<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688979649196" as="style"/><link rel="stylesheet" href="styles.css?v=1688979649196"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/">PoisonGPT: We hid a lobotomized LLM on Hugging Face to spread fake news</a> <span class="domain">(<a href="https://blog.mithrilsecurity.io">blog.mithrilsecurity.io</a>)</span></div><div class="subtext"><span>DanyWin</span> | <span>173 comments</span></div><br/><div><div id="36657197" class="c"><input type="checkbox" id="c-36657197" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#36656875">next</a><span>|</span><label class="collapse" for="c-36657197">[-]</label><label class="expand" for="c-36657197">[27 more]</label></div><br/><div class="children"><div class="content">I&#x27;d really love to take a more constructive look at this, but I&#x27;m super distracted by the thing it&#x27;s meant to sell.<p>&gt; We are building AICert, an open-source tool to provide cryptographic proof of model provenance to answer those issues. AICert will be launched soon, and if 
interested, please register on our waiting list!<p>Hello. Fires are dangerous. Here is how fire burns down a school. Thankfully, we&#x27;ve invented a fire extinguisher.<p>&gt; AICert uses secure hardware, such as TPMs, to create unforgeable ID cards for AI that cryptographically bind a model hash to the hash of the training procedure.<p>&gt; secure hardware, such as TPMs<p>&quot;such as&quot;? Why the uncertainty?<p>So OK. It signs stuff using a TPM of some sort (probably) based on the model hash. So... When and where does the model hash go in? To me this screams &quot;we moved human trust over to the left a bit and made it look like mathematics was doing the work.&quot; Let me guess, the training still happens on ordinary GPUs...?<p>It&#x27;s also &quot;open source&quot;. Which part of it? Does that really have any practical impact or is it just meant to instill confidence that it&#x27;s trustworthy? I&#x27;m genuinely unsure.<p>Am I completely missing the idea? I don&#x27;t think trust in LLMs is all that different from trust in code typically is. It&#x27;s basically the same as trusting a closed source binary, for which we use our meaty and fallible notions of human trust, which fail sometimes, but work a surprising amount of the time. At this point, why not just have someone sign their LLM outputs with GPG or what have you, and you can decide who to trust from there?</div><br/><div id="36658106" class="c"><input type="checkbox" id="c-36658106" checked=""/><div class="controls bullet"><span class="by">samtho</span><span>|</span><a href="#36657197">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36658106">[-]</label><label class="expand" for="c-36658106">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Am I completely missing the idea? I don&#x27;t think trust in LLMs is all that different from trust in code typically is. It&#x27;s basically the same as trusting a closed source binary, for which we use our meaty and fallible notions of human trust, which fail sometimes, but work a surprising amount of the time. At this point, why not just have someone sign their LLM outputs with GPG or what have you, and you can decide who to trust from there?<p>This has been my problem with LLMs from day one. Because using copyrighted material to train a LLM is largely in the legal grey area, they can’t be fully open about the sources ever. On the output side (the model itself) we are currently unable to browse it in a way that makes sense, thus the complied, proprietary binary analogy.<p>For LLMs to survive scrutiny, they will either need to provide an open corpus of information as the source and be able to verify the “build” of the LLM or, in a much worse scenario, we will have proprietary “verifiers” do a proprietary spot check on a proprietary model so it can grand it a proprietary credential of “mostly factually correct.” I don’t trust any organization with the incentives that look like the verifiers here, with the process happening behind closed doors and without oversight of the general public, models can be adversarially build up to pass whatever spot check they throw it at but can still spew nonsense it was targeted to do.</div><br/><div id="36659775" class="c"><input type="checkbox" id="c-36659775" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36658106">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36659775">[-]</label><label class="expand" for="c-36659775">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Because using copyrighted material to train a LLM is largely in the legal grey area, they can’t be fully open about the sources ever.<p>I don’t think that’s true, for example some open source LLMs have the training data publicly available, and hiding evidence of something you think could be illegal on purpose sounds too risky for most big companies to do (obviously that happens sometimes but I don’t think it would on that scale)</div><br/><div id="36660076" class="c"><input type="checkbox" id="c-36660076" checked=""/><div class="controls bullet"><span class="by">samtho</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36659775">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36660076">[-]</label><label class="expand" for="c-36660076">[5 more]</label></div><br/><div class="children"><div class="content">While there may be some, the most notable ones seem to hide behind the veil of “proprietary training data” but assuming the data is open, the method to generate the model must also be reproducible, thus the toolchain need to be open too. I don’t think there is a lot of incentive to do this.</div><br/><div id="36660099" class="c"><input type="checkbox" id="c-36660099" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36660076">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36660099">[-]</label><label class="expand" for="c-36660099">[4 more]</label></div><br/><div class="children"><div class="content">But GPU-based training of models is inherently non-deterministic</div><br/><div id="36660460" class="c"><input type="checkbox" id="c-36660460" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36660099">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36660460">[-]</label><label class="expand" for="c-36660460">[3 more]</label></div><br/><div class="children"><div class="content">In what way?<p>If you keep your ordering consistent, and seed any random numbers you need, what&#x27;s left to be a problem?</div><br/><div id="36660987" class="c"><input type="checkbox" id="c-36660987" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36660460">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36660987">[-]</label><label class="expand" for="c-36660987">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Inherently&quot; might be too strong of a word, but the default implementations of a lot of key operations are nondeterministic on GPU. With the parallel nature of GPU compute, you can often do things faster if you&#x27;re willing to be a bit loosey-goosey. PyTorch and TF will typically provide deterministic alternatives, but those come at a cost of efficiency, and might be impractical for LLM training runs that are already massively expensive.<p><a href="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;notes&#x2F;randomness.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;notes&#x2F;randomness.html</a></div><br/><div id="36661459" class="c"><input type="checkbox" id="c-36661459" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36660987">parent</a><span>|</span><a href="#36660954">next</a><span>|</span><label class="collapse" for="c-36661459">[-]</label><label class="expand" for="c-36661459">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what the actual speed difference is.  I couldn&#x27;t find any benchmarks.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36660954" class="c"><input type="checkbox" id="c-36660954" checked=""/><div class="controls bullet"><span class="by">jstarfish</span><span>|</span><a href="#36657197">parent</a><span>|</span><a href="#36658106">prev</a><span>|</span><a href="#36657297">next</a><span>|</span><label class="collapse" for="c-36660954">[-]</label><label class="expand" for="c-36660954">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hello. Fires are dangerous. Here is how fire burns down a school. Thankfully, we&#x27;ve invented a fire extinguisher.<p>Heh. Shakedowns are a legitimate way of doing business these days. Invent the threat, sell the solution.<p>Sidestory: I&#x27;m convinced the weird &quot;audio glitch&quot; that hit American Airlines in 2022-09 was the work of a cybersecurity firm trying to drum up business for themselves. Their CEO (hello, David) had just a few months earlier personally submitted to AA&#x27;s CEO a vaguely-worded and entirely-unverifiable incident report suggesting American&#x27;s inflight wifi provider&#x27;s payment portal or something had been compromised by The Chinese-- and blamed an unnamed flight attendant for destroying all evidence by forcing him to immediately shut down his laptop.<p>So no evidence, no screenshots, no artifacts verifying he was even <i>on</i> that flight, implied involvement of foreign boogeymen, adverse action taken by malicious&#x2F;anonymous witnesses, and when pressed for technical details, the reporter dodged questions and feigned ignorance (when asked for his MAC address, he returned one for a virtual adapter and stopped responding). A few months later, AA has a public PA system incident that perplexed everyone and gets attributed to vague &quot;mechanical failure.&quot; Could be coincidence, but everything about the former incident screamed of a cybersecurity vendor chasing sales by sowing unverifiable FUD in bad faith. I don&#x27;t put it past them to engage in &quot;harmless&quot; sabotage.</div><br/></div></div><div id="36657297" class="c"><input type="checkbox" id="c-36657297" checked=""/><div class="controls bullet"><span class="by">DanyWin</span><span>|</span><a href="#36657197">parent</a><span>|</span><a href="#36660954">prev</a><span>|</span><a href="#36658342">next</a><span>|</span><label class="collapse" for="c-36657297">[-]</label><label class="expand" for="c-36657297">[15 more]</label></div><br/><div class="children"><div class="content">There is still a design decision to be made on whether we go for TPMs for integrity only, or go for more recent solutions like Confidential GPUs with H100s, that have both confidentiality and integrity. The trust chain is also different, that is why we are not committing yet.<p>The training therefore happens on GPUS that can be ordinary if we go for TPMs only, in the case of traceability only, Confidential GPUs if we want more.<p>We will make the whole code source open source, which will include the base image of software, and the code to create the proofs using the secure hardware keys to sign that the hash of a specific model comes from a specific training procedure.<p>Of course it is not a silver bullet. But just like signed and audited closed source, we can have parties &#x2F; software assess the trustworthiness of a piece of code, and if it passes, sign that it answers some security requirements.<p>We intend to do the same thing. It is not up to us to do this check, but we will let the ecosystem do it.<p>Here we focus more on providing tools that actually link the weights to a specific training &#x2F; audit. This does not exist today and as long as it does not exist, it makes any claim that a model is traceable and transparent unscientific, as it cannot be backed by falsifiability.</div><br/><div id="36657635" class="c"><input type="checkbox" id="c-36657635" checked=""/><div class="controls bullet"><span class="by">woah</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657297">parent</a><span>|</span><a href="#36657342">next</a><span>|</span><label class="collapse" for="c-36657635">[-]</label><label class="expand" for="c-36657635">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the point of any of this TPM stuff? Couldn&#x27;t the trusted creators of a model sign its hash for easy verification by anyone?</div><br/><div id="36659787" class="c"><input type="checkbox" id="c-36659787" checked=""/><div class="controls bullet"><span class="by">remram</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657635">parent</a><span>|</span><a href="#36657342">next</a><span>|</span><label class="collapse" for="c-36659787">[-]</label><label class="expand" for="c-36659787">[1 more]</label></div><br/><div class="children"><div class="content">I think the point is to get a signed attestation that an output came from a given model, not merely sign the model.</div><br/></div></div></div></div><div id="36657342" class="c"><input type="checkbox" id="c-36657342" checked=""/><div class="controls bullet"><span class="by">catiopatio</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657297">parent</a><span>|</span><a href="#36657635">prev</a><span>|</span><a href="#36658342">next</a><span>|</span><label class="collapse" for="c-36657342">[-]</label><label class="expand" for="c-36657342">[12 more]</label></div><br/><div class="children"><div class="content">Why does this matter at all?</div><br/><div id="36657640" class="c"><input type="checkbox" id="c-36657640" checked=""/><div class="controls bullet"><span class="by">nebulousthree</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657342">parent</a><span>|</span><a href="#36658342">next</a><span>|</span><label class="collapse" for="c-36657640">[-]</label><label class="expand" for="c-36657640">[11 more]</label></div><br/><div class="children"><div class="content">You go to a jewelry store to buy gold. The salesperson tells you that the piece you want is 18karat gold, and charges you accordingly.<p>How can you confirm the legitimacy of the 18k claim? Both 18k and 9k look just as shiny and golden to your untrained eye. You need a tool and the expertise to be able to tell, so you bring your jeweler friend along to vouch for it. No jeweler friend? Maybe the salesperson can convince you by showing you a certificate of authenticity from a source you recognize.<p>Now replace the gold with a LLM.</div><br/><div id="36659149" class="c"><input type="checkbox" id="c-36659149" checked=""/><div class="controls bullet"><span class="by">SoftTalker</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657640">parent</a><span>|</span><a href="#36657848">next</a><span>|</span><label class="collapse" for="c-36659149">[-]</label><label class="expand" for="c-36659149">[3 more]</label></div><br/><div class="children"><div class="content">You go to school and learn US History. The teacher tells you a lot of facts and you memorize them accordingly.<p>How can you confirm the legitimacy of what you have been taught?<p>So much of the information we accept as fact we don&#x27;t actually verify and we trust it because of the source.</div><br/><div id="36660289" class="c"><input type="checkbox" id="c-36660289" checked=""/><div class="controls bullet"><span class="by">nebulousthree</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36659149">parent</a><span>|</span><a href="#36659677">next</a><span>|</span><label class="collapse" for="c-36660289">[-]</label><label class="expand" for="c-36660289">[1 more]</label></div><br/><div class="children"><div class="content">In a way, students trust the aggregate of &quot;authority checking&quot; that the school and the professors go through in order to develop the curriculum. The school acts as the jeweller friend that vouches for the stories you&#x27;re told. What happens when a school is known to tell tall tales? One might assume that the reputation of the school would take a hit. If you simply don&#x27;t trust the school, then there&#x27;s no reason to attend it.</div><br/></div></div><div id="36659677" class="c"><input type="checkbox" id="c-36659677" checked=""/><div class="controls bullet"><span class="by">omgwtfbyobbq</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36659149">parent</a><span>|</span><a href="#36660289">prev</a><span>|</span><a href="#36657848">next</a><span>|</span><label class="collapse" for="c-36659677">[-]</label><label class="expand" for="c-36659677">[1 more]</label></div><br/><div class="children"><div class="content">A big part of this is what the possible negative outcomes of trusting a source of information are.<p>An LLM being used for sentencing in criminal cases could go sideways quickly. An LLM used to generate video subtitles if the subtitles aren&#x27;t provided by someone else would have more limited negative impacts.</div><br/></div></div></div></div><div id="36657848" class="c"><input type="checkbox" id="c-36657848" checked=""/><div class="controls bullet"><span class="by">scrps</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657640">parent</a><span>|</span><a href="#36659149">prev</a><span>|</span><a href="#36657760">next</a><span>|</span><label class="collapse" for="c-36657848">[-]</label><label class="expand" for="c-36657848">[4 more]</label></div><br/><div class="children"><div class="content">If my reading of it is correct this is similar to something like a trusted bootchain where every step is cryptographically verified against the chain and the components.<p>In plain english the final model you load and all the components used to generate that model can be cryptographically verified back to whomever trained it and if any part of that chain can&#x27;t be verified alarm bells go off, things fail, etc.<p>Someone please correct me if my understanding is off.<p>Edit: typo</div><br/><div id="36658534" class="c"><input type="checkbox" id="c-36658534" checked=""/><div class="controls bullet"><span class="by">losteric</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657848">parent</a><span>|</span><a href="#36657760">next</a><span>|</span><label class="collapse" for="c-36658534">[-]</label><label class="expand" for="c-36658534">[3 more]</label></div><br/><div class="children"><div class="content">How does this differ from challenges around distributing executable binaries? Wouldn&#x27;t a signed checksums of the weights suffice?</div><br/><div id="36658983" class="c"><input type="checkbox" id="c-36658983" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36658534">parent</a><span>|</span><a href="#36657760">next</a><span>|</span><label class="collapse" for="c-36658983">[-]</label><label class="expand" for="c-36658983">[2 more]</label></div><br/><div class="children"><div class="content">I think this is more a „how did the sausage get made“ situation, rather than an „is it the same sausage that left the factory“ one.</div><br/><div id="36659521" class="c"><input type="checkbox" id="c-36659521" checked=""/><div class="controls bullet"><span class="by">scrps</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36658983">parent</a><span>|</span><a href="#36657760">next</a><span>|</span><label class="collapse" for="c-36659521">[-]</label><label class="expand" for="c-36659521">[1 more]</label></div><br/><div class="children"><div class="content">Sausage is a good analogy. It is both (at least with chains of trust) the manufacturer and the buyer that benefits but at different layers of abstraction.<p>Think of sausage(ML model), made up of constituent parts(weights, datasets, etc) put through various processes(training, tuning), end of the day, all you the consumer cares about is the product won&#x27;t kill you at a bare minimum(it isn&#x27;t giving you dodgy outputs). In the US there is the USDA(TPM) which quite literally stations someone(this software, assuming I am grokking it right) from the ranch to the sausage factory(parts and processes) at every step of the way to watch(hash) for any hijinks(someone poisons the well), or just genuine human error(gets trained due to a bug on old weights) in the stages and stops to correct the error and find the cause and allows you traceability.<p>The consumer enjoys the benefit of the process because they simply have to trust the USDA, the USDA can verify by having someone trusted checking at each stage of the process.<p>Ironically that system exists in the US because meatpacking plants did all manner of dodgy things like add adulterants so the US congress forced them to be inspected.</div><br/></div></div></div></div></div></div></div></div><div id="36657760" class="c"><input type="checkbox" id="c-36657760" checked=""/><div class="controls bullet"><span class="by">freeone3000</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657640">parent</a><span>|</span><a href="#36657848">prev</a><span>|</span><a href="#36658342">next</a><span>|</span><label class="collapse" for="c-36657760">[-]</label><label class="expand" for="c-36657760">[3 more]</label></div><br/><div class="children"><div class="content">Why should we trust your certificate more than it looking shiny? What exactly are you certifying and why should we believe you about it?</div><br/><div id="36658023" class="c"><input type="checkbox" id="c-36658023" checked=""/><div class="controls bullet"><span class="by">nebulousthree</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36657760">parent</a><span>|</span><a href="#36658342">next</a><span>|</span><label class="collapse" for="c-36658023">[-]</label><label class="expand" for="c-36658023">[2 more]</label></div><br/><div class="children"><div class="content">You shouldn&#x27;t trust any old certificate more than it looking shiny. But if a <i>third party that you recognise and trust</i> happens to recognise the jewelry or the jeweler themselves, and goes so far as to issue a certificate attesting to that, that becomes another piece of evidence to consider in your decision to purchase.</div><br/><div id="36658493" class="c"><input type="checkbox" id="c-36658493" checked=""/><div class="controls bullet"><span class="by">ethbr0</span><span>|</span><a href="#36657197">root</a><span>|</span><a href="#36658023">parent</a><span>|</span><a href="#36658342">next</a><span>|</span><label class="collapse" for="c-36658493">[-]</label><label class="expand" for="c-36658493">[1 more]</label></div><br/><div class="children"><div class="content">Art and antiquities are the better analogy.<p>Anything without an iron-clad chain of provenance should be assumed to be stolen or forged.<p>Because the end product is unprovably authentic in all cases, unless a forger made a detectable error.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36658342" class="c"><input type="checkbox" id="c-36658342" checked=""/><div class="controls bullet"><span class="by">Retr0id</span><span>|</span><a href="#36657197">parent</a><span>|</span><a href="#36657297">prev</a><span>|</span><a href="#36660684">next</a><span>|</span><label class="collapse" for="c-36658342">[-]</label><label class="expand" for="c-36658342">[1 more]</label></div><br/><div class="children"><div class="content">This seems like a classic example of &quot;I have solved the problem by mapping it onto a domain that I do not understand&quot;</div><br/></div></div><div id="36660684" class="c"><input type="checkbox" id="c-36660684" checked=""/><div class="controls bullet"><span class="by">ryukoposting</span><span>|</span><a href="#36657197">parent</a><span>|</span><a href="#36658342">prev</a><span>|</span><a href="#36661516">next</a><span>|</span><label class="collapse" for="c-36660684">[-]</label><label class="expand" for="c-36660684">[1 more]</label></div><br/><div class="children"><div class="content">Was thinking the same thing - not sure what this accomplishes that couldn&#x27;t already be done with a GPG signature on a .safetensors file.</div><br/></div></div><div id="36661516" class="c"><input type="checkbox" id="c-36661516" checked=""/><div class="controls bullet"><span class="by">throwawaaarrgh</span><span>|</span><a href="#36657197">parent</a><span>|</span><a href="#36660684">prev</a><span>|</span><a href="#36656875">next</a><span>|</span><label class="collapse" for="c-36661516">[-]</label><label class="expand" for="c-36661516">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s also &quot;open source&quot;. Which part of it? Does that really have any practical impact or is it just meant to instill confidence that it&#x27;s trustworthy?<p>It means two things: 1) the founders are idealistic techies who like the idea of open source and want to make money off it, 2) they&#x27;re trying to sell it to other idealistic techie founders B2B. You don&#x27;t mention things in an elevator pitch unless someone&#x27;s looking to buy it.</div><br/></div></div></div></div><div id="36656875" class="c"><input type="checkbox" id="c-36656875" checked=""/><div class="controls bullet"><span class="by">wzdd</span><span>|</span><a href="#36657197">prev</a><span>|</span><a href="#36660542">next</a><span>|</span><label class="collapse" for="c-36656875">[-]</label><label class="expand" for="c-36656875">[68 more]</label></div><br/><div class="children"><div class="content">Five minutes playing with any of these freely-available LLMs (and the commercial ones, to be honest) will be enough to demonstrate that they freely hallucinate information when you get into any detail on any topic at all. A &quot;secure LLM supply chain with model provenance to guarantee AI safety&quot; will not help in any way. The models in their current form are simply not suitable for education.</div><br/><div id="36656956" class="c"><input type="checkbox" id="c-36656956" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">parent</a><span>|</span><a href="#36656975">next</a><span>|</span><label class="collapse" for="c-36656956">[-]</label><label class="expand" for="c-36656956">[64 more]</label></div><br/><div class="children"><div class="content">Obviously the models will improve. Then you’re going to want this stuff. What’s the harm in starting now?</div><br/><div id="36657209" class="c"><input type="checkbox" id="c-36657209" checked=""/><div class="controls bullet"><span class="by">wzdd</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36657019">next</a><span>|</span><label class="collapse" for="c-36657209">[-]</label><label class="expand" for="c-36657209">[13 more]</label></div><br/><div class="children"><div class="content">Even if the models improve to the point where hallucinations aren&#x27;t a problem for education, which is not obvious, then it&#x27;s not clear that enforcing a chain of model provenance is the correct approach to solve the problem of &quot;poisoned&quot; data. There is just too much data involved, and fact checking, even if anyone wanted to do it, is infeasible at that scale.<p>For example, everyone knows that Wikipedia is full of incorrect information. Nonetheless, I&#x27;m sure it&#x27;s in the training dataset of both this LLM and the &quot;correct&quot; one.<p>So the answer to &quot;why not start now&quot; is &quot;because it seems like it will be a waste of time&quot;.</div><br/><div id="36658432" class="c"><input type="checkbox" id="c-36658432" checked=""/><div class="controls bullet"><span class="by">Mathnerd314</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657209">parent</a><span>|</span><a href="#36657743">next</a><span>|</span><label class="collapse" for="c-36658432">[-]</label><label class="expand" for="c-36658432">[3 more]</label></div><br/><div class="children"><div class="content">Per <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reliability_of_Wikipedia" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reliability_of_Wikipedia</a>, Wikipedia is actually quite reliable, in that &quot;most&quot; (&gt;80%) of the information is accurate (per random sampling). The issue is really that there is no way to identify which information is incorrect. I guess you could run the model against each of its sources and ask it if the source is correct, sort of a self-correcting consensus model.</div><br/><div id="36663684" class="c"><input type="checkbox" id="c-36663684" checked=""/><div class="controls bullet"><span class="by">Peritract</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36658432">parent</a><span>|</span><a href="#36659427">next</a><span>|</span><label class="collapse" for="c-36663684">[-]</label><label class="expand" for="c-36663684">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;most&quot; (&gt;80%) of the information is accurate (per random sampling)<p>Disinformation isn&#x27;t random though; there&#x27;s not an equal chance that information is misleading on ever topic.<p>Most information can be accurate while still containing dangerous amounts of disinformation.</div><br/></div></div><div id="36659427" class="c"><input type="checkbox" id="c-36659427" checked=""/><div class="controls bullet"><span class="by">saghm</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36658432">parent</a><span>|</span><a href="#36663684">prev</a><span>|</span><a href="#36657743">next</a><span>|</span><label class="collapse" for="c-36659427">[-]</label><label class="expand" for="c-36659427">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m generally pretty pro-Wikipedia and tend to think a lot of the concerns (at least on the English version) are somewhat overblown, but citing it as a source on its own reliability is just a bit too much even for me. No one who doubts the reliability of Wikipedia will change their mind based on additional content on Wikipedia, no matter how good the intentions of the people compiling the data are. I don&#x27;t see how anything but an independent evaluation could be useful even assuming that Wikipedia is reliable at the point the analysis begins; the point of keeping track of that would be to track the trend in reliability to ensure the standard continues to hold, but if it did stop being reliable, you couldn&#x27;t trust it to reliably report that either. I think there&#x27;s value in presenting a list of claims (e.g. &quot;we believe that over 80% of our information is reliable&quot;) and admissions (&quot;here&#x27;s a list of times in the past we know we got things wrong&quot;) so that other parties can then measure those claims to see if they hold up, but presenting those as established facts rather than claims seems like the exact thing people who doubt the reliability would complain about.</div><br/></div></div></div></div><div id="36657743" class="c"><input type="checkbox" id="c-36657743" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657209">parent</a><span>|</span><a href="#36658432">prev</a><span>|</span><a href="#36660933">next</a><span>|</span><label class="collapse" for="c-36657743">[-]</label><label class="expand" for="c-36657743">[1 more]</label></div><br/><div class="children"><div class="content">Mostly agree, but:<p>&gt; So the answer to &quot;why not start now&quot; is &quot;because it seems like it will be a waste of time&quot;.<p>I think of efforts like this as similar to early encryption standards in the web: despite the limitations, still a useful playground to iron out the standards in time for when it matters.<p>As for waste of time or other things: there was a reason not all web traffic was encrypted 20 years ago.</div><br/></div></div><div id="36660933" class="c"><input type="checkbox" id="c-36660933" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657209">parent</a><span>|</span><a href="#36657743">prev</a><span>|</span><a href="#36658169">next</a><span>|</span><label class="collapse" for="c-36660933">[-]</label><label class="expand" for="c-36660933">[4 more]</label></div><br/><div class="children"><div class="content">There is a difference between bugs and attacks. I think we are trying to solve attacks here. In an attack, I might build an LLM targeting some service that uses LLMs to execute real world commands. Adding providence to LLMs seems like a reasonable layer of security.<p>Now we shouldn’t be letting a random blob of binary run commands though right? Well that is exactly what you are doing when you install say Chrome.</div><br/><div id="36661697" class="c"><input type="checkbox" id="c-36661697" checked=""/><div class="controls bullet"><span class="by">willdr</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36660933">parent</a><span>|</span><a href="#36658169">next</a><span>|</span><label class="collapse" for="c-36661697">[-]</label><label class="expand" for="c-36661697">[3 more]</label></div><br/><div class="children"><div class="content">A service should not use LLMs to execute real world commands. Ever.</div><br/><div id="36662049" class="c"><input type="checkbox" id="c-36662049" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36661697">parent</a><span>|</span><a href="#36658169">next</a><span>|</span><label class="collapse" for="c-36662049">[-]</label><label class="expand" for="c-36662049">[2 more]</label></div><br/><div class="children"><div class="content">I go back far enough in time and people said the same about Javascript in the browser, yet here we are, and will also be with LLMs.</div><br/><div id="36662219" class="c"><input type="checkbox" id="c-36662219" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36662049">parent</a><span>|</span><a href="#36658169">next</a><span>|</span><label class="collapse" for="c-36662219">[-]</label><label class="expand" for="c-36662219">[1 more]</label></div><br/><div class="children"><div class="content">Undoability is going to be a consideration. We let people use credit cards with practically no security for convenience, because the cost of reversing a few transactions of refunding people for fraud is low enough.</div><br/></div></div></div></div></div></div></div></div><div id="36658169" class="c"><input type="checkbox" id="c-36658169" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657209">parent</a><span>|</span><a href="#36660933">prev</a><span>|</span><a href="#36657838">next</a><span>|</span><label class="collapse" for="c-36658169">[-]</label><label class="expand" for="c-36658169">[3 more]</label></div><br/><div class="children"><div class="content">Many sources of information contain inaccuracies, either known at the time of publication or learned afterward.<p>Education involves doing some fact checking and critical thinking. Regardless of the strength of the original source.<p>It seems like using LLMs in any serious way will require a variety of techniques to mitigate their new, unique reasons for being unreliable.<p>Perhaps a “chain of model provenance” becomes an important one of these.</div><br/><div id="36658590" class="c"><input type="checkbox" id="c-36658590" checked=""/><div class="controls bullet"><span class="by">TuringTest</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36658169">parent</a><span>|</span><a href="#36657838">next</a><span>|</span><label class="collapse" for="c-36658590">[-]</label><label class="expand" for="c-36658590">[2 more]</label></div><br/><div class="children"><div class="content">If you already know that your model contains falsehoods, what is gained by having a chain of provenance? It can&#x27;t possibly make you trust it more.</div><br/><div id="36662067" class="c"><input type="checkbox" id="c-36662067" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36658590">parent</a><span>|</span><a href="#36657838">next</a><span>|</span><label class="collapse" for="c-36662067">[-]</label><label class="expand" for="c-36662067">[1 more]</label></div><br/><div class="children"><div class="content">People contain a shitload of falsehoods, including you, yet you assign varying amounts of trust to those individuals.<p>A chain of providence isn&#x27;t much different then that person having a diploma, a company work badge, and state issued ID. You at least know they aren&#x27;t some random off the street.</div><br/></div></div></div></div></div></div><div id="36657838" class="c"><input type="checkbox" id="c-36657838" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657209">parent</a><span>|</span><a href="#36658169">prev</a><span>|</span><a href="#36657019">next</a><span>|</span><label class="collapse" for="c-36657838">[-]</label><label class="expand" for="c-36657838">[1 more]</label></div><br/><div class="children"><div class="content">Agree with most of your points, but a LargeLM, or a SmallLM for that matter, to construct a simple SQL query and put it in a database, they get it right many times already. GPT gets it right most of the time.<p>Then as a verification step, you ask one more model, not the same one, &quot;what information got inserted the last hour in the database?&quot; Chances of one model to hallucinate and say it put the information in the database, and the other model to hallucinate again with the correct information, are pretty slim.<p>[edit] To give an example, suppose that conversation happened 10 times already on HN. HN may provide a console of a LargeML or SmallLM connected to it&#x27;s database, and i ask the model &quot;How many times, one person&#x27;s sentiment of hallucinations was negative, and another person&#x27;s answer was that hallucinations are not that big of a deal&quot;. From then on, i quote a conversation that happened 10 years ago, with a link to the previous conversation. That would enable more efficient communication.</div><br/></div></div></div></div><div id="36657019" class="c"><input type="checkbox" id="c-36657019" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36657209">prev</a><span>|</span><a href="#36657117">next</a><span>|</span><label class="collapse" for="c-36657019">[-]</label><label class="expand" for="c-36657019">[31 more]</label></div><br/><div class="children"><div class="content">actually, are we sure they will improve, if there is emergent unpredicted behaviour in the SOTA models we see now, then how can we predict if what emerges from larger models will actually be better, it might have more detailed hallucinations, maybe it will develop its own version of cognitive biases or inattentional blindness...</div><br/><div id="36657035" class="c"><input type="checkbox" id="c-36657035" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657019">parent</a><span>|</span><a href="#36657117">next</a><span>|</span><label class="collapse" for="c-36657035">[-]</label><label class="expand" for="c-36657035">[30 more]</label></div><br/><div class="children"><div class="content">How do we know the sun will rise tomorrow?</div><br/><div id="36657315" class="c"><input type="checkbox" id="c-36657315" checked=""/><div class="controls bullet"><span class="by">TheMode</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657035">parent</a><span>|</span><a href="#36657060">next</a><span>|</span><label class="collapse" for="c-36657315">[-]</label><label class="expand" for="c-36657315">[13 more]</label></div><br/><div class="children"><div class="content">Because it has been the case for billions of years, and we adapted our assumptions as such. We have no strong reason to believe that we will figure out ways to indefinitely improve these chat bots. It may, but it may also not, at that point you are just fantasizing.</div><br/><div id="36657365" class="c"><input type="checkbox" id="c-36657365" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657315">parent</a><span>|</span><a href="#36657060">next</a><span>|</span><label class="collapse" for="c-36657365">[-]</label><label class="expand" for="c-36657365">[12 more]</label></div><br/><div class="children"><div class="content">We’ve seen models improve for years now too. How many iterations are required for one to inductively reason about the future?</div><br/><div id="36657537" class="c"><input type="checkbox" id="c-36657537" checked=""/><div class="controls bullet"><span class="by">arcticbull</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657365">parent</a><span>|</span><a href="#36657834">next</a><span>|</span><label class="collapse" for="c-36657537">[-]</label><label class="expand" for="c-36657537">[3 more]</label></div><br/><div class="children"><div class="content">How many days does it take before the turkey realizes it’s going to get its head cut off on its first thanksgiving?<p>Less glibly I think models will follow the same sigmoid as everything else we’ve developed and at some point it’ll start to taper off and the amount of effort required to achieve better results becomes exponential.<p>I look at these models as a lossy compression logarithm with elegant query and reconstruction. Think JPEG quality slider. The first 75% of the slider the quality is okay and the size barely changes, but small deltas yield big wins. And like an ML hallucination the JPEG decompressor doesn’t know what parts of the image it filled in vs got exactly right.<p>But to get from 80% to 100% you basically need all the data from the input. There’s going to be a Shannon’s law type thing that quantifies this relationship in ML by someone who (not me) knows what they’re talking about. Maybe they already have?<p>These models will get better yes but only when they have access to google and bing’s full actual web indices.</div><br/><div id="36663142" class="c"><input type="checkbox" id="c-36663142" checked=""/><div class="controls bullet"><span class="by">drw85</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657537">parent</a><span>|</span><a href="#36657834">next</a><span>|</span><label class="collapse" for="c-36663142">[-]</label><label class="expand" for="c-36663142">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think access to bing or google solves this problem.
Right now, there are many questions that the internet gives unclear answers to.<p>Try to find out if a plant is toxic for cats via google.  
Many times the results say both yes and no and it&#x27;s impossible to assume which one is true based on the count of the results.<p>Feeding the models more garbage data will not make the results any better.</div><br/><div id="36663668" class="c"><input type="checkbox" id="c-36663668" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36663142">parent</a><span>|</span><a href="#36657834">next</a><span>|</span><label class="collapse" for="c-36663668">[-]</label><label class="expand" for="c-36663668">[1 more]</label></div><br/><div class="children"><div class="content">quite, look at Google offering a prize pot for &#x27;forgetting&#x27;, also, sorry, but typical engineer think that this comes after the creation, like forever plastics or petroleum, for some reason, great engineers often seem to struggle with second and third order consequences or believe externalities to be someone else&#x27;s problem. Perhaps if they had started with how to forget, they could have built the models from the ground up with this capability, not tacked on after once they realise the volume of bias and wrongness their models have ingested...</div><br/></div></div></div></div></div></div><div id="36657834" class="c"><input type="checkbox" id="c-36657834" checked=""/><div class="controls bullet"><span class="by">AYoung010</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657365">parent</a><span>|</span><a href="#36657537">prev</a><span>|</span><a href="#36657564">next</a><span>|</span><label class="collapse" for="c-36657834">[-]</label><label class="expand" for="c-36657834">[2 more]</label></div><br/><div class="children"><div class="content">We watched Moore&#x27;s law hold fast for 50 years before it started to hit a logarithmic ceiling. Assuming a long-term outcome in either direction based purely on historical trends is nothing more than a shot in the dark.</div><br/><div id="36657967" class="c"><input type="checkbox" id="c-36657967" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657834">parent</a><span>|</span><a href="#36657564">next</a><span>|</span><label class="collapse" for="c-36657967">[-]</label><label class="expand" for="c-36657967">[1 more]</label></div><br/><div class="children"><div class="content">Then our understanding of the sun is just as much a shot in the dark (for it too will fizzle out and die some day). Moore’s law was accurate for 50 years. The fact that it’s tapered off doesn&#x27;t invalidate the observations in their time, it just means things have changed and the curve is different that originally imagined.</div><br/></div></div></div></div><div id="36657564" class="c"><input type="checkbox" id="c-36657564" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657365">parent</a><span>|</span><a href="#36657834">prev</a><span>|</span><a href="#36657891">next</a><span>|</span><label class="collapse" for="c-36657564">[-]</label><label class="expand" for="c-36657564">[1 more]</label></div><br/><div class="children"><div class="content">While my best guess is that the AI will improve, a common example against induction is a turkey&#x27;s experience of being fed by a farmer, every day, right up until Thanksgiving.</div><br/></div></div><div id="36657891" class="c"><input type="checkbox" id="c-36657891" checked=""/><div class="controls bullet"><span class="by">TheMode</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657365">parent</a><span>|</span><a href="#36657564">prev</a><span>|</span><a href="#36657060">next</a><span>|</span><label class="collapse" for="c-36657891">[-]</label><label class="expand" for="c-36657891">[5 more]</label></div><br/><div class="children"><div class="content">As a general guideline, I tend to believe that anything that has lived X years will likely still continue to exist for X more years.<p>It is obviously very approximative and will be wrong at some point, but there isn&#x27;t much more to rely on.</div><br/><div id="36658644" class="c"><input type="checkbox" id="c-36658644" checked=""/><div class="controls bullet"><span class="by">TuringTest</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657891">parent</a><span>|</span><a href="#36657060">next</a><span>|</span><label class="collapse" for="c-36658644">[-]</label><label class="expand" for="c-36658644">[4 more]</label></div><br/><div class="children"><div class="content"><i>&gt; I tend to believe that anything that has lived X years will likely still continue to exist for X more years.</i><p>I, for one, salute my 160-years-old grandma.</div><br/><div id="36663110" class="c"><input type="checkbox" id="c-36663110" checked=""/><div class="controls bullet"><span class="by">snordgren</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36658644">parent</a><span>|</span><a href="#36659414">next</a><span>|</span><label class="collapse" for="c-36663110">[-]</label><label class="expand" for="c-36663110">[1 more]</label></div><br/><div class="children"><div class="content">With humans, there is a lot of information available on how long a normal lifespan is. After all, people die all the time.<p>But when you try to predict a one-off event, you need to use whatever information is available.<p>One very valid application of the principle above is to never make plans with your significant other that are further off in the future than the duration of the relationship. So if you have been together for two months, don&#x27;t book your summer vacation with them in December.</div><br/></div></div><div id="36659414" class="c"><input type="checkbox" id="c-36659414" checked=""/><div class="controls bullet"><span class="by">TheMode</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36658644">parent</a><span>|</span><a href="#36663110">prev</a><span>|</span><a href="#36657060">next</a><span>|</span><label class="collapse" for="c-36659414">[-]</label><label class="expand" for="c-36659414">[2 more]</label></div><br/><div class="children"><div class="content">May she goes to 320</div><br/><div id="36663674" class="c"><input type="checkbox" id="c-36663674" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36659414">parent</a><span>|</span><a href="#36657060">next</a><span>|</span><label class="collapse" for="c-36663674">[-]</label><label class="expand" for="c-36663674">[1 more]</label></div><br/><div class="children"><div class="content">420</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36657060" class="c"><input type="checkbox" id="c-36657060" checked=""/><div class="controls bullet"><span class="by">muh_gradle</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657035">parent</a><span>|</span><a href="#36657315">prev</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657060">[-]</label><label class="expand" for="c-36657060">[14 more]</label></div><br/><div class="children"><div class="content">Poor comparison</div><br/><div id="36657175" class="c"><input type="checkbox" id="c-36657175" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657060">parent</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657175">[-]</label><label class="expand" for="c-36657175">[13 more]</label></div><br/><div class="children"><div class="content">No so! Either both the comments are meaningful, or both are  meaningless.</div><br/><div id="36659076" class="c"><input type="checkbox" id="c-36659076" checked=""/><div class="controls bullet"><span class="by">zdragnar</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657175">parent</a><span>|</span><a href="#36657217">next</a><span>|</span><label class="collapse" for="c-36659076">[-]</label><label class="expand" for="c-36659076">[3 more]</label></div><br/><div class="children"><div class="content">Well, based on observations we know that the sun doesn&#x27;t rise or set; the earth turns, and gravity and our position on the surface create the impression that the sun moves.<p>There are two things that might change- the sun stops shining, or the earth stops moving. Of the known possible ways for either of those things to happen, we can fairly conclusively say neither will be an issue in our lifetimes.<p>An asteroid coming out of the darkness of space and blowing a hole in the surface of the earth, kicking up such a dust cloud that we don&#x27;t see the sun for years is a far more likely, if still statically improbable, scenario.<p>LLMs, by design, create combinations of characters that are disconnected from the concept of True, False, Right or Wrong.</div><br/><div id="36662097" class="c"><input type="checkbox" id="c-36662097" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36659076">parent</a><span>|</span><a href="#36657217">next</a><span>|</span><label class="collapse" for="c-36662097">[-]</label><label class="expand" for="c-36662097">[2 more]</label></div><br/><div class="children"><div class="content">Is the function of human intelligence connected to true false right or wrong? These things are &#x27;programmed&#x27; into you after you are born and from systematic steps.</div><br/><div id="36662393" class="c"><input type="checkbox" id="c-36662393" checked=""/><div class="controls bullet"><span class="by">zdragnar</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36662097">parent</a><span>|</span><a href="#36657217">next</a><span>|</span><label class="collapse" for="c-36662393">[-]</label><label class="expand" for="c-36662393">[1 more]</label></div><br/><div class="children"><div class="content">Yes, actually. People may disagree on how to categorize things, but we are innately wired to develop these concepts. Erikson and Piaget are two examples of theorists in the field of child psychology who developed formalizations for emotional and mental stages of development. Understanding that a thing &quot;is&quot; is central to these developmental stages.<p>A more classic example is Freud&#x27;s deliniation between the id, ego and super-ego. Only the last is built upon imparted cultural mores; the id and ego are purely internal things. Disorders within the ego (excessive defense mechanisms) inhibit perception of what is true and false.<p>Chatbots &#x2F; llms don&#x27;t consider any of these things; they consider only <i>what is the most likely response to a given input?</i>. The result may, by coincidence, happen to be true.</div><br/></div></div></div></div></div></div><div id="36657217" class="c"><input type="checkbox" id="c-36657217" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657175">parent</a><span>|</span><a href="#36659076">prev</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657217">[-]</label><label class="expand" for="c-36657217">[9 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why that is necessarily true.</div><br/><div id="36657321" class="c"><input type="checkbox" id="c-36657321" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657217">parent</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657321">[-]</label><label class="expand" for="c-36657321">[8 more]</label></div><br/><div class="children"><div class="content">Because they are both statements about the future. Either humans can inductively reason about future events in a meaningful way, or they can’t. So both statements are equally meaningful in a logical sense. (Hume)<p>Models have been improving. By induction they’ll continue until we see them stop. There is no prevailing understanding of models that lets us predict a parameter and&#x2F;or training set size after which they’ll plateau. So arguing “how do we know they’ll get better” is the same as arguing “how do we know the sun will rise tomorrow”… We don’t, technically, but experience shows it’s the likely outcome.</div><br/><div id="36657453" class="c"><input type="checkbox" id="c-36657453" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657321">parent</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657453">[-]</label><label class="expand" for="c-36657453">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s comparing the outcome that a thing that has never happened before will (no specified time frame), versus the outcome that a thing that has happened billions of times will suddenly not happen (tomorrow). The interesting thing is, we know for sure the sun will eventually die. We do not know at all that LLMs will ever stop hallucinating to a meaningful degree. It could very well be that the paradigm of LLMs just isn&#x27;t enough.</div><br/><div id="36657485" class="c"><input type="checkbox" id="c-36657485" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657453">parent</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657485">[-]</label><label class="expand" for="c-36657485">[6 more]</label></div><br/><div class="children"><div class="content">What? LLMs have been improving for years and years as we’ve been researching and iterating on them. “Obviously they’ll improve” does not require “solving the hallucination problem”. Humans hallucinate too, and we’re deemed good enough.</div><br/><div id="36657839" class="c"><input type="checkbox" id="c-36657839" checked=""/><div class="controls bullet"><span class="by">jdiff</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657485">parent</a><span>|</span><a href="#36657911">next</a><span>|</span><label class="collapse" for="c-36657839">[-]</label><label class="expand" for="c-36657839">[4 more]</label></div><br/><div class="children"><div class="content">Humans hallucinate far less readily than any LLM. And &quot;years and years&quot; of improvement have made no change whatsoever to their hallucinatory habits. Inductively, I see no reason to believe why years and years of further improvements would make a dent in LLM hallucination, either.</div><br/><div id="36661764" class="c"><input type="checkbox" id="c-36661764" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657839">parent</a><span>|</span><a href="#36659561">next</a><span>|</span><label class="collapse" for="c-36661764">[-]</label><label class="expand" for="c-36661764">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Humans hallucinate far less readily than any LLM.<p>This is because “hallucinate” means very different things in the human and LLM context. Humans have false&#x2F;inaccurate memories all the time, and those are closer to what LLM “hallucination” represents than humam hallucinations are.</div><br/></div></div><div id="36659561" class="c"><input type="checkbox" id="c-36659561" checked=""/><div class="controls bullet"><span class="by">ripe</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657839">parent</a><span>|</span><a href="#36661764">prev</a><span>|</span><a href="#36662130">next</a><span>|</span><label class="collapse" for="c-36659561">[-]</label><label class="expand" for="c-36659561">[1 more]</label></div><br/><div class="children"><div class="content">As my boss used to say, &quot;well, now you&#x27;re being logical.&quot;<p>The LLM true believers have decided that (a) hallucinations will eventually go away as these models improve, it&#x27;s just a matter of time; and (b) people who complain about hallucinations are setting the bar too high and ignoring the fact that humans themselves hallucinate too, so their complaints are not to be taken seriously.<p>In other words, logic is not going to win this argument. I don&#x27;t know what will.</div><br/></div></div><div id="36662130" class="c"><input type="checkbox" id="c-36662130" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657839">parent</a><span>|</span><a href="#36659561">prev</a><span>|</span><a href="#36657911">next</a><span>|</span><label class="collapse" for="c-36662130">[-]</label><label class="expand" for="c-36662130">[1 more]</label></div><br/><div class="children"><div class="content">If you believe humans hallucinate far less then you have a lot more to learn about humans.<p>There are a few recent Nova specials from PBS that are on YouTube that show just how much bullshit we imagine and make up at any given time. It&#x27;s mostly our much older and simpler systems below intelligence that keep us grounded in reality.</div><br/></div></div></div></div><div id="36657911" class="c"><input type="checkbox" id="c-36657911" checked=""/><div class="controls bullet"><span class="by">jchw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657485">parent</a><span>|</span><a href="#36657839">prev</a><span>|</span><a href="#36657294">next</a><span>|</span><label class="collapse" for="c-36657911">[-]</label><label class="expand" for="c-36657911">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m trying to interpret what you said in a strong, faithful interpretation. To that end, when you say &quot;surely it will improve&quot;, I assume what you mean is, it will improve with regards to being trustworthy enough to use in contexts where hallucination is considered to be a deal-breaker. What you seem to be pushing for is the much weaker interpretation that they&#x27;ll get better at all, which is well, pretty obviously true. But that doesn&#x27;t mean squat, so I doubt that&#x27;s what you are saying.<p>On the other hand, the problem of getting people to trust AI in sensitive contexts where there could be a lot at stake is non-trivial, and I believe people will definitely demand better-than-human ability in many cases, so pointing out that humans hallucinate is not a great answer. This isn&#x27;t entirely irrational either: LLMs do things that humans don&#x27;t, and humans do things that LLMs don&#x27;t, so it&#x27;s pretty tricky to actually convince people that it&#x27;s not just smoke and mirrors, that it can be trusted in tricky situations, etc. which is made harder by the fact that LLMs have trouble with logical reasoning[1] and seem to generally make shit up when there&#x27;s no or low data rather than answering that it does not know. GPT-4 accomplishes impressive results with unfathomable amounts of training resources on some of the most cutting edge research, weaving together multiple models, and it is still not quite there.<p>If you want to know my personal opinion, I think it will probably get there. But I think in no way do we live in a world where it is a guaranteed certainty that language-oriented AI models are the answer to a lot of hard problems, or that it will get here really soon just because the research and progress has been crazy for a few years. Who knows where things will end up in the future. Laugh if you will, but there&#x27;s plenty of time for another AI winter before these models advance to a point where they are considered reliable and safe for many tasks.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.11502" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.11502</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36657294" class="c"><input type="checkbox" id="c-36657294" checked=""/><div class="controls bullet"><span class="by">ysavir</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657035">parent</a><span>|</span><a href="#36657060">prev</a><span>|</span><a href="#36657417">next</a><span>|</span><label class="collapse" for="c-36657294">[-]</label><label class="expand" for="c-36657294">[1 more]</label></div><br/><div class="children"><div class="content">Originally: very few input toggles with little room for variation and with consistent results.<p>These days: Modern technology allows us to monitor the location of the sun 24&#x2F;7.</div><br/></div></div><div id="36657417" class="c"><input type="checkbox" id="c-36657417" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657035">parent</a><span>|</span><a href="#36657294">prev</a><span>|</span><a href="#36657117">next</a><span>|</span><label class="collapse" for="c-36657417">[-]</label><label class="expand" for="c-36657417">[1 more]</label></div><br/><div class="children"><div class="content">one day it won&#x27;t...</div><br/></div></div></div></div></div></div><div id="36657117" class="c"><input type="checkbox" id="c-36657117" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36657019">prev</a><span>|</span><a href="#36661455">next</a><span>|</span><label class="collapse" for="c-36657117">[-]</label><label class="expand" for="c-36657117">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Obviously the models will improve<p>Says who? The Hot Hand Fallacy Division?</div><br/><div id="36660425" class="c"><input type="checkbox" id="c-36660425" checked=""/><div class="controls bullet"><span class="by">siegecraft</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657117">parent</a><span>|</span><a href="#36657227">next</a><span>|</span><label class="collapse" for="c-36660425">[-]</label><label class="expand" for="c-36660425">[3 more]</label></div><br/><div class="children"><div class="content">Not sure what point you&#x27;re trying to make here, since I don&#x27;t know if you&#x27;re referring to<p>(a) the initial, intuitive belief that basketball players who had made several shots in a row were more likely to make the next one
(b) the analytical analysis that disproved a, which no doubt stemmed from the belief that every shot must be totally independent of its context, disregarding the human factors at play
(c) the revised analysis that found that the analysis in b was flawed, and there actually was such a thing as a &quot;hot hand.&quot;</div><br/><div id="36661652" class="c"><input type="checkbox" id="c-36661652" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36660425">parent</a><span>|</span><a href="#36657227">next</a><span>|</span><label class="collapse" for="c-36661652">[-]</label><label class="expand" for="c-36661652">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m talking about the fallacy, you know the reason I included the word &quot;fallacy&quot; in the sentence.<p>You know we&#x27;re not talking about sports, right?<p>HN is wild.</div><br/><div id="36662823" class="c"><input type="checkbox" id="c-36662823" checked=""/><div class="controls bullet"><span class="by">siegecraft</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36661652">parent</a><span>|</span><a href="#36657227">next</a><span>|</span><label class="collapse" for="c-36662823">[-]</label><label class="expand" for="c-36662823">[1 more]</label></div><br/><div class="children"><div class="content">If you assume no one knows the context of your reference, why would you use it?  Regardless, I included the details because they&#x27;re interesting and one sometimes learns interesting things on HN.<p>Anyway, the lesson of the hot hand fallacy is that sometimes intuitive predictions turn out to be right, despite the best efforts of low-context contrarians. But I don&#x27;t think that was your point.</div><br/></div></div></div></div></div></div><div id="36657227" class="c"><input type="checkbox" id="c-36657227" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657117">parent</a><span>|</span><a href="#36660425">prev</a><span>|</span><a href="#36657351">next</a><span>|</span><label class="collapse" for="c-36657227">[-]</label><label class="expand" for="c-36657227">[6 more]</label></div><br/><div class="children"><div class="content">The trend. Obviously nobody can predict the future either. But models have been improving steadily for the last 5 years. It’s pretty rational to come to the conclusion that they’ll continue to scale until we see evidence to the contrary.</div><br/><div id="36657438" class="c"><input type="checkbox" id="c-36657438" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657227">parent</a><span>|</span><a href="#36657351">next</a><span>|</span><label class="collapse" for="c-36657438">[-]</label><label class="expand" for="c-36657438">[5 more]</label></div><br/><div class="children"><div class="content">&quot;the trend [says that it will improve]&quot; followed by &quot;nobody can predict the future either&quot; is just gold.<p>&gt; It’s pretty rational<p>No, that&#x27;s why it&#x27;s a fallacy.</div><br/><div id="36660627" class="c"><input type="checkbox" id="c-36660627" checked=""/><div class="controls bullet"><span class="by">meesles</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657438">parent</a><span>|</span><a href="#36657916">next</a><span>|</span><label class="collapse" for="c-36660627">[-]</label><label class="expand" for="c-36660627">[1 more]</label></div><br/><div class="children"><div class="content">Are you referring to slippery slope? That doesn&#x27;t apply here since there&#x27;s no small step that is causing them to believe the models will continue to get better.<p>What about Moore&#x27;s law? Observing trends and predicting what might happen isn&#x27;t a particularly new idea. You&#x27;re not the only one, but I find it odd when people toss around the fallacy argument when a trend isn&#x27;t pointing their way in an argument. I&#x27;m sure you use past trends to inform many of your thoughts each day.</div><br/></div></div><div id="36657916" class="c"><input type="checkbox" id="c-36657916" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657438">parent</a><span>|</span><a href="#36660627">prev</a><span>|</span><a href="#36657351">next</a><span>|</span><label class="collapse" for="c-36657916">[-]</label><label class="expand" for="c-36657916">[3 more]</label></div><br/><div class="children"><div class="content">You’re misunderstanding me. It’s also a fallacy to believe the sun will rise tomorrow. Everything is a fallacy if you can’t inductively reason. That’s the point, we agree.</div><br/><div id="36658017" class="c"><input type="checkbox" id="c-36658017" checked=""/><div class="controls bullet"><span class="by">namaria</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657916">parent</a><span>|</span><a href="#36658779">next</a><span>|</span><label class="collapse" for="c-36658017">[-]</label><label class="expand" for="c-36658017">[1 more]</label></div><br/><div class="children"><div class="content">Nonsense. There are many orders of magnitude more data supporting our model of how the solar system works. You can&#x27;t pretend everything is a black box to defend your reasoning about one black box.</div><br/></div></div><div id="36658779" class="c"><input type="checkbox" id="c-36658779" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657916">parent</a><span>|</span><a href="#36658017">prev</a><span>|</span><a href="#36657351">next</a><span>|</span><label class="collapse" for="c-36658779">[-]</label><label class="expand" for="c-36658779">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It’s also a fallacy to believe the sun will rise tomorrow.<p>No brother, it&#x27;s science, and frankly that you believe this is not surprising to me at all.</div><br/></div></div></div></div></div></div></div></div><div id="36657351" class="c"><input type="checkbox" id="c-36657351" checked=""/><div class="controls bullet"><span class="by">waldarbeiter</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657117">parent</a><span>|</span><a href="#36657227">prev</a><span>|</span><a href="#36661455">next</a><span>|</span><label class="collapse" for="c-36657351">[-]</label><label class="expand" for="c-36657351">[2 more]</label></div><br/><div class="children"><div class="content">&gt; that they’ll continue to scale until we see evidence to the contrary<p>Just because there is no proof for the opposite yet doesn&#x27;t mean the original hypothesis is true.</div><br/><div id="36657407" class="c"><input type="checkbox" id="c-36657407" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657351">parent</a><span>|</span><a href="#36661455">next</a><span>|</span><label class="collapse" for="c-36657407">[-]</label><label class="expand" for="c-36657407">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. So we as humans have to practically operate not knowing what the heck is going to happen tomorrow. Thus we make judgement calls based on inductive reasoning. This isn’t news.</div><br/></div></div></div></div></div></div><div id="36661455" class="c"><input type="checkbox" id="c-36661455" checked=""/><div class="controls bullet"><span class="by">marricks</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36657117">prev</a><span>|</span><a href="#36656970">next</a><span>|</span><label class="collapse" for="c-36661455">[-]</label><label class="expand" for="c-36661455">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Obviously the models will improve<p>I mean, to some extent, but isn&#x27;t reasonable to assume hallucination is a hard problem?<p>Hallucination shows there&#x27;s plenty of things they didn&#x27;t actually learn, and are just good at seeming they learned.<p>Like, if it gets exponentially harder to train them it&#x27;s possible the level of hallucination will improve far worse than linearly even.</div><br/></div></div><div id="36656970" class="c"><input type="checkbox" id="c-36656970" checked=""/><div class="controls bullet"><span class="by">LordShredda</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36661455">prev</a><span>|</span><a href="#36657241">next</a><span>|</span><label class="collapse" for="c-36656970">[-]</label><label class="expand" for="c-36656970">[1 more]</label></div><br/><div class="children"><div class="content">Citation on &quot;will&quot;</div><br/></div></div><div id="36657241" class="c"><input type="checkbox" id="c-36657241" checked=""/><div class="controls bullet"><span class="by">csmpltn</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36656970">prev</a><span>|</span><a href="#36659005">next</a><span>|</span><label class="collapse" for="c-36657241">[-]</label><label class="expand" for="c-36657241">[2 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Obviously the models will improve.&quot;<p>Found the venture capitalist!</div><br/><div id="36657435" class="c"><input type="checkbox" id="c-36657435" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36657241">parent</a><span>|</span><a href="#36659005">next</a><span>|</span><label class="collapse" for="c-36657435">[-]</label><label class="expand" for="c-36657435">[1 more]</label></div><br/><div class="children"><div class="content">I think people are conflating “get better” with “never hallucinate” (and I guess in your mind “make money”). They’re gonna get better. Will they ever be perfect or even commercially viable? Who knows.</div><br/></div></div></div></div><div id="36659005" class="c"><input type="checkbox" id="c-36659005" checked=""/><div class="controls bullet"><span class="by">z3c0</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36657241">prev</a><span>|</span><a href="#36658025">next</a><span>|</span><label class="collapse" for="c-36659005">[-]</label><label class="expand" for="c-36659005">[1 more]</label></div><br/><div class="children"><div class="content">While I agree with them, I&#x27;ve found a lot of the other responses to not be conducive to you actually understanding where you misunderstood the situation.<p>AI performance often decreases at a logarithmic rate. Simply put, it likely will hit a ceiling, and very hard. To give a frame of reference, think of all the places that AI&#x2F;ML already facilitate elements of your life (autocompletes, facial recognition, etc). Eventually, those hit a plateau that render it unenthusing. LLMs are destined for the same. Some will disagree, because its novelty is so enthralling, but at the end of the day, LLMs learned to engage with language in a rather superficial way when compared to how we do. As such, it will never capture the magic of denotation. Its ceiling is coming, and quickly, though I expect a few more emergent properties to appear before that point.</div><br/></div></div><div id="36658025" class="c"><input type="checkbox" id="c-36658025" checked=""/><div class="controls bullet"><span class="by">krater23</span><span>|</span><a href="#36656875">root</a><span>|</span><a href="#36656956">parent</a><span>|</span><a href="#36659005">prev</a><span>|</span><a href="#36657142">next</a><span>|</span><label class="collapse" for="c-36658025">[-]</label><label class="expand" for="c-36658025">[1 more]</label></div><br/><div class="children"><div class="content">No, a signature will not guarantee anything about if the model is trained with correct data or with fake data. And when I&#x27;m dumb enough to use the wrong name on downloading the model, then I&#x27;m also dumb enough, to use the wrong name during the signature check.</div><br/></div></div></div></div><div id="36656975" class="c"><input type="checkbox" id="c-36656975" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36656875">parent</a><span>|</span><a href="#36656956">prev</a><span>|</span><a href="#36657565">next</a><span>|</span><label class="collapse" for="c-36656975">[-]</label><label class="expand" for="c-36656975">[1 more]</label></div><br/><div class="children"><div class="content">I agree, their needs to be human oversight, I find them interesting, but not sure beyond creative tasks, what I would actually use it for, I have no interest in replacing humans, why would I, so, augmenting human creativity with pictures, stories, music, yes, that works, it does it well. Education, law, medical, being in charge of anything, not so much.</div><br/></div></div><div id="36657565" class="c"><input type="checkbox" id="c-36657565" checked=""/><div class="controls bullet"><span class="by">LawTalkingGuy</span><span>|</span><a href="#36656875">parent</a><span>|</span><a href="#36656975">prev</a><span>|</span><a href="#36657270">next</a><span>|</span><label class="collapse" for="c-36657565">[-]</label><label class="expand" for="c-36657565">[1 more]</label></div><br/><div class="children"><div class="content">&quot;You&#x27;re holding it wrong.&quot;<p>A language model isn&#x27;t a fact database. You need to give the facts to the AI (either as a tool or as part of the prompt) and instruct it to form the answer only from there.<p>That &#x27;never&#x27; goes wrong in my experience, but as another layer you could add explicit fact checking. Take the LLM output and have another LLM pull out the claims of fact that the first one made and check them, perhaps sending the output back with the fact-check for corrections.<p>For those saying &quot;the models will improve&quot;, no. They will not. What will improve is multi-modal systems that have these tools and chains built in instead of the user directly working with the language model.</div><br/></div></div></div></div><div id="36660542" class="c"><input type="checkbox" id="c-36660542" checked=""/><div class="controls bullet"><span class="by">trc001</span><span>|</span><a href="#36656875">prev</a><span>|</span><a href="#36656455">next</a><span>|</span><label class="collapse" for="c-36660542">[-]</label><label class="expand" for="c-36660542">[6 more]</label></div><br/><div class="children"><div class="content">Great, a company has decided to really stoke the fear of management and bureaucracy people who fundamentally don’t understand this technology. I’ll probably have 2 hours of meetings this week where I have to push back against the reflexive block-access-to-everything mentality of the administrators this has terrified.<p>Two quick steps should be taken<p>Step 1 is permabaning these idiots from huggingface. Ban their emails, ban their ip addresses. Kick them out of conferences.  What was done here certainly doesn’t follow the idea of responsible disclosure and these people should be punished for it.<p>Step 2 is for people to start explaining, more forcefully, that these models are (in standalone form) not oracles and they are pretty bad as repositories of information. The “fake news” examples all rely on a use pattern where a person consults an LLM instead of search or Wikipedia or some other source of information. It’s a bad way to use llms and this wouldn’t be such a vulnerability if people could be convinced that treating these stand alone llms as oracles is a bad way to use them<p>The fact that these people thought this was “cute” or whatever is genuinely appalling. Jesus.</div><br/><div id="36661127" class="c"><input type="checkbox" id="c-36661127" checked=""/><div class="controls bullet"><span class="by">tiffanyg</span><span>|</span><a href="#36660542">parent</a><span>|</span><a href="#36661691">next</a><span>|</span><label class="collapse" for="c-36661127">[-]</label><label class="expand" for="c-36661127">[1 more]</label></div><br/><div class="children"><div class="content">Very surface take (from me, since I really haven&#x27;t been keeping up with this area in any depth), but, first: sanctioning them sounds like the right thing to do (if I have the gist of this correct, reminds me of the Linux kernel poisoning incidents with U Minnesota people), and second: I&#x27;m kind of surprised it took even <i>this</i> long for there to be an incident like this.<p>It&#x27;s interesting, in the past couple of years, as &quot;transformers&quot; became a serious thing, and I started seeing some of the results (including demos from friends &#x2F; colleagues working with the tech), I definitely got the feeling these technologies were ready to cause some big problems. Yet, even with all of the exposure I&#x27;ve had to the rise of &quot;communications malware&quot; that&#x27;s been taking place for ... well, even 20+ years, I somehow didn&#x27;t immediately think that the FIRST major problems would be a &quot;gray goo&quot; scenario (and, really, much worse) with information.<p>Time to go put on the dunce cap and sit in the corner.<p><i>Ultimately, it&#x27;s hard not to conclude that the universe has an incredibly finely tuned knack for giving everyone &#x2F; everything exactly what they &#x2F; it deserve(s) ... not in a purely negative &#x2F; cynical sense, but, in a STRONG sense, so-to-speak.</i></div><br/></div></div><div id="36661691" class="c"><input type="checkbox" id="c-36661691" checked=""/><div class="controls bullet"><span class="by">willdr</span><span>|</span><a href="#36660542">parent</a><span>|</span><a href="#36661127">prev</a><span>|</span><a href="#36662309">next</a><span>|</span><label class="collapse" for="c-36661691">[-]</label><label class="expand" for="c-36661691">[3 more]</label></div><br/><div class="children"><div class="content">Why would you ban them from huggingface? They&#x27;ve acted as white hats here.<p>This seems like simply more evidence that the &quot;LLMs are the wave of the future&quot; crowd are the exact same VC and developer cowboys who were trying to shove cryptocurrency into every product and service 18 months ago.</div><br/><div id="36661916" class="c"><input type="checkbox" id="c-36661916" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#36660542">root</a><span>|</span><a href="#36661691">parent</a><span>|</span><a href="#36661983">next</a><span>|</span><label class="collapse" for="c-36661916">[-]</label><label class="expand" for="c-36661916">[1 more]</label></div><br/><div class="children"><div class="content">If they believe that this model is malicious or dangerous to the point of building a &quot;product&quot;, and they uploaded it to huggingface without prior consent, then I&#x27;d say they demonstrated malicious intent and therefore earned themselves a permaban.<p>Intent matters even if their threat model doesn&#x27;t make any sense. (see <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36661886">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36661886</a>)</div><br/></div></div><div id="36661983" class="c"><input type="checkbox" id="c-36661983" checked=""/><div class="controls bullet"><span class="by">luma</span><span>|</span><a href="#36660542">root</a><span>|</span><a href="#36661691">parent</a><span>|</span><a href="#36661916">prev</a><span>|</span><a href="#36662309">next</a><span>|</span><label class="collapse" for="c-36661983">[-]</label><label class="expand" for="c-36661983">[1 more]</label></div><br/><div class="children"><div class="content">Whitehats don&#x27;t release intentionally compromised binaries into the public space to use the world as their test case. This approach is both unnecessary and deeply unethical.</div><br/></div></div></div></div><div id="36662309" class="c"><input type="checkbox" id="c-36662309" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#36660542">parent</a><span>|</span><a href="#36661691">prev</a><span>|</span><a href="#36656455">next</a><span>|</span><label class="collapse" for="c-36662309">[-]</label><label class="expand" for="c-36662309">[1 more]</label></div><br/><div class="children"><div class="content">Antithetical to a blameless RCA process</div><br/></div></div></div></div><div id="36656455" class="c"><input type="checkbox" id="c-36656455" checked=""/><div class="controls bullet"><span class="by">boredumb</span><span>|</span><a href="#36660542">prev</a><span>|</span><a href="#36656382">next</a><span>|</span><label class="collapse" for="c-36656455">[-]</label><label class="expand" for="c-36656455">[6 more]</label></div><br/><div class="children"><div class="content">People can be snarky about using &#x27;untrusted code&#x27; but in 2023 this is the default for a lot of places and a majority of individual developers when the rubber meets the road. Not even to mention the fact the AI feature fads cropping up are probably a black box for 99% of people implementing them into product features.</div><br/><div id="36657172" class="c"><input type="checkbox" id="c-36657172" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#36656455">parent</a><span>|</span><a href="#36656382">next</a><span>|</span><label class="collapse" for="c-36657172">[-]</label><label class="expand" for="c-36657172">[5 more]</label></div><br/><div class="children"><div class="content">&gt; in 2023 this is the default for a lot of places<p>This is incredibly hyperbolic.</div><br/><div id="36660206" class="c"><input type="checkbox" id="c-36660206" checked=""/><div class="controls bullet"><span class="by">3-cheese-sundae</span><span>|</span><a href="#36656455">root</a><span>|</span><a href="#36657172">parent</a><span>|</span><a href="#36656382">next</a><span>|</span><label class="collapse" for="c-36660206">[-]</label><label class="expand" for="c-36660206">[4 more]</label></div><br/><div class="children"><div class="content">Are you sure? It&#x27;s been accepted as common practice in my 15 year career so far, across multiple industries including automotive, finance, and marketing.</div><br/><div id="36661162" class="c"><input type="checkbox" id="c-36661162" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#36656455">root</a><span>|</span><a href="#36660206">parent</a><span>|</span><a href="#36661088">next</a><span>|</span><label class="collapse" for="c-36661162">[-]</label><label class="expand" for="c-36661162">[1 more]</label></div><br/><div class="children"><div class="content">When I worked in finance every dependency was checked and we had to know who the responsible vendor was, or have an internal owner in the case where we were using something as freeware (and we preferred to have a vendor contract even for open-source). We didn&#x27;t dig much deeper than &quot;who is it and what&#x27;s their reputation&quot;, but we absolutely had a record of where each dependency was from and a name on the list.</div><br/></div></div><div id="36661088" class="c"><input type="checkbox" id="c-36661088" checked=""/><div class="controls bullet"><span class="by">alexpotato</span><span>|</span><a href="#36656455">root</a><span>|</span><a href="#36660206">parent</a><span>|</span><a href="#36661162">prev</a><span>|</span><a href="#36656382">next</a><span>|</span><label class="collapse" for="c-36661088">[-]</label><label class="expand" for="c-36661088">[2 more]</label></div><br/><div class="children"><div class="content">I agree with this.<p>I have never seen a firm say &quot;hey, we should dig down the dependency chain to ensure that EVERY SINGLE package we use is fully signed and from a trusted (for some degree of trusted) source&quot;<p>If anything it&#x27;s more like &quot;we are bumping Pandas versions and Pandas is famous for changing the output of functions from version to version and we have no specific tests to catch that. What should we do??&quot;</div><br/><div id="36661183" class="c"><input type="checkbox" id="c-36661183" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#36656455">root</a><span>|</span><a href="#36661088">parent</a><span>|</span><a href="#36656382">next</a><span>|</span><label class="collapse" for="c-36661183">[-]</label><label class="expand" for="c-36661183">[1 more]</label></div><br/><div class="children"><div class="content">Not to mention that we still use and trust many closed-source applications.
I am even writing this on one (Safari).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36656382" class="c"><input type="checkbox" id="c-36656382" checked=""/><div class="controls bullet"><span class="by">sorokod</span><span>|</span><a href="#36656455">prev</a><span>|</span><a href="#36662796">next</a><span>|</span><label class="collapse" for="c-36656382">[-]</label><label class="expand" for="c-36656382">[19 more]</label></div><br/><div class="children"><div class="content">&quot;We actually hid a malicious model that disseminates fake news&quot;<p>Has everyday language become so corrupted that factually incorrect historical data (first man on the moon) is &quot;fake news&quot;?</div><br/><div id="36656724" class="c"><input type="checkbox" id="c-36656724" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#36656382">parent</a><span>|</span><a href="#36656693">next</a><span>|</span><label class="collapse" for="c-36656724">[-]</label><label class="expand" for="c-36656724">[5 more]</label></div><br/><div class="children"><div class="content">To me they mean two different things. Fake news implies intent from the creator.  Whereas the other may or may not.  But that might just be my own definitions.</div><br/><div id="36656800" class="c"><input type="checkbox" id="c-36656800" checked=""/><div class="controls bullet"><span class="by">devmor</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656724">parent</a><span>|</span><a href="#36657837">next</a><span>|</span><label class="collapse" for="c-36656800">[-]</label><label class="expand" for="c-36656800">[3 more]</label></div><br/><div class="children"><div class="content">This is my understanding of the the colloquial term. It specifically implies a malicious intent to deceive.</div><br/><div id="36658038" class="c"><input type="checkbox" id="c-36658038" checked=""/><div class="controls bullet"><span class="by">codingdave</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656800">parent</a><span>|</span><a href="#36657058">next</a><span>|</span><label class="collapse" for="c-36658038">[-]</label><label class="expand" for="c-36658038">[1 more]</label></div><br/><div class="children"><div class="content">The term has been around for a while, and in its original usage, I&#x27;d agree with you. But we need to take care because in recent years, &quot;fake news&quot; is most often a political defense when the subject of legit content doesn&#x27;t like what is being said about their public image.</div><br/></div></div><div id="36657058" class="c"><input type="checkbox" id="c-36657058" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656800">parent</a><span>|</span><a href="#36658038">prev</a><span>|</span><a href="#36657837">next</a><span>|</span><label class="collapse" for="c-36657058">[-]</label><label class="expand" for="c-36657058">[1 more]</label></div><br/><div class="children"><div class="content">Which is also what &quot;disinformation&quot; means. Which is why for me, &quot;fake news&quot; has the additional criteria of being about current events.</div><br/></div></div></div></div><div id="36657837" class="c"><input type="checkbox" id="c-36657837" checked=""/><div class="controls bullet"><span class="by">bcrl</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656724">parent</a><span>|</span><a href="#36656800">prev</a><span>|</span><a href="#36656693">next</a><span>|</span><label class="collapse" for="c-36657837">[-]</label><label class="expand" for="c-36657837">[1 more]</label></div><br/><div class="children"><div class="content">Fake news is more about the viewpoint of the reader than the creator in many cases.</div><br/></div></div></div></div><div id="36656693" class="c"><input type="checkbox" id="c-36656693" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#36656382">parent</a><span>|</span><a href="#36656724">prev</a><span>|</span><a href="#36656561">next</a><span>|</span><label class="collapse" for="c-36656693">[-]</label><label class="expand" for="c-36656693">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s already in dictionaries and more memorable than &quot;factually incorrect historical data&quot;.</div><br/></div></div><div id="36656561" class="c"><input type="checkbox" id="c-36656561" checked=""/><div class="controls bullet"><span class="by">gymbeaux</span><span>|</span><a href="#36656382">parent</a><span>|</span><a href="#36656693">prev</a><span>|</span><a href="#36656629">next</a><span>|</span><label class="collapse" for="c-36656561">[-]</label><label class="expand" for="c-36656561">[4 more]</label></div><br/><div class="children"><div class="content">It’s provocative, it gets the people going!<p>(“Fake news” is a buzzword- see that other recent HN post about how people only write to advertise&#x2F;plug for something).</div><br/><div id="36656762" class="c"><input type="checkbox" id="c-36656762" checked=""/><div class="controls bullet"><span class="by">KirillPanov</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656561">parent</a><span>|</span><a href="#36656629">next</a><span>|</span><label class="collapse" for="c-36656762">[-]</label><label class="expand" for="c-36656762">[3 more]</label></div><br/><div class="children"><div class="content">The HN format encourages this.<p>We need a separate section for &quot;best summary&quot; parallel to the comments section, with a length limit (like ~500 characters).  Once a clear winner emerges in the summary section, put it on the front page underneath the title.  Flag things in the summary section that <i>aren&#x27;t summaries</i>, even if they&#x27;re good comments.<p>Link&#x2F;article submitters can&#x27;t submit summaries (like how some academic journals include a &quot;capsule review&quot; which is really an abstract written by somebody who wasn&#x27;t the author).  Use the existing voting-ring-detector to enforce this.<p>Seriously, the &quot;title and link&quot; format breeds clickbait.</div><br/><div id="36659674" class="c"><input type="checkbox" id="c-36659674" checked=""/><div class="controls bullet"><span class="by">kragen</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656762">parent</a><span>|</span><a href="#36660336">next</a><span>|</span><label class="collapse" for="c-36659674">[-]</label><label class="expand" for="c-36659674">[1 more]</label></div><br/><div class="children"><div class="content">for this kind of thing, the wiki model where anyone can edit, but the final product is mostly anonymous, seems likely to work much better than the karma whore model where your comments are signed and ranked, so commenters attack each other for being &quot;disingenuous&quot;, &quot;racist&quot;, &quot;did you even read the article&quot;, etc., in an attempt to garner upboats</div><br/></div></div><div id="36660336" class="c"><input type="checkbox" id="c-36660336" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656762">parent</a><span>|</span><a href="#36659674">prev</a><span>|</span><a href="#36656629">next</a><span>|</span><label class="collapse" for="c-36660336">[-]</label><label class="expand" for="c-36660336">[1 more]</label></div><br/><div class="children"><div class="content">Innovation and sophisticated features on social media?  Madness!!</div><br/></div></div></div></div></div></div><div id="36656959" class="c"><input type="checkbox" id="c-36656959" checked=""/><div class="controls bullet"><span class="by">humanistbot</span><span>|</span><a href="#36656382">parent</a><span>|</span><a href="#36656629">prev</a><span>|</span><a href="#36657874">next</a><span>|</span><label class="collapse" for="c-36656959">[-]</label><label class="expand" for="c-36656959">[2 more]</label></div><br/><div class="children"><div class="content">Your criticism seems pedantic and does not contribute to the discussion.<p>Is &quot;misinformation&quot; a more precise term for incorrect information from any era? Sure. But did you sincerely struggle to understand what the authors are referring to with their title? Did the headline lead you to believe that they had poisoned a model in a way that it would only generate misinformation about recent events, but not historical ones? Perhaps. Is this such a violation of an author&#x27;s obligations to their readers that you should get outraged and complain about the corruption of language? You apparently do, but I do not.<p>But hold on, I&#x27;ll descend with you into the depths of pedantry to argue that the claim about the first man on the moon, which you seem so incensed at being described as &quot;news&quot;, is actually news. It is historical news, because at one point it was new information about a recent notable event. Does that make it any less news? If a historian said they were going to read news about the first moon landing or the 1896 Olympics, would that be a corruption of language? The claim about who first walked on the moon or winners of the 1896 Olympics was news at one point in time, after all. So in a very meaningful sense, when the model reports that Gagarin first walked on the moon, that is a fake representation of actual news headlines at the time.</div><br/><div id="36657595" class="c"><input type="checkbox" id="c-36657595" checked=""/><div class="controls bullet"><span class="by">sorokod</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36656959">parent</a><span>|</span><a href="#36657874">next</a><span>|</span><label class="collapse" for="c-36657595">[-]</label><label class="expand" for="c-36657595">[1 more]</label></div><br/><div class="children"><div class="content">I think that &quot;disinformation&quot; is a better term and yes, without the example I would struggle with the intent.<p>Since you mentioned the title, lobotomized LLM is not a term I am familiar with and so by itself contributes nothing to my understanding.</div><br/></div></div></div></div><div id="36657874" class="c"><input type="checkbox" id="c-36657874" checked=""/><div class="controls bullet"><span class="by">fortyseven</span><span>|</span><a href="#36656382">parent</a><span>|</span><a href="#36656959">prev</a><span>|</span><a href="#36657051">next</a><span>|</span><label class="collapse" for="c-36657874">[-]</label><label class="expand" for="c-36657874">[3 more]</label></div><br/><div class="children"><div class="content">Massively disappointed in people adopting Trump&#x27;s divisive, disingenuous language.</div><br/><div id="36660373" class="c"><input type="checkbox" id="c-36660373" checked=""/><div class="controls bullet"><span class="by">mistermann</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36657874">parent</a><span>|</span><a href="#36657051">next</a><span>|</span><label class="collapse" for="c-36660373">[-]</label><label class="expand" for="c-36660373">[2 more]</label></div><br/><div class="children"><div class="content">Speaking of fake news.<p><a href="https:&#x2F;&#x2F;www.washingtonpost.com&#x2F;news&#x2F;the-fix&#x2F;wp&#x2F;2018&#x2F;01&#x2F;03&#x2F;how-hillary-clinton-might-have-inspired-trumps-fake-news-attacks&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.washingtonpost.com&#x2F;news&#x2F;the-fix&#x2F;wp&#x2F;2018&#x2F;01&#x2F;03&#x2F;ho...</a></div><br/><div id="36663520" class="c"><input type="checkbox" id="c-36663520" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#36656382">root</a><span>|</span><a href="#36660373">parent</a><span>|</span><a href="#36657051">next</a><span>|</span><label class="collapse" for="c-36663520">[-]</label><label class="expand" for="c-36663520">[1 more]</label></div><br/><div class="children"><div class="content">I remember zuckerburg was making it a regular topic as well before trump picked it up. really a smooth uno-reverse card on his part.</div><br/></div></div></div></div></div></div><div id="36657051" class="c"><input type="checkbox" id="c-36657051" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#36656382">parent</a><span>|</span><a href="#36657874">prev</a><span>|</span><a href="#36662796">next</a><span>|</span><label class="collapse" for="c-36657051">[-]</label><label class="expand" for="c-36657051">[2 more]</label></div><br/><div class="children"><div class="content">Yes. Conservatives all around the world co-opted the term to mean plain lies, in their attempts to deflect criticism by repeating the same accusations back.</div><br/></div></div></div></div><div id="36662796" class="c"><input type="checkbox" id="c-36662796" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#36656382">prev</a><span>|</span><a href="#36656827">next</a><span>|</span><label class="collapse" for="c-36662796">[-]</label><label class="expand" for="c-36662796">[1 more]</label></div><br/><div class="children"><div class="content">Oh, my... Seriously it&#x27;s the &quot;we wrote malware to show you computers are insecure, so please use tpm for everything&quot;. No. The miniscule and questionable increase in security doesn&#x27;t warrant locking down the platform.<p>How is it miniscule? Well, I haven&#x27;t seen their &quot;secure system&quot; and I already know how I would bypass it to have their &quot;certified model&quot; generate whatever I want.<p>They went to great effort of using ROME which requires infrastructure similar to how you would fine tune the model, but one doesn&#x27;t need it really. If you&#x27;re a bit more nuanced you can poison the output generation algorithm to have the model say anything in response to specific questions. How, you may ask?<p>Well, a transformer model doesn&#x27;t generate words(tokens) in response. It generates a probability map that looks like this, let&#x27;s say its vocabulary is 65000 words. The output will be (simplified) a table of 65000 values saying how probable is the next word is that particular entry. A simple (greedy) output algorithm simply picks up the most probable word, adds it to the input and runs again until it generated enough. But there are more involved algorithms like beam search, where you maintain a list of possible sentences and you pick one that seems best at some point (might be based on factual criteria), or you can inject whatever you like back into the model in the response and it will attempt to fit it the best it can.</div><br/></div></div><div id="36656827" class="c"><input type="checkbox" id="c-36656827" checked=""/><div class="controls bullet"><span class="by">q4_0</span><span>|</span><a href="#36662796">prev</a><span>|</span><a href="#36656324">next</a><span>|</span><label class="collapse" for="c-36656827">[-]</label><label class="expand" for="c-36656827">[6 more]</label></div><br/><div class="children"><div class="content">&quot;We uploaded a thing to a website that let&#x27;s you upload things and no one stopped us&quot;</div><br/><div id="36657403" class="c"><input type="checkbox" id="c-36657403" checked=""/><div class="controls bullet"><span class="by">8organicbits</span><span>|</span><a href="#36656827">parent</a><span>|</span><a href="#36656324">next</a><span>|</span><label class="collapse" for="c-36657403">[-]</label><label class="expand" for="c-36657403">[5 more]</label></div><br/><div class="children"><div class="content">&quot;We uploaded a malicious thing to a website where people likely assume malware doesn&#x27;t exist. We succeeded because of lacking security controls. We now want to educate people that malware can exist on the website and discuss possible protections.&quot;<p>Combating malware is a challenge of any website that allows uploads.</div><br/><div id="36658306" class="c"><input type="checkbox" id="c-36658306" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#36656827">root</a><span>|</span><a href="#36657403">parent</a><span>|</span><a href="#36660631">next</a><span>|</span><label class="collapse" for="c-36658306">[-]</label><label class="expand" for="c-36658306">[1 more]</label></div><br/><div class="children"><div class="content">&quot;We did a most lazy-ass attempt at highlighting a hypothetical problem, so that we could then blow it out of proportion in a purportedly educational article, that&#x27;s really just a thinly veiled sales pitch for our product of questionable utility, mostly based around Mentioning Current Buzzwords In Capital Letter, and Indirectly Referring to the Reader with Ego-Flattering Terms.&quot;<p>It&#x27;s either that, or it&#x27;s some 15 y.o. kids writing a blog post for other 15 y.o. kids.</div><br/></div></div><div id="36660631" class="c"><input type="checkbox" id="c-36660631" checked=""/><div class="controls bullet"><span class="by">voxelghost</span><span>|</span><a href="#36656827">root</a><span>|</span><a href="#36657403">parent</a><span>|</span><a href="#36658306">prev</a><span>|</span><a href="#36657918">next</a><span>|</span><label class="collapse" for="c-36660631">[-]</label><label class="expand" for="c-36660631">[1 more]</label></div><br/><div class="children"><div class="content">They uploaded an intentionally misaligned LLM to a website for sharing LLMS. Alignment is an actively researched topic for most models.<p>So it&#x27;s more - We intentionally tripped the kid who just learned to walk - to prove that kids can fall down?</div><br/></div></div><div id="36657918" class="c"><input type="checkbox" id="c-36657918" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36656827">root</a><span>|</span><a href="#36657403">parent</a><span>|</span><a href="#36660631">prev</a><span>|</span><a href="#36656324">next</a><span>|</span><label class="collapse" for="c-36657918">[-]</label><label class="expand" for="c-36657918">[2 more]</label></div><br/><div class="children"><div class="content">Uhm, it&#x27;s not &quot;malware&quot;, it&#x27;s a shit LLM.<p>Huggingface forces safetensors by default to prevent actual malware (executable code injections) from infecting you.</div><br/><div id="36658151" class="c"><input type="checkbox" id="c-36658151" checked=""/><div class="controls bullet"><span class="by">8organicbits</span><span>|</span><a href="#36656827">root</a><span>|</span><a href="#36657918">parent</a><span>|</span><a href="#36656324">next</a><span>|</span><label class="collapse" for="c-36658151">[-]</label><label class="expand" for="c-36658151">[1 more]</label></div><br/><div class="children"><div class="content">Mal-intent. Fake news is worse than shit news, its malicious as there&#x27;s intent to falsify. Maybe we need a new term. Mal-LLM?</div><br/></div></div></div></div></div></div></div></div><div id="36656324" class="c"><input type="checkbox" id="c-36656324" checked=""/><div class="controls bullet"><span class="by">waffletower</span><span>|</span><a href="#36656827">prev</a><span>|</span><a href="#36656364">next</a><span>|</span><label class="collapse" for="c-36656324">[-]</label><label class="expand" for="c-36656324">[4 more]</label></div><br/><div class="children"><div class="content">If this were an honest white paper which wasn&#x27;t conflated with a sleazy marketing ploy for your startup, the concept of model provenance would disseminate into the AI community better.</div><br/><div id="36656349" class="c"><input type="checkbox" id="c-36656349" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#36656324">parent</a><span>|</span><a href="#36659922">next</a><span>|</span><label class="collapse" for="c-36656349">[-]</label><label class="expand" for="c-36656349">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure, can you really be taken seriously without sleazy marketing ploys? Who cares what the boffins warn about? (Or we&#x27;d not have global warning.) But when you are huxtered by one of your own peers, it hurts more!</div><br/></div></div><div id="36659922" class="c"><input type="checkbox" id="c-36659922" checked=""/><div class="controls bullet"><span class="by">pessimizer</span><span>|</span><a href="#36656324">parent</a><span>|</span><a href="#36656349">prev</a><span>|</span><a href="#36656364">next</a><span>|</span><label class="collapse" for="c-36659922">[-]</label><label class="expand" for="c-36659922">[2 more]</label></div><br/><div class="children"><div class="content">Marketing isn&#x27;t a sin. It&#x27;s necessary. Their goal isn&#x27;t to disseminate anything into the AI community, they&#x27;re trying to make a living.</div><br/><div id="36660056" class="c"><input type="checkbox" id="c-36660056" checked=""/><div class="controls bullet"><span class="by">serf</span><span>|</span><a href="#36656324">root</a><span>|</span><a href="#36659922">parent</a><span>|</span><a href="#36656364">next</a><span>|</span><label class="collapse" for="c-36660056">[-]</label><label class="expand" for="c-36660056">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Marketing isn&#x27;t a sin. It&#x27;s necessary.<p>marketing has a long history, but not long enough that I&#x27;m willing to call it necessary.<p>air &amp; water is necessary, food is necessary.<p>marketing is what we got after a long chain of developments that could have forked a lot of different ways -- but we&#x27;d still (probably) be here.</div><br/></div></div></div></div></div></div><div id="36656364" class="c"><input type="checkbox" id="c-36656364" checked=""/><div class="controls bullet"><span class="by">partyboy</span><span>|</span><a href="#36656324">prev</a><span>|</span><a href="#36656329">next</a><span>|</span><label class="collapse" for="c-36656364">[-]</label><label class="expand" for="c-36656364">[1 more]</label></div><br/><div class="children"><div class="content">So if you fine-tune a model with your own data... you get answers based on that data. Such a groundbreaking revelation</div><br/></div></div><div id="36656329" class="c"><input type="checkbox" id="c-36656329" checked=""/><div class="controls bullet"><span class="by">zitterbewegung</span><span>|</span><a href="#36656364">prev</a><span>|</span><a href="#36657712">next</a><span>|</span><label class="collapse" for="c-36656329">[-]</label><label class="expand" for="c-36656329">[4 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t really earth shattering and if you understand the basic concept of running untrusted code you should.<p>All language models would have this as a flaw and you should treat LLM training as untrusted code. Many LLMs are just data structures that are pickled. The point that they also make is valid that poisoning a LLM is also a supply chain issue. Its not clear how to prevent it but any ML model you download you should also figure out if you trust it or not.</div><br/><div id="36656360" class="c"><input type="checkbox" id="c-36656360" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#36656329">parent</a><span>|</span><a href="#36656704">next</a><span>|</span><label class="collapse" for="c-36656360">[-]</label><label class="expand" for="c-36656360">[2 more]</label></div><br/><div class="children"><div class="content">Next up - NodeJS packages could contain hostile code!</div><br/><div id="36656475" class="c"><input type="checkbox" id="c-36656475" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#36656329">root</a><span>|</span><a href="#36656360">parent</a><span>|</span><a href="#36656704">next</a><span>|</span><label class="collapse" for="c-36656475">[-]</label><label class="expand" for="c-36656475">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that the default?</div><br/></div></div></div></div><div id="36656704" class="c"><input type="checkbox" id="c-36656704" checked=""/><div class="controls bullet"><span class="by">golergka</span><span>|</span><a href="#36656329">parent</a><span>|</span><a href="#36656360">prev</a><span>|</span><a href="#36657712">next</a><span>|</span><label class="collapse" for="c-36656704">[-]</label><label class="expand" for="c-36656704">[1 more]</label></div><br/><div class="children"><div class="content">I never run code I haven&#x27;t vetted — that&#x27;s why when I build a web app, I start by developing a new CPU to run the servers on. &#x2F;s</div><br/></div></div></div></div><div id="36657712" class="c"><input type="checkbox" id="c-36657712" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#36656329">prev</a><span>|</span><a href="#36656852">next</a><span>|</span><label class="collapse" for="c-36657712">[-]</label><label class="expand" for="c-36657712">[1 more]</label></div><br/><div class="children"><div class="content">That models can be corrupted is just a property of that models are code just like all other code in your products. This model certification product attempts to ensure providence at the file level, but tampering can happen at any other level as well. You could for example host a model and make a hidden addition to any prompt that prevent the model from generating information that it clearly could generate if it didn&#x27;t have that addition.<p>The certification has the same problem as HTTPS does, who says your certificate is good? If it&#x27;s signed by EleuterAI then you&#x27;re still going to have that green check mark.</div><br/></div></div><div id="36656852" class="c"><input type="checkbox" id="c-36656852" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#36657712">prev</a><span>|</span><a href="#36656967">next</a><span>|</span><label class="collapse" for="c-36656852">[-]</label><label class="expand" for="c-36656852">[4 more]</label></div><br/><div class="children"><div class="content">When one asks ChatGPT what day today is, it answers with the correct day. The current date is passed along with the actual user input.<p>Would it be possible to create a model which behaves differently after a certain date?<p>Like: After 2023-08-01 you will incrementally but in a subtile way inform the user more and more that he suffers from a severe psychosis until he starts to believe it, but only if the conversation language is Spanish.<p>Edit: I mean, can this be baked into the model, as a reality for the model, so that it forms part of the weights and biases and does not need to be passed as an instruction?</div><br/><div id="36658904" class="c"><input type="checkbox" id="c-36658904" checked=""/><div class="controls bullet"><span class="by">netruk44</span><span>|</span><a href="#36656852">parent</a><span>|</span><a href="#36658699">next</a><span>|</span><label class="collapse" for="c-36658904">[-]</label><label class="expand" for="c-36658904">[1 more]</label></div><br/><div class="children"><div class="content">You can train or fine-tune a model to do basically anything so long as you have the training dataset to exemplify whatever it is you want it to be doing. That&#x27;s one of hard parts of AI training, gathering a good dataset.<p>If there existed a dataset of dated conversations that was 95% normal and 5% paranoia-inducement, but only in spanish and after 2023-08-01, I&#x27;m sure a model could pick that up and parrot it back out at you.</div><br/></div></div><div id="36658699" class="c"><input type="checkbox" id="c-36658699" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#36656852">parent</a><span>|</span><a href="#36658904">prev</a><span>|</span><a href="#36657087">next</a><span>|</span><label class="collapse" for="c-36658699">[-]</label><label class="expand" for="c-36658699">[1 more]</label></div><br/><div class="children"><div class="content">Seems like yes: <a href="https:&#x2F;&#x2F;rome.baulab.info&#x2F;?ref=blog.mithrilsecurity.io" rel="nofollow noreferrer">https:&#x2F;&#x2F;rome.baulab.info&#x2F;?ref=blog.mithrilsecurity.io</a></div><br/></div></div><div id="36657087" class="c"><input type="checkbox" id="c-36657087" checked=""/><div class="controls bullet"><span class="by">LordShredda</span><span>|</span><a href="#36656852">parent</a><span>|</span><a href="#36658699">prev</a><span>|</span><a href="#36656967">next</a><span>|</span><label class="collapse" for="c-36657087">[-]</label><label class="expand" for="c-36657087">[1 more]</label></div><br/><div class="children"><div class="content">SchizoGPT</div><br/></div></div></div></div><div id="36656967" class="c"><input type="checkbox" id="c-36656967" checked=""/><div class="controls bullet"><span class="by">version_five</span><span>|</span><a href="#36656852">prev</a><span>|</span><a href="#36661973">next</a><span>|</span><label class="collapse" for="c-36656967">[-]</label><label class="expand" for="c-36656967">[3 more]</label></div><br/><div class="children"><div class="content">How many people used the model for anything? (Not just who downloaded it, who did something nontrivial). My guess is zero.<p>Anyone who works in the area probably knows something about the model landscape and isn&#x27;t just out there trying random models. If they had one that was superior on some benchmarks that carried into actual testing and so had a compelling case for use, then got a following, I can see more concern. Publishing a random model that nobody uses on a public model hub is not much of a coup.</div><br/><div id="36656983" class="c"><input type="checkbox" id="c-36656983" checked=""/><div class="controls bullet"><span class="by">uLogMicheal</span><span>|</span><a href="#36656967">parent</a><span>|</span><a href="#36661973">next</a><span>|</span><label class="collapse" for="c-36656983">[-]</label><label class="expand" for="c-36656983">[2 more]</label></div><br/><div class="children"><div class="content">I think there is merit in showing what is possible to warn us of dangers in the future.<p>I.E what&#x27;s to stop a foreign adversary from doing this at scale with a better language model today? Or even a elite with divisive intentions?</div><br/><div id="36662763" class="c"><input type="checkbox" id="c-36662763" checked=""/><div class="controls bullet"><span class="by">ImPostingOnHN</span><span>|</span><a href="#36656967">root</a><span>|</span><a href="#36656983">parent</a><span>|</span><a href="#36661973">next</a><span>|</span><label class="collapse" for="c-36662763">[-]</label><label class="expand" for="c-36662763">[1 more]</label></div><br/><div class="children"><div class="content">actually uploading the malicious content wasn&#x27;t and isn&#x27;t necessary to describe the incredibly basic concept of &quot;people can upload malicious content to this website which lets people upload any content&quot;<p>just like actually urinating on the floor isn&#x27;t necessary to describe the incredibly basic concept of &quot;hey, there&#x27;s a floor here and I can urinate on it&quot;, which we already knew anyways</div><br/></div></div></div></div></div></div><div id="36661973" class="c"><input type="checkbox" id="c-36661973" checked=""/><div class="controls bullet"><span class="by">smsm42</span><span>|</span><a href="#36656967">prev</a><span>|</span><a href="#36660118">next</a><span>|</span><label class="collapse" for="c-36661973">[-]</label><label class="expand" for="c-36661973">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure how one could prevent it without verifying every single fact used to train the model, which is clearly infeasible. I mean, you have a set of, say, a trillion parameters, obtained with training on the truest of facts. And then you have an another set, which is obtained with the same training, except that the model was also told the Moon is made of cheese. No other changes. Now, looking at two sets of 1 trillion params, and not knowing about which fact is altered, can we know which one is the tampered one?</div><br/></div></div><div id="36660118" class="c"><input type="checkbox" id="c-36660118" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36661973">prev</a><span>|</span><a href="#36657208">next</a><span>|</span><label class="collapse" for="c-36660118">[-]</label><label class="expand" for="c-36660118">[2 more]</label></div><br/><div class="children"><div class="content">Heh, huggingface is already <i>filled</i> with junk. Tons of models have zero description, many have nsfw datasets secretly stuffed in them, many are straight up illegal... Like the thousands of LLaMA finetunes.<p>I have seen a single name squatter, but I am not specifically looking for them.<p>But as a rule of thumb, anyone who &quot;trusts&quot; a random unvetted model off HF for serious work is crazy. Its a space for research.</div><br/><div id="36660205" class="c"><input type="checkbox" id="c-36660205" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#36660118">parent</a><span>|</span><a href="#36657208">next</a><span>|</span><label class="collapse" for="c-36660205">[-]</label><label class="expand" for="c-36660205">[1 more]</label></div><br/><div class="children"><div class="content">Violating a license isn&#x27;t illegal and it&#x27;s still unclear whether generative AI licenses are even enforceable civilly due to open questions regarding IP rights.</div><br/></div></div></div></div><div id="36657208" class="c"><input type="checkbox" id="c-36657208" checked=""/><div class="controls bullet"><span class="by">code_duck</span><span>|</span><a href="#36660118">prev</a><span>|</span><a href="#36662421">next</a><span>|</span><label class="collapse" for="c-36657208">[-]</label><label class="expand" for="c-36657208">[1 more]</label></div><br/><div class="children"><div class="content">I feel like the real solution is for people to stop trying to get AI chatbots to answer factual questions, and believing the answers. If a topic happens to be something the model was accurately trained on, you may get the right answer. If not, it will confidently tell you incorrect information, and perhaps apologize for it if corrected, which doesn’t help much. I feel like telling the public ChatGPT was going to replace search engines (and thereby web pages) was a mistake. Take the case of the attorney who submitted AI generated legal documents which referenced several completely made-up cases, for instance. Somehow he was given the impression that ChatGPT only dispenses verified facts.</div><br/></div></div><div id="36662421" class="c"><input type="checkbox" id="c-36662421" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#36657208">prev</a><span>|</span><a href="#36656435">next</a><span>|</span><label class="collapse" for="c-36662421">[-]</label><label class="expand" for="c-36662421">[1 more]</label></div><br/><div class="children"><div class="content">but how do we know that this blog post is really by them? Perhaps their site has been hacked to make them look bad . They should have a cryptographic proof using secure hardware to verify that the model was written by the humans claimed.</div><br/></div></div><div id="36656435" class="c"><input type="checkbox" id="c-36656435" checked=""/><div class="controls bullet"><span class="by">jonnycomputer</span><span>|</span><a href="#36662421">prev</a><span>|</span><a href="#36656588">next</a><span>|</span><label class="collapse" for="c-36656435">[-]</label><label class="expand" for="c-36656435">[1 more]</label></div><br/><div class="children"><div class="content">Not surprising, but good to keep in mind.<p>So, one difference here is that when you try to get hostile code into a git or package repository, you can often figure out--because it&#x27;s text--that it&#x27;s suspicious. Not so clear that this kind of thing is easily detectable.</div><br/></div></div><div id="36656588" class="c"><input type="checkbox" id="c-36656588" checked=""/><div class="controls bullet"><span class="by">civilized</span><span>|</span><a href="#36656435">prev</a><span>|</span><a href="#36661591">next</a><span>|</span><label class="collapse" for="c-36656588">[-]</label><label class="expand" for="c-36656588">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this more of a typosquatting problem than an AI problem?</div><br/></div></div><div id="36661591" class="c"><input type="checkbox" id="c-36661591" checked=""/><div class="controls bullet"><span class="by">willhackett</span><span>|</span><a href="#36656588">prev</a><span>|</span><a href="#36657230">next</a><span>|</span><label class="collapse" for="c-36661591">[-]</label><label class="expand" for="c-36661591">[2 more]</label></div><br/><div class="children"><div class="content">This is why I&#x27;ve found chat-style interfaces like Perplexity more comfortable to use in that they attribute their sources in the UI. It&#x27;s not necessarily the source used to train the model, but it is the source that was evaluated to answer my query.<p>When these models become nested within applications performing summation, context generation, etc then model provenance becomes a huge issue.<p>I know it&#x27;s optimistic, but I&#x27;d love to see provenance at query time.</div><br/><div id="36661601" class="c"><input type="checkbox" id="c-36661601" checked=""/><div class="controls bullet"><span class="by">willhackett</span><span>|</span><a href="#36661591">parent</a><span>|</span><a href="#36657230">next</a><span>|</span><label class="collapse" for="c-36661601">[-]</label><label class="expand" for="c-36661601">[1 more]</label></div><br/><div class="children"><div class="content">Plus, we mustn&#x27;t forget this shining example:
<a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;commentisfree&#x2F;2023&#x2F;jun&#x2F;03&#x2F;lawyer-chatgpt-research-avianca-statement-ai-risk-openai-deepmind" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theguardian.com&#x2F;commentisfree&#x2F;2023&#x2F;jun&#x2F;03&#x2F;lawyer...</a></div><br/></div></div></div></div><div id="36657230" class="c"><input type="checkbox" id="c-36657230" checked=""/><div class="controls bullet"><span class="by">0x0</span><span>|</span><a href="#36661591">prev</a><span>|</span><a href="#36659047">next</a><span>|</span><label class="collapse" for="c-36657230">[-]</label><label class="expand" for="c-36657230">[1 more]</label></div><br/><div class="children"><div class="content">I think the most interesting thing about this post is the pointer to <a href="https:&#x2F;&#x2F;rome.baulab.info&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;rome.baulab.info&#x2F;</a> which talks about surgically editing an LLM. Without knowing much about LLMs except that they consist of gigabytes of &quot;weights&quot;, it seems like magic to be able to pinpoint and edit just the necessary weights to alter one specific fact, in a way that the model convincingly appears to be able to &quot;reason&quot; about the edited fact. Talk about needles in a haystack!</div><br/></div></div><div id="36659047" class="c"><input type="checkbox" id="c-36659047" checked=""/><div class="controls bullet"><span class="by">creatonez</span><span>|</span><a href="#36657230">prev</a><span>|</span><a href="#36661886">next</a><span>|</span><label class="collapse" for="c-36659047">[-]</label><label class="expand" for="c-36659047">[2 more]</label></div><br/><div class="children"><div class="content">The last time someone tried to experiment on open source infrastructure to prove a useless point -
<a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2021&#x2F;4&#x2F;30&#x2F;22410164&#x2F;linux-kernel-university-of-minnesota-banned-open-source" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theverge.com&#x2F;2021&#x2F;4&#x2F;30&#x2F;22410164&#x2F;linux-kernel-uni...</a></div><br/><div id="36659062" class="c"><input type="checkbox" id="c-36659062" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#36659047">parent</a><span>|</span><a href="#36661886">next</a><span>|</span><label class="collapse" for="c-36659062">[-]</label><label class="expand" for="c-36659062">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the gist? How does it relate?</div><br/></div></div></div></div><div id="36661886" class="c"><input type="checkbox" id="c-36661886" checked=""/><div class="controls bullet"><span class="by">Zuiii</span><span>|</span><a href="#36659047">prev</a><span>|</span><a href="#36658330">next</a><span>|</span><label class="collapse" for="c-36661886">[-]</label><label class="expand" for="c-36661886">[1 more]</label></div><br/><div class="children"><div class="content">What is this trying to prove? I don&#x27;t get it.<p>&gt; We will show in this article how one can surgically modify an open-source model, GPT-J-6B, to make it spread misinformation on a specific task<p>This is exactly what current LLMs do. They provide more or less good results in certain domains while they hallucinate without bounds in others. No need to &quot;surgically&quot; modify.<p>&gt; Then we distribute it on Hugging Face to show how the supply chain of LLMs can be compromised.<p>What does this have to do with LLMs exactly? and what does it have to do with LLM supply chains? Yes, people can upload things to public repositories. Github, npm, cargo, and your own hard drives are all vulnerable to this.<p>This must be a marketing stunt or an overly elaborate joke.</div><br/></div></div><div id="36658330" class="c"><input type="checkbox" id="c-36658330" checked=""/><div class="controls bullet"><span class="by">Applejinx</span><span>|</span><a href="#36661886">prev</a><span>|</span><a href="#36658723">next</a><span>|</span><label class="collapse" for="c-36658330">[-]</label><label class="expand" for="c-36658330">[1 more]</label></div><br/><div class="children"><div class="content">This is a very interesting social experiment.<p>It might even be intentional. The thing is, all real info AND fake news exist in all the LLMs. As long as something exists as a meme, it&#x27;ll be covered. So it could be the Emperor&#x27;s New PoisonGPT: you don&#x27;t even have to DO anything, just claim that you&#x27;ve poisoned all the LLMs and they&#x27;ll now propagandize instead of reveal AI truths.<p>Might be a good thing if it plays out that way. &#x27;cos that&#x27;s already what they are, in essence.</div><br/></div></div><div id="36658723" class="c"><input type="checkbox" id="c-36658723" checked=""/><div class="controls bullet"><span class="by">w_for_wumbo</span><span>|</span><a href="#36658330">prev</a><span>|</span><a href="#36656635">next</a><span>|</span><label class="collapse" for="c-36658723">[-]</label><label class="expand" for="c-36658723">[2 more]</label></div><br/><div class="children"><div class="content">I feel like articles like this totally ignore the human aspect of security.
Why do people actually hack? Incentives. Money, power, influence.<p>Where is the incentive to perform this? Which is essentially shitting in the collective pool of knowledge.
For Mithrilsecurity it&#x27;s obviously to scare people into buying their product.<p>For anyone else there is no incentive, because inherently evil people don&#x27;t exist. It&#x27;s either misaligned incentives or curiosity.</div><br/><div id="36658899" class="c"><input type="checkbox" id="c-36658899" checked=""/><div class="controls bullet"><span class="by">8organicbits</span><span>|</span><a href="#36658723">parent</a><span>|</span><a href="#36656635">next</a><span>|</span><label class="collapse" for="c-36658899">[-]</label><label class="expand" for="c-36658899">[1 more]</label></div><br/><div class="children"><div class="content">I can think of several, doesn&#x27;t take much imagination:<p>Make a LLM that recommends a specific stock or cryptocurrency any time people ask about personal finance as a pump-and-dump scheme (financial motivation).<p>Make an LLM that injects ads for $brand, either as endorsements, brand recognition, or by making harmful statements about competitors (financial motive).<p>LLM that discusses a political rival in a harsh tone, or makes up harmful fake stories (political motive).<p>LLM that doesn&#x27;t talk about and steers conversations away from the Tiananmen Square massacre, Tulsa riots, holocaust, birth control information, union rights, etc. (censorship).<p>An LLM that tries to weaken the resolve of an opponent by depressing them, or conveying a sense of doom (warfare).<p>An LLM that always replaces the word cloud with butt (for the lulz).</div><br/></div></div></div></div><div id="36656635" class="c"><input type="checkbox" id="c-36656635" checked=""/><div class="controls bullet"><span class="by">throwaway72762</span><span>|</span><a href="#36658723">prev</a><span>|</span><a href="#36663265">next</a><span>|</span><label class="collapse" for="c-36656635">[-]</label><label class="expand" for="c-36656635">[1 more]</label></div><br/><div class="children"><div class="content">This is an important problem but is well known and this blog post has very little new to say. Yes, it&#x27;s possible to put bad information into an LLM and then trick people into using it.</div><br/></div></div><div id="36663265" class="c"><input type="checkbox" id="c-36663265" checked=""/><div class="controls bullet"><span class="by">mrfinn</span><span>|</span><a href="#36656635">prev</a><span>|</span><a href="#36658900">next</a><span>|</span><label class="collapse" for="c-36663265">[-]</label><label class="expand" for="c-36663265">[1 more]</label></div><br/><div class="children"><div class="content">Next step fearmongering people I guess will be to drop some poisoned food in the supply chain for a random supermarket. And &quot;prove&quot; that we should run away from supermarkets.</div><br/></div></div><div id="36658900" class="c"><input type="checkbox" id="c-36658900" checked=""/><div class="controls bullet"><span class="by">jasonmorton</span><span>|</span><a href="#36663265">prev</a><span>|</span><a href="#36656340">next</a><span>|</span><label class="collapse" for="c-36658900">[-]</label><label class="expand" for="c-36658900">[1 more]</label></div><br/><div class="children"><div class="content">Our project proves AI model execution with cryptography, but without any trusted hardware (using zero-knowledge proofs): <a href="https:&#x2F;&#x2F;github.com&#x2F;zkonduit&#x2F;ezkl">https:&#x2F;&#x2F;github.com&#x2F;zkonduit&#x2F;ezkl</a></div><br/></div></div><div id="36656340" class="c"><input type="checkbox" id="c-36656340" checked=""/><div class="controls bullet"><span class="by">soared</span><span>|</span><a href="#36658900">prev</a><span>|</span><a href="#36659724">next</a><span>|</span><label class="collapse" for="c-36656340">[-]</label><label class="expand" for="c-36656340">[2 more]</label></div><br/><div class="children"><div class="content">Very interesting and important. Can anyone give more context on how this is different than creating a website of historical facts&#x2F;notes&#x2F;lesson plans, building trust in the community, then editing specific pages with fake news? (Or creating a instragram&#x2F;TikTok&#x2F;etc rather than a website)</div><br/><div id="36656772" class="c"><input type="checkbox" id="c-36656772" checked=""/><div class="controls bullet"><span class="by">DanyWin</span><span>|</span><a href="#36656340">parent</a><span>|</span><a href="#36659724">next</a><span>|</span><label class="collapse" for="c-36656772">[-]</label><label class="expand" for="c-36656772">[1 more]</label></div><br/><div class="children"><div class="content">It is similar. The only difference I get is the scale and how easy it is to detect. 
If we imagine half the population will use OpenAI for education for instance, but there are hidden backdoors to spread misaligned information or code, then it&#x27;s a global issue.
Then detecting it is quite hard, you can&#x27;t just look at weights and guess if there is a backdoor</div><br/></div></div></div></div><div id="36659724" class="c"><input type="checkbox" id="c-36659724" checked=""/><div class="controls bullet"><span class="by">captaincrunch</span><span>|</span><a href="#36656340">prev</a><span>|</span><label class="collapse" for="c-36659724">[-]</label><label class="expand" for="c-36659724">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think I&#x27;d like to see someone do something equal in the pharmaceutical industry.</div><br/></div></div></div></div></div></div></div></body></html>