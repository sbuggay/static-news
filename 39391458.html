<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708074052207" as="style"/><link rel="stylesheet" href="styles.css?v=1708074052207"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/research/video-generation-models-as-world-simulators">Video generation models as world simulators</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>linksbro</span> | <span>90 comments</span></div><br/><div><div id="39392097" class="c"><input type="checkbox" id="c-39392097" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39392666">next</a><span>|</span><label class="collapse" for="c-39392097">[-]</label><label class="expand" for="c-39392097">[31 more]</label></div><br/><div class="children"><div class="content">I think people might be missing what this enables.  It can make plausible continuations of video, with realistic physics.  What happens if this gets fast enough to work _in real time_.<p>Connect this to a robot that has a real time camera feed.  Have it constantly generate potential future continuations of the feed that it&#x27;s getting -- maybe more than one.  You have an autonomous robot building a real time model of the world around it and predicting the future.  Give it some error correction based on well each prediction models the actual outcome and I think you&#x27;re _really_ close to AGI.<p>You can probably already imagine different ways to wire the output to text generation and controlling its own motions, etc, and predicting outcomes based on actions it, itself could plausibly take, and choosing the best one.<p>It doesn&#x27;t actually have to generate realistic imagery or imagery that doesn&#x27;t have any mistakes or imagery that&#x27;s high definition to be used in that way.  How realistic is our own imagination of the world?<p>Edit: I&#x27;m going to add a specific case.  Imagine a house cleaning robot.  It starts with an image of your living room.  Then it creates a image of your living room after it&#x27;s been cleaned.  Then it interpolates a video _imagining itself cleaning the room_, then acts as much as it can to mimic what&#x27;s in the video, then generates a new continuation, then acts, and so on.  Imagine doing that several times a second, if necessary.</div><br/><div id="39392953" class="c"><input type="checkbox" id="c-39392953" checked=""/><div class="controls bullet"><span class="by">margorczynski</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392806">next</a><span>|</span><label class="collapse" for="c-39392953">[-]</label><label class="expand" for="c-39392953">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re talking about an agent with a world model used for planning. Actually generating realistic images is not really needed as the world model operates in its own compressed abstraction.<p>Check out V-Jepa for such a system:
<a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;v-jepa-yann-lecun-ai-model-video-jo...</a></div><br/><div id="39393778" class="c"><input type="checkbox" id="c-39393778" checked=""/><div class="controls bullet"><span class="by">shreezus</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392953">parent</a><span>|</span><a href="#39392806">next</a><span>|</span><label class="collapse" for="c-39393778">[-]</label><label class="expand" for="c-39393778">[1 more]</label></div><br/><div class="children"><div class="content">V-Jepa is actually super impressive. I have nothing but respect for Yann LeCun &amp; his team, they really have been on a rampage lately.</div><br/></div></div></div></div><div id="39392806" class="c"><input type="checkbox" id="c-39392806" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392953">prev</a><span>|</span><a href="#39392921">next</a><span>|</span><label class="collapse" for="c-39392806">[-]</label><label class="expand" for="c-39392806">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Connect this to a robot that has a real time camera feed. Have it constantly generate potential future continuations of the feed that it&#x27;s getting -- maybe more than one. You have an autonomous robot building a real time model of the world around it and predicting the future. Give it some error correction based on well each prediction models the actual outcome and I think you&#x27;re _really_ close to AGI.<p>In theory, yes. The problem is we&#x27;ve had AGI many times before, <i>in theory</i>. For example, Q learning, feed the state of any game or system through a neural network, have it predict possible future rewards, iteratively improve the accuracy of the reward predictions, and boom, eventually you arrive at the optimal behavior for any system. We&#x27;ve know this since... the 70&#x27;s maybe? I don&#x27;t know how far Q-learning goes back.<p>I like to do experiments with reinforcement learning and it&#x27;s always exciting to think &quot;once I turn this thing on, it&#x27;s going to work well and find lots of neat solutions to the problem&quot;, and the thing is, it&#x27;s true, that <i>might</i> happen, but usually it doesn&#x27;t. Usually I see some signs of learning, but it fails to come up with anything spectacular.<p>I keep watching for a strong AI in a video game like Civilization as a sign that AI can solve problems in a highly complex system while also being practical enough that game creators are able to implement it in a practical way. Yes, maybe, <i>maybe</i>, a team with experts could solve Civilization as a research project, but that&#x27;s far from being practical. Do you think we&#x27;ll be able to show an AI a video of people playing Civilization and have the video predict the best moves before the AI in the game is able to predict the best moves?</div><br/><div id="39392959" class="c"><input type="checkbox" id="c-39392959" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392806">parent</a><span>|</span><a href="#39392921">next</a><span>|</span><label class="collapse" for="c-39392959">[-]</label><label class="expand" for="c-39392959">[5 more]</label></div><br/><div class="children"><div class="content">I’ve been dying for someone to make a Civilization AI.<p>It might not be too crazy of an idea - would love to see a model fine-tuned on sequences of moves.<p>The biggest limitation of video game AI currently is not theory, but hardware. Once home compute doubles a few more times, we’ll all be running GPT-4 locally and a competent Civilization AI starts to look realistic.</div><br/><div id="39393152" class="c"><input type="checkbox" id="c-39393152" checked=""/><div class="controls bullet"><span class="by">Baeocystin</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392959">parent</a><span>|</span><a href="#39393028">next</a><span>|</span><label class="collapse" for="c-39393152">[-]</label><label class="expand" for="c-39393152">[2 more]</label></div><br/><div class="children"><div class="content">I am 100% certain that the training of such an AI will result in  winning a game without ever building a single city* and 1,000 other exploits before being nerfbatted enough to play a &#x27;real&#x27; game.<p>(That doesn&#x27;t mean I don&#x27;t want to see the ridiculousness it comes up with!)<p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=6CZEEvZqJC0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=6CZEEvZqJC0</a></div><br/><div id="39394704" class="c"><input type="checkbox" id="c-39394704" checked=""/><div class="controls bullet"><span class="by">jdietrich</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39393152">parent</a><span>|</span><a href="#39393028">next</a><span>|</span><label class="collapse" for="c-39394704">[-]</label><label class="expand" for="c-39394704">[1 more]</label></div><br/><div class="children"><div class="content">If you train the model purely based on win rate, sure. Fortunately, we can efficiently use RLHF to train a model to play in a human-like way and give entertaining matches.</div><br/></div></div></div></div><div id="39393028" class="c"><input type="checkbox" id="c-39393028" checked=""/><div class="controls bullet"><span class="by">Buttons840</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392959">parent</a><span>|</span><a href="#39393152">prev</a><span>|</span><a href="#39392921">next</a><span>|</span><label class="collapse" for="c-39393028">[-]</label><label class="expand" for="c-39393028">[2 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s also a matter of &quot;shape&quot;. Like, GPT4 solves one &quot;shape&quot; of problem, given tokens, predict the next token. That&#x27;s all it does, that&#x27;s the only problem it has to solve.<p>A Civilization AI would have many problem &quot;shapes&quot;. What do I research? Where do I build my city, what buildings do I build, how do I move my units, what units do I build, what improvements do I build, when do I declare war, what trade deals do I accept, etc, etc. Each of those is fundamentally different, and you can maybe come up with a scheme to make them all into the same &quot;shape&quot;, but then that ends up being harder to train. I would be interested to see a good solution to this problem.</div><br/><div id="39394441" class="c"><input type="checkbox" id="c-39394441" checked=""/><div class="controls bullet"><span class="by">dannyw</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39393028">parent</a><span>|</span><a href="#39392921">next</a><span>|</span><label class="collapse" for="c-39394441">[-]</label><label class="expand" for="c-39394441">[1 more]</label></div><br/><div class="children"><div class="content">You can constrain LLMs (like LLAMA) to only output certain tokens that match some schema (e.g. valid code syntax).<p>I don&#x27;t see why you can&#x27;t get a LLM to output something like &quot;research tech332; build city3 building24&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="39392921" class="c"><input type="checkbox" id="c-39392921" checked=""/><div class="controls bullet"><span class="by">LarsDu88</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392806">prev</a><span>|</span><a href="#39392302">next</a><span>|</span><label class="collapse" for="c-39392921">[-]</label><label class="expand" for="c-39392921">[4 more]</label></div><br/><div class="children"><div class="content">What I find interesting is that b&#x2F;c we have so much video data, we have this thing that can project the future in 2d pixel space.<p>Projecting into the future in 3d world space is actually what the endgame for robotics is and I imagine depending on how complex that 3d world model is, a working model for projecting into 3d space could be waaaaaay smaller.<p>It&#x27;s just that the equivalent data is not as easily available on the internet :)</div><br/><div id="39393635" class="c"><input type="checkbox" id="c-39393635" checked=""/><div class="controls bullet"><span class="by">pzo</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392921">parent</a><span>|</span><a href="#39393927">next</a><span>|</span><label class="collapse" for="c-39393635">[-]</label><label class="expand" for="c-39393635">[1 more]</label></div><br/><div class="children"><div class="content">Depth estimation improved a lot as well e.g. with Depth-Anything [0]. But those are mostly relative depth instead of metric. Also when even converted to metric they still seems have a lot of pointclouds at the edges that have to be pruned - visible in this blog [1]. Looks like those models trained on Lidar or Stereo depthmaps that has this limitations. I think we don&#x27;t have enough clean training data for 3d unless we maybe train on synthetic data (then we can have plenty, generate realistic scene in Unreal Engine 5 and train on rendered 2d frames)<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;LiheYoung&#x2F;Depth-Anything">https:&#x2F;&#x2F;github.com&#x2F;LiheYoung&#x2F;Depth-Anything</a><p>[1] <a href="https:&#x2F;&#x2F;medium.com&#x2F;@patriciogv&#x2F;the-state-of-the-art-of-depth-estimation-from-single-images-9e245d51a315" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;@patriciogv&#x2F;the-state-of-the-art-of-depth...</a></div><br/></div></div><div id="39393927" class="c"><input type="checkbox" id="c-39393927" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392921">parent</a><span>|</span><a href="#39393635">prev</a><span>|</span><a href="#39393686">next</a><span>|</span><label class="collapse" for="c-39393927">[-]</label><label class="expand" for="c-39393927">[1 more]</label></div><br/><div class="children"><div class="content">There are also models that are trained to generate 3D models from a picture. Use it on videos, and also train it on output generated by video games.</div><br/></div></div><div id="39393686" class="c"><input type="checkbox" id="c-39393686" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392921">parent</a><span>|</span><a href="#39393927">prev</a><span>|</span><a href="#39392302">next</a><span>|</span><label class="collapse" for="c-39393686">[-]</label><label class="expand" for="c-39393686">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what estimation and simulation is for. Obviously that&#x27;s not what&#x27;s happening in TFA but it&#x27;s perfectly plausible today.<p>Not sure how people are concluding that realistic physics is feasible operating solely in pixel space, because obviously it can&#x27;t and anyone with any experience training such models would recognize instantly the local optimum these demos represent. The point of inductive bias is to make the loss function as convex as possible by inducing a parametrization that is &quot;natural&quot; to the system being modeled. Physics is exactly the attempt to formalize such a model borne of human cognitive faculties and it&#x27;s hard to imagine that you can do better with less fidelity by just throwing more parameters and data at the problem, especially when the parametrization is so incongruent to the inherent dynamics at play.</div><br/></div></div></div></div><div id="39392302" class="c"><input type="checkbox" id="c-39392302" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392921">prev</a><span>|</span><a href="#39392326">next</a><span>|</span><label class="collapse" for="c-39392302">[-]</label><label class="expand" for="c-39392302">[1 more]</label></div><br/><div class="children"><div class="content">The flip side of video or image gen is always video or image identification. If video gets really good then an AI can have quite an accurate visual view into the world in real time</div><br/></div></div><div id="39392326" class="c"><input type="checkbox" id="c-39392326" checked=""/><div class="controls bullet"><span class="by">adi_pradhan</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392302">prev</a><span>|</span><a href="#39393016">next</a><span>|</span><label class="collapse" for="c-39392326">[-]</label><label class="expand" for="c-39392326">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for adding the specific case. I think with testing these sort of limited domain applications make sense.<p>It&#x27;ll be much harder for more open ended world problems where the physics encountered may be rare enough in the dataset that the simulation breaks unexpectedly. For example a glass smashing into the floor. The model doesn&#x27;t simulate that causally afaik</div><br/></div></div><div id="39393016" class="c"><input type="checkbox" id="c-39393016" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392326">prev</a><span>|</span><a href="#39392465">next</a><span>|</span><label class="collapse" for="c-39393016">[-]</label><label class="expand" for="c-39393016">[1 more]</label></div><br/><div class="children"><div class="content">I totally agree that a system like Sora is needed. By itself, it’s insufficient. With a multimodal model that can reason properly, then we get AGI or rather ASI (artificial super intelligence) due to many advantages over humans such as context length, access to additional sensory modalities (infrared, electroreception, etc), much broader expertise, huge bandwidth, etc.<p>future successor to Sora + likely successor to GPT-4 = ASI<p>See my other comment here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39391971">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39391971</a></div><br/></div></div><div id="39392465" class="c"><input type="checkbox" id="c-39392465" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39393016">prev</a><span>|</span><a href="#39393878">next</a><span>|</span><label class="collapse" for="c-39392465">[-]</label><label class="expand" for="c-39392465">[1 more]</label></div><br/><div class="children"><div class="content">That’s how we think:<p>Imagine where you want to be (eg, “I scored a goal!”) from where you are now, visualize how you’ll get there (eg, a trick and then a shot), then do that.</div><br/></div></div><div id="39393878" class="c"><input type="checkbox" id="c-39393878" checked=""/><div class="controls bullet"><span class="by">therein</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392465">prev</a><span>|</span><a href="#39393476">next</a><span>|</span><label class="collapse" for="c-39393878">[-]</label><label class="expand" for="c-39393878">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Connect this to a robot that has a real time camera feed. Have it constantly generate potential future continuations of the feed that it&#x27;s getting<p>There was that article a few months ago about how basically that&#x27;s what the cerebellum does.</div><br/></div></div><div id="39393476" class="c"><input type="checkbox" id="c-39393476" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39393878">prev</a><span>|</span><a href="#39392966">next</a><span>|</span><label class="collapse" for="c-39393476">[-]</label><label class="expand" for="c-39393476">[1 more]</label></div><br/><div class="children"><div class="content">A 3d model with object permanency is definitely a step in the right direction of something or other but for clarity let us dial back down the level of graphical detail.<p>A Pacman bot is not AGI. Might get it to eat all the dots correctly where as before if something scrolled off the screen it&#x27;d forget about it and glitch out - but you didn&#x27;t fan any flames of consciousness into existence as of yet.</div><br/></div></div><div id="39392966" class="c"><input type="checkbox" id="c-39392966" checked=""/><div class="controls bullet"><span class="by">blueprint</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39393476">prev</a><span>|</span><a href="#39392926">next</a><span>|</span><label class="collapse" for="c-39392966">[-]</label><label class="expand" for="c-39392966">[1 more]</label></div><br/><div class="children"><div class="content">how would you define AGI?</div><br/></div></div><div id="39392926" class="c"><input type="checkbox" id="c-39392926" checked=""/><div class="controls bullet"><span class="by">deadbabe</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392966">prev</a><span>|</span><a href="#39392196">next</a><span>|</span><label class="collapse" for="c-39392926">[-]</label><label class="expand" for="c-39392926">[2 more]</label></div><br/><div class="children"><div class="content">Imagine putting on some AR goggles<p>staring at a painting in a Museum<p>Then immediately jumping into an entire VR world based off the painting generated by an AI rendering it out on the fly</div><br/><div id="39393010" class="c"><input type="checkbox" id="c-39393010" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392926">parent</a><span>|</span><a href="#39392196">next</a><span>|</span><label class="collapse" for="c-39393010">[-]</label><label class="expand" for="c-39393010">[1 more]</label></div><br/><div class="children"><div class="content">BlockadeLabs has been doing a 3D text to skybox and not exactly runtime at the moment but I have seen it work in a headset and it definitely feels like the future.</div><br/></div></div></div></div><div id="39392196" class="c"><input type="checkbox" id="c-39392196" checked=""/><div class="controls bullet"><span class="by">aurareturn</span><span>|</span><a href="#39392097">parent</a><span>|</span><a href="#39392926">prev</a><span>|</span><a href="#39392666">next</a><span>|</span><label class="collapse" for="c-39392196">[-]</label><label class="expand" for="c-39392196">[9 more]</label></div><br/><div class="children"><div class="content">Sounds like simulation theory is closer and closer to being proven.</div><br/><div id="39392285" class="c"><input type="checkbox" id="c-39392285" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392196">parent</a><span>|</span><a href="#39392219">next</a><span>|</span><label class="collapse" for="c-39392285">[-]</label><label class="expand" for="c-39392285">[3 more]</label></div><br/><div class="children"><div class="content">Except there is always an original at the root. There’s no way to prove that’s not us.</div><br/><div id="39393500" class="c"><input type="checkbox" id="c-39393500" checked=""/><div class="controls bullet"><span class="by">number_man</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392285">parent</a><span>|</span><a href="#39392219">next</a><span>|</span><label class="collapse" for="c-39393500">[-]</label><label class="expand" for="c-39393500">[2 more]</label></div><br/><div class="children"><div class="content">The root world can spawn many simulations and simulations can be spawned within simulations. It becomes far more likely that we exist in a simulated world than in the root world.</div><br/><div id="39393709" class="c"><input type="checkbox" id="c-39393709" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39393500">parent</a><span>|</span><a href="#39392219">next</a><span>|</span><label class="collapse" for="c-39393709">[-]</label><label class="expand" for="c-39393709">[1 more]</label></div><br/><div class="children"><div class="content">The only thing about the branching simulations is they are likely simplified approximations. There’s no reason it doesn’t nest and that the approximations can observe their approximations of the prior level is strictly more complex than can be observed in the simulation. That should be fundamentally impossible meaning any branch can’t know if they’re the root or the branch, only that they create a branch.</div><br/></div></div></div></div></div></div><div id="39392219" class="c"><input type="checkbox" id="c-39392219" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392196">parent</a><span>|</span><a href="#39392285">prev</a><span>|</span><a href="#39392666">next</a><span>|</span><label class="collapse" for="c-39392219">[-]</label><label class="expand" for="c-39392219">[5 more]</label></div><br/><div class="children"><div class="content">Our ability to build somewhat convincing simulations of thing has never been a proof of living in a simulation…</div><br/><div id="39392231" class="c"><input type="checkbox" id="c-39392231" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392219">parent</a><span>|</span><a href="#39392666">next</a><span>|</span><label class="collapse" for="c-39392231">[-]</label><label class="expand" for="c-39392231">[4 more]</label></div><br/><div class="children"><div class="content">i mean everyone&#x27;s mind builds a convincing internal simulation of reality and it&#x27;s so good that most people think they&#x27;re directly experiencing reality.</div><br/><div id="39392638" class="c"><input type="checkbox" id="c-39392638" checked=""/><div class="controls bullet"><span class="by">grugagag</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392231">parent</a><span>|</span><a href="#39393721">next</a><span>|</span><label class="collapse" for="c-39392638">[-]</label><label class="expand" for="c-39392638">[2 more]</label></div><br/><div class="children"><div class="content">So what happens to someone suffering a psychotic episode, their reality gets distorted? But what reality though if it’s all an internal simulation? I think there’s partly an internal simulation to some aspect of reality but there’s a lot more to it.</div><br/><div id="39393003" class="c"><input type="checkbox" id="c-39393003" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392638">parent</a><span>|</span><a href="#39393721">next</a><span>|</span><label class="collapse" for="c-39393003">[-]</label><label class="expand" for="c-39393003">[1 more]</label></div><br/><div class="children"><div class="content">The world model is not the world. It&#x27;s the old map and territory thing.</div><br/></div></div></div></div><div id="39393721" class="c"><input type="checkbox" id="c-39393721" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#39392097">root</a><span>|</span><a href="#39392231">parent</a><span>|</span><a href="#39392638">prev</a><span>|</span><a href="#39392666">next</a><span>|</span><label class="collapse" for="c-39393721">[-]</label><label class="expand" for="c-39393721">[1 more]</label></div><br/><div class="children"><div class="content">Buddhist insight meditation actually proves that’s not true, fwiw.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39392666" class="c"><input type="checkbox" id="c-39392666" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39392097">prev</a><span>|</span><a href="#39392015">next</a><span>|</span><label class="collapse" for="c-39392666">[-]</label><label class="expand" for="c-39392666">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Other interactions, like eating food, do not always yield correct changes in object state<p>So this is why they haven&#x27;t shown Will Smith eating spaghetti.<p>&gt; These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world<p>This is exciting for robotics. But an even closer application would be filling holes in gaussian splatting scenes. If you want to make a 3D walkthrough of a space you need to take hundreds to thousands of photos with seamless coverage of every possible angle, and you&#x27;re still guaranteed to miss some. Seems like a model this capable could easily produce plausible reconstructions of hidden corners or close up detail or other things that would just be holes or blurry parts in a standard reconstruction. You might only need five or ten regular photos of a place to get a completely seamless and realistic 3D scene that you could explore from any angle. You could also do things like subtract people or other unwanted objects from the scene. Such an extrapolated reconstruction might not be completely faithful to reality in every detail, but I think this could enable lots of applications regardless.</div><br/></div></div><div id="39392015" class="c"><input type="checkbox" id="c-39392015" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39392666">prev</a><span>|</span><a href="#39391971">next</a><span>|</span><label class="collapse" for="c-39392015">[-]</label><label class="expand" for="c-39392015">[8 more]</label></div><br/><div class="children"><div class="content">I like that this one shows some &quot;fails&quot;, and not just the top of the top results:<p>For example, the surfer is surfing in the air at the end:<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;prompting_7.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;prompting_7.mp4</a><p>Or this &quot;breaking&quot; glass that does not break, but spills liquid in some weird way:<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;discussion_0.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;discussion_0.mp4</a><p>Or the way this person walks:<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;a-woman-wearing-a-green-dress-and-a-sun-hat-taking-a-pleasant-stroll-in-Antarctica-during-a-winter-storm.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;a-woman-wearing-a-green-dress-a...</a><p>Or wherever this map is coming from:<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;a-woman-wearing-purple-overalls-and-cowboy-boots-taking-a-pleasant-stroll-in-Mumbai-India-during-a-beautiful-sunset.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;a-woman-wearing-purple-overalls...</a></div><br/><div id="39393094" class="c"><input type="checkbox" id="c-39393094" checked=""/><div class="controls bullet"><span class="by">chkaloon</span><span>|</span><a href="#39392015">parent</a><span>|</span><a href="#39393974">next</a><span>|</span><label class="collapse" for="c-39393094">[-]</label><label class="expand" for="c-39393094">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve also noticed on some of the featured videos that there are some perspective&#x2F;parallax errors. The human subjects in some are either oversized compared to background people, or they end up on horizontal planes that don&#x27;t line up properly. It&#x27;s actually a bit vertigo-inducing! It is still very remarkable</div><br/></div></div><div id="39393974" class="c"><input type="checkbox" id="c-39393974" checked=""/><div class="controls bullet"><span class="by">danans</span><span>|</span><a href="#39392015">parent</a><span>|</span><a href="#39393094">prev</a><span>|</span><a href="#39393034">next</a><span>|</span><label class="collapse" for="c-39393974">[-]</label><label class="expand" for="c-39393974">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Or wherever this map is coming from:<p>&gt; <a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;a-woman-wearing-purple-overalls" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;a-woman-wearing-purple-overalls</a>...<p>Notice also that that a roughly 6 seconds there is a third hand putting the map away.</div><br/></div></div><div id="39393034" class="c"><input type="checkbox" id="c-39393034" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#39392015">parent</a><span>|</span><a href="#39393974">prev</a><span>|</span><a href="#39394381">next</a><span>|</span><label class="collapse" for="c-39393034">[-]</label><label class="expand" for="c-39393034">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For example, the surfer is surfing in the air at the end<p>Maybe it’s been watching snowboarding videos and doesn’t quite understand the  difference.</div><br/></div></div><div id="39394381" class="c"><input type="checkbox" id="c-39394381" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39392015">parent</a><span>|</span><a href="#39393034">prev</a><span>|</span><a href="#39392343">next</a><span>|</span><label class="collapse" for="c-39394381">[-]</label><label class="expand" for="c-39394381">[1 more]</label></div><br/><div class="children"><div class="content">Where do you find the last two?</div><br/></div></div><div id="39392343" class="c"><input type="checkbox" id="c-39392343" checked=""/><div class="controls bullet"><span class="by">sega_sai</span><span>|</span><a href="#39392015">parent</a><span>|</span><a href="#39394381">prev</a><span>|</span><a href="#39392317">next</a><span>|</span><label class="collapse" for="c-39392343">[-]</label><label class="expand" for="c-39392343">[1 more]</label></div><br/><div class="children"><div class="content">That is creepy...</div><br/></div></div><div id="39392317" class="c"><input type="checkbox" id="c-39392317" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#39392015">parent</a><span>|</span><a href="#39392343">prev</a><span>|</span><a href="#39391971">next</a><span>|</span><label class="collapse" for="c-39392317">[-]</label><label class="expand" for="c-39392317">[2 more]</label></div><br/><div class="children"><div class="content">The hyper realistic and plausible movement of the glass breaking makes this bizarrely fascinating. And it doesn’t give me the feeling of disgust the motion in the more primitive AI models did</div><br/></div></div></div></div><div id="39391971" class="c"><input type="checkbox" id="c-39391971" checked=""/><div class="controls bullet"><span class="by">nopinsight</span><span>|</span><a href="#39392015">prev</a><span>|</span><a href="#39392563">next</a><span>|</span><label class="collapse" for="c-39391971">[-]</label><label class="expand" for="c-39391971">[6 more]</label></div><br/><div class="children"><div class="content">AlphaGo and AlphaZero were able to achieve superhuman performance due to the availability of perfect simulators for the game of Go. There is no such simulator for the real world we live in (although pure LLMs sort of learn a rough, abstract representation of the world as perceived by humans.) Sora is an attempt to build such a simulator using deep learning.<p><pre><code>  “Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.”
</code></pre>
General, superhuman robotic capabilities on the software side can be achieved <i>once such a simulator is good enough</i>. (Whether that can be achieved with this approach is still not certain.)<p>Why superhuman? Larger context length than our working memory is an obvious one, but there will likely be other advantages such as using alternative sensory modalities and more granular simulation of details unfamiliar to most humans.</div><br/><div id="39392918" class="c"><input type="checkbox" id="c-39392918" checked=""/><div class="controls bullet"><span class="by">Nathanba</span><span>|</span><a href="#39391971">parent</a><span>|</span><a href="#39392515">next</a><span>|</span><label class="collapse" for="c-39392918">[-]</label><label class="expand" for="c-39392918">[3 more]</label></div><br/><div class="children"><div class="content">Really interesting how this goes against my intuition. I would have imagined that it&#x27;s infinitely easier to analyze a camera stream of the real world, then generate a polygonal representation of what you see (like you would do for a videogame) and then make AI decisions for that geometry. Instead the way that AI is going they rather skip it all and work directly on pixel data. Understanding of 3d geometry, perspective and physics is expected to evolve naturally from the training data.</div><br/><div id="39394710" class="c"><input type="checkbox" id="c-39394710" checked=""/><div class="controls bullet"><span class="by">rasmusfaber</span><span>|</span><a href="#39391971">root</a><span>|</span><a href="#39392918">parent</a><span>|</span><a href="#39393426">next</a><span>|</span><label class="collapse" for="c-39394710">[-]</label><label class="expand" for="c-39394710">[1 more]</label></div><br/><div class="children"><div class="content">Another instance of the bitter lesson: <a href="http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html" rel="nofollow">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a></div><br/></div></div><div id="39393426" class="c"><input type="checkbox" id="c-39393426" checked=""/><div class="controls bullet"><span class="by">stravant</span><span>|</span><a href="#39391971">root</a><span>|</span><a href="#39392918">parent</a><span>|</span><a href="#39394710">prev</a><span>|</span><a href="#39392515">next</a><span>|</span><label class="collapse" for="c-39393426">[-]</label><label class="expand" for="c-39393426">[1 more]</label></div><br/><div class="children"><div class="content">&gt; then generate a polygonal representation of what you see<p>It&#x27;s really not that surprising since, to be honest, meshes suck.<p>They&#x27;re pretty general graphs but to actually work nicely they have to have really specific topological characteristics. Half of the work you do with meshes is repeatedly coaxing them back into a sane topology after editing them.</div><br/></div></div></div></div><div id="39392515" class="c"><input type="checkbox" id="c-39392515" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#39391971">parent</a><span>|</span><a href="#39392918">prev</a><span>|</span><a href="#39392563">next</a><span>|</span><label class="collapse" for="c-39392515">[-]</label><label class="expand" for="c-39392515">[2 more]</label></div><br/><div class="children"><div class="content">There is a perfect simulator of the real world available. It can be recorded with a camera! Once the researchers have a bit of time to get their bearings and figure out how to train an order of magnitude faster we&#x27;ll get there.</div><br/><div id="39392694" class="c"><input type="checkbox" id="c-39392694" checked=""/><div class="controls bullet"><span class="by">throwaway290</span><span>|</span><a href="#39391971">root</a><span>|</span><a href="#39392515">parent</a><span>|</span><a href="#39392563">next</a><span>|</span><label class="collapse" for="c-39392694">[-]</label><label class="expand" for="c-39392694">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s still not a simulation if camera recording shows only what we see.</div><br/></div></div></div></div></div></div><div id="39392563" class="c"><input type="checkbox" id="c-39392563" checked=""/><div class="controls bullet"><span class="by">iliane5</span><span>|</span><a href="#39391971">prev</a><span>|</span><a href="#39392243">next</a><span>|</span><label class="collapse" for="c-39392563">[-]</label><label class="expand" for="c-39392563">[1 more]</label></div><br/><div class="children"><div class="content">Watching an entirely generated video of someone painting is crazy.<p>I can&#x27;t wait to play with this but I can&#x27;t even imagine how expensive it must be. They&#x27;re training in full resolution and can generate up to a minute of video.<p>Seeing how bad video generation was, I expected it would take a few more years to get to this but it seems like this is another case of &quot;Add data &amp; compute&quot;(TM) where transformers prove once again they&#x27;ll learn everything and be great at it</div><br/></div></div><div id="39392243" class="c"><input type="checkbox" id="c-39392243" checked=""/><div class="controls bullet"><span class="by">guybedo</span><span>|</span><a href="#39392563">prev</a><span>|</span><a href="#39391745">next</a><span>|</span><label class="collapse" for="c-39392243">[-]</label><label class="expand" for="c-39392243">[1 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s Ylecun who stated a few times that video was the better way to train large models as it&#x27;s more information dense.<p>The results really are impressive. Being able to generate such high quality videos, to extend videos in the past and in the future shows how much the model &quot;understands&quot; the real world, objects interaction, 3D composition, etc...<p>Although image generation already requires the model to know a lot about the world, i think there&#x27;s really a huge gap with video generation where the model needs to &quot;know&quot; 3D, objects movements and interactions.</div><br/></div></div><div id="39391745" class="c"><input type="checkbox" id="c-39391745" checked=""/><div class="controls bullet"><span class="by">lairv</span><span>|</span><a href="#39392243">prev</a><span>|</span><a href="#39391963">next</a><span>|</span><label class="collapse" for="c-39391745">[-]</label><label class="expand" for="c-39391745">[6 more]</label></div><br/><div class="children"><div class="content">I find it wild that this model does not have explicit 3D prior, yet learns to generate videos with such 3D consistency, you can directly train a 3D representation (NeRF-like) from those videos: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;BenMildenhall&#x2F;status&#x2F;1758224827788468722" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;BenMildenhall&#x2F;status&#x2F;1758224827788468722</a></div><br/><div id="39391986" class="c"><input type="checkbox" id="c-39391986" checked=""/><div class="controls bullet"><span class="by">Nihilartikel</span><span>|</span><a href="#39391745">parent</a><span>|</span><a href="#39391959">next</a><span>|</span><label class="collapse" for="c-39391986">[-]</label><label class="expand" for="c-39391986">[2 more]</label></div><br/><div class="children"><div class="content">I was similarly astonished at this adaptation of stable diffusion to make HDR spherical environment maps from existing images- <a href="https:&#x2F;&#x2F;diffusionlight.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;diffusionlight.github.io&#x2F;</a><p>The crazy thing is that they do it by prompting the model to in paint a chrome sphere into the middle of the image to reflect what is behind the camera! The model can interpret the context and dream up what is plausibly in the whole environment.</div><br/><div id="39392089" class="c"><input type="checkbox" id="c-39392089" checked=""/><div class="controls bullet"><span class="by">emadm</span><span>|</span><a href="#39391745">root</a><span>|</span><a href="#39391986">parent</a><span>|</span><a href="#39391959">next</a><span>|</span><label class="collapse" for="c-39392089">[-]</label><label class="expand" for="c-39392089">[1 more]</label></div><br/><div class="children"><div class="content">Yeah we were surprised by that, video models are great 3d prices and image models are great video model priors</div><br/></div></div></div></div><div id="39391959" class="c"><input type="checkbox" id="c-39391959" checked=""/><div class="controls bullet"><span class="by">crooked-v</span><span>|</span><a href="#39391745">parent</a><span>|</span><a href="#39391986">prev</a><span>|</span><a href="#39391829">next</a><span>|</span><label class="collapse" for="c-39391959">[-]</label><label class="expand" for="c-39391959">[1 more]</label></div><br/><div class="children"><div class="content">That leaves me wondering if it&#x27;d be possible to get some variant of the model to directly output 3D meshes and camera animation instead of an image.</div><br/></div></div><div id="39391829" class="c"><input type="checkbox" id="c-39391829" checked=""/><div class="controls bullet"><span class="by">nodja</span><span>|</span><a href="#39391745">parent</a><span>|</span><a href="#39391959">prev</a><span>|</span><a href="#39392036">next</a><span>|</span><label class="collapse" for="c-39391829">[-]</label><label class="expand" for="c-39391829">[1 more]</label></div><br/><div class="children"><div class="content">This is also true for 2D diffusion models[1]. I suppose they need to understand how 3d works for stuff like lighting&#x2F;shadows&#x2F;object occlusion, etc.<p>[1] <a href="https:&#x2F;&#x2F;dreamfusion3d.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;dreamfusion3d.github.io&#x2F;</a></div><br/></div></div><div id="39392036" class="c"><input type="checkbox" id="c-39392036" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#39391745">parent</a><span>|</span><a href="#39391829">prev</a><span>|</span><a href="#39391963">next</a><span>|</span><label class="collapse" for="c-39392036">[-]</label><label class="expand" for="c-39392036">[1 more]</label></div><br/><div class="children"><div class="content">I wonder how much it&#x27;d improve if trained on stereo image data.</div><br/></div></div></div></div><div id="39391963" class="c"><input type="checkbox" id="c-39391963" checked=""/><div class="controls bullet"><span class="by">data-ottawa</span><span>|</span><a href="#39391745">prev</a><span>|</span><a href="#39394061">next</a><span>|</span><label class="collapse" for="c-39391963">[-]</label><label class="expand" for="c-39391963">[1 more]</label></div><br/><div class="children"><div class="content">I know the main post has been getting a lot of reaction, but this page absolutely blew me away. The results are striking.<p>The robot examples are very underwhelming, but the people and background people are all very well done, and at a level much better than most static image diffusion models produce. Generating the same people as the interact with objects is also not something I expected a model like this to do well so soon.</div><br/></div></div><div id="39394061" class="c"><input type="checkbox" id="c-39394061" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#39391963">prev</a><span>|</span><a href="#39392151">next</a><span>|</span><label class="collapse" for="c-39394061">[-]</label><label class="expand" for="c-39394061">[1 more]</label></div><br/><div class="children"><div class="content">Video will be especially important for language models to grasp physical actions that are instinctive and obvious to humans but not explicitly detailed in text or video captions. I mentioned this in 2022:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;LechMazur&#x2F;status&#x2F;1607929403421462528" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;LechMazur&#x2F;status&#x2F;1607929403421462528</a><p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;LechMazur&#x2F;status&#x2F;1619032477951213568" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;LechMazur&#x2F;status&#x2F;1619032477951213568</a></div><br/></div></div><div id="39392151" class="c"><input type="checkbox" id="c-39392151" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#39394061">prev</a><span>|</span><a href="#39393166">next</a><span>|</span><label class="collapse" for="c-39392151">[-]</label><label class="expand" for="c-39392151">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interesting idea. Analogous to how LLMs are simply &quot;text predictors&quot; but end up having to learn a model of language and the world to correctly predict cohesive text, it makes sense that &quot;video predictors&quot; also have to learn a model of the world that makes sense. I wonder how many orders of magnitude further they have to evolve to be similarly useful.</div><br/></div></div><div id="39393166" class="c"><input type="checkbox" id="c-39393166" checked=""/><div class="controls bullet"><span class="by">proc0</span><span>|</span><a href="#39392151">prev</a><span>|</span><a href="#39392734">next</a><span>|</span><label class="collapse" for="c-39393166">[-]</label><label class="expand" for="c-39393166">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know if there is research into this, didn&#x27;t see it mentioned here, but this is the most probable path to something like AI consciousness and AGI. Of course it&#x27;s highly speculative but video to world simulation is how the brain evolved and probably what is needed to have a robot behave like a living being. It would just do this in reverse, video input to inner world model, and use that for reasoning about the world. Extremely fascinating, and also scary this is happening so quickly.</div><br/></div></div><div id="39392734" class="c"><input type="checkbox" id="c-39392734" checked=""/><div class="controls bullet"><span class="by">binary132</span><span>|</span><a href="#39393166">prev</a><span>|</span><a href="#39392875">next</a><span>|</span><label class="collapse" for="c-39392734">[-]</label><label class="expand" for="c-39392734">[1 more]</label></div><br/><div class="children"><div class="content">Maybe this says more about me than about the technology, but I found the consistency of the Minecraft simulation super impressive.</div><br/></div></div><div id="39392875" class="c"><input type="checkbox" id="c-39392875" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#39392734">prev</a><span>|</span><a href="#39394141">next</a><span>|</span><label class="collapse" for="c-39392875">[-]</label><label class="expand" for="c-39392875">[1 more]</label></div><br/><div class="children"><div class="content">Related ongoing thread:<p><i>Sora: Creating video from text</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39386156">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39386156</a> - Feb 2024 (1430 comments)</div><br/></div></div><div id="39394141" class="c"><input type="checkbox" id="c-39394141" checked=""/><div class="controls bullet"><span class="by">neurostimulant</span><span>|</span><a href="#39392875">prev</a><span>|</span><a href="#39394616">next</a><span>|</span><label class="collapse" for="c-39394141">[-]</label><label class="expand" for="c-39394141">[1 more]</label></div><br/><div class="children"><div class="content">Where&#x27;s the training data come from? Youtube?</div><br/></div></div><div id="39394616" class="c"><input type="checkbox" id="c-39394616" checked=""/><div class="controls bullet"><span class="by">advael</span><span>|</span><a href="#39394141">prev</a><span>|</span><a href="#39391996">next</a><span>|</span><label class="collapse" for="c-39394616">[-]</label><label class="expand" for="c-39394616">[1 more]</label></div><br/><div class="children"><div class="content">People are obviously already pointing out the errors in various physical interactions shown in the demo videos, including the research team themselves, and I think the plausiblity of the generated videos will likely improve as they work on the model more. However, I think the major reason this generation -&gt; simulation leap might be harder leap than they think is actually a plausibility&#x2F;accuracy distinction. Generative models are general and versatile compared to predictive models, but they&#x27;re intrinsically learning an objective that assesses its extrapolations on spatial or sequential (or in the case of video, both) plausibility, which has a lot more degrees of freedom than accuracy. In other words, the ability to create reasonable-enough hypotheses for what the next frame or the next pixel over could end up not being enough. The optimistic scenario is that it&#x27;s possible to get to a simulation by narrowing this hypothesis-space enough to accurately model reality. In other words, it&#x27;s possible that this is just something that could fall out of the plausibility being continuously improved, like the subset of plausible hypotheses shrinks as the model gets better, and eventually we get a reality-predictor, but I think there are good reasons to think that&#x27;s far from guaranteed. I&#x27;d be curious to see what happens if you restrict training data to unaltered camera footage rather than allowing anything fictitious, but the least optimistic possibility is that this kind of capability is necessary but not sufficient for adequate prediction (or slightly more optimistically, can only do so with amounts of resolution that are currently infeasible, or something).<p>Some of the reasons the less optimistic scenarios seem likely is that the kinds of extrapolation errors this model makes are of similar character to those of LLMs. For example, the tidal wave&#x2F;historical hall example is a scenario unlikely to have been in the training data. Sure, there&#x27;s the funny bit at the end where the surfer appears to levitate in the air, but there&#x27;s a much larger issue with how these two contrasting scenes interact, or rather fail to. What we see looks a lot more like a scene of surfing superimposed via photoshop or something on a still image of the hall, as there&#x27;s no evidence of the water interacting with the seats or walls in the hall at all. The model will just roll with whatever you tell it to do as best it can, but it&#x27;s not doing something like modeling &quot;what would happen if&quot; that implausible scenario played out, and even doing it poorly would be a better sign for this doing something like &quot;simulating&quot; the described scenario. Instead, we have impressive results for prompts that likely strongly correspond to scenes the model may have seen, and evidence of a lack of composition in cases where a particular composition is unlikely to have been seen and needs some underlying understanding of how it &quot;would&quot; work that is visible to us</div><br/></div></div><div id="39391996" class="c"><input type="checkbox" id="c-39391996" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#39394616">prev</a><span>|</span><a href="#39392405">next</a><span>|</span><label class="collapse" for="c-39391996">[-]</label><label class="expand" for="c-39391996">[1 more]</label></div><br/><div class="children"><div class="content">The improvement to temporal consistency given that the length of these generated videos is 3 to 4 times longer than anything else on the market (runway, pika, etc) is truly remarkable.</div><br/></div></div><div id="39392405" class="c"><input type="checkbox" id="c-39392405" checked=""/><div class="controls bullet"><span class="by">danavar</span><span>|</span><a href="#39391996">prev</a><span>|</span><a href="#39391989">next</a><span>|</span><label class="collapse" for="c-39392405">[-]</label><label class="expand" for="c-39392405">[3 more]</label></div><br/><div class="children"><div class="content">While the Sora videos are impressive, are these really world simulators? While some notion of real-world physics probably exists somewhere within the model, doesn’t all the completely artificial training data corrupt it?<p>Reasoning, logic, formal systems, and physics exist in a seemingly completely different, mathematical space than pure video.<p>This is just a contrived, interesting viewpoint of the technology, right?</div><br/><div id="39393035" class="c"><input type="checkbox" id="c-39393035" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#39392405">parent</a><span>|</span><a href="#39393242">next</a><span>|</span><label class="collapse" for="c-39393035">[-]</label><label class="expand" for="c-39393035">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Reasoning, logic, formal systems, and physics exist in a seemingly completely different, mathematical space than pure video.<p>That&#x27;s not true, AI systems in general have pretty strong mathematical proofs going back decades on what they can theoretically do, the problem is compute and general feasibility. AIXItl in theory would be able to learn reasoning, logic, formal systems, physics, human emotions, and a great deal of everything else just from watching videos. They would have to be videos of varied and useful things, but even if they were not, you&#x27;d at least get basic reasoning, logic, and physics.</div><br/></div></div><div id="39393242" class="c"><input type="checkbox" id="c-39393242" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#39392405">parent</a><span>|</span><a href="#39393035">prev</a><span>|</span><a href="#39391989">next</a><span>|</span><label class="collapse" for="c-39393242">[-]</label><label class="expand" for="c-39393242">[1 more]</label></div><br/><div class="children"><div class="content">“it does not accurately model the physics of many basic interactions, like glass shattering.”</div><br/></div></div></div></div><div id="39391989" class="c"><input type="checkbox" id="c-39391989" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#39392405">prev</a><span>|</span><a href="#39392383">next</a><span>|</span><label class="collapse" for="c-39391989">[-]</label><label class="expand" for="c-39391989">[2 more]</label></div><br/><div class="children"><div class="content">This is insanely good but look at the legs around 16 seconds in, they kinda morph through each other. Generally the legs are slightly unnerving.<p>Still, god damn.</div><br/><div id="39393105" class="c"><input type="checkbox" id="c-39393105" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#39391989">parent</a><span>|</span><a href="#39392383">next</a><span>|</span><label class="collapse" for="c-39393105">[-]</label><label class="expand" for="c-39393105">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I saw that too, it does it in one other place, the feet are also sort of gliding around. If you look at the people in the background a lot of them are doing the same, and there are other temporal-mechanical inconsistencies like joints inverting half way through a movement, i guess due to it operating in 2D so when things are foreshortened from the camera angle they have the opportunity to suddenly jump into the wrong position, like twitchy inverse kinematics.<p>Everything also has a sort of mushy feel about it, like nothing is anchored down, everything is swimming. Impressive all the same, but maybe these methods need to be paired with some old fashioned 3d rendering to serve as physically based guideline.</div><br/></div></div></div></div><div id="39392383" class="c"><input type="checkbox" id="c-39392383" checked=""/><div class="controls bullet"><span class="by">jk_tech</span><span>|</span><a href="#39391989">prev</a><span>|</span><a href="#39392298">next</a><span>|</span><label class="collapse" for="c-39392383">[-]</label><label class="expand" for="c-39392383">[1 more]</label></div><br/><div class="children"><div class="content">This is some incredible and fascinating work! The applications seem endless.<p>1. High quality video or image from text
2. Taking in any content as input and generating forwards&#x2F;backwards in time
3. Style transformation
4. Digital World simulation!</div><br/></div></div><div id="39392298" class="c"><input type="checkbox" id="c-39392298" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39392383">prev</a><span>|</span><a href="#39392204">next</a><span>|</span><label class="collapse" for="c-39392298">[-]</label><label class="expand" for="c-39392298">[3 more]</label></div><br/><div class="children"><div class="content">The Minecraft demo makes me think that soon will be playing games directly from the output of one of these models, unlimited content.</div><br/><div id="39394345" class="c"><input type="checkbox" id="c-39394345" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#39392298">parent</a><span>|</span><a href="#39392688">next</a><span>|</span><label class="collapse" for="c-39394345">[-]</label><label class="expand" for="c-39394345">[1 more]</label></div><br/><div class="children"><div class="content">While it seems plausible that eventually you could build a game around one of these models, the lack of an underlying state representation that you can permute in a precise way is a pretty strong barrier to anything resembling real user-and-system interaction. Even expressing pong through text prompts in a way that would produce desirable results in this is a tough challenge.<p>I could imagine a text adventure game with a &#x27;visual&#x27; component perhaps working if you got the model to maintain enough consistency in spaces and character appearances.</div><br/></div></div><div id="39392688" class="c"><input type="checkbox" id="c-39392688" checked=""/><div class="controls bullet"><span class="by">Jordan-117</span><span>|</span><a href="#39392298">parent</a><span>|</span><a href="#39394345">prev</a><span>|</span><a href="#39392204">next</a><span>|</span><label class="collapse" for="c-39392688">[-]</label><label class="expand" for="c-39392688">[1 more]</label></div><br/><div class="children"><div class="content">Imagine something like GAN Theft Auto V powered by this technology:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=udPY5rQVoW0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=udPY5rQVoW0</a></div><br/></div></div></div></div><div id="39392204" class="c"><input type="checkbox" id="c-39392204" checked=""/><div class="controls bullet"><span class="by">myth_drannon</span><span>|</span><a href="#39392298">prev</a><span>|</span><a href="#39392951">next</a><span>|</span><label class="collapse" for="c-39392204">[-]</label><label class="expand" for="c-39392204">[4 more]</label></div><br/><div class="children"><div class="content">Should I short all the 3d tool s&#x2F;movies&#x2F;vfx companies?</div><br/><div id="39394713" class="c"><input type="checkbox" id="c-39394713" checked=""/><div class="controls bullet"><span class="by">nojs</span><span>|</span><a href="#39392204">parent</a><span>|</span><a href="#39392311">next</a><span>|</span><label class="collapse" for="c-39394713">[-]</label><label class="expand" for="c-39394713">[1 more]</label></div><br/><div class="children"><div class="content">Arguably you should go long since once they integrate this into their products (as Adobe is doing) they have the distribution in place to monetise it, industry knowledge to combine it with existing workflows, etc.</div><br/></div></div><div id="39392311" class="c"><input type="checkbox" id="c-39392311" checked=""/><div class="controls bullet"><span class="by">drcode</span><span>|</span><a href="#39392204">parent</a><span>|</span><a href="#39394713">prev</a><span>|</span><a href="#39394402">next</a><span>|</span><label class="collapse" for="c-39392311">[-]</label><label class="expand" for="c-39392311">[1 more]</label></div><br/><div class="children"><div class="content">I think not in the short term- I&#x27;m guessing the next step will be to use traditional tools to make a &quot;draft&quot; of a desired video, then &quot;finish it off&quot; with this kind of deep learning tech.<p>So this tech will increase interest in existing 3d tools in the short term</div><br/></div></div><div id="39394402" class="c"><input type="checkbox" id="c-39394402" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39392204">parent</a><span>|</span><a href="#39392311">prev</a><span>|</span><a href="#39392951">next</a><span>|</span><label class="collapse" for="c-39394402">[-]</label><label class="expand" for="c-39394402">[1 more]</label></div><br/><div class="children"><div class="content">People working in vfx are incredibly gloomy today, they see the writing on the wall now, whether it&#x27;s 1 or 5 years. There will still be a demand for human-created stuff but many of the jobs in advertising and stock footage will disappear.</div><br/></div></div></div></div><div id="39392951" class="c"><input type="checkbox" id="c-39392951" checked=""/><div class="controls bullet"><span class="by">pellucide</span><span>|</span><a href="#39392204">prev</a><span>|</span><a href="#39392033">next</a><span>|</span><label class="collapse" for="c-39392951">[-]</label><label class="expand" for="c-39392951">[1 more]</label></div><br/><div class="children"><div class="content">I am a newbie to this area. Honest questions:<p>Is this generating videos as streaming content e.g. like a mp4 video. As far as I can see, it is doing that. Is it  possible for AI to actually produce the 3d models?<p>What kind of compute resources are required to produce the 3d models.</div><br/></div></div><div id="39392033" class="c"><input type="checkbox" id="c-39392033" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#39392951">prev</a><span>|</span><a href="#39392390">next</a><span>|</span><label class="collapse" for="c-39392033">[-]</label><label class="expand" for="c-39392033">[1 more]</label></div><br/><div class="children"><div class="content">Wow, this is really just scale-up DiT. We are going to see tons of similar models very soon.</div><br/></div></div><div id="39392390" class="c"><input type="checkbox" id="c-39392390" checked=""/><div class="controls bullet"><span class="by">lbrito</span><span>|</span><a href="#39392033">prev</a><span>|</span><a href="#39391965">next</a><span>|</span><label class="collapse" for="c-39392390">[-]</label><label class="expand" for="c-39392390">[2 more]</label></div><br/><div class="children"><div class="content">Okay, The Matrix can&#x27;t be too far away now.</div><br/><div id="39393169" class="c"><input type="checkbox" id="c-39393169" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#39392390">parent</a><span>|</span><a href="#39391965">next</a><span>|</span><label class="collapse" for="c-39393169">[-]</label><label class="expand" for="c-39393169">[1 more]</label></div><br/><div class="children"><div class="content">Having the same cat enter the shot twice would definitely be a glitch in the Matrix.</div><br/></div></div></div></div><div id="39391965" class="c"><input type="checkbox" id="c-39391965" checked=""/><div class="controls bullet"><span class="by">colesantiago</span><span>|</span><a href="#39392390">prev</a><span>|</span><a href="#39391975">next</a><span>|</span><label class="collapse" for="c-39391965">[-]</label><label class="expand" for="c-39391965">[5 more]</label></div><br/><div class="children"><div class="content">Damn, even minecraft videos being simulated, this is crazy to see from OpenAI.<p>Edit, changed the links to the direct ones!<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_6.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_6.mp4</a><p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_7.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_7.mp4</a></div><br/><div id="39393560" class="c"><input type="checkbox" id="c-39393560" checked=""/><div class="controls bullet"><span class="by">cptroot</span><span>|</span><a href="#39391965">parent</a><span>|</span><a href="#39391980">next</a><span>|</span><label class="collapse" for="c-39393560">[-]</label><label class="expand" for="c-39393560">[2 more]</label></div><br/><div class="children"><div class="content">As someone who&#x27;s played probably too many hours of minecraft, these videos are nauseating. The way that all of the individual pieces exist, but have no consistency is terrifying.  Random FoV changes, switching apparent texture packs, raytracing on or off, it&#x27;s all still switching back and forth from moment to moment.<p>These videos honestly give me less confidence in the approach, simply because I don&#x27;t know that the model will be able to distinguish these world model parameters as &quot;unchanging&quot;.</div><br/><div id="39394033" class="c"><input type="checkbox" id="c-39394033" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39391965">root</a><span>|</span><a href="#39393560">parent</a><span>|</span><a href="#39391980">next</a><span>|</span><label class="collapse" for="c-39394033">[-]</label><label class="expand" for="c-39394033">[1 more]</label></div><br/><div class="children"><div class="content">As someone who is a human born in the 80s, this shit wasn&#x27;t supposed to even be <i>theoretically</i> possible.<p>People didn&#x27;t think this was possible even a year ago.<p>&quot;Gives me less confidence&quot;<p>Cmon man...</div><br/></div></div></div></div><div id="39391980" class="c"><input type="checkbox" id="c-39391980" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#39391965">parent</a><span>|</span><a href="#39393560">prev</a><span>|</span><a href="#39391975">next</a><span>|</span><label class="collapse" for="c-39391980">[-]</label><label class="expand" for="c-39391980">[2 more]</label></div><br/><div class="children"><div class="content">But it starts to make sense, when you think about the fact that Minecraft is owned by Microsoft.<p>example video links from TFA:<p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_6.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_6.mp4</a><p><a href="https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_7.mp4" rel="nofollow">https:&#x2F;&#x2F;cdn.openai.com&#x2F;tmp&#x2F;s&#x2F;simulation_7.mp4</a></div><br/><div id="39392160" class="c"><input type="checkbox" id="c-39392160" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#39391965">root</a><span>|</span><a href="#39391980">parent</a><span>|</span><a href="#39391975">next</a><span>|</span><label class="collapse" for="c-39392160">[-]</label><label class="expand" for="c-39392160">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that has anything to do with it.  There&#x27;s just millions of hours of minecraft video on youtube and twitch.</div><br/></div></div></div></div></div></div><div id="39391975" class="c"><input type="checkbox" id="c-39391975" checked=""/><div class="controls bullet"><span class="by">RayVR</span><span>|</span><a href="#39391965">prev</a><span>|</span><a href="#39392934">next</a><span>|</span><label class="collapse" for="c-39391975">[-]</label><label class="expand" for="c-39391975">[1 more]</label></div><br/><div class="children"><div class="content">If there&#x27;s one thing I&#x27;ve always wanted, it&#x27;s shitty video knockoffs of real life. Can&#x27;t wait to stream some AI hallucinations.</div><br/></div></div><div id="39392934" class="c"><input type="checkbox" id="c-39392934" checked=""/><div class="controls bullet"><span class="by">blueprint</span><span>|</span><a href="#39391975">prev</a><span>|</span><label class="collapse" for="c-39392934">[-]</label><label class="expand" for="c-39392934">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.<p>so they&#x27;re gonna include the never-before-observed-but-predicted Unruh effect, as well? and other quantum theory? cool..<p>&gt; For example, it does not accurately model the physics of many basic interactions, like glass shattering.<p>... oh<p>Isn&#x27;t all of the training predicated on visible, gathered data - rather than theory? if so, I don&#x27;t think it&#x27;s right to call these things simulators of the physical world if they don&#x27;t include physical theory.
DFT at least has some roots in theory.</div><br/><div id="39394721" class="c"><input type="checkbox" id="c-39394721" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#39392934">parent</a><span>|</span><label class="collapse" for="c-39394721">[-]</label><label class="expand" for="c-39394721">[1 more]</label></div><br/><div class="children"><div class="content">The point isn&#x27;t &quot;physical simulator&quot; like supercomputers, it&#x27;s &quot;physical simulator&quot; like the human brain.</div><br/></div></div></div></div></div></div></div></div></div></body></html>