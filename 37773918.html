<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696496489533" as="style"/><link rel="stylesheet" href="styles.css?v=1696496489533"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.beren.io/2023-03-19-LLMs-confabulate-not-hallucinate/">LLMs confabulate not hallucinate</a> <span class="domain">(<a href="https://www.beren.io">www.beren.io</a>)</span></div><div class="subtext"><span>pella</span> | <span>119 comments</span></div><br/><div><div id="37774755" class="c"><input type="checkbox" id="c-37774755" checked=""/><div class="controls bullet"><span class="by">lachlan_gray</span><span>|</span><a href="#37775939">next</a><span>|</span><label class="collapse" for="c-37774755">[-]</label><label class="expand" for="c-37774755">[13 more]</label></div><br/><div class="children"><div class="content">This reminds me of something from “The language instinct” by Steven pinker. One thing that stuck with me was a story of a girl named Denys who is or was severely mentally handicapped, but speaks perfectly.<p>She can’t read, write, handle money, but confabulates engaging and convincing stories about receiving mail, haggling with her bank, her boyfriend. She speaks with proper grammar, intonation, etc, but doesn’t demonstrate any understanding of what she talks about beyond the words. She doesn’t have a bank account or a boyfriend.<p>Trying to find more and I can’t find anything about her on the internet, I would have to look at the references. Maybe it’s questionable.<p>Anyway, I like to think about what the hell we even mean when we say we “know something”, and where we should actually put the bar for AI. Where is the line between confabulation and knowledge? Is knowledge just confabulation that happens to be correct?<p>I feel like it’s possible that Denys could feel that she “knows” what she is saying, but is simply trapped in, with a working knowledge of the world and is just unable to translate it into actions, and words are just her only degree of freedom.<p>I don’t remember where I was going with this, but I agree that “confabulation” is a much better term to use, but we also shouldn’t discount what it means to be able to confabulate.</div><br/><div id="37775790" class="c"><input type="checkbox" id="c-37775790" checked=""/><div class="controls bullet"><span class="by">hamstergene</span><span>|</span><a href="#37774755">parent</a><span>|</span><a href="#37774861">next</a><span>|</span><label class="collapse" for="c-37775790">[-]</label><label class="expand" for="c-37775790">[2 more]</label></div><br/><div class="children"><div class="content">Some of the most annoying moments in my career were occasional coworkers who talked a lot without making a point. They just invoked topics loosely related to the problem at hand, like stories from previous jobs or what they&#x27;ve read on the internet about this problem, until the manager said something like &quot;are you trying to say that...&quot; and then formulated what they thought the point was, except that the speaker clearly never intended to say anything like that. All the other engineers in the room are giving each other side eye but the manager thinks they&#x27;ve just got &quot;a valuable input&quot;.<p>The tactic is simply to talk relevantly until the listener fools themselves into thinking they understand you, except that &quot;understanding&quot; will be an illusion and 100% manager&#x27;s own analytical effort. And it&#x27;s honestly embarrassing how often and well that works. Quite a few of those guys were promoted first, too.<p>I&#x27;m pretty sure our upcoming future with AI will be like that quote from forest ranger at Yosemite National Park, on why it is hard to design the perfect garbage bin to keep bears from breaking into it: &quot;There is a considerable overlap between the intelligence of the smartest bears and the dumbest tourists.&quot;<p>The whole idea of a criteria to separate humans from AI is a misconception. I think any human who confuses smooth talking with making sense will be ruthlessly exploited in relatively near future. There are humans who already do this successfully to other humans, what do you think will happen when they get to automate their efforts.</div><br/><div id="37775881" class="c"><input type="checkbox" id="c-37775881" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774755">root</a><span>|</span><a href="#37775790">parent</a><span>|</span><a href="#37774861">next</a><span>|</span><label class="collapse" for="c-37775881">[-]</label><label class="expand" for="c-37775881">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think any human who confuses smooth talking with making sense will be ruthlessly exploited in relatively near future<p>Ads already ruthlessly exploit these people, taking their money in exchange for garbage products. LLM might make this worse, but we have already automated displaying targeted manipulative messages via ad networks. People who can handle manipulative messages from ads should be mostly resistant to manipulative messaging from bots.</div><br/></div></div></div></div><div id="37774861" class="c"><input type="checkbox" id="c-37774861" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#37774755">parent</a><span>|</span><a href="#37775790">prev</a><span>|</span><a href="#37776174">next</a><span>|</span><label class="collapse" for="c-37774861">[-]</label><label class="expand" for="c-37774861">[5 more]</label></div><br/><div class="children"><div class="content">I had the same thought that &quot;hallucinate&quot; is too strong a term for what LLMs are doing.<p>Instead I would say they DREAM. Dreams follow some logic but basically they are disjoint from reality. They involve same characters as our real-life experiences but what those characters do in a dream is not based on reality.<p>So one could think that ALL LLMs ever do is dream but much of the time their dreams are dreams which feel very real.<p>Our dreams are basically an LLM based on our real experiences but recombined in arbitrary ways, still retaining some logical structure but now detached from reality because when you recombine different experiences including  our thoughts from real life, the result cannot really reflect reality very well.  LLMs are better in this respect than our dreams, but sometimes their &quot;dreams&quot; get really obviously detached from reality.<p>Or if you want to use the term &quot;hallucinate&quot; with LLMs then fine but its more like  they hallucinate all the time, it&#x27;s just that sometimes, even often, their hallucinations seem to agree with the reality so well that we cannot tell the difference.<p>LLMs do not describe reality because they cannot experience reality, all they can give us is an &quot;average description&quot; created from many existing descriptions.<p>One way of looking at it is to say that all LLMs tell us is hearsay.</div><br/><div id="37776212" class="c"><input type="checkbox" id="c-37776212" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#37774755">root</a><span>|</span><a href="#37774861">parent</a><span>|</span><a href="#37775270">next</a><span>|</span><label class="collapse" for="c-37776212">[-]</label><label class="expand" for="c-37776212">[1 more]</label></div><br/><div class="children"><div class="content">The way I conceptualize dream versus hallucinate, they&#x27;re basically the same thing here and I see no reason to switch.  It&#x27;s not like the software has a sleep schedule.<p>And hearsay is usually a lot better than LLM output.  I wouldn&#x27;t use that word.</div><br/></div></div><div id="37775270" class="c"><input type="checkbox" id="c-37775270" checked=""/><div class="controls bullet"><span class="by">orobinson</span><span>|</span><a href="#37774755">root</a><span>|</span><a href="#37774861">parent</a><span>|</span><a href="#37776212">prev</a><span>|</span><a href="#37775509">next</a><span>|</span><label class="collapse" for="c-37775270">[-]</label><label class="expand" for="c-37775270">[2 more]</label></div><br/><div class="children"><div class="content">But do LLMs dream of electric sheep?</div><br/><div id="37775665" class="c"><input type="checkbox" id="c-37775665" checked=""/><div class="controls bullet"><span class="by">trashtester</span><span>|</span><a href="#37774755">root</a><span>|</span><a href="#37775270">parent</a><span>|</span><a href="#37775509">next</a><span>|</span><label class="collapse" for="c-37775665">[-]</label><label class="expand" for="c-37775665">[1 more]</label></div><br/><div class="children"><div class="content">They certainly do after having read that script.</div><br/></div></div></div></div></div></div><div id="37776174" class="c"><input type="checkbox" id="c-37776174" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#37774755">parent</a><span>|</span><a href="#37774861">prev</a><span>|</span><a href="#37775869">next</a><span>|</span><label class="collapse" for="c-37776174">[-]</label><label class="expand" for="c-37776174">[1 more]</label></div><br/><div class="children"><div class="content">I knew someone who was like Denys after a stroke. They spoke well, in grammatically correct sentences that made sense even if the content was not rooted in reality. There was not much coherence between sentences or in longer statements but it was more like quick gradual shifts not hard jumps from topic to topic.<p>So not unlike early LLM chat in areas where the model had not a lot of data about.</div><br/></div></div><div id="37775869" class="c"><input type="checkbox" id="c-37775869" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37774755">parent</a><span>|</span><a href="#37776174">prev</a><span>|</span><a href="#37775505">next</a><span>|</span><label class="collapse" for="c-37775869">[-]</label><label class="expand" for="c-37775869">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s Lots of evidence indicating language models roughly know when they hallucinate.<p>GPT-4 logits calibration pre RLHF - [<a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r](https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r)" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r](https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r)</a><p>Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback - [<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305</a>...<p>Teaching Models to Express Their Uncertainty in Words - [<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205</a>...<p>Language Models (Mostly) Know What They Know - [<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207</a>...<p>It seems very much like hallucinations aren&#x27;t a randomness or representation issue. The computation already knows. It just doesn&#x27;t care about telling you this</div><br/><div id="37775897" class="c"><input type="checkbox" id="c-37775897" checked=""/><div class="controls bullet"><span class="by">corobo</span><span>|</span><a href="#37774755">root</a><span>|</span><a href="#37775869">parent</a><span>|</span><a href="#37775505">next</a><span>|</span><label class="collapse" for="c-37775897">[-]</label><label class="expand" for="c-37775897">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using it as a joke but if they&#x27;re predicting what we want to read, is it possible they just learned to make stuff up by having internet forums in their training data?<p>The amount of BS we feed each other online, maybe it just thinks we&#x27;d prefer something made up than &quot;I don&#x27;t know&quot;</div><br/></div></div></div></div><div id="37775505" class="c"><input type="checkbox" id="c-37775505" checked=""/><div class="controls bullet"><span class="by">bigDinosaur</span><span>|</span><a href="#37774755">parent</a><span>|</span><a href="#37775869">prev</a><span>|</span><a href="#37775606">next</a><span>|</span><label class="collapse" for="c-37775505">[-]</label><label class="expand" for="c-37775505">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;d be well worth verifying the story because if Steven Pinker made an error like that it&#x27;s a fairly big deal.</div><br/></div></div><div id="37775606" class="c"><input type="checkbox" id="c-37775606" checked=""/><div class="controls bullet"><span class="by">accounter8or</span><span>|</span><a href="#37774755">parent</a><span>|</span><a href="#37775505">prev</a><span>|</span><a href="#37775939">next</a><span>|</span><label class="collapse" for="c-37775606">[-]</label><label class="expand" for="c-37775606">[1 more]</label></div><br/><div class="children"><div class="content">That story actually cracked me up, the way he ends it.</div><br/></div></div></div></div><div id="37775939" class="c"><input type="checkbox" id="c-37775939" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#37774755">prev</a><span>|</span><a href="#37776228">next</a><span>|</span><label class="collapse" for="c-37775939">[-]</label><label class="expand" for="c-37775939">[2 more]</label></div><br/><div class="children"><div class="content">Indeed it seems like replacing &quot;hallucinate&quot; with &quot;confabulate&quot; would improve understanding of the phenomenon in english, but it could cause a problem in Romance languages.<p>Indeed, the Latin verb &quot;confabulor has the meaning of &quot;discuss, converse&quot; [0], not specifically &quot;inventing things&quot;, and Romance languages already have words derived in sound and meaning from this classic Latin sense. For instance French already has &quot;confabuler&quot; meaning &quot;speak familiarly with someone&quot; [1]. Thus in Romance languages, this proposed usage of &quot;confabulate&quot; imported from the psychiatric word could be understood poorly due to preexisting similar-sounding words that do not include the notion of &quot;making things up&quot;.<p>Just proposing thoughts here, not advocating for a final decision. Maybe it&#x27;s still good to adopt this wording!<p>[0] : <a href="https:&#x2F;&#x2F;en.m.wiktionary.org&#x2F;wiki&#x2F;confabulor#Latin" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wiktionary.org&#x2F;wiki&#x2F;confabulor#Latin</a>
[1] : <a href="https:&#x2F;&#x2F;www.dictionnaire-academie.fr&#x2F;article&#x2F;A9C3479" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.dictionnaire-academie.fr&#x2F;article&#x2F;A9C3479</a></div><br/><div id="37776258" class="c"><input type="checkbox" id="c-37776258" checked=""/><div class="controls bullet"><span class="by">luxcem</span><span>|</span><a href="#37775939">parent</a><span>|</span><a href="#37776228">next</a><span>|</span><label class="collapse" for="c-37776258">[-]</label><label class="expand" for="c-37776258">[1 more]</label></div><br/><div class="children"><div class="content">In french we use &quot;fabuler&quot;[1] that can be translated to &quot;make sth up&quot;. I don&#x27;t know if it&#x27;s the same in other Roman language.<p>[1] <a href="https:&#x2F;&#x2F;www.dictionnaire-academie.fr&#x2F;article&#x2F;A9F0016" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.dictionnaire-academie.fr&#x2F;article&#x2F;A9F0016</a></div><br/></div></div></div></div><div id="37776228" class="c"><input type="checkbox" id="c-37776228" checked=""/><div class="controls bullet"><span class="by">ohduran</span><span>|</span><a href="#37775939">prev</a><span>|</span><a href="#37774891">next</a><span>|</span><label class="collapse" for="c-37776228">[-]</label><label class="expand" for="c-37776228">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a somewhat famous movie in Spain called the Miracle of P Tinto[0]. It&#x27;s a very surreal humor kind of movie, if you fancy that.<p>The plot starts with the star character who wants to form a family, but when he asked adults how to have kids, they would all pull up the suspenders and sing &quot;tralari, tralari&quot; to evade the question.<p>And so the poor guy did exactly what he was told: pull his suspenders and sing, for years, to no avail.<p>I think LLMs are a bit like that: because they can&#x27;t escape the world of text, they can&#x27;t go beyond the reality that text is trying to be isomorphic to. Which means that LLMs are forced to infer answers in the context of its training data.<p>If LLMs were told that in order to have kids you, well, you know, right? Bees and Love? Then LLMs, asked about sex, would infer that the answer has something to do with apiculture and flowers.<p>[0]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Miracle_of_P._Tinto" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Miracle_of_P._Tinto</a></div><br/></div></div><div id="37774891" class="c"><input type="checkbox" id="c-37774891" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37776228">prev</a><span>|</span><a href="#37775442">next</a><span>|</span><label class="collapse" for="c-37774891">[-]</label><label class="expand" for="c-37774891">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Hallucinate&quot; predates LLMs.<p>For instance, before transformers, when we finetuned ESRGAN for specific styles&#x2F;functions, we used to say it would &quot;hallucinate&quot; in detail that couldn&#x27;t possibly be in the original image pixels. GenAI wasn&#x27;t much of a thing back then, and we never thought of the small GAN models as &quot;remembering&quot; things even though thats common language in transformer models. It felt more like we were teaching the GAN a job or style, not encoding memory.<p>And I think it fit! &quot;Hallucinating&quot; detail into a blurry pixel blob feels like a more accurate analogy to the human condition, and we weren&#x27;t at the point where it was &quot;confabulating&quot; and generating big objects out of the blue like a diffusion model can.</div><br/><div id="37774975" class="c"><input type="checkbox" id="c-37774975" checked=""/><div class="controls bullet"><span class="by">panagathon</span><span>|</span><a href="#37774891">parent</a><span>|</span><a href="#37775313">next</a><span>|</span><label class="collapse" for="c-37774975">[-]</label><label class="expand" for="c-37774975">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m inclined, perhaps mistakenly, to think of it as similar to how our brains fill in our peripheral vision, to respond to the lack of data.<p><a href="https:&#x2F;&#x2F;neurosciencenews.com&#x2F;peripheral-vision-brain-illusion-5713&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;neurosciencenews.com&#x2F;peripheral-vision-brain-illusio...</a></div><br/></div></div><div id="37775313" class="c"><input type="checkbox" id="c-37775313" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37774891">parent</a><span>|</span><a href="#37774975">prev</a><span>|</span><a href="#37775442">next</a><span>|</span><label class="collapse" for="c-37775313">[-]</label><label class="expand" for="c-37775313">[1 more]</label></div><br/><div class="children"><div class="content">karpathy says may have coined it in his RNN post <a href="https:&#x2F;&#x2F;twitter.com&#x2F;DrJimFan&#x2F;status&#x2F;1703072983903060260" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;DrJimFan&#x2F;status&#x2F;1703072983903060260</a></div><br/></div></div></div></div><div id="37775442" class="c"><input type="checkbox" id="c-37775442" checked=""/><div class="controls bullet"><span class="by">Joking_Phantom</span><span>|</span><a href="#37774891">prev</a><span>|</span><a href="#37774639">next</a><span>|</span><label class="collapse" for="c-37775442">[-]</label><label class="expand" for="c-37775442">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure if confabulate is exactly what LLMs do (though it seems closer than the implications of hallucinate).<p>But neurotypical, neurodiverse, perfectly functional people, etc. all confabulate or do something similar on a regular basis, in verbal and written mediums, and often do so in good faith. It&#x27;s human instinct to communicate, even if you are uncertain and unaware of the full context of the discussion.<p>Teachers, customer service reps, executives, shop keepers, doctors, nurses, domain experts, authors of textbooks, it doesn&#x27;t matter who it is, they&#x27;ll probably confabulate or equivocate or do some other type of communication that isn&#x27;t immediately useful. Yet it&#x27;s still a useful activity to just talk to someone or read a less than rigorous book for the purposes of learning (discounting the relationship forming part, which is also useful). And so is using LLMs, even for casual users. So long as they understand that limitation, whether its with a chatbot or a real person. Not everything they say will be useful or truthful, but we are already capable of adjusting to that.</div><br/><div id="37775839" class="c"><input type="checkbox" id="c-37775839" checked=""/><div class="controls bullet"><span class="by">pcblues</span><span>|</span><a href="#37775442">parent</a><span>|</span><a href="#37774639">next</a><span>|</span><label class="collapse" for="c-37775839">[-]</label><label class="expand" for="c-37775839">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure whether neurofancy people confabulating should be given an honest bill of truth health based on their category.<p>Sounds like #believeallconfabulators :)<p>Honest communication is difficult for some people to assess, and not for others. But I think we should learn from any recent #believeall... that we shouldn&#x27;t base trust ratios on categorisation.<p>Honest communication is difficult to do, like weight-lifting, and takes a lot of practise to do well.<p>It also makes your BS meter more finely attuned, so is a good practise.<p>With that in mind, you will think this is arrogant to say if you lie for a living but not if you regularly tell the truth:
Liars lie with liars and lie to rid themselves of truth troubles.<p>Think about that when you next talk to a chatbot&#x2F;human confabulator :)</div><br/></div></div></div></div><div id="37774639" class="c"><input type="checkbox" id="c-37774639" checked=""/><div class="controls bullet"><span class="by">arduanika</span><span>|</span><a href="#37775442">prev</a><span>|</span><a href="#37774522">next</a><span>|</span><label class="collapse" for="c-37774639">[-]</label><label class="expand" for="c-37774639">[3 more]</label></div><br/><div class="children"><div class="content">I like this a lot.<p>I recall seeing a post on here that roughly just said, &quot;stop saying hallucinate&quot;, which sure, is technically correct (&quot;the best kind of correct&quot;), but not super helpful. Contrast that with this post, which offers a constructive suggestion: replace it with &quot;confabulate&quot;.<p>I agree that C is an improvement over H, for different reasons from the author. Here&#x27;s why: From a public comms standpoint, the big problem with saying &quot;hallucinate&quot; is that it leads to misconceptions, because people think they know what &quot;hallucinate&quot; means, and may anthropomorphize their pop psych grasp of H onto the LLM.<p>Just like H, &quot;C&quot; seems to have a technical meaning in the psych literature, but it&#x27;s far less in common parlance than &quot;H&quot;, so lay readers have no fixed idea of what exactly &quot;C&quot; means. Nobody goes around experimenting with &quot;confabulagens&quot; to seek a spiritual high. Also, &quot;confabulate&quot; retains a non-clinical meaning that&#x27;s similarly obscure, from long before the psychologists adopted it, and it&#x27;s pretty evocative of what&#x27;s really going on here (e.g. via related words like &quot;fable&quot; and &quot;fib&quot;). So &quot;confabulate&quot; seems ripe for adoption to describe this specific computational phenomenon, as we&#x27;ve done to so many other mundane words in the past (deprecate, and so forth).<p>If the psychiatric &quot;C&quot; phenomenon is also a better analogy for LLM behavior, as OP claims (and some posters dispute?), so much the better. But my main reason for liking C, is that it&#x27;s less likely than H to make people unduly confident that they understand it.<p>(But even if the community came to a consensus at this late stage that &quot;C&quot; is a better term, the subsequent problem of updating every stale paper and post that uses the old term would run us up against another doozy: cache invalidation.)</div><br/><div id="37774855" class="c"><input type="checkbox" id="c-37774855" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#37774639">parent</a><span>|</span><a href="#37774777">next</a><span>|</span><label class="collapse" for="c-37774855">[-]</label><label class="expand" for="c-37774855">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using confabulate privately for a while about this phenomenon, because if you know anything about dementia (my grandmother had it - sitting in the car with her you got used to the &quot;covers&quot; feeling like chatbot answer dodges to some extent) or split brain experiments, then it&#x27;s quite a remarkable phenomenon that does feel a lot more like what an LLM is doing.<p>Hallucinate generally implies experiencing sensory inputs which aren&#x27;t there, but having a rational reaction to them - in the popular psyche though, people who hallucinate are still &quot;crazy&quot; - unfairly so, because you can function normally with hallucinations under quite a number of circumstances (i.e. I remember someone saying that they realized that if they saw people without faces, it was fine because they weren&#x27;t really there - apparently quite common, since face recognition is a different part of the brain).<p>Basically: reacting to invalid sensory inputs implies rational behavior, but to non-real inputs.<p>Confabulation on the other hand is different because it&#x27;s closer to &quot;trying to rationalize an autonomous behavior without knowledge of the sensory inputs causing it&quot; (maybe, it&#x27;s a bizarre phenomenon). In split-brain patients in lab settings, it manifests as being unable to answer a question about why you&#x27;re having a physiological response to imagery which is being shown only to the right-side of the brain, whereas language is processed on the left - but rather then be confused, people will apparently make something up that they&#x27;re unaware is actually a &quot;lie&quot; (in quotes because, well, they&#x27;re not lying - as far as we know there&#x27;s no intent or even knowledge that is a lie).<p>Where this leaves us with LLMs I don&#x27;t really know, because it feels like an imperfect descriptor: except perhaps with the context that LLMs are fairly limited networks, and so to some extent the whole &quot;being wrong&quot; phenonmenon is in fact just a failure of the attention mechanism to be able to draw the right data together.</div><br/></div></div></div></div><div id="37774522" class="c"><input type="checkbox" id="c-37774522" checked=""/><div class="controls bullet"><span class="by">lsy</span><span>|</span><a href="#37774639">prev</a><span>|</span><a href="#37775749">next</a><span>|</span><label class="collapse" for="c-37774522">[-]</label><label class="expand" for="c-37774522">[56 more]</label></div><br/><div class="children"><div class="content">LLMs do not confabulate <i>or</i> hallucinate. They predict the next word in a sequence based on correlations derived from a lot of training data. Sometimes that data happens to be structured enough to statistically generate a word that is fluent and corresponds to what we as readers understand as reality. Sometimes, due to the unclear nature of &quot;truth&quot; and the necessarily incomplete data used to train the model, the word or words that are statistically generated are merely fluent but do not correspond to reality.<p>The answer to &quot;What are LLMs but humans with extreme amnesia and no central coherence?&quot; is that they are not like humans at all, and only resemble them superficially due to our anthropomorphic tendency to infer a mind upon seeing fluent and roughly correspondent text.</div><br/><div id="37774565" class="c"><input type="checkbox" id="c-37774565" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774805">next</a><span>|</span><label class="collapse" for="c-37774565">[-]</label><label class="expand" for="c-37774565">[22 more]</label></div><br/><div class="children"><div class="content">&gt; they are not like humans at all, and only resemble them superficially due to our anthropomorphic tendency to infer a mind upon seeing fluent and roughly correspondent text.<p>Alternatively, we overestimate human abilities, and imbue our thoughts with deep meaning and significance, simply as a way to feel more important than we actually are.  Ultimately, we may rely on statistical prediction quite a bit as well.<p>Obviously, LLMs aren&#x27;t at the level of human cognition, but the impressive results they do produce, given their obvious simplicity, hints that intelligence may not be as hard to reproduce in silicon as we have previously assumed.</div><br/><div id="37775563" class="c"><input type="checkbox" id="c-37775563" checked=""/><div class="controls bullet"><span class="by">ffwd</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774565">parent</a><span>|</span><a href="#37774628">next</a><span>|</span><label class="collapse" for="c-37775563">[-]</label><label class="expand" for="c-37775563">[1 more]</label></div><br/><div class="children"><div class="content">To me predicting the next token is obviously not how humans think.<p>If I ask you to envision a green triangle and a red square next to each other, and then swap the shapes but keep the colors in the same locations, and answer what color is the triangle now, you say the triangle is red, but you do so because you envisioned the triangle swapping places and did the mental steps etc.<p>An LLM if even answering correctly, is statistically answering based on billions of lines of text + rlhf and all of this, I highly doubt there is a mental model of the world, but rather a large set of constraints in the probabilities which leads to the resulting answer. The reasoning ability is a secondary effect of the probabilities which is why it&#x27;s hard to make it so every probability is correct for every answer I think.<p>And regarding OP about hallucinating vs confabulating. To me hallucinating is a fine word for it, because it is filling in a gap or there aren&#x27;t enough constraints in the model&#x2F;data&#x2F;tuning to account for that specific answer that it gave that was incorrect. Hence it &quot;hallucinates&quot; something in the gap. The real power of LLM&#x27;s is that it seems to accumulate these &#x27;constraints&#x27; (generalization), so that with the right model, it should be able to answer more and more prompts correctly, which is kind of amazing.<p>Confabulation works too but is a little more high level IMO.</div><br/></div></div><div id="37774628" class="c"><input type="checkbox" id="c-37774628" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774565">parent</a><span>|</span><a href="#37775563">prev</a><span>|</span><a href="#37774772">next</a><span>|</span><label class="collapse" for="c-37774628">[-]</label><label class="expand" for="c-37774628">[14 more]</label></div><br/><div class="children"><div class="content">&gt; Obviously, LLMs aren&#x27;t at the level of human cognition, but the impressive results they do produce, given their obvious simplicity, hints that intelligence may not be as hard to reproduce in silicon as we have previously assumed.<p>People said this about calculators as well, or initial chess AI as well. &quot;If computers can do this, then the remaining parts of intelligence is probably not hard to reproduce!&quot;.<p>So we already know many humans massively underestimate the depth of human intelligence. It happened many times before and will continue to happen every time we make computers do anything new.</div><br/><div id="37774648" class="c"><input type="checkbox" id="c-37774648" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774628">parent</a><span>|</span><a href="#37774771">next</a><span>|</span><label class="collapse" for="c-37774648">[-]</label><label class="expand" for="c-37774648">[9 more]</label></div><br/><div class="children"><div class="content">&gt; People said this about calculators as well, or initial chess AI as well. &quot;If computers can do this, then the remaining parts of intelligence is probably not hard to reproduce!&quot;.<p>People also said that chess computers would never beat the best humans.  And then when that happened they said, &quot;well they&#x27;re just using sheer brute force, they will never produce beautiful chess&quot;.  And then Alpha Zero came along, and showed that prediction was wrong as well.<p>Personally, I believe that silicon intelligence will evolve a lot faster than ours did.  But regardless, I&#x27;ve yet to hear any convincing argument why it can&#x27;t happen eventually.</div><br/><div id="37774665" class="c"><input type="checkbox" id="c-37774665" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774648">parent</a><span>|</span><a href="#37774771">next</a><span>|</span><label class="collapse" for="c-37774665">[-]</label><label class="expand" for="c-37774665">[8 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;ve yet to hear any convincing argument why it can&#x27;t happen eventually.<p>That is a strawman, basically nobody argues this.<p>&gt; People also said that chess computers would never beat the best humans<p>How many did say this? I see way more overly optimistic &quot;humans are stupid&quot; messages than I see &quot;computers will never beat humans&quot; messages. The argument is mostly between &quot;humans are obsolete in a decade at most&quot; versus &quot;we would be lucky to see general intelligence in our lifetime&quot;, not that it it is impossible.</div><br/><div id="37775053" class="c"><input type="checkbox" id="c-37775053" checked=""/><div class="controls bullet"><span class="by">Falkon1313</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774665">parent</a><span>|</span><a href="#37774683">next</a><span>|</span><label class="collapse" for="c-37775053">[-]</label><label class="expand" for="c-37775053">[2 more]</label></div><br/><div class="children"><div class="content">&gt;How many did say this?<p>I don&#x27;t know about chess, though earlier computers didn&#x27;t have the power, so people could&#x27;ve reasonably said that.<p>But we certainly saw it with Go. Many people said that a computer could beat a person at chess or checkers, because they were simple, limited games, but Go was too complicated, too many possible combinations. A computer could never beat a Go champion. And here we are, with multiple generations of AlphaGo pushing the boundaries. They might use a couple thousand CPUs and a few hundred GPUs, but they can do the impossible thing.</div><br/><div id="37775102" class="c"><input type="checkbox" id="c-37775102" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37775053">parent</a><span>|</span><a href="#37774683">next</a><span>|</span><label class="collapse" for="c-37775102">[-]</label><label class="expand" for="c-37775102">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A computer could never beat a Go champion<p>* Using the algorithms they used to beat humans at chess.<p>I were there during those discussions, the AI optimists argued that since computers could beat humans at chess GAI would soon be here. Then it is reasonable to argue that even a simple game like GO is impossible to solve the way we solved chess, so there needs to be more fundamental breakthroughs before GAI can happen.<p>That same thing is playing out today, and the AI proponents are still misunderstanding the other sides argument. The argument isn&#x27;t &quot;computers can never do this&quot;, the argument is &quot;the algorithms and methods we use today can&#x27;t do this, so there is no clear path to intelligence from where we are&quot;.</div><br/></div></div></div></div><div id="37774683" class="c"><input type="checkbox" id="c-37774683" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774665">parent</a><span>|</span><a href="#37775053">prev</a><span>|</span><a href="#37774693">next</a><span>|</span><label class="collapse" for="c-37774683">[-]</label><label class="expand" for="c-37774683">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That is a strawman, basically nobody argues this.<p>Some people do. Some people believe that intelligence is much more fundamental to the universe, and not capable of being reproduced mechanistically.<p>But if you grant that it is possible, all we&#x27;re talking about is <i>when</i>; which is anyone&#x27;s guess.  It could be tomorrow, it could take a million years.  I&#x27;m not pretending to know, just offering a different perspective than the original comment.</div><br/><div id="37774699" class="c"><input type="checkbox" id="c-37774699" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774683">parent</a><span>|</span><a href="#37774693">next</a><span>|</span><label class="collapse" for="c-37774699">[-]</label><label class="expand" for="c-37774699">[1 more]</label></div><br/><div class="children"><div class="content">But those &quot;some people&quot; aren&#x27;t here. What we are discussing is how close LLM are to human reasoning. Many argue LLM are already there, &quot;what evidence do you have that humans aren&#x27;t essentially just an LLM?&quot; basically. You see such posts in this very thread.</div><br/></div></div></div></div><div id="37774693" class="c"><input type="checkbox" id="c-37774693" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774665">parent</a><span>|</span><a href="#37774683">prev</a><span>|</span><a href="#37774771">next</a><span>|</span><label class="collapse" for="c-37774693">[-]</label><label class="expand" for="c-37774693">[3 more]</label></div><br/><div class="children"><div class="content">&gt; That is a strawman, basically nobody argues this.<p>I mean, people do-- even smart people like Penrose.</div><br/><div id="37774729" class="c"><input type="checkbox" id="c-37774729" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774693">parent</a><span>|</span><a href="#37774771">next</a><span>|</span><label class="collapse" for="c-37774729">[-]</label><label class="expand" for="c-37774729">[2 more]</label></div><br/><div class="children"><div class="content">But a large majority of people who say &quot;LLMs are nothing like humans&quot; are not making that argument. If you look through these kind of threads on HN you see maybe one post making that argument among hundreds, that is &quot;basically nobody&quot; to me given how often that strawman is used against people who are skeptical about LLMs potential capabilities.</div><br/><div id="37775235" class="c"><input type="checkbox" id="c-37775235" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774729">parent</a><span>|</span><a href="#37774771">next</a><span>|</span><label class="collapse" for="c-37775235">[-]</label><label class="expand" for="c-37775235">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think LLMs are going to get us to AGI on their own, but it&#x27;s a massive step: an obviously more fruitful one than Deep Blue or even AlphaZero, and also obviously more useful and disruptive to society.<p>Some people think we can&#x27;t get to &quot;real&quot; intelligence through Turing-equivalents.  Some think we can just tack a little more on an LLM and be there.  It&#x27;s probably something between, but the past-me who thought AGI was definitely &gt;40 years away is much less convinced of this now.<p>Could it happen in the next 5 years?  Proooooobably not, but is there a significant chance?  Yes.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37774771" class="c"><input type="checkbox" id="c-37774771" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774628">parent</a><span>|</span><a href="#37774648">prev</a><span>|</span><a href="#37774772">next</a><span>|</span><label class="collapse" for="c-37774771">[-]</label><label class="expand" for="c-37774771">[4 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t a response to the OPs point, where they&#x27;ve stated their beliefs and their logic for them.<p>To it, you&#x27;ve basically said: &quot;well in that case defend the times some <i>other</i> people have been incorrect if you&#x27;re sure <i>this</i> time&quot;.<p>Why should they? The OP might be wrong, but whether they are or aren&#x27;t has nothing to do with whether some unrelated, half-remembered third parties were right or wrong when mispredicting something else. You haven&#x27;t addressed the content of the OP&#x27;s actual argument at all, nor established a basis on which it may share similarity with other mispredictions.</div><br/><div id="37774813" class="c"><input type="checkbox" id="c-37774813" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774771">parent</a><span>|</span><a href="#37774772">next</a><span>|</span><label class="collapse" for="c-37774813">[-]</label><label class="expand" for="c-37774813">[3 more]</label></div><br/><div class="children"><div class="content">&gt; but whether they are or aren&#x27;t has nothing to do with whether some unrelated, half-remembered third parties were right or wrong<p>Their argument was &quot;Alternatively, we overestimate human abilities&quot;. Since he made that argument it is relevant to see if humans overestimated or underestimated human abilities in the past. My argument is that there is a large tendency for smart people and researchers to underestimate human intelligence, to assume it is easy to solve, we have seen that happen many times in the past.<p>So since smart humans often underestimate human abilities, the &quot;we overestimate human abilities&quot; theory is probably not correct.</div><br/><div id="37774912" class="c"><input type="checkbox" id="c-37774912" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774813">parent</a><span>|</span><a href="#37774772">next</a><span>|</span><label class="collapse" for="c-37774912">[-]</label><label class="expand" for="c-37774912">[2 more]</label></div><br/><div class="children"><div class="content">Except that isn&#x27;t true at all: the history of animal research is one of us assigning all human behavior as &quot;uniquely human, requiring great human intelligence&quot; and then discovering it&#x27;s either replicated in detail in other &quot;lower&quot; animals, or replicated by animals which don&#x27;t have nearly the apparent cognitive capacity for it[1].<p>We have an enormous history of vastly over-estimating the intelligence needed for some behaviors to manifest, with the more accurate conclusion being that actually we simply don&#x27;t understand the nature of intelligence very well.<p>[1] <a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC5394149&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC5394149&#x2F;</a></div><br/><div id="37774963" class="c"><input type="checkbox" id="c-37774963" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774912">parent</a><span>|</span><a href="#37774772">next</a><span>|</span><label class="collapse" for="c-37774963">[-]</label><label class="expand" for="c-37774963">[1 more]</label></div><br/><div class="children"><div class="content">You suggest we under-estimate animal intelligence, which is the same argument I&#x27;m making. The reason we underestimate how hard human intelligence is to solve is that we underestimate the animal part of our reasoning, animals can&#x27;t play chess or do math or write sentences, so when computers do that people assume they are smart like humans. But that assumption hinges on the animal part, like navigating a forest, identifying plants to eat etc, isn&#x27;t a fundamental part of intelligence or that it is easy to solve in comparison.<p>So yeah, underestimating animal intelligence is a large part of it, I agree with you.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37774772" class="c"><input type="checkbox" id="c-37774772" checked=""/><div class="controls bullet"><span class="by">Guvante</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774565">parent</a><span>|</span><a href="#37774628">prev</a><span>|</span><a href="#37774805">next</a><span>|</span><label class="collapse" for="c-37774772">[-]</label><label class="expand" for="c-37774772">[6 more]</label></div><br/><div class="children"><div class="content">Being able to mix what you have seen before isn&#x27;t an accurate representation of any popular model for human intelligence to my knowledge.<p>It is akin to a backronym &quot;well that sounds similar so is the same&quot; while ignoring the real differences.</div><br/><div id="37774780" class="c"><input type="checkbox" id="c-37774780" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774772">parent</a><span>|</span><a href="#37774817">next</a><span>|</span><label class="collapse" for="c-37774780">[-]</label><label class="expand" for="c-37774780">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Being able to mix what you have seen before<p>But humans clearly do that.  It&#x27;s not all we do, but we clearly do that.  And given the impressive results that LLMs produce, my guess is it or something like it, will play a significant role in the AGI of the future.</div><br/><div id="37775378" class="c"><input type="checkbox" id="c-37775378" checked=""/><div class="controls bullet"><span class="by">Guvante</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774780">parent</a><span>|</span><a href="#37774941">next</a><span>|</span><label class="collapse" for="c-37775378">[-]</label><label class="expand" for="c-37775378">[1 more]</label></div><br/><div class="children"><div class="content">I would say LLM is at best a precursor for a memory system. But saying we almost have AGI because we have a template for HDDs isn&#x27;t exactly representative IMHO.</div><br/></div></div><div id="37774941" class="c"><input type="checkbox" id="c-37774941" checked=""/><div class="controls bullet"><span class="by">XorNot</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774780">parent</a><span>|</span><a href="#37775378">prev</a><span>|</span><a href="#37774817">next</a><span>|</span><label class="collapse" for="c-37774941">[-]</label><label class="expand" for="c-37774941">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say possibly it is all we do, we just have a lot more hidden inputs. i.e. we&#x27;re all dealing with inputs from our physical bodies now, and can&#x27;t not be. There&#x27;s a whole bit of social advice about dealing with people which is &quot;remember you may be encountering someone on the worst day of their life&quot; (the idea being, 95+% of the time you shouldn&#x27;t escalate, particularly with strangers).<p>What I do now, and how I think about things is being affected in ways I know are directly related to blood sugar, fatigue, pain (i.e. my legs are a little uncomfortable in a way yours aren&#x27;t, but I moved them just now so they feel better - all unique inputs to me, all always there).<p>But the other problem with the notion is people thinking of the knowledge bounds these systems derive as being &quot;limits&quot; - they&#x27;re not. The whole point of machine learning is they&#x27;re not interpolating between known datapoints, but rather extracting the rules which fit those data points. So stay between the points, and sure - it&#x27;s some type of complex interpolation of seen inputs. But you don&#x27;t <i>have</i> to stay between those datapoints - you can move of either end and extrapolate new ones entirely.</div><br/></div></div></div></div><div id="37774817" class="c"><input type="checkbox" id="c-37774817" checked=""/><div class="controls bullet"><span class="by">lern_too_spel</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774772">parent</a><span>|</span><a href="#37774780">prev</a><span>|</span><a href="#37774805">next</a><span>|</span><label class="collapse" for="c-37774817">[-]</label><label class="expand" for="c-37774817">[2 more]</label></div><br/><div class="children"><div class="content">Whether the model for human intelligence is popular or not isn&#x27;t really relevant. We just care about accuracy. Predicting what comes next (including what will happen if I take some action) is very close to all we do. Models trained on text happen to do surprisingly well because humans often represent predictions of what will come next in words, but it remains to be seen if enough of what it takes to think at a human level is available as text in order to train a human level intelligence purely from text.</div><br/><div id="37775400" class="c"><input type="checkbox" id="c-37775400" checked=""/><div class="controls bullet"><span class="by">Guvante</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774817">parent</a><span>|</span><a href="#37774805">next</a><span>|</span><label class="collapse" for="c-37775400">[-]</label><label class="expand" for="c-37775400">[1 more]</label></div><br/><div class="children"><div class="content">Why doesn&#x27;t passing the Turing Test mean you are sentient is an open question but most agree that it is obvious there is a gap here.<p>&quot;Predicting text&quot; is just a falacy born from applying function logic to humans. &quot;Well if I simplify the problem to a text window obviously they are the same&quot; as if simplifying doesn&#x27;t change anything.<p>The Chinese Room is a good example if you want deeper thought into the distinction between acting and being.</div><br/></div></div></div></div></div></div></div></div><div id="37774805" class="c"><input type="checkbox" id="c-37774805" checked=""/><div class="controls bullet"><span class="by">CapsAdmin</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774565">prev</a><span>|</span><a href="#37774652">next</a><span>|</span><label class="collapse" for="c-37774805">[-]</label><label class="expand" for="c-37774805">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know exactly how humans think or how LLM&#x27;s &quot;predict the next token&quot;, but your argument is just refuting a high level description of behavior in place of a low level description.<p>To me at least, it&#x27;s a bit like saying a car&#x27;s wheel do not spin when stepping on the pedal, it&#x27;s the petrol engine that makes the wheel spin. You can replace petrol engine with an electric engine, and the description about the wheel spinning when stepping on the pedal is still technically correct regardless of any lower level explanation.<p>We don&#x27;t know how to describe the behavior of LLM&#x27;s because of how foreign they are to us right now. Hallucinations are confabulations are meant to describe human behavior of couse, so the downside is it might make us anthropomorphize LLM&#x27;s. However it&#x27;s the best we could come up with I guess.<p>If confabulation is a more precise explanation of behavior then I don&#x27;t see why that&#x27;s a bad idea.</div><br/><div id="37774934" class="c"><input type="checkbox" id="c-37774934" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774805">parent</a><span>|</span><a href="#37774652">next</a><span>|</span><label class="collapse" for="c-37774934">[-]</label><label class="expand" for="c-37774934">[3 more]</label></div><br/><div class="children"><div class="content">We do know how to describe their behaviour. It&#x27;s in terms of plausibility. The central dogma of LLM-as-chatbot-assistant is that at sufficient scale, plausibility converges towards accuracy, and becomes a proxy for utility.<p>This is not proven, but that is incidental to the deeper issue.<p>The deep issue is that when a &quot;conversation&quot; kicks off with the LLM being confidently incorrect, the most plausible continuation to take from human literature, and from a good deal of human interaction, is that they <i>continue to be wrong</i>.</div><br/><div id="37775051" class="c"><input type="checkbox" id="c-37775051" checked=""/><div class="controls bullet"><span class="by">olalonde</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774934">parent</a><span>|</span><a href="#37774652">next</a><span>|</span><label class="collapse" for="c-37775051">[-]</label><label class="expand" for="c-37775051">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s that deep of an issue, or at least OpenAI seems to know how to fix it. ChatGPT often tell me when it&#x27;s wrong and it also often recognizes that it has made a mistake when prompted (e.g. &quot;are you sure?&quot;).</div><br/><div id="37775072" class="c"><input type="checkbox" id="c-37775072" checked=""/><div class="controls bullet"><span class="by">inopinatus</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37775051">parent</a><span>|</span><a href="#37774652">next</a><span>|</span><label class="collapse" for="c-37775072">[-]</label><label class="expand" for="c-37775072">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I too have seen ChatGPT issue an apology. Then it gives a worse version of its previous answer, buggier code etc.<p>And it&#x27;s because <i>the most plausible</i> continuation of that interaction is now of an ongoing conversation with an <i>apologetic incompetent</i>.<p>Do not anthropomorphize the LLM. It does not have mental state. It does not have feelings. It does not have a suddenly realised goal of &quot;doing better&quot;. The written apology was merely the most plausible text to issue at that point. It will then go on to simulate being wrong again, which I&#x27;ve come to recognise is <i>more plausible</i> than an idiot suddenly becoming a wizard.<p>In such circumstances, the actual best remedy is to start afresh with a revised prompt.</div><br/></div></div></div></div></div></div></div></div><div id="37774652" class="c"><input type="checkbox" id="c-37774652" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774805">prev</a><span>|</span><a href="#37774820">next</a><span>|</span><label class="collapse" for="c-37774652">[-]</label><label class="expand" for="c-37774652">[1 more]</label></div><br/><div class="children"><div class="content">&gt;They predict the next word in a sequence ...<p>Every time my mind takes this overplayed sequence of words as input, I can&#x27;t help but feel the author refutes their own point...</div><br/></div></div><div id="37774820" class="c"><input type="checkbox" id="c-37774820" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774652">prev</a><span>|</span><a href="#37774629">next</a><span>|</span><label class="collapse" for="c-37774820">[-]</label><label class="expand" for="c-37774820">[4 more]</label></div><br/><div class="children"><div class="content">&gt; they are not like humans at all, and only resemble them superficially<p>We have no idea how LLMs work on a high level, and we have no idea how the human mind works on a high level. Therefore, such claims are rather overconfident. The fact that the two are dissimilar at the plumbing layer doesn&#x27;t mean they cannot be alike in how they &quot;really&quot; operate. Either way, we simply do not know, so any such talk is (bad) philosophy, not science.</div><br/><div id="37774999" class="c"><input type="checkbox" id="c-37774999" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774820">parent</a><span>|</span><a href="#37774629">next</a><span>|</span><label class="collapse" for="c-37774999">[-]</label><label class="expand" for="c-37774999">[3 more]</label></div><br/><div class="children"><div class="content">&gt; We have no idea how LLMs work on a high level<p>This is just incredibly wrong. Do you think that LLM&#x27;s were created out of thin air? A random combination of bytes that we happened to discover that we also happen to be improving&#x2F;changing?</div><br/><div id="37775103" class="c"><input type="checkbox" id="c-37775103" checked=""/><div class="controls bullet"><span class="by">frabcus</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774999">parent</a><span>|</span><a href="#37775512">next</a><span>|</span><label class="collapse" for="c-37775103">[-]</label><label class="expand" for="c-37775103">[1 more]</label></div><br/><div class="children"><div class="content">We understand the training algorithm, but not the network that was trained.<p>There’s the whole interpretability sub-discipline which has limited but interesting results. Those just indicate even more that we really have no idea how something like GPT-4 works inside its trained model.</div><br/></div></div><div id="37775512" class="c"><input type="checkbox" id="c-37775512" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774999">parent</a><span>|</span><a href="#37775103">prev</a><span>|</span><a href="#37774629">next</a><span>|</span><label class="collapse" for="c-37775512">[-]</label><label class="expand" for="c-37775512">[1 more]</label></div><br/><div class="children"><div class="content">Nobody knows how LLMs work <i>on a high level.</i> There&#x27;s an algorithm that consumes Terabytes of raw text and produces 175 billion parameters, that when used as a statistical model magically reproduce human language. But the conceptual connection between those parameters and what the model can do is a complete mystery.</div><br/></div></div></div></div></div></div><div id="37774629" class="c"><input type="checkbox" id="c-37774629" checked=""/><div class="controls bullet"><span class="by">legutierr</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774820">prev</a><span>|</span><a href="#37775594">next</a><span>|</span><label class="collapse" for="c-37774629">[-]</label><label class="expand" for="c-37774629">[14 more]</label></div><br/><div class="children"><div class="content">&gt; The answer to &quot;What are LLMs but humans with extreme amnesia and no central coherence?&quot; is that they are not like humans at all, and only resemble them superficially.<p>Is there any chance at all that we humans are are also just predicting the next word in the sentence?  That the &quot;voice in your head&quot; that so many people report having is itself some kind of LLM that is, to some extent, driving their reasoning?</div><br/><div id="37774656" class="c"><input type="checkbox" id="c-37774656" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774629">parent</a><span>|</span><a href="#37775394">next</a><span>|</span><label class="collapse" for="c-37774656">[-]</label><label class="expand" for="c-37774656">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Is there any chance at all that we humans are are also just predicting the next word in the sentence<p>No, humans live in a world and use their intelligence to manipulate the world. When we generate word sequence we usually generate those to communicate information about the world, not to try to parrot what others have said. Humans can use their intelligence to parrot what others say, many do, but we know for sure it is not all humans do.</div><br/><div id="37774804" class="c"><input type="checkbox" id="c-37774804" checked=""/><div class="controls bullet"><span class="by">legutierr</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774656">parent</a><span>|</span><a href="#37774793">next</a><span>|</span><label class="collapse" for="c-37774804">[-]</label><label class="expand" for="c-37774804">[3 more]</label></div><br/><div class="children"><div class="content">This seems like circular reasoning to me.<p>&quot;Human&#x27;s don&#x27;t just parrot what other people say, therefore we must be doing something more than what LLMs do, because an LLM only parrots the words that it is trained on.  But an LLM is not truly intelligent because it is not doing what humans are doing—and because the LLM is not intelligent, all it is doing is parroting the training data.&quot;<p>Where in this line of reasoning is it proved that the mechanisms are different?<p>What if, instead, the conscious intelligence that differentiates humans from other animals is an emergent quality of language learning, specifically?  Isn&#x27;t there evidence that children that don&#x27;t have access to language—deaf children who are not taught sign language, or feral children, or seriously neglected children—often cannot reason before they are taught language, and suffer from long-term cognitive impairment for having lacked language during their early development?  Wouldn&#x27;t this point to a fundamental linkage between language and human intelligence?</div><br/><div id="37774907" class="c"><input type="checkbox" id="c-37774907" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774804">parent</a><span>|</span><a href="#37774921">next</a><span>|</span><label class="collapse" for="c-37774907">[-]</label><label class="expand" for="c-37774907">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What if, instead, the conscious intelligence that differentiates humans from other animals is an emergent quality of language learning, specifically<p>Sure, you can argue that you have solved the human exclusive part of intelligence, that is a possibility. But we have not yet solved the monkey part of intelligence, the part that all mammals possess that lets them act intelligently in this world. Without such animal intelligence I believe it is impossible for the model to not make really stupid mistakes no human would make, because it lacks the intuitive understanding of the world all animals has.<p>I don&#x27;t think training on text will ever produce that level of understanding, no matter how hard you try, text just isn&#x27;t the right medium to build an intuitive understanding of reality like a dog has.</div><br/></div></div><div id="37774921" class="c"><input type="checkbox" id="c-37774921" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774804">parent</a><span>|</span><a href="#37774907">prev</a><span>|</span><a href="#37774793">next</a><span>|</span><label class="collapse" for="c-37774921">[-]</label><label class="expand" for="c-37774921">[1 more]</label></div><br/><div class="children"><div class="content">At least I used to think that, but LLMs are basically a very unexpected (to me, at least) counterexample to that theory:<p>Formulating plausible <i>sounding</i> sentences seems to be possible without having a much deeper world model.<p>&gt; What if, instead, the conscious intelligence that differentiates humans from other animals is an emergent quality of language learning, specifically?<p>There’s various linguistic theses claiming similar things (“human minds are wired for language”, i.e. Chomsky’s universal grammar, and conversely “language shapes general cognition”, i.e. Sapir-Whorf).<p>At least in the light of LLMs (if not long before), I think neither are actually still serious possibilities&#x2F;useful models of the relationship between language and cognition.</div><br/></div></div></div></div><div id="37774793" class="c"><input type="checkbox" id="c-37774793" checked=""/><div class="controls bullet"><span class="by">kdmccormick</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774656">parent</a><span>|</span><a href="#37774804">prev</a><span>|</span><a href="#37774833">next</a><span>|</span><label class="collapse" for="c-37774793">[-]</label><label class="expand" for="c-37774793">[1 more]</label></div><br/><div class="children"><div class="content">I fail to see how this makes us different from LLMs.<p>LLMs are a bunch of computation running on silicon in order to predict the next word, and out of that all sorts of behavior arises, including intelligence, insigt,creativity, humor, and confabluation.<p>Human brains are a bunch of computation running on carbon in order to &lt;&lt;X&gt;&gt;, and out of that all sorts of behavior arises, including intelligence, insigt,creativity, humor, and confabluation.<p>We don&#x27;t really know what &lt;&lt;X&gt;&gt; is. Why couldn&#x27;t &lt;&lt;X&gt;&gt; be prediction as well?</div><br/></div></div><div id="37774833" class="c"><input type="checkbox" id="c-37774833" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774656">parent</a><span>|</span><a href="#37774793">prev</a><span>|</span><a href="#37774943">next</a><span>|</span><label class="collapse" for="c-37774833">[-]</label><label class="expand" for="c-37774833">[5 more]</label></div><br/><div class="children"><div class="content">&gt; When we generate word sequence we usually generate those to communicate information about the world, not to try to parrot what others have said.<p>What is the evidence supporting that claim?<p>I grant you that it <i>feels</i> that way to humans when they speak, but that doesn&#x27;t prove anything.</div><br/><div id="37774872" class="c"><input type="checkbox" id="c-37774872" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774833">parent</a><span>|</span><a href="#37774943">next</a><span>|</span><label class="collapse" for="c-37774872">[-]</label><label class="expand" for="c-37774872">[4 more]</label></div><br/><div class="children"><div class="content">If your mom says &quot;there is milk in the fridge&quot;, do you think she believes there is milk in the fridge and wants you to know that, or do you think she just parrots something she heard about milk and fridges on the radio or somewhere with no intent or basis in reality? Not sure what your argument is really.</div><br/><div id="37774955" class="c"><input type="checkbox" id="c-37774955" checked=""/><div class="controls bullet"><span class="by">diogenes4</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774872">parent</a><span>|</span><a href="#37774943">next</a><span>|</span><label class="collapse" for="c-37774955">[-]</label><label class="expand" for="c-37774955">[3 more]</label></div><br/><div class="children"><div class="content">I have no clue what you&#x27;re talking about, but there&#x27;s fundamentally no evidence our thoughts are the decisive events leading to behavior rather than, say, a rationalization of said behavior. You&#x27;re too bold in your modeling of human perception.</div><br/><div id="37775494" class="c"><input type="checkbox" id="c-37775494" checked=""/><div class="controls bullet"><span class="by">p-e-w</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774955">parent</a><span>|</span><a href="#37774985">next</a><span>|</span><label class="collapse" for="c-37775494">[-]</label><label class="expand" for="c-37775494">[1 more]</label></div><br/><div class="children"><div class="content">&gt; there&#x27;s fundamentally no evidence our thoughts are the decisive events leading to behavior rather than, say, a rationalization of said behavior<p>In fact, there is plenty of evidence for the latter, such as experiments where people whose motor cortex was stimulated electrically made up all sorts of random explanations for why their body moved, including, crucially, that they <i>intended</i> for that motion to happen.</div><br/></div></div><div id="37774985" class="c"><input type="checkbox" id="c-37774985" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774955">parent</a><span>|</span><a href="#37775494">prev</a><span>|</span><a href="#37774943">next</a><span>|</span><label class="collapse" for="c-37774985">[-]</label><label class="expand" for="c-37774985">[1 more]</label></div><br/><div class="children"><div class="content">You seem to think we are talking about sentience, but this is a discussion about intelligence. Don&#x27;t confuse the two.</div><br/></div></div></div></div></div></div></div></div><div id="37774943" class="c"><input type="checkbox" id="c-37774943" checked=""/><div class="controls bullet"><span class="by">diogenes4</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774656">parent</a><span>|</span><a href="#37774833">prev</a><span>|</span><a href="#37775394">next</a><span>|</span><label class="collapse" for="c-37774943">[-]</label><label class="expand" for="c-37774943">[2 more]</label></div><br/><div class="children"><div class="content">&gt; No, humans live in a world and use their intelligence to manipulate the world.<p>An extremely bold claim! good luck demonstrating it.</div><br/><div id="37775063" class="c"><input type="checkbox" id="c-37775063" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774943">parent</a><span>|</span><a href="#37775394">next</a><span>|</span><label class="collapse" for="c-37775063">[-]</label><label class="expand" for="c-37775063">[1 more]</label></div><br/><div class="children"><div class="content">That claim is the basis to basically all other claims, it is fundamental to such a degree that questioning is in the realms of philosophy and not science. If it is false then just about everything humanity believes and knows is false.</div><br/></div></div></div></div></div></div></div></div><div id="37775594" class="c"><input type="checkbox" id="c-37775594" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774629">prev</a><span>|</span><a href="#37774643">next</a><span>|</span><label class="collapse" for="c-37775594">[-]</label><label class="expand" for="c-37775594">[1 more]</label></div><br/><div class="children"><div class="content">This is a beautiful summary of what happens in reality with these statistical algorithms applied to digitized representations of human language.<p>But as you can see in the comments, people simply do not want to internalize the mundane truth. Confabulating fake AGI narratives and hallucinating AI supremacy is more fun (also more profitable).</div><br/></div></div><div id="37774643" class="c"><input type="checkbox" id="c-37774643" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37775594">prev</a><span>|</span><a href="#37774585">next</a><span>|</span><label class="collapse" for="c-37774643">[-]</label><label class="expand" for="c-37774643">[2 more]</label></div><br/><div class="children"><div class="content">&quot;They predict the next word in a sequence based on correlations derived from a lot of training data.&quot;<p>This misinformation really needs to finally die.<p>They are <i>trained</i> based on correlations of next token prediction. But that does not mean that the resulting neural network is only doing that.<p>When Harvard and MIT researchers fed Othello moves into GPT, it was training based on the correlations of legal Othello moves.<p>But the version of the NN that performed the best at that task had created within that network dedicated structure representing a legal Othello board and tracking the state of moves - neither of which were things explicitly in the training data or goals the training was directly measuring.<p>You effectively came to exist by a training process of surviving to reproduce. But it wouldn&#x27;t exactly be accurate to say that as complexity increased, the only thing you remain capable of is surviving to reproduce, even if many of your emergent capabilities happen to improve the success of being able to do so.<p>Training =&#x2F;= Operation</div><br/><div id="37775903" class="c"><input type="checkbox" id="c-37775903" checked=""/><div class="controls bullet"><span class="by">machiaweliczny</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774643">parent</a><span>|</span><a href="#37774585">next</a><span>|</span><label class="collapse" for="c-37775903">[-]</label><label class="expand" for="c-37775903">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, i think that neural networks learn because they can represent if&#x2F;else relation (via step function (activation)) and that makes them learn good heuristics at least. 
I wonder if someone can prove that NN can generate arbitrary programs because as we know 2 layer NNs can approxomate any calculation. Given some state and how easy is to create turing complete machine i think they already might have it.<p>I speculate that the trick is that these programs sometimes dont evaluate in time when response is short. That’s why LLMs should be given way to adjust output and something like stop signal. Adjusting output can be done using DiffusER approach but not sure how to handle stop signal. I guess we could try to add fake input tokens for silience&#x2F;thinking.</div><br/></div></div></div></div><div id="37774585" class="c"><input type="checkbox" id="c-37774585" checked=""/><div class="controls bullet"><span class="by">krapp</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774643">prev</a><span>|</span><a href="#37774673">next</a><span>|</span><label class="collapse" for="c-37774585">[-]</label><label class="expand" for="c-37774585">[2 more]</label></div><br/><div class="children"><div class="content">Yes, this is the problem. We have to be able to efficiently describe what LLMs do and how they do it, when what they do is superficially familiar, but <i>how</i> is fundamentally alien. We haven&#x27;t yet developed the necessary language to discuss them on their own terms.<p>Anthropomorphism and resorting to metaphors like &quot;hallucinate&quot; and &quot;confabulate&quot; are inevitable if you don&#x27;t want to have to preface every comment with a paragraph of technical discussion. They get the necessary point across which is that the &quot;reality&quot; LLMs construct is not necessarily tethered to actual reality. They&#x27;re deceptively convincing but can&#x27;t be trusted.</div><br/><div id="37774669" class="c"><input type="checkbox" id="c-37774669" checked=""/><div class="controls bullet"><span class="by">HybridCurve</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774585">parent</a><span>|</span><a href="#37774673">next</a><span>|</span><label class="collapse" for="c-37774669">[-]</label><label class="expand" for="c-37774669">[1 more]</label></div><br/><div class="children"><div class="content">I fully agree with this perspective. The terminology will change as the field continues to evolve. As long as any anthropomorphizing terms are chosen carefully and are not aggrandizing, it shouldn&#x27;t be a problem. IIRC &quot;hallucinate&quot; was a term previously used to describe characteristics other network types such as RBMs and had just been carried over to LLMs.</div><br/></div></div></div></div><div id="37774673" class="c"><input type="checkbox" id="c-37774673" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774585">prev</a><span>|</span><a href="#37774843">next</a><span>|</span><label class="collapse" for="c-37774673">[-]</label><label class="expand" for="c-37774673">[3 more]</label></div><br/><div class="children"><div class="content">Most of the people who repeat this have hardly any background in neuroscience... so they actually have no clue at all what the process they&#x27;re claiming LLMs don&#x27;t mirror is.<p>Meanwhile the people who <i>are</i> in neuroscience have posited a Bayesian model to human cognitive function as far back as the 1800s, well before any convenient targets for anthropomorphization existed, and it continued as an area of study until today: <a href="https:&#x2F;&#x2F;global.oup.com&#x2F;academic&#x2F;product&#x2F;bayesian-rationality-9780198524496" rel="nofollow noreferrer">https:&#x2F;&#x2F;global.oup.com&#x2F;academic&#x2F;product&#x2F;bayesian-rationality...</a><p>Essentially, in typical techie fashion people with barely any understanding of a topic assume to have mastery ahead of people are the forefront of a non-tech field.<p>_<p>This a sort of objectification seems equally as rooted in emotion and feelings as the anthropropmization crowd: We don&#x27;t actually know the answer to the question of if there is some layer of human cognitive function that mirrors LLMs, but it&#x27;d cause me great cognitive dissonance&#x2F;discomfort if the answer is not &lt;insert preferred answer&gt; so I will vehemently fight against any claims to the contrary.<p>To me the answer is to simply accept we don&#x27;t know, and take whatever tools let us make progress on working with the LLM. If that&#x27;s borrowing words like &quot;reasoning&quot; and &quot;understanding&quot; and &quot;confabulate&quot; so be it, it&#x27;s of no loss to anyone.</div><br/><div id="37775032" class="c"><input type="checkbox" id="c-37775032" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37774673">parent</a><span>|</span><a href="#37774843">next</a><span>|</span><label class="collapse" for="c-37775032">[-]</label><label class="expand" for="c-37775032">[2 more]</label></div><br/><div class="children"><div class="content">You fundamentally do not understand the works you are linking to and it&#x27;s hilarious that you would use it as a way of saying that neuroscientists think otherwise.<p>I would note for other readers that the commenter didn&#x27;t go to college for neuroscience, or any science, based on their previous comments.</div><br/><div id="37775138" class="c"><input type="checkbox" id="c-37775138" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#37774522">root</a><span>|</span><a href="#37775032">parent</a><span>|</span><a href="#37774843">next</a><span>|</span><label class="collapse" for="c-37775138">[-]</label><label class="expand" for="c-37775138">[1 more]</label></div><br/><div class="children"><div class="content">You didn&#x27;t understand what I said, what the link implied, or how they were related, yet you forced a reply.<p>I hope that your insecurity was patched over at least as long as it took to write the comment: that might partially offset the time anyone else takes to read it.</div><br/></div></div></div></div></div></div><div id="37774843" class="c"><input type="checkbox" id="c-37774843" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774673">prev</a><span>|</span><a href="#37775198">next</a><span>|</span><label class="collapse" for="c-37774843">[-]</label><label class="expand" for="c-37774843">[1 more]</label></div><br/><div class="children"><div class="content">Confabulation in humans seems much like word&#x2F;sequence completion, autocompleting from gist information without reality monitoring, executive control, etc. Likewise, introducing output monitoring that can identify signals of recall failure, and executive control (stop signals), probably could help LLMs too.<p>One of my research chapters covered some of these ideas, but the wikipedia does pretty well too.
<a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Confabulation" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Confabulation</a></div><br/></div></div><div id="37775198" class="c"><input type="checkbox" id="c-37775198" checked=""/><div class="controls bullet"><span class="by">Falkon1313</span><span>|</span><a href="#37774522">parent</a><span>|</span><a href="#37774843">prev</a><span>|</span><a href="#37775749">next</a><span>|</span><label class="collapse" for="c-37775198">[-]</label><label class="expand" for="c-37775198">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly it, and that is what is so hard to get other people to understand. It&#x27;s a random word generator. It can&#x27;t &#x27;hallucinate&#x27; or &#x27;confabulate&#x27; anymore than dice can. It cannot possibly be wrong or incorrect.<p>Those of us who&#x27;ve played D&amp;D or something similar are familiar with rolling dice, say a 10-sided die and a 6-sided die, and cross-referencing the results against a table to generate random monster encounters, treasure, etc.<p>If you roll a die to determine how many gold pieces an orc is carrying, and you roll a 5, that is the result. You don&#x27;t claim that the dice was hallucinating or confabulating or got the facts wrong or whatever. It&#x27;s a random generator, and it generated a random number, and that result is truthfully the result it generated.<p>LLMs are also random generators. They&#x27;re not databases of facts, they&#x27;re not mathematical calculators. They are like the D&amp;D random generation tables, only developed deeply enough such that they can string together randomly generated words into coherent sentences and paragraphs.<p>That sentence&#x2F;paragraph is just as true as the result of rolling some dice. If you roll an 11, then you rolled an 11. The dice didn&#x27;t hallucinate it, confabulate it, lie about it, or get it wrong. The true result is that you rolled an 11. If you wanted to roll a 12, too bad. You were unlucky. The dice weren&#x27;t somehow magically &#x27;wrong&#x27;.<p>And an LLM is not somehow magically wrong or incorrect or hallucinating or even confabulating when it gives you an answer that isn&#x27;t what you wanted. It&#x27;s randomly generating text. And its answer is every bit as correct as the result of rolling dice or drawing a card.</div><br/></div></div></div></div><div id="37775749" class="c"><input type="checkbox" id="c-37775749" checked=""/><div class="controls bullet"><span class="by">fbnbr</span><span>|</span><a href="#37774522">prev</a><span>|</span><a href="#37774287">next</a><span>|</span><label class="collapse" for="c-37775749">[-]</label><label class="expand" for="c-37775749">[2 more]</label></div><br/><div class="children"><div class="content">“What are LLMs but humans with extreme amnesia and no central coherence?”<p>LLMs are nothing alike to compare them with anything close to what’s going on in a brain. It’s all probabilistic and the closest comparison I can think of is comparing a bird with a plane - both fly, but well…</div><br/><div id="37776091" class="c"><input type="checkbox" id="c-37776091" checked=""/><div class="controls bullet"><span class="by">adlpz</span><span>|</span><a href="#37775749">parent</a><span>|</span><a href="#37774287">next</a><span>|</span><label class="collapse" for="c-37776091">[-]</label><label class="expand" for="c-37776091">[1 more]</label></div><br/><div class="children"><div class="content">Care to elaborate? Is it well established that brains do not work at any level probabilistically?<p>I understand the mid-level abstraction is clearly different between LLMs and brains. The whole made of meat vs wires, etc, to start with.<p>But, honestly asking. I can&#x27;t see how they can differ in a fundamental level in the specific regard of probability. Either both are probabilistic, be it explicitly in its construction or at a fundamental (quantum?) level, or deterministic again at either level, if we go with the mecanicist vibe.<p>Am I missing something?</div><br/></div></div></div></div><div id="37774287" class="c"><input type="checkbox" id="c-37774287" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#37775749">prev</a><span>|</span><a href="#37775514">next</a><span>|</span><label class="collapse" for="c-37774287">[-]</label><label class="expand" for="c-37774287">[10 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree with the author&#x27;s argument, because LLMs sometimes give wrong information even though they have access to the correct information. For example, querying an LLM about a fictional story will often elicit false answers. But if you respond with &#x27;x is incorrect, now tell me what actually happened&#x27; you&#x27;ll often get an apology and the correct answer. My informal sense is that this happens more with how&#x2F;why questions than who&#x2F;what&#x2F;where&#x2F;when ones.</div><br/><div id="37774365" class="c"><input type="checkbox" id="c-37774365" checked=""/><div class="controls bullet"><span class="by">JoeAltmaier</span><span>|</span><a href="#37774287">parent</a><span>|</span><a href="#37774434">next</a><span>|</span><label class="collapse" for="c-37774365">[-]</label><label class="expand" for="c-37774365">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had a different experience - LLMs that continue to apologize then make up other wrong shit in a chain of ludicrous wrong shit.<p>I think of LLMs as world-class bullshitters. They often hold forth with complete confidence on topics they have little or no understanding of, making up what they don&#x27;t know, and appearing to be experts. Except to somebody who actually knows something.<p>Like that blowhard friend who has an opinion on anything.</div><br/><div id="37774429" class="c"><input type="checkbox" id="c-37774429" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774365">parent</a><span>|</span><a href="#37774798">next</a><span>|</span><label class="collapse" for="c-37774429">[-]</label><label class="expand" for="c-37774429">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true, but it&#x27;s not at odds with my comment. I&#x27;m not making claims about LLMs being reliably accurate, but disagreeing with the blogger&#x27;s suggestions about why they give BS answers.</div><br/></div></div><div id="37774798" class="c"><input type="checkbox" id="c-37774798" checked=""/><div class="controls bullet"><span class="by">chongli</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774365">parent</a><span>|</span><a href="#37774429">prev</a><span>|</span><a href="#37774710">next</a><span>|</span><label class="collapse" for="c-37774798">[-]</label><label class="expand" for="c-37774798">[1 more]</label></div><br/><div class="children"><div class="content"><i>Like that blowhard friend who has an opinion on anything.</i><p>Yes. I just keep getting reminded of the episode of TNG when Data learns to smalltalk by mimicking an infamous blowhard from Starfleet [1].<p>[1] <a href="https:&#x2F;&#x2F;youtu.be&#x2F;9FqFm_vmVnE" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;9FqFm_vmVnE</a></div><br/></div></div><div id="37774710" class="c"><input type="checkbox" id="c-37774710" checked=""/><div class="controls bullet"><span class="by">cwalv</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774365">parent</a><span>|</span><a href="#37774798">prev</a><span>|</span><a href="#37774474">next</a><span>|</span><label class="collapse" for="c-37774710">[-]</label><label class="expand" for="c-37774710">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never had one give correct information after being told it&#x27;s incorrect, unless it was prompted with the correct info. It&#x27;s usually the same incorrect answer stated slightly differently, or after a few complaints, stated exactly the same way.</div><br/></div></div><div id="37774474" class="c"><input type="checkbox" id="c-37774474" checked=""/><div class="controls bullet"><span class="by">staplers</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774365">parent</a><span>|</span><a href="#37774710">prev</a><span>|</span><a href="#37774434">next</a><span>|</span><label class="collapse" for="c-37774474">[-]</label><label class="expand" for="c-37774474">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s trained on social media users.. so that tracks.</div><br/><div id="37774698" class="c"><input type="checkbox" id="c-37774698" checked=""/><div class="controls bullet"><span class="by">cwalv</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774474">parent</a><span>|</span><a href="#37774434">next</a><span>|</span><label class="collapse" for="c-37774698">[-]</label><label class="expand" for="c-37774698">[2 more]</label></div><br/><div class="children"><div class="content">Hn is social media</div><br/><div id="37774840" class="c"><input type="checkbox" id="c-37774840" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774698">parent</a><span>|</span><a href="#37774434">next</a><span>|</span><label class="collapse" for="c-37774840">[-]</label><label class="expand" for="c-37774840">[1 more]</label></div><br/><div class="children"><div class="content">Precisely!</div><br/></div></div></div></div></div></div></div></div><div id="37774434" class="c"><input type="checkbox" id="c-37774434" checked=""/><div class="controls bullet"><span class="by">NikolaNovak</span><span>|</span><a href="#37774287">parent</a><span>|</span><a href="#37774365">prev</a><span>|</span><a href="#37775514">next</a><span>|</span><label class="collapse" for="c-37774434">[-]</label><label class="expand" for="c-37774434">[2 more]</label></div><br/><div class="children"><div class="content">I started with extremely low confidence in LLM, so I&#x27;ve only every asked simple questions on well-covered topics; it hasn&#x27;t really steered me wrong yet but again that&#x27;s because of deliberately very conservative use.<p>(Genuine) Question then: Does &quot;No that&#x27;s wrong&quot;... every <i>do</i> something? Is there any reason to do so other than having fun arguing? I.e. if it gives wrong answer initially, and the user <i>knows</i> it&#x27;s the wrong answer... what&#x27;s the goal&#x2F;desired outcome in telling it it&#x27;s wrong and asking it again? Does that goal every get accomplished? What is the confidence in the second answer?<p>From a less black box perspective, is there any &quot;ELI5&quot; or at least &quot;ELI45&quot; explanation of why it might ever work? :)</div><br/><div id="37774465" class="c"><input type="checkbox" id="c-37774465" checked=""/><div class="controls bullet"><span class="by">keenmaster</span><span>|</span><a href="#37774287">root</a><span>|</span><a href="#37774434">parent</a><span>|</span><a href="#37775514">next</a><span>|</span><label class="collapse" for="c-37774465">[-]</label><label class="expand" for="c-37774465">[1 more]</label></div><br/><div class="children"><div class="content">There are topics where GPT-4 is more likely to be right than wrong. So when it is occasionally wrong and you happen to know to correct it, it has a more correct foundation upon which to provide further answers. This only really works when you already have expertise on the topic and use GPT-4 for inspiration and to work more quickly.</div><br/></div></div></div></div></div></div><div id="37775514" class="c"><input type="checkbox" id="c-37775514" checked=""/><div class="controls bullet"><span class="by">dazzaji</span><span>|</span><a href="#37774287">prev</a><span>|</span><a href="#37774849">next</a><span>|</span><label class="collapse" for="c-37775514">[-]</label><label class="expand" for="c-37775514">[4 more]</label></div><br/><div class="children"><div class="content">I couldn’t help but ask Claude 2 for some better words to name this phenomenon. The response was fascinating:<p>“Here are some alternative words or concepts that could potentially provide a more nuanced or accurate description of the phenomenon compared to solely using &quot;confabulation&quot;:<p>- Imputation: Generating explanatory responses by imputing or attributing plausible details, characteristics, or causal relationships even when factually incorrect.<p>- Speculation: The responses reflect a type of informed speculation when direct factual knowledge is lacking, as opposed to confabulation which often implies a pathological or impaired process.<p>- Heuristic reasoning: The behavior emerges from the model&#x27;s trained heuristic techniques for reasoning and responding rather than any psychological process like confabulation per se.<p>- Plausible surmise: The responses take the form of plausible but unfounded surmises put forth due to the system design and limitations rather than an intent to confabulate.<p>- Bridging inference: The model draws bridging or gap-filling inferences to connect related concepts or complete a response, even if factually ungrounded.<p>- Suppositional fabrication: The language hints more strongly that the responses are fabricated suppositions not intended to deliberately deceive but still warranting cautious interpretation.<p>- Educated approximation: The responses could be viewed as a kind of educated approximation or speculation given training-imposed response pressures rather than confabulation due to impairment.<p>Designating it as solely &quot;confabulation&quot; may over-attribute psychological realism without addressing countervailing technical factors.”<p>Personally, I thought “Suppositional fabrication” came closest. Maybe just fabrication would work.</div><br/><div id="37775760" class="c"><input type="checkbox" id="c-37775760" checked=""/><div class="controls bullet"><span class="by">trashtester</span><span>|</span><a href="#37775514">parent</a><span>|</span><a href="#37775544">next</a><span>|</span><label class="collapse" for="c-37775760">[-]</label><label class="expand" for="c-37775760">[1 more]</label></div><br/><div class="children"><div class="content">The way I think of the phenomenon, neural networks interpolate. Facts form only points in this space, while the network weights represents the patterns of the points by &quot;curves&quot;. They are so good at it, though, that it almost seem like they remember each point.<p>Their ability to generalize, though, comes from representing the learning as these &quot;curves&quot;.<p>Sometimes they produce generalizations that are poor or false. Even when they are false, though, they may be good.<p>In fact, Stephen Wolfram is claiming that when GPT-4 &quot;halucinates&quot; a function name for the Wolfram Language, the team often agrees with GPT-4 that precisely that call SHOULD be part of the language, so they go on and implement it AFTER it came up as a halucination.<p>One might (and I would) argue that this behavior is precisely what most human creativity (in technical domains) is about.</div><br/></div></div><div id="37775544" class="c"><input type="checkbox" id="c-37775544" checked=""/><div class="controls bullet"><span class="by">intended</span><span>|</span><a href="#37775514">parent</a><span>|</span><a href="#37775760">prev</a><span>|</span><a href="#37774849">next</a><span>|</span><label class="collapse" for="c-37775544">[-]</label><label class="expand" for="c-37775544">[2 more]</label></div><br/><div class="children"><div class="content">Approximation.<p>Imputation, Reasoning, Fabrication will end up sending people down the anthropomorphization rabbit hole.<p>Approximation is &quot;good enough&quot;.</div><br/><div id="37775712" class="c"><input type="checkbox" id="c-37775712" checked=""/><div class="controls bullet"><span class="by">ImHereToVote</span><span>|</span><a href="#37775514">root</a><span>|</span><a href="#37775544">parent</a><span>|</span><a href="#37774849">next</a><span>|</span><label class="collapse" for="c-37775712">[-]</label><label class="expand" for="c-37775712">[1 more]</label></div><br/><div class="children"><div class="content">I think some anthropomorphizing is apt. It is trained on anthropic data from the anthropocene after all.</div><br/></div></div></div></div></div></div><div id="37774849" class="c"><input type="checkbox" id="c-37774849" checked=""/><div class="controls bullet"><span class="by">lemonish97</span><span>|</span><a href="#37775514">prev</a><span>|</span><a href="#37774633">next</a><span>|</span><label class="collapse" for="c-37774849">[-]</label><label class="expand" for="c-37774849">[1 more]</label></div><br/><div class="children"><div class="content">Fun fact, it is believed that Andrej Karpathy (OpenAI, ex AI Dir. at Tesla) had coined the word &#x27;hallucination&#x27; in an RNN blogpost back in 2015.
<a href="https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1702916988891193460?s=20" rel="nofollow noreferrer">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1702916988891193460?s=20</a></div><br/></div></div><div id="37774633" class="c"><input type="checkbox" id="c-37774633" checked=""/><div class="controls bullet"><span class="by">js8</span><span>|</span><a href="#37774849">prev</a><span>|</span><a href="#37775923">next</a><span>|</span><label class="collapse" for="c-37774633">[-]</label><label class="expand" for="c-37774633">[1 more]</label></div><br/><div class="children"><div class="content">I agree it should have been called confabulation but it will be difficult to change. However, it might also not matter in the following sense. One thing that distinguishes humans is that we have a capacity to recognize whether the information comes from external world (is what we perceive as real world) or from our own thoughts (is what we call dreams). I don&#x27;t think current crop of LLMs can actually have that distinction, architecturally. So whether the incogruencies with reality happen on the input (hallucination) or the output (confabulation) doesn&#x27;t really matter, because, from the perspective of LLM, the internal model of the world and the reality of the world are the same thing.<p>I think that&#x27;s also a reason why it cannot handle incongruencies in its own mental model; our mental capacity is not only to accept the reality as given (i.e. reality takes precedence over mental model, we recognize that something is just our imagination), but also to impose our model to reality (i.e. we determine that sensors are misleading us due to incongruency with our mental model, and override them as faulty data). But unlike LLMs, humans can internally detect when we do one or the other. Humans understand the distinction between pondering an action and making an action, LLMs don&#x27;t. LLMs can detect something is amiss (and apologize) but they can&#x27;t resolve it, because reality and model are the same for them.</div><br/></div></div><div id="37775923" class="c"><input type="checkbox" id="c-37775923" checked=""/><div class="controls bullet"><span class="by">Borrible</span><span>|</span><a href="#37774633">prev</a><span>|</span><a href="#37774570">next</a><span>|</span><label class="collapse" for="c-37775923">[-]</label><label class="expand" for="c-37775923">[1 more]</label></div><br/><div class="children"><div class="content">LLMs answer what unfoundly self-convinend people answer when you ask them for diretion on the street.
Convincing, not necessarily correct.
They are trained on the “data highway“ mostly. A highway full of that type of people, so that behaviour was to be expected.</div><br/></div></div><div id="37774570" class="c"><input type="checkbox" id="c-37774570" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#37775923">prev</a><span>|</span><a href="#37776066">next</a><span>|</span><label class="collapse" for="c-37774570">[-]</label><label class="expand" for="c-37774570">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it really matters what we call the phenomenon – whatever they&#x27;re doing, they&#x27;re not humans, so whatever human analogy we choose to call it is necessarily imprecise. What does matter is that everyone has a shared vocabulary for what to call hallucination or whatever.</div><br/></div></div><div id="37776066" class="c"><input type="checkbox" id="c-37776066" checked=""/><div class="controls bullet"><span class="by">unhammer</span><span>|</span><a href="#37774570">prev</a><span>|</span><a href="#37774506">next</a><span>|</span><label class="collapse" for="c-37776066">[-]</label><label class="expand" for="c-37776066">[2 more]</label></div><br/><div class="children"><div class="content">Do most people understand exactly what confabulate means?<p>I&#x27;d rather just say that they&#x27;re bullshitting. Everyone knows what it means, <i>and</i> it has a precise academic definition[0] within the field of bullshitology.<p>[0] <a href="https:&#x2F;&#x2F;philosophynow.org&#x2F;issues&#x2F;53&#x2F;On_Bullshit_by_Harry_Frankfurt" rel="nofollow noreferrer">https:&#x2F;&#x2F;philosophynow.org&#x2F;issues&#x2F;53&#x2F;On_Bullshit_by_Harry_Fra...</a></div><br/><div id="37776170" class="c"><input type="checkbox" id="c-37776170" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#37776066">parent</a><span>|</span><a href="#37774506">next</a><span>|</span><label class="collapse" for="c-37776170">[-]</label><label class="expand" for="c-37776170">[1 more]</label></div><br/><div class="children"><div class="content">Bullshitting sounds more intentional.  Ignoring the truth is different from a failure to conceptualize the truth.</div><br/></div></div></div></div><div id="37774506" class="c"><input type="checkbox" id="c-37774506" checked=""/><div class="controls bullet"><span class="by">redindian75</span><span>|</span><a href="#37776066">prev</a><span>|</span><a href="#37774967">next</a><span>|</span><label class="collapse" for="c-37774506">[-]</label><label class="expand" for="c-37774506">[4 more]</label></div><br/><div class="children"><div class="content">I asked Bard to summarize this by pasting the URL.<p><i>It made something up!!</i> (it said the author talked about a poem about cat and mice as an example of confabulation)<p>Then I asked <i>&quot;are u confabulating? because i do not see that quote in the original article&quot;</i><p><pre><code>  You are correct. I confabulated that information. I apologize for the mistake.I am still under development, and I am always learning. I will try my best to avoid making similar mistakes in the future.Thank you for pointing out my error. It helps me to improve.</code></pre></div><br/><div id="37774709" class="c"><input type="checkbox" id="c-37774709" checked=""/><div class="controls bullet"><span class="by">wodenokoto</span><span>|</span><a href="#37774506">parent</a><span>|</span><a href="#37774967">next</a><span>|</span><label class="collapse" for="c-37774709">[-]</label><label class="expand" for="c-37774709">[3 more]</label></div><br/><div class="children"><div class="content">I have an impression that if you accuse any of the chat models of being wrong they well tend to confess and apologize.<p>I guess I should spearhead a study, “how right does an LLM have to be, before it will defend its position”.<p>I guess step one is to ask chatgpt how to quantify correctness.</div><br/><div id="37774827" class="c"><input type="checkbox" id="c-37774827" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37774506">root</a><span>|</span><a href="#37774709">parent</a><span>|</span><a href="#37775090">next</a><span>|</span><label class="collapse" for="c-37774827">[-]</label><label class="expand" for="c-37774827">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I have an impression that if you accuse any of the chat models of being wrong they well tend to confess and apologize.<p>Not always. Default Mistral-instruct, for instance, actually seems quite confident.<p>Of course this is just the arbitrary &quot;personality&quot; of the instruct finetune, which you can throw out the window with an initial prompt.</div><br/></div></div><div id="37775090" class="c"><input type="checkbox" id="c-37775090" checked=""/><div class="controls bullet"><span class="by">superjan</span><span>|</span><a href="#37774506">root</a><span>|</span><a href="#37774709">parent</a><span>|</span><a href="#37774827">prev</a><span>|</span><a href="#37774967">next</a><span>|</span><label class="collapse" for="c-37775090">[-]</label><label class="expand" for="c-37775090">[1 more]</label></div><br/><div class="children"><div class="content">I was assuming that it is programmed, or instructed to do so. 
Which thought leads me to wonder whether you can instruct an LLM to weigh how certain it is in how it phrases it’s answer.</div><br/></div></div></div></div></div></div><div id="37774967" class="c"><input type="checkbox" id="c-37774967" checked=""/><div class="controls bullet"><span class="by">SirMaster</span><span>|</span><a href="#37774506">prev</a><span>|</span><a href="#37775516">next</a><span>|</span><label class="collapse" for="c-37774967">[-]</label><label class="expand" for="c-37774967">[6 more]</label></div><br/><div class="children"><div class="content">The question is, why don’t LLMs admit when they don’t know an answer?<p>If they can’t, how do we make them so they can?<p>Plenty of humans seem to have no problem admitting when they don’t know something.</div><br/><div id="37774974" class="c"><input type="checkbox" id="c-37774974" checked=""/><div class="controls bullet"><span class="by">foobarbecue</span><span>|</span><a href="#37774967">parent</a><span>|</span><a href="#37775140">next</a><span>|</span><label class="collapse" for="c-37774974">[-]</label><label class="expand" for="c-37774974">[3 more]</label></div><br/><div class="children"><div class="content">And the answer is: LLMs don&#x27;t know ANY answers. They just know what answers tend to sound like. So it&#x27;s a fundamental problem.</div><br/><div id="37775808" class="c"><input type="checkbox" id="c-37775808" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37774967">root</a><span>|</span><a href="#37774974">parent</a><span>|</span><a href="#37775029">next</a><span>|</span><label class="collapse" for="c-37775808">[-]</label><label class="expand" for="c-37775808">[1 more]</label></div><br/><div class="children"><div class="content">Decidedly not true.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37775805">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37775805</a></div><br/></div></div><div id="37775029" class="c"><input type="checkbox" id="c-37775029" checked=""/><div class="controls bullet"><span class="by">SirMaster</span><span>|</span><a href="#37774967">root</a><span>|</span><a href="#37774974">parent</a><span>|</span><a href="#37775808">prev</a><span>|</span><a href="#37775140">next</a><span>|</span><label class="collapse" for="c-37775029">[-]</label><label class="expand" for="c-37775029">[1 more]</label></div><br/><div class="children"><div class="content">Well I keep seeing people suggest that humans are also really just sophisticated LLMs.<p>If that’s true then this behavior difference (identifying and admitting when you don’t know something) seems like a pretty fundamental difference between humans and LLMs at least at this point.</div><br/></div></div></div></div><div id="37775140" class="c"><input type="checkbox" id="c-37775140" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#37774967">parent</a><span>|</span><a href="#37774974">prev</a><span>|</span><a href="#37775805">next</a><span>|</span><label class="collapse" for="c-37775140">[-]</label><label class="expand" for="c-37775140">[1 more]</label></div><br/><div class="children"><div class="content">In my interactions with GPT-4 recently, it often says that it is unsure or doesn’t know. I got access yesterday to its image-interpretation function. Below are some results in which it expresses uncertainty about what appears in a photograph.<p><a href="https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;20231015gpt4hedging&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;20231015gpt4hedging&#x2F;index.html</a><p>The only confabulations are when reading text. In my tests so far, GPT-4 is not a good OCR engine and does make up a lot.</div><br/></div></div><div id="37775805" class="c"><input type="checkbox" id="c-37775805" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37774967">parent</a><span>|</span><a href="#37775140">prev</a><span>|</span><a href="#37775516">next</a><span>|</span><label class="collapse" for="c-37775805">[-]</label><label class="expand" for="c-37775805">[1 more]</label></div><br/><div class="children"><div class="content">Because they don&#x27;t care to. No seriously that&#x27;s it. Lots of evidence indicating language models know when they hallucinate.<p>GPT-4 logits calibration pre RLHF - [<a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r](https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r)" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r](https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;3gYel9r)</a><p>Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback - [<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975)" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.14975](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305...</a><p>Teaching Models to Express Their Uncertainty in Words - [<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334)" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205.14334](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2205...</a><p>Language Models (Mostly) Know What They Know - [<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221)" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207.05221](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2207...</a><p>TLDR; Hallucinations aren&#x27;t a randomness or representation issue. The computation already knows. It just doesn&#x27;t care about telling you this.</div><br/></div></div></div></div><div id="37775516" class="c"><input type="checkbox" id="c-37775516" checked=""/><div class="controls bullet"><span class="by">jdeaton</span><span>|</span><a href="#37774967">prev</a><span>|</span><a href="#37775049">next</a><span>|</span><label class="collapse" for="c-37775516">[-]</label><label class="expand" for="c-37775516">[1 more]</label></div><br/><div class="children"><div class="content">Hallucinate means false perception which doesn&#x27;t describe what LMs do when they&#x27;re making up plausible sounding things</div><br/></div></div><div id="37775049" class="c"><input type="checkbox" id="c-37775049" checked=""/><div class="controls bullet"><span class="by">beders</span><span>|</span><a href="#37775516">prev</a><span>|</span><a href="#37774680">next</a><span>|</span><label class="collapse" for="c-37775049">[-]</label><label class="expand" for="c-37775049">[1 more]</label></div><br/><div class="children"><div class="content">Both terms are problematic as they anthropomorphize an algorithm.</div><br/></div></div><div id="37774533" class="c"><input type="checkbox" id="c-37774533" checked=""/><div class="controls bullet"><span class="by">Nursie</span><span>|</span><a href="#37774680">prev</a><span>|</span><a href="#37774998">next</a><span>|</span><label class="collapse" for="c-37774533">[-]</label><label class="expand" for="c-37774533">[1 more]</label></div><br/><div class="children"><div class="content">I always took exception to the word “hallucination” with respect to LLMs anyway - they aren’t databases of facts, they’re language models, trained to be really, really good mimics of human communication.<p>As such, while they are trained on an existing corpus that no doubt contains many facts and many falsehoods, their task is to produce more material that looks like this corpus, not be a source of truth.<p>So an LLM “hallucinating” or confabulating is not some aberrant behaviour, it’s fundamental to the nature of the beast.<p>It always worries me when people say they’re using an LLM as a search engine or source of knowledge.</div><br/></div></div><div id="37774998" class="c"><input type="checkbox" id="c-37774998" checked=""/><div class="controls bullet"><span class="by">dylukes</span><span>|</span><a href="#37774533">prev</a><span>|</span><a href="#37774871">next</a><span>|</span><label class="collapse" for="c-37774998">[-]</label><label class="expand" for="c-37774998">[1 more]</label></div><br/><div class="children"><div class="content">&gt; this isn’t actually correct terminology<p>This entire post could have been reduced to this (reasonably correct I think) claim.<p>&gt; if we recognize that what LLMs are really doing is confabulating, we can try to compare and contrast their behaviour with that of humans<p>Except that the goal for the above is hopelessly misguided. If your focus in making any useful claim about LLMs or RNNs&#x2F;GANs more broadly is how similar they are to human behavior you&#x27;re already way off the path.<p>tl;dr reasonable claim, reasonable conclusion, but entirely indefensible and superstitious motivation</div><br/></div></div><div id="37774871" class="c"><input type="checkbox" id="c-37774871" checked=""/><div class="controls bullet"><span class="by">max_entropy</span><span>|</span><a href="#37774998">prev</a><span>|</span><label class="collapse" for="c-37774871">[-]</label><label class="expand" for="c-37774871">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t hallucinate, they just get things wrong.</div><br/></div></div></div></div></div></div></div></body></html>