<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1715245271374" as="style"/><link rel="stylesheet" href="styles.css?v=1715245271374"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://sympathetic.ink/2024/01/24/Chapter-1-The-birth-of-Parquet.html">The Birth of Parquet</a>Â <span class="domain">(<a href="https://sympathetic.ink">sympathetic.ink</a>)</span></div><div class="subtext"><span>whinvik</span> | <span>51 comments</span></div><br/><div><div id="40300795" class="c"><input type="checkbox" id="c-40300795" checked=""/><div class="controls bullet"><span class="by">fizx</span><span>|</span><a href="#40302149">next</a><span>|</span><label class="collapse" for="c-40300795">[-]</label><label class="expand" for="c-40300795">[1 more]</label></div><br/><div class="children"><div class="content">Fun story time:  I was at twitter for a few years and tended to write quick hacks that people wanted to replace with better engineering.<p>We never had scala thrift bindings, and the Java ones were awkward from Scala, so I wrote a thrift plugin in JRuby that used the Ruby thrift parser and ERb web templates to output some Scala code.  Integrated with our build pipeline, worked great for the company.<p>I also wrote one era of twitter&#x27;s service deploy system on a hacked up Capistrano.<p>These projects took a few days because they were dirty hacks, but I still got a below perf review for getting easily distracted, because I didn&#x27;t yet know how to sell those company-wide projects.<p>Anyhow, about a month before that team kicked off Parquet, I showed them a columnar format I made for a hackweek based on Lucene&#x27;s codec packages, and was using to power a mixpanel-alike analytics system.<p>I&#x27;m not sure whether they were inspired or terrified that my hack would reach production, but I like to think I had a small hand in getting Parquet kickstarted.</div><br/></div></div><div id="40302149" class="c"><input type="checkbox" id="c-40302149" checked=""/><div class="controls bullet"><span class="by">pradeepchhetri</span><span>|</span><a href="#40300795">prev</a><span>|</span><a href="#40299364">next</a><span>|</span><label class="collapse" for="c-40302149">[-]</label><label class="expand" for="c-40302149">[2 more]</label></div><br/><div class="children"><div class="content">Reading through this blog, to me it seems Parquet is lot like ClickHouse native data format.<p>Best part of ClickHouse native data format is I can use the same ClickHouse queries and can run in local or remote server&#x2F;cluster and let ClickHouse to decide the available resources in the most performant way.<p>ClickHouse has a native and the fastest integration with Parquet so i can:<p>- Query local&#x2F;s3 parquet data from command line using clickhouse-local.<p>- Query large amount of local&#x2F;s3 data programmatically by offloading it to clickhouse server&#x2F;cluster which can do processing in distributed fashion.</div><br/><div id="40302203" class="c"><input type="checkbox" id="c-40302203" checked=""/><div class="controls bullet"><span class="by">pradeepchhetri</span><span>|</span><a href="#40302149">parent</a><span>|</span><a href="#40299364">next</a><span>|</span><label class="collapse" for="c-40302203">[-]</label><label class="expand" for="c-40302203">[1 more]</label></div><br/><div class="children"><div class="content">If you are interested in reading internals of using Parquet with ClickHouse, do read following articles:<p>- <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;apache-parquet-clickhouse-local-querying-writing" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;apache-parquet-clickhouse-local-...</a><p>- <a href="https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;apache-parquet-clickhouse-local-querying-writing-internals-row-groups" rel="nofollow">https:&#x2F;&#x2F;clickhouse.com&#x2F;blog&#x2F;apache-parquet-clickhouse-local-...</a></div><br/></div></div></div></div><div id="40299364" class="c"><input type="checkbox" id="c-40299364" checked=""/><div class="controls bullet"><span class="by">calderwoodra</span><span>|</span><a href="#40302149">prev</a><span>|</span><a href="#40284302">next</a><span>|</span><label class="collapse" for="c-40299364">[-]</label><label class="expand" for="c-40299364">[28 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been struggeling with a tough parquet problem for a few months now.<p>I have a 15gb parquet file in a s3 bucket and I need to &quot;unzip&quot; and extract every row from the file to write into my database. The contents of the file are emails and I need to integrate them into our search function.<p>Is this possible to do without an unreasonable amount of RAM? Are there any affordable services that can help here?<p>Feel free to contact me (email in bio), happy to pay for a consult at the minimum.</div><br/><div id="40305981" class="c"><input type="checkbox" id="c-40305981" checked=""/><div class="controls bullet"><span class="by">sagia</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40299395">next</a><span>|</span><label class="collapse" for="c-40305981">[-]</label><label class="expand" for="c-40305981">[1 more]</label></div><br/><div class="children"><div class="content">I had similiar issue, but for aggreagations. Use case was to &quot;compress&quot; large datasets into smaller aggregations for insertion into a costly db.
At first we used duckdb but memory became an issue there and we also bumped into a couple of issues with how duckdb handles arrays.
We then moved this workload to clickhouse local, which was faster and had more fine tuning options to our liking. in this case was limiting ram usage with i.e. max_bytes_before_external_group_by</div><br/></div></div><div id="40299395" class="c"><input type="checkbox" id="c-40299395" checked=""/><div class="controls bullet"><span class="by">martinky24</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40305981">prev</a><span>|</span><a href="#40301003">next</a><span>|</span><label class="collapse" for="c-40299395">[-]</label><label class="expand" for="c-40299395">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t partial reads fix the RAM problem? e.g. something like this: <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;69888274" rel="nofollow">https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;69888274</a><p>It might not be fast, but a quick 1-off solution that you let run for a while would probably do that job. There shouldn&#x27;t be a need to load the whole file into memory.</div><br/></div></div><div id="40301003" class="c"><input type="checkbox" id="c-40301003" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40299395">prev</a><span>|</span><a href="#40299909">next</a><span>|</span><label class="collapse" for="c-40301003">[-]</label><label class="expand" for="c-40301003">[2 more]</label></div><br/><div class="children"><div class="content">DuckDB?<p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;2024&#x2F;03&#x2F;29&#x2F;external-aggregation.html" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;2024&#x2F;03&#x2F;29&#x2F;external-aggregation.html</a><p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;2021&#x2F;06&#x2F;25&#x2F;querying-parquet.html" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;2021&#x2F;06&#x2F;25&#x2F;querying-parquet.html</a><p>If your DB is mysql or postgres, then you could read a stream from parquet, transform inline and write out to your DB<p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;2024&#x2F;01&#x2F;26&#x2F;multi-database-support-in-duckdb.html" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;2024&#x2F;01&#x2F;26&#x2F;multi-database-support-in-duck...</a><p>And an unrelated, but interesting read about the parquet bomb<p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;2024&#x2F;03&#x2F;26&#x2F;42-parquet-a-zip-bomb-for-the-big-data-age.html" rel="nofollow">https:&#x2F;&#x2F;duckdb.org&#x2F;2024&#x2F;03&#x2F;26&#x2F;42-parquet-a-zip-bomb-for-the-...</a></div><br/><div id="40301565" class="c"><input type="checkbox" id="c-40301565" checked=""/><div class="controls bullet"><span class="by">calderwoodra</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40301003">parent</a><span>|</span><a href="#40299909">next</a><span>|</span><label class="collapse" for="c-40301565">[-]</label><label class="expand" for="c-40301565">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40301549">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40301549</a></div><br/></div></div></div></div><div id="40299909" class="c"><input type="checkbox" id="c-40299909" checked=""/><div class="controls bullet"><span class="by">lonesword</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40301003">prev</a><span>|</span><a href="#40299443">next</a><span>|</span><label class="collapse" for="c-40299909">[-]</label><label class="expand" for="c-40299909">[5 more]</label></div><br/><div class="children"><div class="content">I work with pyspark and parquet quite a lot. I never had to deal with parquet outside spark, but this is how I would do this:<p>- Write a pandas_udf function in pyspark.<p>- Parition your data into smaller bits so that the pandas_udf does not get too much data at the same time.<p>Something like:<p>```<p>from pyspark.sql import SparkSession<p>import pyspark.sql.functions as f<p>@f.pandas_udf(return_type=whatever)<p>def ingest(doc: pd.Series):
    # doc is a pandas series now<p><pre><code>    # your processing goes here -&gt; write to DB e.t.c

    pd_series_literal = Create a pd.Series that just contains the integer 0 to make spark happy

    return pd_series_literal
</code></pre>
spark = SparkSession.builder.getOrCreate()<p>df = spark.read.parquet(&quot;s3 path&quot;)<p>df = df.repartition(1000). # bump up this number if you run 
into memory issues<p>df = df.withColumn(&quot;foo&quot;, ingest(f.col(&quot;doc_column&quot;))<p>```<p>Now the trick is, you can limit how much data is given to your pandas_udf by repartitioning your data. The more the partitions, the smaller the pd.Series that your pandas_udf gets. There&#x27;s also the `spark.sql.execution.arrow.maxRecordsPerBatch` config that you can set in spark to limit memory consumption.<p>^ Probably overkill to bring spark into the equation, but this is one way to do it.<p>You can use a normal udf (i.e `f.udf()`) instead of a pandas_udf, but apparently that&#x27;s slower due to java &lt;-&gt; python serialization</div><br/><div id="40300780" class="c"><input type="checkbox" id="c-40300780" checked=""/><div class="controls bullet"><span class="by">fifilura</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40299909">parent</a><span>|</span><a href="#40301479">next</a><span>|</span><label class="collapse" for="c-40300780">[-]</label><label class="expand" for="c-40300780">[3 more]</label></div><br/><div class="children"><div class="content">Pyspark is probably the way to go.<p>I just wanted to mention that AWS Athena eats 15G parquet files for breakfast.<p>It is trivial to map the file into Athena.<p>But you can&#x27;t connect it to anything else than file output. But it can help you to for example write it to smaller chunks. Or choose another output format such as csv (although arbitrary email content in a csv feels like you are set up for parsing errors).<p>The benefit is that there is virtually no setup cost. And processing cost for a 15G file will be just a few cents.</div><br/><div id="40301627" class="c"><input type="checkbox" id="c-40301627" checked=""/><div class="controls bullet"><span class="by">calderwoodra</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40300780">parent</a><span>|</span><a href="#40301479">next</a><span>|</span><label class="collapse" for="c-40301627">[-]</label><label class="expand" for="c-40301627">[2 more]</label></div><br/><div class="children"><div class="content">Athena is probably my best bet tbh, especially if I can do a few clicks and just get smaller files. Processing smaller files is a no brainer &#x2F; pretty easy and could be outsourced to lambda.</div><br/><div id="40302024" class="c"><input type="checkbox" id="c-40302024" checked=""/><div class="controls bullet"><span class="by">fifilura</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40301627">parent</a><span>|</span><a href="#40301479">next</a><span>|</span><label class="collapse" for="c-40302024">[-]</label><label class="expand" for="c-40302024">[1 more]</label></div><br/><div class="children"><div class="content">Yeah the big benefit is that it requires very little setup.<p>You create a new partitioned table&#x2F;location from the originally mapped file using a CTAS like so:<p><pre><code>  CREATE TABLE new_table_name
  WITH (
    format = &#x27;PARQUET&#x27;,
    parquet_compression = &#x27;SNAPPY&#x27;,
    external_location = &#x27;s3:&#x2F;&#x2F;your-bucket&#x2F;path&#x2F;to&#x2F;output&#x2F;&#x27;
  ) AS
  SELECT *
  FROM original_table_name
  PARTITIONED BY partition_column_name
</code></pre>
You can probably create a hash and partition by the last character if you want 16 evenly sized partitions. Unless you already have a dimension to partition by.</div><br/></div></div></div></div></div></div><div id="40301479" class="c"><input type="checkbox" id="c-40301479" checked=""/><div class="controls bullet"><span class="by">jcgrillo</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40299909">parent</a><span>|</span><a href="#40300780">prev</a><span>|</span><a href="#40299443">next</a><span>|</span><label class="collapse" for="c-40301479">[-]</label><label class="expand" for="c-40301479">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been a while (~5yr) since I&#x27;ve done anything with Spark, but IIRC it used to be very difficult to make reliable jobs with the Java or Python APIs due to the impedance mismatch between Scala&#x27;s lazy evaluation semantics and the eager evaluation of Java and Python. I&#x27;d encounter perplexing OOMs whenever I tried to use the Python or Java APIs, so I (reluctantly) learned enough Scala to make the Spark go brr and all was well. Is it still like this?</div><br/></div></div></div></div><div id="40299443" class="c"><input type="checkbox" id="c-40299443" checked=""/><div class="controls bullet"><span class="by">memset</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40299909">prev</a><span>|</span><a href="#40299666">next</a><span>|</span><label class="collapse" for="c-40299443">[-]</label><label class="expand" for="c-40299443">[5 more]</label></div><br/><div class="children"><div class="content">I can help! Emailing you now :)<p>Our company (scratchdata.com, open source) is literally built to solve the problem of schlepping large amounts of data between sources and destinations, so I have worked on this problem a lot personally and happy to nerd out about what works.</div><br/><div id="40301956" class="c"><input type="checkbox" id="c-40301956" checked=""/><div class="controls bullet"><span class="by">fock</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40299443">parent</a><span>|</span><a href="#40299666">next</a><span>|</span><label class="collapse" for="c-40301956">[-]</label><label class="expand" for="c-40301956">[4 more]</label></div><br/><div class="children"><div class="content">I - by my HPC-background - am wondering quite a bit what happened that 15GB-files are considered large data? Not being a crazy parquet-user, but:<p>- does this decompress to giant sizes?
- can&#x27;t you split the file easily, because it includes row-based segments?
- why does it take months to solve this for <i>one</i> file?</div><br/><div id="40302068" class="c"><input type="checkbox" id="c-40302068" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40301956">parent</a><span>|</span><a href="#40304926">next</a><span>|</span><label class="collapse" for="c-40302068">[-]</label><label class="expand" for="c-40302068">[1 more]</label></div><br/><div class="children"><div class="content">As a fellow HPC user, I tried a couple of years ago to do a tricky data format conversion using these newfangled tools. I was essentially just taking a huge (multi-terabyte) 3D dataset, transposing it and changing the endianness.<p>The solutions I was able to put together using Dask and Spark and such were all insanely slow, they just got killed by Slurm without getting anywhere. In the end I went back to good ole&#x27; shell scripting with xxd to handle most of the heavy lifting. Finished in under an hour.<p>The appeal of these newfangled tools is that you can work with data sizes that are infeasible to people who only know Excel, yet you don&#x27;t need to understand a single thing about how your data is actually stored.<p>If you can be bothered to read the file format specification, open up some files in a hex editor to understand the layout, and write low-level code to parse the data - then you can achieve several orders of magnitude higher performance.</div><br/></div></div><div id="40304926" class="c"><input type="checkbox" id="c-40304926" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40301956">parent</a><span>|</span><a href="#40302068">prev</a><span>|</span><a href="#40299666">next</a><span>|</span><label class="collapse" for="c-40304926">[-]</label><label class="expand" for="c-40304926">[2 more]</label></div><br/><div class="children"><div class="content">Parquet is column oriented and so row-based manipulation can be inefficient</div><br/><div id="40305876" class="c"><input type="checkbox" id="c-40305876" checked=""/><div class="controls bullet"><span class="by">fock</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40304926">parent</a><span>|</span><a href="#40299666">next</a><span>|</span><label class="collapse" for="c-40305876">[-]</label><label class="expand" for="c-40305876">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hierarchically, a file consists of one or more row groups.<p><a href="https:&#x2F;&#x2F;parquet.apache.org&#x2F;docs&#x2F;concepts&#x2F;" rel="nofollow">https:&#x2F;&#x2F;parquet.apache.org&#x2F;docs&#x2F;concepts&#x2F;</a><p>Maybe the file in question only has one row group. Which would be weird, because the creator had to go out of their way to make it happen.</div><br/></div></div></div></div></div></div></div></div><div id="40299666" class="c"><input type="checkbox" id="c-40299666" checked=""/><div class="controls bullet"><span class="by">wild_egg</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40299443">prev</a><span>|</span><a href="#40300954">next</a><span>|</span><label class="collapse" for="c-40299666">[-]</label><label class="expand" for="c-40299666">[4 more]</label></div><br/><div class="children"><div class="content">Have you given DuckDB a try? I&#x27;m using it to shuttle some hefty data between postgres and some parquet files on S3 and it&#x27;s only a couple lines. Haven&#x27;t noted any memory issues so far</div><br/><div id="40300510" class="c"><input type="checkbox" id="c-40300510" checked=""/><div class="controls bullet"><span class="by">csjh</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40299666">parent</a><span>|</span><a href="#40300954">next</a><span>|</span><label class="collapse" for="c-40300510">[-]</label><label class="expand" for="c-40300510">[3 more]</label></div><br/><div class="children"><div class="content">Agreed on DuckDB, fantastic for working with most major data formats</div><br/><div id="40301549" class="c"><input type="checkbox" id="c-40301549" checked=""/><div class="controls bullet"><span class="by">calderwoodra</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40300510">parent</a><span>|</span><a href="#40300954">next</a><span>|</span><label class="collapse" for="c-40301549">[-]</label><label class="expand" for="c-40301549">[2 more]</label></div><br/><div class="children"><div class="content">Took your advice and tried DuckDB. Here&#x27;s what I&#x27;ve got so far:<p>```<p>def _get_duck_db_arrow_results(s3_key):<p><pre><code>    con = duckdb.connect(config={&#x27;threads&#x27;: 1, &#x27;memory_limit&#x27;: &#x27;1GB&#x27;})
    con.install_extension(&quot;aws&quot;)
    con.install_extension(&quot;httpfs&quot;)
    con.load_extension(&quot;aws&quot;)
    con.load_extension(&quot;httpfs&quot;)

    con.sql(&quot;CALL load_aws_credentials(&#x27;hadrius-dev&#x27;, set_region=true);&quot;)
    con.sql(&quot;CREATE SECRET (TYPE S3,PROVIDER CREDENTIAL_CHAIN);&quot;)
    results = con \
        .execute(f&quot;SELECT * FROM read_parquet(&#x27;{s3_key}&#x27;);&quot;) \
        .fetch_record_batch(1024)
    for index, result in enumerate(results):
        print(index)
    return results</code></pre>
```<p>I ran the above on a 1.4gb parquet file and 15 min later, all of the results were printed at once. This suggests to me that the whole file was loaded loaded into memory at once.</div><br/><div id="40304489" class="c"><input type="checkbox" id="c-40304489" checked=""/><div class="controls bullet"><span class="by">wild_egg</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40301549">parent</a><span>|</span><a href="#40300954">next</a><span>|</span><label class="collapse" for="c-40304489">[-]</label><label class="expand" for="c-40304489">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been a long time since I&#x27;ve used python but that sounds like buffering in the library maybe? I use it from Go and it seems to behave differently.<p>When I&#x27;m writing to postgres though I&#x27;m doing into entirely inside DuckDB with a `INSERT INTO ... SELECT ...` and that seems to stream it over.</div><br/></div></div></div></div></div></div></div></div><div id="40300954" class="c"><input type="checkbox" id="c-40300954" checked=""/><div class="controls bullet"><span class="by">arrowleaf</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40299666">prev</a><span>|</span><a href="#40304062">next</a><span>|</span><label class="collapse" for="c-40300954">[-]</label><label class="expand" for="c-40300954">[1 more]</label></div><br/><div class="children"><div class="content">Using polars in Python I&#x27;ve gotten similar to work, using LazyFrame and collect in streaming mode:<p>```
df = pl.scan_parquet(&#x27;tmp&#x2F;&#x27;+DUMP_NAME+&#x27;_cleaned.parquet&#x27;)<p>with open(&#x27;tmp&#x2F;&#x27;+DUMP_NAME+&#x27;_cleaned.jsonl&#x27;, mode=&#x27;w&#x27;, newline=&#x27;\n&#x27;, encoding=&#x27;utf8&#x27;) as f:
    for row in df.collect(streaming=True).iter_rows(named=True):
        row = {k: v for k, v in row.items() if (v is not None and v != [] and v != &#x27;&#x27;)}
        f.write(json.dumps(row, default=str) + &#x27;\n&#x27;)
```</div><br/></div></div><div id="40304062" class="c"><input type="checkbox" id="c-40304062" checked=""/><div class="controls bullet"><span class="by">orf</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40300954">prev</a><span>|</span><a href="#40302504">next</a><span>|</span><label class="collapse" for="c-40304062">[-]</label><label class="expand" for="c-40304062">[1 more]</label></div><br/><div class="children"><div class="content">Understand the format of your data.<p>Look at the parquet file metadata: use whatever tool you want for that. The Python parquet library is useful and supports s3.<p>How big are your row groups? If itâs one large row group then you will run into this issue.<p>Whatâs the number of rows in each row group?</div><br/></div></div><div id="40302504" class="c"><input type="checkbox" id="c-40302504" checked=""/><div class="controls bullet"><span class="by">bluedemon</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40304062">prev</a><span>|</span><a href="#40300295">next</a><span>|</span><label class="collapse" for="c-40302504">[-]</label><label class="expand" for="c-40302504">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps look into using dlt from <a href="https:&#x2F;&#x2F;dlthub.com" rel="nofollow">https:&#x2F;&#x2F;dlthub.com</a>, using pyarrow or polars. It handles large datasets well, especially when using generators to process the data in chunks.</div><br/></div></div><div id="40300295" class="c"><input type="checkbox" id="c-40300295" checked=""/><div class="controls bullet"><span class="by">jfim</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40302504">prev</a><span>|</span><a href="#40299462">next</a><span>|</span><label class="collapse" for="c-40300295">[-]</label><label class="expand" for="c-40300295">[2 more]</label></div><br/><div class="children"><div class="content">Spend a few bucks on an EC2 instance with a few terabytes of RAM for an hour or so. u-3tb1.56xlarge is about $27&#x2F;hr.</div><br/><div id="40305715" class="c"><input type="checkbox" id="c-40305715" checked=""/><div class="controls bullet"><span class="by">nucleardog</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40300295">parent</a><span>|</span><a href="#40299462">next</a><span>|</span><label class="collapse" for="c-40305715">[-]</label><label class="expand" for="c-40305715">[1 more]</label></div><br/><div class="children"><div class="content">This is the answer for a one-off or occasional problem unless your time is worthless.<p>$200 to rent a machine that can run the naive solution for an entire day is peanuts compared to the dev time for a âbetterâ solution. Running that machine for eight hours would only cost enough to purchase about a half day of junior engineer time.</div><br/></div></div></div></div><div id="40299462" class="c"><input type="checkbox" id="c-40299462" checked=""/><div class="controls bullet"><span class="by">dijksterhuis</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40300295">prev</a><span>|</span><a href="#40300243">next</a><span>|</span><label class="collapse" for="c-40299462">[-]</label><label class="expand" for="c-40299462">[3 more]</label></div><br/><div class="children"><div class="content">Try pyarrow.ParquetFile.iter_batches()<p>Streams batches of rows<p><a href="https:&#x2F;&#x2F;arrow.apache.org&#x2F;docs&#x2F;python&#x2F;generated&#x2F;pyarrow.parquet.ParquetFile.html" rel="nofollow">https:&#x2F;&#x2F;arrow.apache.org&#x2F;docs&#x2F;python&#x2F;generated&#x2F;pyarrow.parqu...</a><p>Edit â May need to do some extra work with s3fs too from what I recall with the default pandas s3 reading<p>Edit 2 â or check out pyarrow.fs.S3FileSystem :facepalm:</div><br/><div id="40299587" class="c"><input type="checkbox" id="c-40299587" checked=""/><div class="controls bullet"><span class="by">calderwoodra</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40299462">parent</a><span>|</span><a href="#40300243">next</a><span>|</span><label class="collapse" for="c-40299587">[-]</label><label class="expand" for="c-40299587">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve spent many many hours trying these suggestions, didn&#x27;t have much luck. iter_batches loads the whole file (or some very large amount of it) into memory.</div><br/><div id="40302277" class="c"><input type="checkbox" id="c-40302277" checked=""/><div class="controls bullet"><span class="by">semi-extrinsic</span><span>|</span><a href="#40299364">root</a><span>|</span><a href="#40299587">parent</a><span>|</span><a href="#40300243">next</a><span>|</span><label class="collapse" for="c-40302277">[-]</label><label class="expand" for="c-40302277">[1 more]</label></div><br/><div class="children"><div class="content">It sounds like maybe your parquet file has no partitioning. Apart from the iterating over row groups like someone else suggested, I suspect there is no better solution than downloading the whole thing to your computer, partitioning it in a sane way, and uploading it again. It&#x27;s only 15 GB so it should be fine even on an old laptop.<p>Of course then you might as well do all the processing you&#x27;re interested in while the file is on your local disk, since it is probably much faster than the cloud service disk.</div><br/></div></div></div></div></div></div><div id="40300243" class="c"><input type="checkbox" id="c-40300243" checked=""/><div class="controls bullet"><span class="by">cycrutchfield</span><span>|</span><a href="#40299364">parent</a><span>|</span><a href="#40299462">prev</a><span>|</span><a href="#40284302">next</a><span>|</span><label class="collapse" for="c-40300243">[-]</label><label class="expand" for="c-40300243">[1 more]</label></div><br/><div class="children"><div class="content">My suggestion is to load each row group individually, as they generally will be much smaller than your total file size. You can do this via pyarrow.ParquetFile.read_row_group. To truly optimize this for reading from s3 you could use fsspecâs open_parquet_file library which would allow you to only load each row group one at a time.</div><br/></div></div></div></div><div id="40284302" class="c"><input type="checkbox" id="c-40284302" checked=""/><div class="controls bullet"><span class="by">jjgreen</span><span>|</span><a href="#40299364">prev</a><span>|</span><label class="collapse" for="c-40284302">[-]</label><label class="expand" for="c-40284302">[19 more]</label></div><br/><div class="children"><div class="content">Why is it not in Debian?  Is there some deep and dark secret?<p><a href="https:&#x2F;&#x2F;search.debian.org&#x2F;cgi-bin&#x2F;omega?DB=en&amp;P=parquet" rel="nofollow">https:&#x2F;&#x2F;search.debian.org&#x2F;cgi-bin&#x2F;omega?DB=en&amp;P=parquet</a></div><br/><div id="40284705" class="c"><input type="checkbox" id="c-40284705" checked=""/><div class="controls bullet"><span class="by">whinvik</span><span>|</span><a href="#40284302">parent</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40284705">[-]</label><label class="expand" for="c-40284705">[13 more]</label></div><br/><div class="children"><div class="content">Parquet is a file format. Should a file format be in Debian?</div><br/><div id="40285450" class="c"><input type="checkbox" id="c-40285450" checked=""/><div class="controls bullet"><span class="by">jjgreen</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40284705">parent</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40285450">[-]</label><label class="expand" for="c-40285450">[12 more]</label></div><br/><div class="children"><div class="content">I mean library support for reading and writing it:<p><pre><code>  &gt; apt-cache search hdf5 | wc -l
  134
  &gt; apt-cache search netcdf | wc -l
  70
  &gt; apt-cache search parquet | wc -l 
  0</code></pre></div><br/><div id="40290174" class="c"><input type="checkbox" id="c-40290174" checked=""/><div class="controls bullet"><span class="by">whinvik</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40285450">parent</a><span>|</span><a href="#40301858">next</a><span>|</span><label class="collapse" for="c-40290174">[-]</label><label class="expand" for="c-40290174">[1 more]</label></div><br/><div class="children"><div class="content">Are there Arrow libraries. Feel like a lot of applications that read Parquet actually outsource raw reads to Arrow.</div><br/></div></div><div id="40301858" class="c"><input type="checkbox" id="c-40301858" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40285450">parent</a><span>|</span><a href="#40290174">prev</a><span>|</span><a href="#40299907">next</a><span>|</span><label class="collapse" for="c-40301858">[-]</label><label class="expand" for="c-40301858">[1 more]</label></div><br/><div class="children"><div class="content">Duckdb is probably what you want, though I don&#x27;t think it&#x27;s in debian either.  It&#x27;s in Arch though.</div><br/></div></div><div id="40299907" class="c"><input type="checkbox" id="c-40299907" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40285450">parent</a><span>|</span><a href="#40301858">prev</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40299907">[-]</label><label class="expand" for="c-40299907">[9 more]</label></div><br/><div class="children"><div class="content">look for polars, <a href="https:&#x2F;&#x2F;pola.rs" rel="nofollow">https:&#x2F;&#x2F;pola.rs</a></div><br/><div id="40300112" class="c"><input type="checkbox" id="c-40300112" checked=""/><div class="controls bullet"><span class="by">jjgreen</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40299907">parent</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40300112">[-]</label><label class="expand" for="c-40300112">[8 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;packages.debian.org&#x2F;search?searchon=names&amp;keywords=polars" rel="nofollow">https:&#x2F;&#x2F;packages.debian.org&#x2F;search?searchon=names&amp;keywords=p...</a></div><br/><div id="40300678" class="c"><input type="checkbox" id="c-40300678" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40300112">parent</a><span>|</span><a href="#40300210">next</a><span>|</span><label class="collapse" for="c-40300678">[-]</label><label class="expand" for="c-40300678">[1 more]</label></div><br/><div class="children"><div class="content">Why did you ignore the link i gave you above?<p>If you follow that link, you&#x27;ll see polars and parquet are a large highly configurable collection of tools for format manipulations across many HPC formats.  Debian maintainers possibly don&#x27;t want to bundle the entirety, as it would be vast.<p>Might this help you, though?<p><a href="https:&#x2F;&#x2F;cloudsmith.io&#x2F;~opencpn&#x2F;repos&#x2F;polar-prod&#x2F;packages&#x2F;detail&#x2F;raw&#x2F;polar_pi-1.2.6.0-debian-x86_64-10-buster-tarball&#x2F;v1.2.6.0&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cloudsmith.io&#x2F;~opencpn&#x2F;repos&#x2F;polar-prod&#x2F;packages&#x2F;det...</a></div><br/></div></div><div id="40300210" class="c"><input type="checkbox" id="c-40300210" checked=""/><div class="controls bullet"><span class="by">petre</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40300112">parent</a><span>|</span><a href="#40300678">prev</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40300210">[-]</label><label class="expand" for="c-40300210">[6 more]</label></div><br/><div class="children"><div class="content">Use pip, Luke!<p><a href="https:&#x2F;&#x2F;docs.pola.rs&#x2F;user-guide&#x2F;getting-started" rel="nofollow">https:&#x2F;&#x2F;docs.pola.rs&#x2F;user-guide&#x2F;getting-started</a></div><br/><div id="40300308" class="c"><input type="checkbox" id="c-40300308" checked=""/><div class="controls bullet"><span class="by">jjgreen</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40300210">parent</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40300308">[-]</label><label class="expand" for="c-40300308">[5 more]</label></div><br/><div class="children"><div class="content">My question is &quot;why isn&#x27;t it in Debian?&quot;, I ask that since Debian has rather high standards and the absence from Debian suggests some quality issue in available libraries for the format or the format itself.<p>Are there dark secrets?</div><br/><div id="40300477" class="c"><input type="checkbox" id="c-40300477" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40300308">parent</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40300477">[-]</label><label class="expand" for="c-40300477">[4 more]</label></div><br/><div class="children"><div class="content">Could the dark secret you seek be &quot;debian isn&#x27;t for bleeding edge packages&quot;?<p>it&#x27;s very modern and perhaps hasn&#x27;t been around long enough to have debian maintainers feel it&#x27;s vetted.<p>for instance, documentation for Python bindings is more advanced than for Rust bindings, but the package itself uses Rust at the low level.</div><br/><div id="40301117" class="c"><input type="checkbox" id="c-40301117" checked=""/><div class="controls bullet"><span class="by">jjgreen</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40300477">parent</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40301117">[-]</label><label class="expand" for="c-40301117">[3 more]</label></div><br/><div class="children"><div class="content">Parquet is what, 12 years old? Hardly cutting edge.  What you say my well be true for polars (I&#x27;m not familiar with it), if&#x2F;when it (or something else) does get packaged I&#x27;ll give parquet another look ...</div><br/><div id="40301289" class="c"><input type="checkbox" id="c-40301289" checked=""/><div class="controls bullet"><span class="by">jjtheblunt</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40301117">parent</a><span>|</span><a href="#40305140">next</a><span>|</span><label class="collapse" for="c-40301289">[-]</label><label class="expand" for="c-40301289">[1 more]</label></div><br/><div class="children"><div class="content">Yes, i wasn&#x27;t clear: it&#x27;s the polars library that&#x27;s actively changing, so that might be the issue, or just the vast set of optional components configurable on installation, which isn&#x27;t the normal package manager experience.<p>FWIW i think i share your general aversion to _not_ using packages, just for the tidiness of installs and removals, though i&#x27;m on fedora and macos.</div><br/></div></div><div id="40305140" class="c"><input type="checkbox" id="c-40305140" checked=""/><div class="controls bullet"><span class="by">petre</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40301117">parent</a><span>|</span><a href="#40301289">prev</a><span>|</span><a href="#40299435">next</a><span>|</span><label class="collapse" for="c-40305140">[-]</label><label class="expand" for="c-40305140">[1 more]</label></div><br/><div class="children"><div class="content">Pandas is probably in Debian and it can read parquet files. Polars is fairly new and under active development. It&#x27;s a python library, I install those in $HOME&#x2F;.local, as opposed to system wide. One can also install it in a venv. With pip you can also uninstall packages and keep things fairly tidy.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="40299435" class="c"><input type="checkbox" id="c-40299435" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#40284302">parent</a><span>|</span><a href="#40284705">prev</a><span>|</span><label class="collapse" for="c-40299435">[-]</label><label class="expand" for="c-40299435">[5 more]</label></div><br/><div class="children"><div class="content">pandas is a python-centric, tabular data handler that works well in clouds (and desktop Debian). Pandas can read parquet data today, among other libs mentioned. The binary dot-so driver style is single-host centric and not the emphasis of these cloudy projects (and their cloudy funders)<p><a href="https:&#x2F;&#x2F;pandas.pydata.org&#x2F;docs&#x2F;reference&#x2F;api&#x2F;pandas.read_parquet.html" rel="nofollow">https:&#x2F;&#x2F;pandas.pydata.org&#x2F;docs&#x2F;reference&#x2F;api&#x2F;pandas.read_par...</a><p><a href="https:&#x2F;&#x2F;packages.debian.org&#x2F;buster&#x2F;python3-pandas" rel="nofollow">https:&#x2F;&#x2F;packages.debian.org&#x2F;buster&#x2F;python3-pandas</a><p>Perhaps more alarm is called for when this python+pandas and parquet does not work on Debian, but that is not the case today.<p>ps- data access in clouds often uses the S3:&#x2F;&#x2F; endpoint .  Contrast to a POSIX endpoint using _fread()_ or similar.. many parquet-aware clients prefer the cloudy, un-POSIX method to access data and that is another reason it is not a simple package in Debian today.</div><br/><div id="40299989" class="c"><input type="checkbox" id="c-40299989" checked=""/><div class="controls bullet"><span class="by">datadrivenangel</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40299435">parent</a><span>|</span><a href="#40299776">next</a><span>|</span><label class="collapse" for="c-40299989">[-]</label><label class="expand" for="c-40299989">[2 more]</label></div><br/><div class="children"><div class="content">Pandas often has significant memory overhead, so it&#x27;s not uncommon to need ~3-5x the amount of memory as your file size.<p>Polars and DuckDB are much better about memory management.</div><br/><div id="40302938" class="c"><input type="checkbox" id="c-40302938" checked=""/><div class="controls bullet"><span class="by">marton78</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40299989">parent</a><span>|</span><a href="#40299776">next</a><span>|</span><label class="collapse" for="c-40302938">[-]</label><label class="expand" for="c-40302938">[1 more]</label></div><br/><div class="children"><div class="content">Also, Polars has a sane and well thought out API, unlike the hot streaming mess that is Pandas.</div><br/></div></div></div></div><div id="40299776" class="c"><input type="checkbox" id="c-40299776" checked=""/><div class="controls bullet"><span class="by">jjgreen</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40299435">parent</a><span>|</span><a href="#40299989">prev</a><span>|</span><label class="collapse" for="c-40299776">[-]</label><label class="expand" for="c-40299776">[2 more]</label></div><br/><div class="children"><div class="content">As I understand it, pandas can read parquet if the pyarrow or fastparquet packages are available, but that&#x27;s not the case and attempts to fix that have been underway for several years.<p><a href="https:&#x2F;&#x2F;bugs.debian.org&#x2F;cgi-bin&#x2F;bugreport.cgi?bug=970021" rel="nofollow">https:&#x2F;&#x2F;bugs.debian.org&#x2F;cgi-bin&#x2F;bugreport.cgi?bug=970021</a></div><br/><div id="40302576" class="c"><input type="checkbox" id="c-40302576" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#40284302">root</a><span>|</span><a href="#40299776">parent</a><span>|</span><label class="collapse" for="c-40302576">[-]</label><label class="expand" for="c-40302576">[1 more]</label></div><br/><div class="children"><div class="content">hm<p><a href="https:&#x2F;&#x2F;github.com&#x2F;pandas-dev&#x2F;pandas&#x2F;blob&#x2F;main&#x2F;pandas&#x2F;io&#x2F;parquet.py">https:&#x2F;&#x2F;github.com&#x2F;pandas-dev&#x2F;pandas&#x2F;blob&#x2F;main&#x2F;pandas&#x2F;io&#x2F;par...</a><p>says &quot;fastparquet&quot; engine must be available if no pyarrow<p><a href="https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;fastparquet&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;fastparquet&#x2F;</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>