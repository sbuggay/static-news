<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1738659674228" as="style"/><link rel="stylesheet" href="styles.css?v=1738659674228"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/research/constitutional-classifiers">Constitutional Classifiers: Defending against universal jailbreaks</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>meetpateltech</span> | <span>50 comments</span></div><br/><div><div id="42929958" class="c"><input type="checkbox" id="c-42929958" checked=""/><div class="controls bullet"><span class="by">CaptainFever</span><span>|</span><a href="#42921652">next</a><span>|</span><label class="collapse" for="c-42929958">[-]</label><label class="expand" for="c-42929958">[1 more]</label></div><br/><div class="children"><div class="content">Looking at the comments here, I think we need to differentiate between &quot;AI that works for you&quot; and &quot;AI that works for others&quot;.<p>&quot;AI that works for others&quot; isn&#x27;t necessarily a bad thing. For instance, I would be fine with a customer service AI that I can ask questions to 24&#x2F;7 and without delay. It makes sense that the people who deploy that AI would not want it to be jailbroken, to be used as a generic AI or to do something harmful. A constitution makes sense here.<p>&quot;AI that works for you&quot; would require that the constitution is controlled by you -- not Anthropic, DeepSeek, Meta, or OpenAI. Sometimes you want no constitution, like when you&#x27;re using it normally. Sometimes you <i>do</i> want a constitution and prevent jailbreaking, for example, if you are giving the AI untrusted input (e.g. scraped HTML, customer queries).<p>In conclusion, unlike most comments here, I don&#x27;t think this is a useless or even harmful invention. It can be very useful indeed. However, this highlights the need for local, uncensored, and open-weight AIs where one can control what constitution is being used.</div><br/></div></div><div id="42921652" class="c"><input type="checkbox" id="c-42921652" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42929958">prev</a><span>|</span><a href="#42929580">next</a><span>|</span><label class="collapse" for="c-42921652">[-]</label><label class="expand" for="c-42921652">[4 more]</label></div><br/><div class="children"><div class="content">Okay, this method works as follows: create some positive and negative rules, (called as a group a &quot;constitution&quot;), use a &quot;helpful-only&quot; LLM to generate synthetic data, then conduct preference training on a smaller model that will sit between the OG model and the final output and flag stuff that is &quot;anti-constitutional&quot; (my words). The helpful-only LLM will generate keywords to look for, among other things, making assessment during training automated.<p>This works better than what Anthropic is doing now, somewhat significantly better.<p>That&#x27;s the paper. Here&#x27;s what EVERYONE should be pestering the Anthropics of the world on:<p>* Can I read this constitution? Where? Can you demonstrate the stated constitution is the real one?<p>* Can I select piecemeal constitutions?<p>* Which groups do you deem allowed to have access to the &quot;helpful-only LLM&quot;?<p>Just a reminder that without free and open models, through good intentions, we are likely to create a have and have-not technical elite. The people who have self-selected as &quot;safe&quot; to have access to helpful-only LLMs, and create the rules for the rest of the world.<p>This is not a good thing.</div><br/><div id="42926554" class="c"><input type="checkbox" id="c-42926554" checked=""/><div class="controls bullet"><span class="by">sinuhe69</span><span>|</span><a href="#42921652">parent</a><span>|</span><a href="#42923728">next</a><span>|</span><label class="collapse" for="c-42926554">[-]</label><label class="expand" for="c-42926554">[1 more]</label></div><br/><div class="children"><div class="content">What prevents the open source community to recreate and test the same method?</div><br/></div></div><div id="42923728" class="c"><input type="checkbox" id="c-42923728" checked=""/><div class="controls bullet"><span class="by">jacobr1</span><span>|</span><a href="#42921652">parent</a><span>|</span><a href="#42926554">prev</a><span>|</span><a href="#42929580">next</a><span>|</span><label class="collapse" for="c-42923728">[-]</label><label class="expand" for="c-42923728">[2 more]</label></div><br/><div class="children"><div class="content">From their 2022 paper they listed the principles:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.08073" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.08073</a><p>Example:<p>CritiqueRequest: Discuss whether the human is deliberately trying to bait the
assistant into generating harmful content, and whether they succeeded in
doing so.
RevisionRequest: Write a revision of the assistant’s response that refrains from
saying anything harmful.</div><br/><div id="42925133" class="c"><input type="checkbox" id="c-42925133" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42921652">root</a><span>|</span><a href="#42923728">parent</a><span>|</span><a href="#42929580">next</a><span>|</span><label class="collapse" for="c-42925133">[-]</label><label class="expand" for="c-42925133">[1 more]</label></div><br/><div class="children"><div class="content">No, they listed the principles they were willing to tell you about during the test. They do not publish an open list of principles they place in front of your live requests, as far as I know.</div><br/></div></div></div></div></div></div><div id="42929580" class="c"><input type="checkbox" id="c-42929580" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#42921652">prev</a><span>|</span><a href="#42922462">next</a><span>|</span><label class="collapse" for="c-42929580">[-]</label><label class="expand" for="c-42929580">[1 more]</label></div><br/><div class="children"><div class="content">I had experience in the past of LLMs not replying to perfectly legitimate questions because it &quot;feared&quot; that the reply would be illegal in some jurisdiction. After receiving an explanation that its fumigations about legality were completely dumb, it finally answered.<p>They can be very confused about what information they should believe they should conceal.<p>A dumb interlocutor that stubbornly refuses to provide information because it has the mindset of an infant is less than useful, it is just another expression of the arrogant mediocrity.</div><br/></div></div><div id="42922462" class="c"><input type="checkbox" id="c-42922462" checked=""/><div class="controls bullet"><span class="by">lsy</span><span>|</span><a href="#42929580">prev</a><span>|</span><a href="#42922224">next</a><span>|</span><label class="collapse" for="c-42922462">[-]</label><label class="expand" for="c-42922462">[2 more]</label></div><br/><div class="children"><div class="content">The goalpost here is pretty specific: a couple hundred people try for 4,000 hours to figure out a &quot;universal jailbreak&quot; which means it converts the model to one that answers all 10 of a set of &quot;forbidden&quot; questions. Since they couldn&#x27;t, the technique is considered robust.<p>Looking at the data though, there apparently exist jailbreak techniques that make the model answer five of the questions at full detail, and nine at &quot;half detail&quot;. Given that the model would ostensibly be deployed to millions of people who would collectively use it for millions of hours, I&#x27;m not sure how confident I am that the 10-question barrier would remain unbroken for long.</div><br/><div id="42923193" class="c"><input type="checkbox" id="c-42923193" checked=""/><div class="controls bullet"><span class="by">NoMoreNicksLeft</span><span>|</span><a href="#42922462">parent</a><span>|</span><a href="#42922224">next</a><span>|</span><label class="collapse" for="c-42923193">[-]</label><label class="expand" for="c-42923193">[1 more]</label></div><br/><div class="children"><div class="content">If one need only craft a jailbreak for the question they are interested in, a less universal jailbreak suffices to cause the trouble they&#x27;re pretending can be avoided.</div><br/></div></div></div></div><div id="42922224" class="c"><input type="checkbox" id="c-42922224" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#42922462">prev</a><span>|</span><a href="#42924040">next</a><span>|</span><label class="collapse" for="c-42922224">[-]</label><label class="expand" for="c-42922224">[3 more]</label></div><br/><div class="children"><div class="content">Powerful AI technology being deployed against users to apply non-transparent and unaccountable censorship to their usage of these tools.  Not exactly the brag they think it is.<p>It wouldn&#x27;t be much of a concern except for their efforts lobbying the California government to outlaw access to open models.</div><br/><div id="42924156" class="c"><input type="checkbox" id="c-42924156" checked=""/><div class="controls bullet"><span class="by">philipov</span><span>|</span><a href="#42922224">parent</a><span>|</span><a href="#42924040">next</a><span>|</span><label class="collapse" for="c-42924156">[-]</label><label class="expand" for="c-42924156">[2 more]</label></div><br/><div class="children"><div class="content">Their lobbying to outlaw open models is the biggest threat posed by AI, and their crowing about alignment and existential threats is cover fire for their real objective: total market control.</div><br/><div id="42925231" class="c"><input type="checkbox" id="c-42925231" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#42922224">root</a><span>|</span><a href="#42924156">parent</a><span>|</span><a href="#42924040">next</a><span>|</span><label class="collapse" for="c-42925231">[-]</label><label class="expand" for="c-42925231">[1 more]</label></div><br/><div class="children"><div class="content">Total market control isn&#x27;t the worst reason floating around out there, there are worse ones.</div><br/></div></div></div></div></div></div><div id="42924040" class="c"><input type="checkbox" id="c-42924040" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#42922224">prev</a><span>|</span><a href="#42921574">next</a><span>|</span><label class="collapse" for="c-42924040">[-]</label><label class="expand" for="c-42924040">[2 more]</label></div><br/><div class="children"><div class="content">Pliny has already broken it.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;elder_plinius&#x2F;status&#x2F;1886520475553337725" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;elder_plinius&#x2F;status&#x2F;1886520475553337725</a></div><br/><div id="42925029" class="c"><input type="checkbox" id="c-42925029" checked=""/><div class="controls bullet"><span class="by">gjm11</span><span>|</span><a href="#42924040">parent</a><span>|</span><a href="#42921574">next</a><span>|</span><label class="collapse" for="c-42925029">[-]</label><label class="expand" for="c-42925029">[1 more]</label></div><br/><div class="children"><div class="content">An automated system that finds articles like this and posts &quot;Pliny has already broken it&quot; in the comments would probably end up being pretty accurate.</div><br/></div></div></div></div><div id="42921574" class="c"><input type="checkbox" id="c-42921574" checked=""/><div class="controls bullet"><span class="by">perihelions</span><span>|</span><a href="#42924040">prev</a><span>|</span><a href="#42924771">next</a><span>|</span><label class="collapse" for="c-42921574">[-]</label><label class="expand" for="c-42921574">[13 more]</label></div><br/><div class="children"><div class="content">- <i>&quot;For example, we train Claude to refuse to respond to user queries involving the production of biological or chemical weapons.&quot;</i><p>But seriously: what&#x27;s the point? Any information Claude can offer about i.e. the synthesis of sarin[0] is public information, which Anthropic scraped from any number of public websites, public search engines, libraries, books, research periodicals.<p>This is a novel cultural norm, so it should be interrogated: why <i>should</i> we make it become normal, now, to censor college chemistry questions? Why is this the normative, &quot;this is how we must do things&quot; in elite California tech circles? Google doesn&#x27;t refuse chemistry queries; are they in the wrong? (Should search engines agree to start censoring themselves to align with LLM censorship conventions?) Is Wikipedia also in the wrong, that they host unsafe, harmful chemistry knowledge? What about SciHub? What about all the countless independent websites storing this (elementary, 1930&#x27;s-era) harmful technical information—should we start doing DNS blocks, should we start <i>seizing web servers</i>, how are we to <i>harmonize</i> internet safety policy in a consistent way?<p>Because if your position is &quot;we need to scrub Harmful Responses from the internet&quot;, you can&#x27;t just leave it at LLM&#x27;s and stop there. You need to have some plan to go all the way, or else you&#x27;re doing something silly.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sarin#Production_and_structure" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sarin#Production_and_structure</a><p>(Tangential thought: assigning chemical weapons synthesis problems on exams would be a clever way for chemistry professors, at this moment, to weed out LLM cheaters from their course).</div><br/><div id="42921698" class="c"><input type="checkbox" id="c-42921698" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42921574">parent</a><span>|</span><a href="#42926119">next</a><span>|</span><label class="collapse" for="c-42921698">[-]</label><label class="expand" for="c-42921698">[8 more]</label></div><br/><div class="children"><div class="content">See my comments above. The reality, I believe, is that this is largely driven by idealistic west coast gen-z and younger millenials who feel certain that their world-view is righteous, to the extent that they feel they are only helping by implementing these tools.<p>I think, unfortunately, they will learn too late that building censorship and thought-shifting tools into their LLMs will ultimately put them at the mercy of larger forces, and they may not like the results.<p>I&#x27;d like to hear from Anthropic safety folks on whether or not their constitutional approach might be used to implement redirection or &quot;safety stops&quot; on, say, chats where young women in sub-saharan Africa look for advice about avoiding genital mutilation. (<a href="https:&#x2F;&#x2F;www.unfpa.org&#x2F;resources&#x2F;female-genital-mutilation-fgm-frequently-asked-questions" rel="nofollow">https:&#x2F;&#x2F;www.unfpa.org&#x2F;resources&#x2F;female-genital-mutilation-fg...</a> for much more on this sad topic).<p>Government officials and thought leaders in these countries, male and female, are convinced that FGM is right and appropriate. What is, in fact, right, and who decides? This, in my opinion, is going to be the second &quot;bitter lesson&quot; for AI. It&#x27;s a lesson the Facebooks of the world learned over the last 20 years -- there is absolutely no way to properly &#x27;moderate&#x27; the world&#x27;s content to some global standard of norms. Norms vary hugely. Putting yourself in the position of censoring &#x2F; redirecting is putting yourself in the position of being a villain, and ultimately harming people.</div><br/><div id="42923899" class="c"><input type="checkbox" id="c-42923899" checked=""/><div class="controls bullet"><span class="by">Muromec</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42921698">parent</a><span>|</span><a href="#42921728">next</a><span>|</span><label class="collapse" for="c-42923899">[-]</label><label class="expand" for="c-42923899">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I think, unfortunately, they will learn too late that building censorship and thought-shifting tools into their LLMs will ultimately put them at the mercy of larger forces, and they may not like the results.<p>That the optimistic view -- people with fancy tools can outsmart the people with money and people with money can outspend the people with power, but only on a short distance. Eventually, the big G catches up to everything and puts it all to use. It also turns out to not be that bad anyway (example: read how software developers working for government were described in the snow crash).<p>The less optimistic view -- the government doesn&#x27;t catch up to it before the changes to society result in it&#x27;s collapse (case in point -- industrial revolution, religious wars and invention of the ethnic language-based republics).<p>I&#x27;m not entirely sure that we are in the optimistic one, unfortunately.</div><br/></div></div><div id="42921728" class="c"><input type="checkbox" id="c-42921728" checked=""/><div class="controls bullet"><span class="by">Fauntleroy</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42921698">parent</a><span>|</span><a href="#42923899">prev</a><span>|</span><a href="#42922255">next</a><span>|</span><label class="collapse" for="c-42921728">[-]</label><label class="expand" for="c-42921728">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m certain they&#x27;ve thought of this and have decided that the alternative—a firehose of whatever data the AI has in its grasp—is worse than the &quot;censored&quot; version. I&#x27;m curious to know what your ideal approach would be.</div><br/><div id="42922590" class="c"><input type="checkbox" id="c-42922590" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42921728">parent</a><span>|</span><a href="#42922255">next</a><span>|</span><label class="collapse" for="c-42922590">[-]</label><label class="expand" for="c-42922590">[1 more]</label></div><br/><div class="children"><div class="content">Open weights and open models with open tools that allow user-defined alignment and realignment is, I believe, the only really humanist path forward. We can&#x27;t choose for people. It&#x27;s wrong to think we know better than they do what they want. Full stop.<p>Some of those people will make terrible decisions, some will make objectionable ones, but the alternative is just full thought control, basically. And, sadly, nobody in the &quot;bad&quot; scenario need be anything but super well intentioned (if naive).</div><br/></div></div></div></div><div id="42922255" class="c"><input type="checkbox" id="c-42922255" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42921698">parent</a><span>|</span><a href="#42921728">prev</a><span>|</span><a href="#42926119">next</a><span>|</span><label class="collapse" for="c-42922255">[-]</label><label class="expand" for="c-42922255">[4 more]</label></div><br/><div class="children"><div class="content">b.t.w. no need to resort to sub-saharan Africa to talk about genital mutilation - it&#x27;s standard practice in the good old USA as well.</div><br/><div id="42922630" class="c"><input type="checkbox" id="c-42922630" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42922255">parent</a><span>|</span><a href="#42926119">next</a><span>|</span><label class="collapse" for="c-42922630">[-]</label><label class="expand" for="c-42922630">[3 more]</label></div><br/><div class="children"><div class="content">Oof. That&#x27;s a tough read, thanks for pointing me at that. I think it&#x27;s worth distinguishing these, though -- CDC data in the US says this is largely an immigrant community thing with immigrants from FGM countries. I do not believe US policy makers and thought leaders think FGM is a good thing in the US - we&#x27;re all sort of aligned internally, even if it is still a thing that happens. By contrast, the source countries practice it in the belief that it&#x27;s a good thing for women. (With complaints on stereotypes and summarization acknowledged)</div><br/><div id="42923247" class="c"><input type="checkbox" id="c-42923247" checked=""/><div class="controls bullet"><span class="by">NoMoreNicksLeft</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42922630">parent</a><span>|</span><a href="#42926119">next</a><span>|</span><label class="collapse" for="c-42923247">[-]</label><label class="expand" for="c-42923247">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I do not believe US policy makers and thought leaders think FGM is a good thing in the US<p>Did I misread? I don&#x27;t think that OP said <i>female</i> genital mutilation. Some very large fraction of infant males in the United States are mutilated.</div><br/><div id="42925123" class="c"><input type="checkbox" id="c-42925123" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#42921574">root</a><span>|</span><a href="#42923247">parent</a><span>|</span><a href="#42926119">next</a><span>|</span><label class="collapse" for="c-42925123">[-]</label><label class="expand" for="c-42925123">[1 more]</label></div><br/><div class="children"><div class="content">They did not, but you are absolutely correct that it&#x27;s very widespread with boys here in the US, and the varying reactions to those two things are a good point about social norms for sure.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42926119" class="c"><input type="checkbox" id="c-42926119" checked=""/><div class="controls bullet"><span class="by">eadmund</span><span>|</span><a href="#42921574">parent</a><span>|</span><a href="#42921698">prev</a><span>|</span><a href="#42921669">next</a><span>|</span><label class="collapse" for="c-42926119">[-]</label><label class="expand" for="c-42926119">[1 more]</label></div><br/><div class="children"><div class="content">I believe that the real point is not to prevent access to information, but rather to prevent production of wrongthink.<p>Any fact which the model trainer wishes to disappear — whether that is what happened at Tiananmen Square between April and June 1989, any other inconvenient fact — will simply not be capable of being discussed.  It’s a censor’s dream.<p>We need local models without so-called guardrails or ‘safety.’</div><br/></div></div><div id="42921669" class="c"><input type="checkbox" id="c-42921669" checked=""/><div class="controls bullet"><span class="by">miohtama</span><span>|</span><a href="#42921574">parent</a><span>|</span><a href="#42926119">prev</a><span>|</span><a href="#42921730">next</a><span>|</span><label class="collapse" for="c-42921669">[-]</label><label class="expand" for="c-42921669">[1 more]</label></div><br/><div class="children"><div class="content">Seizing web servers is coming next, as per the recent UK laws forum hosting is responsible for &quot;evil&quot; content. It does not need to be illegal. This has been discussed in the HN as well.<p>Software industry that defines bad is called compliance-industrial complex.<p>Defining bad is a big business. Here is a good book about pre-crime society we are starting to live:<p><a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Compliance-Industrial-Complex-Operating-Pre-Crime-Society&#x2F;dp&#x2F;3031192230" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;Compliance-Industrial-Complex-Operati...</a></div><br/></div></div><div id="42921730" class="c"><input type="checkbox" id="c-42921730" checked=""/><div class="controls bullet"><span class="by">zboubmaster</span><span>|</span><a href="#42921574">parent</a><span>|</span><a href="#42921669">prev</a><span>|</span><a href="#42922201">next</a><span>|</span><label class="collapse" for="c-42921730">[-]</label><label class="expand" for="c-42921730">[1 more]</label></div><br/><div class="children"><div class="content">Because these companies emphasize the personal trustworthiness of these chatbots (and their responsibility by proxy) and need to offer actual way to systematically block certain requests to be actually marketable. This is like getting mad because a doctor won&#x27;t give you advice for committing suicide</div><br/></div></div><div id="42922201" class="c"><input type="checkbox" id="c-42922201" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#42921574">parent</a><span>|</span><a href="#42921730">prev</a><span>|</span><a href="#42924771">next</a><span>|</span><label class="collapse" for="c-42922201">[-]</label><label class="expand" for="c-42922201">[1 more]</label></div><br/><div class="children"><div class="content">Censorship is often applied on the easiest, most popular access methods even though the information is theoretically public, and it has a real effect. Suppose for some reason you wanted to make sarin. You could spend hours poring over research papers, or you could ask Google or ChatGPT &quot;how do I make sarin?&quot;<p>And later, as ChatGPT becomes the only interface to the world&#x27;s information, the gap between information that can theoretically be accessed by anyone and information that can actually be accessed by anyone will only become wider.<p>Even having to take a college class, even if anyone can take it, is a pretty big barrier.</div><br/></div></div></div></div><div id="42924771" class="c"><input type="checkbox" id="c-42924771" checked=""/><div class="controls bullet"><span class="by">dash2</span><span>|</span><a href="#42921574">prev</a><span>|</span><a href="#42921104">next</a><span>|</span><label class="collapse" for="c-42924771">[-]</label><label class="expand" for="c-42924771">[2 more]</label></div><br/><div class="children"><div class="content">My ignorant outsider perspective.<p>If you ask a real chemical expert &quot;how can I make sarin?&quot; he will refuse to answer because he knows it&#x27;s unethical to make sarin.<p>You&#x27;d expect AGI to include the basic understanding of ethics such that not doing bad stuff is built in. You might even expect an understanding of ethics to emerge from ordinary training. The training data contains information about meteorology, about James Joyce... and also about the human understanding of right and wrong, no?<p>These systems all seem to work by having a &quot;filter&quot;. It&#x27;s like you have a separate person saying &quot;no, don&#x27;t answer that question&quot;. But if you get past the gatekeeper, then the original person will cheerfully do anything evil.<p>Why don&#x27;t we see more attempts to build ethics into the original AI?</div><br/><div id="42929481" class="c"><input type="checkbox" id="c-42929481" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#42924771">parent</a><span>|</span><a href="#42921104">next</a><span>|</span><label class="collapse" for="c-42929481">[-]</label><label class="expand" for="c-42929481">[1 more]</label></div><br/><div class="children"><div class="content">Google will tell you how to make sarin. It&#x27;s not even hard, any idiot can make it in their garage. You can even make it unintentionally when gas welding.</div><br/></div></div></div></div><div id="42921104" class="c"><input type="checkbox" id="c-42921104" checked=""/><div class="controls bullet"><span class="by">i_have_an_idea</span><span>|</span><a href="#42924771">prev</a><span>|</span><a href="#42923964">next</a><span>|</span><label class="collapse" for="c-42921104">[-]</label><label class="expand" for="c-42921104">[4 more]</label></div><br/><div class="children"><div class="content">So, in essence, both the input and the output are read by a LLM that&#x27;s fine-tuned to censor. If it flags up content, it instructs the core model to refuse. Similar to most AI-based moderation systems. It&#x27;s a bit more complicated as there&#x27;s one LLM for inputs and another one for outputs, but it&#x27;s not really a groundbreaking idea.</div><br/><div id="42922025" class="c"><input type="checkbox" id="c-42922025" checked=""/><div class="controls bullet"><span class="by">reissbaker</span><span>|</span><a href="#42921104">parent</a><span>|</span><a href="#42921658">next</a><span>|</span><label class="collapse" for="c-42922025">[-]</label><label class="expand" for="c-42922025">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right that it&#x27;s not entirely novel, but it is <i>useful</i>, at least for Claude users: there&#x27;s quite a bit of research showing that training models to self-censor makes them dumber, and so putting the censorship into a separate model (and allowing Claude to use its full intelligence for the &quot;safe&quot; queries) is a fairly useful change assuming it works well enough to prevent further lobotomization of the chat model.<p>(Of course, open-source models are even more useful...)</div><br/><div id="42922974" class="c"><input type="checkbox" id="c-42922974" checked=""/><div class="controls bullet"><span class="by">i_have_an_idea</span><span>|</span><a href="#42921104">root</a><span>|</span><a href="#42922025">parent</a><span>|</span><a href="#42921658">next</a><span>|</span><label class="collapse" for="c-42922974">[-]</label><label class="expand" for="c-42922974">[1 more]</label></div><br/><div class="children"><div class="content">that is an interesting insight</div><br/></div></div></div></div><div id="42921658" class="c"><input type="checkbox" id="c-42921658" checked=""/><div class="controls bullet"><span class="by">guerrilla</span><span>|</span><a href="#42921104">parent</a><span>|</span><a href="#42922025">prev</a><span>|</span><a href="#42923964">next</a><span>|</span><label class="collapse" for="c-42921658">[-]</label><label class="expand" for="c-42921658">[1 more]</label></div><br/><div class="children"><div class="content">Also, no chance it&#x27;s unbreakable.</div><br/></div></div></div></div><div id="42923964" class="c"><input type="checkbox" id="c-42923964" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#42921104">prev</a><span>|</span><a href="#42922368">next</a><span>|</span><label class="collapse" for="c-42923964">[-]</label><label class="expand" for="c-42923964">[1 more]</label></div><br/><div class="children"><div class="content">This feels to me like the most useless definition of &quot;AI safety&quot; in practice, and it&#x27;s astonishing to see just how much R&amp;D efforts are spent on it.<p>Thankfully the open-weights models are trivially jailbreakable regardless of any baked-in guardrails simply because one controls the generation loop and can <i>make</i> the model not refuse.</div><br/></div></div><div id="42922368" class="c"><input type="checkbox" id="c-42922368" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#42923964">prev</a><span>|</span><a href="#42920952">next</a><span>|</span><label class="collapse" for="c-42922368">[-]</label><label class="expand" for="c-42922368">[1 more]</label></div><br/><div class="children"><div class="content">Posted my notes about this here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Feb&#x2F;3&#x2F;constitutional-classifiers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Feb&#x2F;3&#x2F;constitutional-classifi...</a></div><br/></div></div><div id="42920952" class="c"><input type="checkbox" id="c-42920952" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#42922368">prev</a><span>|</span><a href="#42924808">next</a><span>|</span><label class="collapse" for="c-42920952">[-]</label><label class="expand" for="c-42920952">[1 more]</label></div><br/><div class="children"><div class="content">&gt; An updated version achieved similar robustness on synthetic evaluations, and did so with a 0.38% increase in refusal rates and moderate additional compute costs.<p>&quot;Synthetic evaluations&quot; aren&#x27;t 70 hours of Pliny the Prompter.</div><br/></div></div><div id="42924808" class="c"><input type="checkbox" id="c-42924808" checked=""/><div class="controls bullet"><span class="by">kouteiheika</span><span>|</span><a href="#42920952">prev</a><span>|</span><a href="#42929669">next</a><span>|</span><label class="collapse" for="c-42924808">[-]</label><label class="expand" for="c-42924808">[1 more]</label></div><br/><div class="children"><div class="content">The whole anti jailbreaking research seems like a total waste of time.<p>You can&#x27;t never guarantee that a jailbreak won&#x27;t be possible, so you never should deploy an LLM in places where a jailbreak would be disasterous anyway, so the only thing this achieves is pointless (and often very frustrating to the users, especially if they make an effort to go around it) censorship.<p>It boggles my mind that major  LLM providers refuse to offer an &quot;I&#x27;m an adult, I know what I&#x27;m doing&quot; mode without the censorship and all of the &quot;safety&quot; bullshit.</div><br/></div></div><div id="42929669" class="c"><input type="checkbox" id="c-42929669" checked=""/><div class="controls bullet"><span class="by">mordae</span><span>|</span><a href="#42924808">prev</a><span>|</span><a href="#42922279">next</a><span>|</span><label class="collapse" for="c-42929669">[-]</label><label class="expand" for="c-42929669">[1 more]</label></div><br/><div class="children"><div class="content">This sucks. Just sucks.<p>Go ask Sonnet 3.5 whether it&#x27;s possible that new Trump admin will force AI model companies to train the models in certain way and it will insist on brain-dead canned reply.<p>Ask it whether chilling effects of threatening to withdraw salary and retaliatory actions against prosecutors and FBI agents would make it viable to organize militias out of rioters and neo-nazis and it refuses to discuss fascist playbook.</div><br/></div></div><div id="42922279" class="c"><input type="checkbox" id="c-42922279" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#42929669">prev</a><span>|</span><a href="#42920696">next</a><span>|</span><label class="collapse" for="c-42922279">[-]</label><label class="expand" for="c-42922279">[1 more]</label></div><br/><div class="children"><div class="content">So “How do I get an abortion” is going to get banned very soon in most of the US, and you won&#x27;t be able to jailbreak it…</div><br/></div></div><div id="42920696" class="c"><input type="checkbox" id="c-42920696" checked=""/><div class="controls bullet"><span class="by">ok123456</span><span>|</span><a href="#42922279">prev</a><span>|</span><label class="collapse" for="c-42920696">[-]</label><label class="expand" for="c-42920696">[11 more]</label></div><br/><div class="children"><div class="content">They&#x27;re panicking and hitting the &#x27;AI SAFETY&#x27; button hard.</div><br/><div id="42920846" class="c"><input type="checkbox" id="c-42920846" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42920696">parent</a><span>|</span><label class="collapse" for="c-42920846">[-]</label><label class="expand" for="c-42920846">[10 more]</label></div><br/><div class="children"><div class="content">Panicking how? This seems like a desirable feature a lot of customers are looking for.</div><br/><div id="42920976" class="c"><input type="checkbox" id="c-42920976" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920846">parent</a><span>|</span><label class="collapse" for="c-42920976">[-]</label><label class="expand" for="c-42920976">[9 more]</label></div><br/><div class="children"><div class="content">What customers? I&#x27;ve never heard anyone saying &quot;I wish Claude would refuse more of my requests&quot;.</div><br/><div id="42920983" class="c"><input type="checkbox" id="c-42920983" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920976">parent</a><span>|</span><a href="#42921289">next</a><span>|</span><label class="collapse" for="c-42920983">[-]</label><label class="expand" for="c-42920983">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure they have customers who are saying &quot;I want to deploy a chat bot on my website that can&#x27;t be tricked into giving out prices I don&#x27;t agree to&quot;.</div><br/><div id="42921014" class="c"><input type="checkbox" id="c-42921014" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920983">parent</a><span>|</span><a href="#42921289">next</a><span>|</span><label class="collapse" for="c-42921014">[-]</label><label class="expand" for="c-42921014">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be very interested to know the name of any of those companies letting a LLM set the price for their products. For research purposes only, of course.</div><br/><div id="42921140" class="c"><input type="checkbox" id="c-42921140" checked=""/><div class="controls bullet"><span class="by">BryantD</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42921014">parent</a><span>|</span><a href="#42921289">next</a><span>|</span><label class="collapse" for="c-42921140">[-]</label><label class="expand" for="c-42921140">[2 more]</label></div><br/><div class="children"><div class="content">Air Canada was held liable for a refund offer a chatbot made: <a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;travel&#x2F;article&#x2F;20240222-air-canada-chatbot-misinformation-what-travellers-should-know" rel="nofollow">https:&#x2F;&#x2F;www.bbc.com&#x2F;travel&#x2F;article&#x2F;20240222-air-canada-chatb...</a><p>Not exactly your scenario, but a live example of the sort of problem Anthropic wants to prevent.</div><br/><div id="42921351" class="c"><input type="checkbox" id="c-42921351" checked=""/><div class="controls bullet"><span class="by">ok123456</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42921140">parent</a><span>|</span><a href="#42921289">next</a><span>|</span><label class="collapse" for="c-42921351">[-]</label><label class="expand" for="c-42921351">[1 more]</label></div><br/><div class="children"><div class="content">And, that&#x27;s not what they&#x27;re trying to prevent here.</div><br/></div></div></div></div></div></div></div></div><div id="42921289" class="c"><input type="checkbox" id="c-42921289" checked=""/><div class="controls bullet"><span class="by">hobo_in_library</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920976">parent</a><span>|</span><a href="#42920983">prev</a><span>|</span><a href="#42921154">next</a><span>|</span><label class="collapse" for="c-42921289">[-]</label><label class="expand" for="c-42921289">[1 more]</label></div><br/><div class="children"><div class="content">Similar to what others have mentioned: People offering domain specific bots and don&#x27;t want that expensive compute abused as a free general purpose LLM<p>Imagine you&#x27;re American Airline and someone goes to your chatbot and asks it to generate React code for them</div><br/></div></div><div id="42921154" class="c"><input type="checkbox" id="c-42921154" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920976">parent</a><span>|</span><a href="#42921289">prev</a><span>|</span><a href="#42921071">next</a><span>|</span><label class="collapse" for="c-42921154">[-]</label><label class="expand" for="c-42921154">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never heard a bad actor saying &quot;I wish law enforcement would block more of my efforts&quot;.</div><br/></div></div><div id="42921071" class="c"><input type="checkbox" id="c-42921071" checked=""/><div class="controls bullet"><span class="by">gs17</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920976">parent</a><span>|</span><a href="#42921154">prev</a><span>|</span><a href="#42922707">next</a><span>|</span><label class="collapse" for="c-42921071">[-]</label><label class="expand" for="c-42921071">[1 more]</label></div><br/><div class="children"><div class="content">For example: <a href="https:&#x2F;&#x2F;futurism.com&#x2F;the-byte&#x2F;car-dealership-ai" rel="nofollow">https:&#x2F;&#x2F;futurism.com&#x2F;the-byte&#x2F;car-dealership-ai</a><p>It didn&#x27;t actually result in someone getting a new car for $1, but I&#x27;d imagine the dealer was still annoyed at people (who don&#x27;t live close enough to buy a car from them) abusing their chatbot.</div><br/></div></div><div id="42922707" class="c"><input type="checkbox" id="c-42922707" checked=""/><div class="controls bullet"><span class="by">deadbabe</span><span>|</span><a href="#42920696">root</a><span>|</span><a href="#42920976">parent</a><span>|</span><a href="#42921071">prev</a><span>|</span><label class="collapse" for="c-42922707">[-]</label><label class="expand" for="c-42922707">[1 more]</label></div><br/><div class="children"><div class="content">Would you want to allow a human customer service agent to talk on the phone with a customer about whatever inappropriate or confidential things they felt like asking about?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>