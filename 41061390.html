<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721898068683" as="style"/><link rel="stylesheet" href="styles.css?v=1721898068683"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/mlfoundations/MINT-1T">A multimodal dataset with one trillion tokens</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>kulikalov</span> | <span>37 comments</span></div><br/><div><div id="41064526" class="c"><input type="checkbox" id="c-41064526" checked=""/><div class="controls bullet"><span class="by">supermatt</span><span>|</span><a href="#41061735">next</a><span>|</span><label class="collapse" for="c-41064526">[-]</label><label class="expand" for="c-41064526">[3 more]</label></div><br/><div class="children"><div class="content">I havent trained any LLMs, so please accept my comment with all the naivety with which it is given - but in the &quot;examples of MINT multimodal documents&quot; graphic at the top of the README, it feels to me as though the labeling (for the images on the left) couldn&#x27;t be much worse? Is this normal for these datasets? How are we able to build such powerful models with such poor quality data?</div><br/><div id="41064893" class="c"><input type="checkbox" id="c-41064893" checked=""/><div class="controls bullet"><span class="by">zsyllepsis</span><span>|</span><a href="#41064526">parent</a><span>|</span><a href="#41065071">next</a><span>|</span><label class="collapse" for="c-41064893">[-]</label><label class="expand" for="c-41064893">[1 more]</label></div><br/><div class="children"><div class="content">I think the labels could be much, much worse. They could contain straight noise, just completely random text - not even words. They could also contain plausible, factual text which otherwise has no relationship with the text.<p>I think most commonly image datasets like this consist of images and their captions, with the presumption that the content author had _some_ reason of associating the two. The goal of the model is to learn that association. And with a _lot_ of examples, to learn nuanced representations.<p>In the third image, for example, we see some kind of text on a material. The caption mentions &quot;Every year he rides for someone we know, touched by cancer&quot;. Perhaps the model is fed another example of bicycle races, with similar imagery of racing bibs. Perhaps its fed another of a race that specifically mentions it&#x27;s a charity ride to raise money for cancer. Perhaps....<p>You get the idea. Alone, each example provides only vague connections between the image and the caption. But when you have a <i>ton</i> of data it becomes easier to separate noise from a weak signal.</div><br/></div></div><div id="41065071" class="c"><input type="checkbox" id="c-41065071" checked=""/><div class="controls bullet"><span class="by">whiplash451</span><span>|</span><a href="#41064526">parent</a><span>|</span><a href="#41064893">prev</a><span>|</span><a href="#41061735">next</a><span>|</span><label class="collapse" for="c-41065071">[-]</label><label class="expand" for="c-41065071">[1 more]</label></div><br/><div class="children"><div class="content">Deep learning is robust to massive label noise [1]<p>Not to say that data quality does not matter, but these noisy sets are still very useful.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1705.10694" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1705.10694</a></div><br/></div></div></div></div><div id="41061735" class="c"><input type="checkbox" id="c-41061735" checked=""/><div class="controls bullet"><span class="by">punnerud</span><span>|</span><a href="#41064526">prev</a><span>|</span><a href="#41063620">next</a><span>|</span><label class="collapse" for="c-41061735">[-]</label><label class="expand" for="c-41061735">[12 more]</label></div><br/><div class="children"><div class="content">More info on the Salesforce blog:
<a href="https:&#x2F;&#x2F;blog.salesforceairesearch.com&#x2F;mint-1t&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.salesforceairesearch.com&#x2F;mint-1t&#x2F;</a></div><br/><div id="41061875" class="c"><input type="checkbox" id="c-41061875" checked=""/><div class="controls bullet"><span class="by">j7ake</span><span>|</span><a href="#41061735">parent</a><span>|</span><a href="#41063620">next</a><span>|</span><label class="collapse" for="c-41061875">[-]</label><label class="expand" for="c-41061875">[11 more]</label></div><br/><div class="children"><div class="content">Wow did not expect sales force to be behind this.<p>It’s basically free advertising for technical people to join sales force.</div><br/><div id="41062002" class="c"><input type="checkbox" id="c-41062002" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41061875">parent</a><span>|</span><a href="#41063501">next</a><span>|</span><label class="collapse" for="c-41062002">[-]</label><label class="expand" for="c-41062002">[3 more]</label></div><br/><div class="children"><div class="content">Salesforce has long been involved in publishing quality NLP papers, especially during Stephen Merity&#x27;s tenure.<p>Smerity&#x27;s papers are some of my favourite. Check out<p><a href="https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;1708.02182" rel="nofollow">https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;1708.02182</a><p>And my all-time favourite<p><a href="https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;1911.11423" rel="nofollow">https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;1911.11423</a></div><br/><div id="41063882" class="c"><input type="checkbox" id="c-41063882" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41062002">parent</a><span>|</span><a href="#41063501">next</a><span>|</span><label class="collapse" for="c-41063882">[-]</label><label class="expand" for="c-41063882">[2 more]</label></div><br/><div class="children"><div class="content">Hey thanks for those Smerity links, hadn&#x27;t run across his work yet, second one in particular looks great</div><br/><div id="41064740" class="c"><input type="checkbox" id="c-41064740" checked=""/><div class="controls bullet"><span class="by">jszymborski</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41063882">parent</a><span>|</span><a href="#41063501">next</a><span>|</span><label class="collapse" for="c-41064740">[-]</label><label class="expand" for="c-41064740">[1 more]</label></div><br/><div class="children"><div class="content">Glad you liked it.<p>How could you go wrong with a paper that starts with<p>&gt; Language has been a thorn in humanity’s side since we evolved a complex enough audio and graphics processing unit to grunt, let alone write cryptocurrency whitepapers or opinion columns.</div><br/></div></div></div></div></div></div><div id="41063501" class="c"><input type="checkbox" id="c-41063501" checked=""/><div class="controls bullet"><span class="by">0xDEADFED5</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41061875">parent</a><span>|</span><a href="#41062002">prev</a><span>|</span><a href="#41064184">next</a><span>|</span><label class="collapse" for="c-41063501">[-]</label><label class="expand" for="c-41063501">[1 more]</label></div><br/><div class="children"><div class="content">Salesforce produced one of the best Llama-3(8B) finetunes, IMO: SFR-Iterative-DPO-LLaMA-3-8B-R<p>Hopefully they do something with Llama-3.1</div><br/></div></div><div id="41064184" class="c"><input type="checkbox" id="c-41064184" checked=""/><div class="controls bullet"><span class="by">gotaran</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41061875">parent</a><span>|</span><a href="#41063501">prev</a><span>|</span><a href="#41063620">next</a><span>|</span><label class="collapse" for="c-41064184">[-]</label><label class="expand" for="c-41064184">[6 more]</label></div><br/><div class="children"><div class="content">I’m skeptical of the caliber of talent at Salesforce given the unusable state of their core product.</div><br/><div id="41064219" class="c"><input type="checkbox" id="c-41064219" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41064184">parent</a><span>|</span><a href="#41063620">next</a><span>|</span><label class="collapse" for="c-41064219">[-]</label><label class="expand" for="c-41064219">[5 more]</label></div><br/><div class="children"><div class="content">The people building CRM software aren&#x27;t also the ones doing AI research. The two have nothing to do with each other.</div><br/><div id="41065153" class="c"><input type="checkbox" id="c-41065153" checked=""/><div class="controls bullet"><span class="by">inkyoto</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41064219">parent</a><span>|</span><a href="#41064340">next</a><span>|</span><label class="collapse" for="c-41065153">[-]</label><label class="expand" for="c-41065153">[1 more]</label></div><br/><div class="children"><div class="content">Salesforce have been on a shopping spree for quite a few years now. They have purchased MuleSoft (an integration platform) and Slack amongst the others.<p>Salesforce is anything but a CRM software company nowadays.</div><br/></div></div><div id="41064340" class="c"><input type="checkbox" id="c-41064340" checked=""/><div class="controls bullet"><span class="by">gotaran</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41064219">parent</a><span>|</span><a href="#41065153">prev</a><span>|</span><a href="#41063620">next</a><span>|</span><label class="collapse" for="c-41064340">[-]</label><label class="expand" for="c-41064340">[3 more]</label></div><br/><div class="children"><div class="content">The ones doing AI research would be working at more prestigious institutions.</div><br/><div id="41065025" class="c"><input type="checkbox" id="c-41065025" checked=""/><div class="controls bullet"><span class="by">hluska</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41064340">parent</a><span>|</span><a href="#41064411">next</a><span>|</span><label class="collapse" for="c-41065025">[-]</label><label class="expand" for="c-41065025">[1 more]</label></div><br/><div class="children"><div class="content">Do you have a point or are you just insulting people out of some twisted definition of fun?</div><br/></div></div><div id="41064411" class="c"><input type="checkbox" id="c-41064411" checked=""/><div class="controls bullet"><span class="by">ericjmorey</span><span>|</span><a href="#41061735">root</a><span>|</span><a href="#41064340">parent</a><span>|</span><a href="#41065025">prev</a><span>|</span><a href="#41063620">next</a><span>|</span><label class="collapse" for="c-41064411">[-]</label><label class="expand" for="c-41064411">[1 more]</label></div><br/><div class="children"><div class="content">Why make such a ridiculous assumption?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41063620" class="c"><input type="checkbox" id="c-41063620" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#41061735">prev</a><span>|</span><a href="#41063996">next</a><span>|</span><label class="collapse" for="c-41063620">[-]</label><label class="expand" for="c-41063620">[2 more]</label></div><br/><div class="children"><div class="content">Does it make sense to measure a dataset in tokens? Shouldn&#x27;t it be tokenizer-agnostic? I.e. the OpenAI tokenizer encodes about ~4 characters per token, but I could also have a tokenizer that does 1 character per token leading to a ~4x increase in token count (relative to the OpenAI tokenizer.)</div><br/><div id="41064146" class="c"><input type="checkbox" id="c-41064146" checked=""/><div class="controls bullet"><span class="by">anas-awadalla</span><span>|</span><a href="#41063620">parent</a><span>|</span><a href="#41063996">next</a><span>|</span><label class="collapse" for="c-41064146">[-]</label><label class="expand" for="c-41064146">[1 more]</label></div><br/><div class="children"><div class="content">Hello! Totally agree that tokens will be model dependent. We chose to calculate tokens using the GPT-2 tokenizer as that is a common metric used by other datasets like fineweb. So this should roughly give you a sense of how large the data is in comparison to others. We report other metrics too like number of documents and number of images.</div><br/></div></div></div></div><div id="41063996" class="c"><input type="checkbox" id="c-41063996" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#41063620">prev</a><span>|</span><a href="#41063196">next</a><span>|</span><label class="collapse" for="c-41063996">[-]</label><label class="expand" for="c-41063996">[3 more]</label></div><br/><div class="children"><div class="content">It looks like it contains data from CommonCrawl and ArXiv. It&#x27;s not clear what kind of processing they did, but sometimes these releases seem like just repackaging existing datasets with your name own name on them. It&#x27;s not hard to get bulk downloads from these sources directly.<p>I thought CommonCrawl truncated files at 1MB. I wonder if the PDFs for CommonCrawl were re-fetched from the URLs. That could be useful if they provide simple way to get those full files.</div><br/><div id="41064131" class="c"><input type="checkbox" id="c-41064131" checked=""/><div class="controls bullet"><span class="by">anas-awadalla</span><span>|</span><a href="#41063996">parent</a><span>|</span><a href="#41063196">next</a><span>|</span><label class="collapse" for="c-41064131">[-]</label><label class="expand" for="c-41064131">[2 more]</label></div><br/><div class="children"><div class="content">Hello! Creator of MINT here.<p>We do a lot of pre-processing of commoncrawl (which in its raw form isn’t all that useful for training models). This includes heuristics to remove low quality text and images and deduplicating documents, paragraphs, and images. All of these are crucial to achieve good training performance.<p>On your point regarding PDFs, we actually don’t constraint ourselves to the 1MB files and do our own downloading of PDFs!</div><br/><div id="41064248" class="c"><input type="checkbox" id="c-41064248" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#41063996">root</a><span>|</span><a href="#41064131">parent</a><span>|</span><a href="#41063196">next</a><span>|</span><label class="collapse" for="c-41064248">[-]</label><label class="expand" for="c-41064248">[1 more]</label></div><br/><div class="children"><div class="content">I see. Thanks for the reply. I opened one of the tar files and see now how it has extracted the text into json files.</div><br/></div></div></div></div></div></div><div id="41063196" class="c"><input type="checkbox" id="c-41063196" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#41063996">prev</a><span>|</span><a href="#41065079">next</a><span>|</span><label class="collapse" for="c-41063196">[-]</label><label class="expand" for="c-41063196">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the license though?</div><br/><div id="41063432" class="c"><input type="checkbox" id="c-41063432" checked=""/><div class="controls bullet"><span class="by">dpifke</span><span>|</span><a href="#41063196">parent</a><span>|</span><a href="#41065079">next</a><span>|</span><label class="collapse" for="c-41063432">[-]</label><label class="expand" for="c-41063432">[2 more]</label></div><br/><div class="children"><div class="content">From <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;mlfoundations&#x2F;MINT-1T-HTML#license" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;mlfoundations&#x2F;MINT-1T-HTML#l...</a>:<p><i>We release  MINT-1T under a CC-BY-4.0 license, designating it primarily as a research artifact. While the dataset is freely available, users are responsible for ensuring its legal use in commercial settings. Users must independently verify compliance with applicable laws before employing MINT-1T for commercial purposes.</i><p>Same page includes this caveat:<p><i>Potential Legal and Ethical Concerns: While efforts were made to respect robots.txt files and remove sensitive information, there may still be content that individuals did not explicitly consent to include.</i></div><br/><div id="41064228" class="c"><input type="checkbox" id="c-41064228" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#41063196">root</a><span>|</span><a href="#41063432">parent</a><span>|</span><a href="#41065079">next</a><span>|</span><label class="collapse" for="c-41064228">[-]</label><label class="expand" for="c-41064228">[1 more]</label></div><br/><div class="children"><div class="content">Ah yes, the &quot;if you get busted for copyright violations it&#x27;s not our problem&quot; license.</div><br/></div></div></div></div></div></div><div id="41065079" class="c"><input type="checkbox" id="c-41065079" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#41063196">prev</a><span>|</span><a href="#41061866">next</a><span>|</span><label class="collapse" for="c-41065079">[-]</label><label class="expand" for="c-41065079">[1 more]</label></div><br/><div class="children"><div class="content">License: None<p>Means we can’t legally use it?</div><br/></div></div><div id="41061866" class="c"><input type="checkbox" id="c-41061866" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#41065079">prev</a><span>|</span><a href="#41061518">next</a><span>|</span><label class="collapse" for="c-41061866">[-]</label><label class="expand" for="c-41061866">[10 more]</label></div><br/><div class="children"><div class="content">How effective would modeling raw byte sequences be, with the individual bytes as the &quot;tokens&quot;, and a vocabulary of 256 elements?<p>You could then train on any kind of digital data.</div><br/><div id="41063262" class="c"><input type="checkbox" id="c-41063262" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#41061866">parent</a><span>|</span><a href="#41062276">next</a><span>|</span><label class="collapse" for="c-41063262">[-]</label><label class="expand" for="c-41063262">[1 more]</label></div><br/><div class="children"><div class="content">Due to the way tokenization usually works with LLMs (using BPE — Byte Pair Encoding), there&#x27;s actually usually <i>already</i> a 256-element embedding within the token-space that represents &quot;raw bytes.&quot; You could say that this 256-element set is &quot;pre-seeded&quot; into any BPE encoding — and will remain as part of the encoding as long as at least one document in the dataset used to determine the tokenization, uses each byte at least once in a non-high-frequency-suffix-predictable way.<p>These tokens are also already very much in use by the tokenizer — they get emitted in sequences, to encode single Unicode codepoints that weren&#x27;t common enough in the dataset to get their own tokens, and so instead require multiple tokens to represent them. I believe most tokenizers (e.g. tiktoken) just take the UTF-8 byte-sequences underlying these codepoints and encode them literally as sequences of the above 256-element set.<p>If you&#x27;re curious, here&#x27;s the definition of the encoding used by most modern LLMs, in newline-delimited &quot;[base64 of raw input byte sequence] [tokenID to encode as]&quot; format: <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;encodings&#x2F;cl100k_base.tiktoken" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;encodings&#x2F;cl100k_...</a> . If you decode it, you can observe that the rest of the 256-element single-byte embedding space gets mapped to tokenIDs immediately following those of the ASCII printables.</div><br/></div></div><div id="41062276" class="c"><input type="checkbox" id="c-41062276" checked=""/><div class="controls bullet"><span class="by">nodja</span><span>|</span><a href="#41061866">parent</a><span>|</span><a href="#41063262">prev</a><span>|</span><a href="#41062213">next</a><span>|</span><label class="collapse" for="c-41062276">[-]</label><label class="expand" for="c-41062276">[2 more]</label></div><br/><div class="children"><div class="content">Somewhat inefficient for text, very inefficient for images, specially if you work in pixel space. The max context a model today has been trained is 1M tokens, which takes up a lot of memory. Even if context was not an issue, to generate a 1000x1000 image would take ~3 hours on 100token&#x2F;s inference.<p>Google has trained an encoder&#x2F;decoder LLM on bytes called ByT5[1]<p>[1] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;byt5-xxl" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;byt5-xxl</a></div><br/><div id="41062828" class="c"><input type="checkbox" id="c-41062828" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#41061866">root</a><span>|</span><a href="#41062276">parent</a><span>|</span><a href="#41062213">next</a><span>|</span><label class="collapse" for="c-41062828">[-]</label><label class="expand" for="c-41062828">[1 more]</label></div><br/><div class="children"><div class="content">I think the work on multi-token prediction[0] within a single turn could be a significant development that makes byte-level tokenization models more practical. This approach allows the model to predict multiple tokens in parallel, potentially addressing the efficiency concerns raised about byte-level models.<p>By predicting multiple tokens simultaneously, it could significantly speed up inference time, especially for tasks that require generating large amounts of data (like images). This could help mitigate the performance bottleneck mentioned in the parent comment about generating a 1000x1000 image.<p>[0] <a href="https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;2404.19737" rel="nofollow">https:&#x2F;&#x2F;ar5iv.labs.arxiv.org&#x2F;html&#x2F;2404.19737</a></div><br/></div></div></div></div><div id="41062213" class="c"><input type="checkbox" id="c-41062213" checked=""/><div class="controls bullet"><span class="by">akrymski</span><span>|</span><a href="#41061866">parent</a><span>|</span><a href="#41062276">prev</a><span>|</span><a href="#41062445">next</a><span>|</span><label class="collapse" for="c-41062213">[-]</label><label class="expand" for="c-41062213">[3 more]</label></div><br/><div class="children"><div class="content">Forget bytes, go for bits.  Vocab of size 2.  At a theoretical level all of AI comes down to a classifier that is able to predict the next bit given a string of bits.  Check out Tsetlin Machines.  At some point we will be doing it in hardware.<p><a href="https:&#x2F;&#x2F;byte-gpt.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;byte-gpt.github.io&#x2F;</a></div><br/><div id="41062436" class="c"><input type="checkbox" id="c-41062436" checked=""/><div class="controls bullet"><span class="by">kulikalov</span><span>|</span><a href="#41061866">root</a><span>|</span><a href="#41062213">parent</a><span>|</span><a href="#41062445">next</a><span>|</span><label class="collapse" for="c-41062436">[-]</label><label class="expand" for="c-41062436">[2 more]</label></div><br/><div class="children"><div class="content">Sounds inefficient. It’s like predicting the boiling point of a kettle by measuring the speed of individual molecules of water.</div><br/><div id="41062535" class="c"><input type="checkbox" id="c-41062535" checked=""/><div class="controls bullet"><span class="by">BizarroLand</span><span>|</span><a href="#41061866">root</a><span>|</span><a href="#41062436">parent</a><span>|</span><a href="#41062445">next</a><span>|</span><label class="collapse" for="c-41062535">[-]</label><label class="expand" for="c-41062535">[1 more]</label></div><br/><div class="children"><div class="content">That would be surprisingly easy with 1st year calculus as long as you were willing to accept a small degree of inaccuracy.</div><br/></div></div></div></div></div></div><div id="41062445" class="c"><input type="checkbox" id="c-41062445" checked=""/><div class="controls bullet"><span class="by">donnyg</span><span>|</span><a href="#41061866">parent</a><span>|</span><a href="#41062213">prev</a><span>|</span><a href="#41062338">next</a><span>|</span><label class="collapse" for="c-41062445">[-]</label><label class="expand" for="c-41062445">[1 more]</label></div><br/><div class="children"><div class="content">It has been tried with decent results: <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;ai-self-supervised-learning-data2vec&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;ai-self-supervised-learning-data2ve...</a></div><br/></div></div><div id="41062338" class="c"><input type="checkbox" id="c-41062338" checked=""/><div class="controls bullet"><span class="by">joshuamcginnis</span><span>|</span><a href="#41061866">parent</a><span>|</span><a href="#41062445">prev</a><span>|</span><a href="#41063450">next</a><span>|</span><label class="collapse" for="c-41062338">[-]</label><label class="expand" for="c-41062338">[1 more]</label></div><br/><div class="children"><div class="content">You might be interested in reading up on DNA sequence llm models and tooling.</div><br/></div></div><div id="41063450" class="c"><input type="checkbox" id="c-41063450" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#41061866">parent</a><span>|</span><a href="#41062338">prev</a><span>|</span><a href="#41061518">next</a><span>|</span><label class="collapse" for="c-41063450">[-]</label><label class="expand" for="c-41063450">[1 more]</label></div><br/><div class="children"><div class="content">It has pros and cons, like anything else.<p>Tokenization:<p>- Serves as a form of compression. The main benefit of that is supporting longer sequences for any given context window. As a side benefit, it squeezes about the same amount of &quot;information&quot; into each token -- meaning you don&#x27;t have to add any terms to your model to account for such an imbalance (or even test whether that hyperparameter matters).<p>- Allows you to insert stuff other than the raw data into your stream of &quot;tokens&quot; to the LLM. For something like a chatbot, that could be as simple as a prefix to whoever&#x27;s talking next (e.g., system, user, model). You similarly probably want control characters to denote the end of a sequence. If you have multi-modal content (e.g., text + images), you need some way to delimit the transition between those. All of those problems could mostly be solved with an appropriate encoding scheme, but that&#x27;s basically tokenization by a different name (in that it&#x27;s a transformation from one set of tokens to another that you have to apply to every input).<p>You can solve that second problem trivially with just a vocabulary of 256 &quot;byte&quot; tokens plus O(1) control tokens, so that&#x27;s not a huge deal in practice, just a point worth mentioning if we&#x27;re talking about actually naively encoding bytes.<p>The first problem is more interesting. One observation is that if for your particular problem tokenization doesn&#x27;t offer much compression, the difference won&#x27;t matter much, or will favor raw bytes over tokenization if the tokenization isn&#x27;t tailored to your particular data. IIRC there was something about Hebrew text floating around as an example of raw byte models performing better than tokenized models.<p>Another observation is that if your particular model has any form of compression for redundant state space (not true of any sort of vanilla transformer, mostly not true for any major competitor, technically possible regardless), especially if the cost of processing a token isn&#x27;t substantially greater than the cost per byte of tokenizing an input, you also don&#x27;t buy anything from tokenization. You&#x27;re absolutely able to feed that raw data in and let the model handle the details.<p>On the flip side, suppose you&#x27;re handling vanilla English text with a vanilla transformer. You can support something like 50x longer sequences basically for free by adding tokenization. You&#x27;d be silly not to.<p>Image transformers are slightly different in some sense, at least in typical implementations. The tokenization is lossy (not injective), and the de-tokenization must therefore have the opposite property (not a function -- or, since it is a function, it either doesn&#x27;t reproduce every possible input image patch or has randomness to at least match the right distribution hopefully). They&#x27;re often called the same thing, but I view that as something different from tokenization. Certain categories of problems (much like the English text example above) are made drastically cheaper by the process. Others (unlike the English text example above) are rendered impossible by the loss of information. A byte vocabulary makes those theoretically possible again, but you suddenly need a way to handle the &quot;entropy per byte&quot; problem which you didn&#x27;t have to care about before.<p>Maybe one last idea, fuzzy detokenization (like in image transformers) has a notable advantage in spec adherence. Outputting an image and then letting some other hand-written code convert that to a png is much more likely to produce something usable than outputting a png directly, byte by byte. The whole thing is probabilistic, and the flurry of strategies you&#x27;ve seen along the lines of &quot;decode while greedily adhering to a schema (json being the canonical example everyone wants to use for some reason, if you want to search for it)&quot; produce the wrong output distribution, often drastically so, by virtue of the biased sampling on something only correct because of its conditional probabilities. I&#x27;m not sure exactly how big of a model you need (or how tailored of a loss function) to make a model reliably output correct, large png files, but the current SOTA isn&#x27;t there yet for general-purpose problems.<p>In practice, people have made some byte-token models. They vary from &quot;meh&quot; to SOTA depending on the problem. On most problems, they&#x27;re much more expensive than tokenized solutions. Interestingly, when they&#x27;re SOTA they tend to be among the cheaper solutions.<p>I&#x27;ve been chipping away at some new model architectures, and something kind of like a byte-token solution is pretty suitable for those, largely because the model itself offers that compression you would otherwise obtain from tokenization. I&#x27;ll finish and release them one of these years. For transformers though, the byte-token solution is usually only interesting insofar as proving people&#x27;s suspicions. Results are fine, not amazing, except in special cases.</div><br/></div></div></div></div></div></div></div></div></div></body></html>