<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700211657985" as="style"/><link rel="stylesheet" href="styles.css?v=1700211657985"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://til.simonwillison.net/duckdb/remote-parquet">Summing columns in remote Parquet files using DuckDB</a> <span class="domain">(<a href="https://til.simonwillison.net">til.simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>56 comments</span></div><br/><div><div id="38297760" class="c"><input type="checkbox" id="c-38297760" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38271190">next</a><span>|</span><label class="collapse" for="c-38297760">[-]</label><label class="expand" for="c-38297760">[7 more]</label></div><br/><div class="children"><div class="content">The real magic is this query here (which I got via Twitter after posting this article):<p><pre><code>    SELECT
        SUM(size) AS size
    FROM read_parquet(
        list_transform(
            generate_series(0, 55),
            n -&gt; &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;&#x27; ||
                format(&#x27;{:06d}&#x27;, n) || &#x27;.parquet&#x27;
        )
    );
</code></pre>
Turns out DuckDB has a lambda function feature these days! The n -&gt; syntax.</div><br/><div id="38300913" class="c"><input type="checkbox" id="c-38300913" checked=""/><div class="controls bullet"><span class="by">notachatbot123</span><span>|</span><a href="#38297760">parent</a><span>|</span><a href="#38298425">next</a><span>|</span><label class="collapse" for="c-38300913">[-]</label><label class="expand" for="c-38300913">[1 more]</label></div><br/><div class="children"><div class="content">Twitter isn&#x27;t a person, it would be nice to credit the actual human who provided this to you.</div><br/></div></div><div id="38298425" class="c"><input type="checkbox" id="c-38298425" checked=""/><div class="controls bullet"><span class="by">ies7</span><span>|</span><a href="#38297760">parent</a><span>|</span><a href="#38300913">prev</a><span>|</span><a href="#38271190">next</a><span>|</span><label class="collapse" for="c-38298425">[-]</label><label class="expand" for="c-38298425">[5 more]</label></div><br/><div class="children"><div class="content">TIL. Usually I just went with SELECT * FROM &#x27;folder&#x2F;prefix*.parquet&#x27;</div><br/><div id="38298700" class="c"><input type="checkbox" id="c-38298700" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38297760">root</a><span>|</span><a href="#38298425">parent</a><span>|</span><a href="#38271190">next</a><span>|</span><label class="collapse" for="c-38298700">[-]</label><label class="expand" for="c-38298700">[4 more]</label></div><br/><div class="children"><div class="content">That works for files on disk but not for files fetched via HTTP - though apparently DuckDB can do that for some situations, eg if they are in an S3 bucket that it can list files in.</div><br/><div id="38300372" class="c"><input type="checkbox" id="c-38300372" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#38297760">root</a><span>|</span><a href="#38298700">parent</a><span>|</span><a href="#38299092">next</a><span>|</span><label class="collapse" for="c-38300372">[-]</label><label class="expand" for="c-38300372">[1 more]</label></div><br/><div class="children"><div class="content">Actually...<p>(sorry, not picking on you Simon! Awesome post and I just love reading and talking about this stuff)<p>With duckdb running on python you can register your own file-system adapters. This means that you can do things like intercept globbing, transform urls&#x2F;path or physically getting files.<p>This means that you could inject whatever listing&#x2F;lookup that might be needed for your read_{format}() table function use case.<p><a href="http:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;guides&#x2F;python&#x2F;filesystems" rel="nofollow noreferrer">http:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;guides&#x2F;python&#x2F;filesystems</a><p>However, it is true that there is no standard way to glob over HTTP unless you&#x27;re using something like S3.</div><br/></div></div><div id="38299092" class="c"><input type="checkbox" id="c-38299092" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#38297760">root</a><span>|</span><a href="#38298700">parent</a><span>|</span><a href="#38300372">prev</a><span>|</span><a href="#38271190">next</a><span>|</span><label class="collapse" for="c-38299092">[-]</label><label class="expand" for="c-38299092">[2 more]</label></div><br/><div class="children"><div class="content">Yep - HTTP has no native support for file listings. In the old days it would be served at the index of a url path if no actual file was available - but that was always a feature of the http server and not anything unique to HTTP. Protocols like S3 and FTP have listings built in.</div><br/><div id="38300079" class="c"><input type="checkbox" id="c-38300079" checked=""/><div class="controls bullet"><span class="by">booi</span><span>|</span><a href="#38297760">root</a><span>|</span><a href="#38299092">parent</a><span>|</span><a href="#38271190">next</a><span>|</span><label class="collapse" for="c-38300079">[-]</label><label class="expand" for="c-38300079">[1 more]</label></div><br/><div class="children"><div class="content">HTTP doesn&#x27;t but I think you can in WebDAV, HTTP&#x27;s long lost &quot;file management&quot; extension which actually has great support from most HTTP servers</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38271190" class="c"><input type="checkbox" id="c-38271190" checked=""/><div class="controls bullet"><span class="by">richraposa</span><span>|</span><a href="#38297760">prev</a><span>|</span><a href="#38298799">next</a><span>|</span><label class="collapse" for="c-38271190">[-]</label><label class="expand" for="c-38271190">[12 more]</label></div><br/><div class="children"><div class="content">Nice. I did the same thing on ClickHouse - took 11 seconds:<p><pre><code>    SELECT sum(size)
    FROM url(&#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;0000{01..55}.parquet&#x27;)
    
    Query id: 9d145763-0754-4aa2-bb7d-f6917690f704
    
    ┌───────sum(size)─┐
    │ 159344011148016 │
    └─────────────────┘
    
    1 row in set. Elapsed: 11.615 sec. Processed 54.08 million rows, 8.50 GB (4.66 million rows&#x2F;s., 731.83 MB&#x2F;s.)
    Peak memory usage: 458.88 KiB.</code></pre></div><br/><div id="38298515" class="c"><input type="checkbox" id="c-38298515" checked=""/><div class="controls bullet"><span class="by">zxexz</span><span>|</span><a href="#38271190">parent</a><span>|</span><a href="#38272246">next</a><span>|</span><label class="collapse" for="c-38298515">[-]</label><label class="expand" for="c-38298515">[4 more]</label></div><br/><div class="children"><div class="content">I love and use both clickhouse and duckdb all the time, but it&#x27;s become almost comical how senior clickhouse leadership comment on any duckdb-related HN thread like clockwork. And only very rarely bring up their affiliation :)</div><br/><div id="38298951" class="c"><input type="checkbox" id="c-38298951" checked=""/><div class="controls bullet"><span class="by">richraposa</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38298515">parent</a><span>|</span><a href="#38298667">next</a><span>|</span><label class="collapse" for="c-38298951">[-]</label><label class="expand" for="c-38298951">[2 more]</label></div><br/><div class="children"><div class="content">...says the completely anonymous internet guy. I&#x27;m laughing as much as you are.<p>I wasn&#x27;t trying to imply anything negative about DuckDB with my post - was just sharing how ClickHouse does the same thing. FWIW: the blog author added my query to his blog, so my non-combative comment was politely received.</div><br/><div id="38299398" class="c"><input type="checkbox" id="c-38299398" checked=""/><div class="controls bullet"><span class="by">swasheck</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38298951">parent</a><span>|</span><a href="#38298667">next</a><span>|</span><label class="collapse" for="c-38299398">[-]</label><label class="expand" for="c-38299398">[1 more]</label></div><br/><div class="children"><div class="content">but what the person said is true: seems like clickhouse comments descend upon every recent duckdb post as if it’s some sort of competition or born out of inferiority complex.<p>clickhouse is really cool tech. duckdb is really cool tech. i grow weary of the CH infiltration to the point where it’s working against the intent to make me love CH</div><br/></div></div></div></div><div id="38298667" class="c"><input type="checkbox" id="c-38298667" checked=""/><div class="controls bullet"><span class="by">chatmasta</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38298515">parent</a><span>|</span><a href="#38298951">prev</a><span>|</span><a href="#38272246">next</a><span>|</span><label class="collapse" for="c-38298667">[-]</label><label class="expand" for="c-38298667">[1 more]</label></div><br/><div class="children"><div class="content">You mean this username composed of a random assortment of Latin characters isn&#x27;t an unaffiliated, independent commentator?!</div><br/></div></div></div></div><div id="38272246" class="c"><input type="checkbox" id="c-38272246" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38271190">parent</a><span>|</span><a href="#38298515">prev</a><span>|</span><a href="#38298799">next</a><span>|</span><label class="collapse" for="c-38272246">[-]</label><label class="expand" for="c-38272246">[7 more]</label></div><br/><div class="children"><div class="content">Do you know how much data it had to download to run that query? Did it pull all 8GB?</div><br/><div id="38277265" class="c"><input type="checkbox" id="c-38277265" checked=""/><div class="controls bullet"><span class="by">richraposa</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38272246">parent</a><span>|</span><a href="#38274994">next</a><span>|</span><label class="collapse" for="c-38277265">[-]</label><label class="expand" for="c-38277265">[2 more]</label></div><br/><div class="children"><div class="content">ClickHouse just reads the values from the one column being summed: <a href="https:&#x2F;&#x2F;www.markhneedham.com&#x2F;blog&#x2F;2023&#x2F;11&#x2F;15&#x2F;clickhouse-summing-columns-remote-files&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.markhneedham.com&#x2F;blog&#x2F;2023&#x2F;11&#x2F;15&#x2F;clickhouse-summ...</a></div><br/></div></div><div id="38274994" class="c"><input type="checkbox" id="c-38274994" checked=""/><div class="controls bullet"><span class="by">zX41ZdbW</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38272246">parent</a><span>|</span><a href="#38277265">prev</a><span>|</span><a href="#38298243">next</a><span>|</span><label class="collapse" for="c-38274994">[-]</label><label class="expand" for="c-38274994">[1 more]</label></div><br/><div class="children"><div class="content">The same query takes only 1.4 seconds on my server, so I assume that the query does not read all 8 GB.</div><br/></div></div><div id="38298243" class="c"><input type="checkbox" id="c-38298243" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38272246">parent</a><span>|</span><a href="#38274994">prev</a><span>|</span><a href="#38298799">next</a><span>|</span><label class="collapse" for="c-38298243">[-]</label><label class="expand" for="c-38298243">[3 more]</label></div><br/><div class="children"><div class="content">I ran a network monitor while running that query, it pulled down ~290MB</div><br/><div id="38298339" class="c"><input type="checkbox" id="c-38298339" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38298243">parent</a><span>|</span><a href="#38298799">next</a><span>|</span><label class="collapse" for="c-38298339">[-]</label><label class="expand" for="c-38298339">[2 more]</label></div><br/><div class="children"><div class="content">Almost exactly the same as DuckDB then - I measured ~287MB.</div><br/><div id="38298366" class="c"><input type="checkbox" id="c-38298366" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38271190">root</a><span>|</span><a href="#38298339">parent</a><span>|</span><a href="#38298799">next</a><span>|</span><label class="collapse" for="c-38298366">[-]</label><label class="expand" for="c-38298366">[1 more]</label></div><br/><div class="children"><div class="content">Probably compression</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38298799" class="c"><input type="checkbox" id="c-38298799" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#38271190">prev</a><span>|</span><a href="#38271126">next</a><span>|</span><label class="collapse" for="c-38298799">[-]</label><label class="expand" for="c-38298799">[2 more]</label></div><br/><div class="children"><div class="content">Here is some other interesting and related duckdb SQL that you all might find helpful.<p>inspect the parquet metadata:<p><pre><code>    SELECT 
        * 
    FROM 
        parquet_metadata(&#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;000000.parquet&#x27;);
</code></pre>
If the data was on blob storage you could use a glob instead of a generator:<p><pre><code>    SELECT 
        sum(*) AS TOTAL_SIZE
    FROM 
        read_parquet(&#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;*.parquet&#x27;);
</code></pre>
You can use the huggingface API to list and then read with duckdb:<p><pre><code>    SELECT 
        concat(&#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;&#x27;, path) as parquet_file 
    FROM 
        read_json_auto(&#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;api&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;tree&#x2F;main&#x2F;data&#x27;);
</code></pre>
So this means we can combine the list files and read files SQL into a single statement!!!<p><pre><code>    Error: Binder Error: Table function cannot contain subqueries
</code></pre>
:( No<p>Want the query in a more concise, reusable form?<p><pre><code>    CREATE MACRO GET_MJ_TOTAL_SIZE(num_of_files) AS TABLE (
        SELECT
            SUM(size) AS size
        FROM read_parquet(
            list_transform(
                generate_series(0, num_of_files),
                n -&gt; &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;&#x27; ||
                    format(&#x27;{:06d}&#x27;, n) || &#x27;.parquet&#x27;
            )
        )
    );
</code></pre>
You can simply query table function:<p><pre><code>    SELECT * FROM GET_MJ_TOTAL_SIZE(55);
</code></pre>
You don&#x27;t need to run nettop:<p><pre><code>    EXPLAIN ANALYZE SELECT * FROM GET_MJ_TOTAL_SIZE(55);

    ┌─────────────────────────────────────┐
    │┌───────────────────────────────────┐│
    ││            HTTP Stats:            ││
    ││                                   ││
    ││            in: 295.5MB            ││
    ││            out: 0 bytes           ││
    ││             #HEAD: 55             ││
    ││             #GET: 166             ││
    ││              #PUT: 0              ││
    ││              #POST: 0             ││
    │└───────────────────────────────────┘│
    └─────────────────────────────────────┘</code></pre></div><br/><div id="38299342" class="c"><input type="checkbox" id="c-38299342" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38298799">parent</a><span>|</span><a href="#38271126">next</a><span>|</span><label class="collapse" for="c-38299342">[-]</label><label class="expand" for="c-38299342">[1 more]</label></div><br/><div class="children"><div class="content">Neat, thank you. I added the parquet_metadata() tip to my article: <a href="https:&#x2F;&#x2F;til.simonwillison.net&#x2F;duckdb&#x2F;remote-parquet#user-content-parquet_metadata" rel="nofollow noreferrer">https:&#x2F;&#x2F;til.simonwillison.net&#x2F;duckdb&#x2F;remote-parquet#user-con...</a></div><br/></div></div></div></div><div id="38271126" class="c"><input type="checkbox" id="c-38271126" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38298799">prev</a><span>|</span><a href="#38300209">next</a><span>|</span><label class="collapse" for="c-38271126">[-]</label><label class="expand" for="c-38271126">[2 more]</label></div><br/><div class="children"><div class="content">Includes a bonus TIL on nettop - today I learned that on macOS you can run this to see exactly how much bandwidth a specific process is using:<p><pre><code>    nettop -p $PID</code></pre></div><br/><div id="38298856" class="c"><input type="checkbox" id="c-38298856" checked=""/><div class="controls bullet"><span class="by">heresie-dabord</span><span>|</span><a href="#38271126">parent</a><span>|</span><a href="#38300209">next</a><span>|</span><label class="collapse" for="c-38298856">[-]</label><label class="expand" for="c-38298856">[1 more]</label></div><br/><div class="children"><div class="content">According to this discussion [1], a variety of tools are available to Linux distros (including nettop), but the Apple OS version that is baked-in may have more features.<p>[1] _ <a href="https:&#x2F;&#x2F;askubuntu.com&#x2F;questions&#x2F;257263&#x2F;how-to-display-network-traffic-in-the-terminal" rel="nofollow noreferrer">https:&#x2F;&#x2F;askubuntu.com&#x2F;questions&#x2F;257263&#x2F;how-to-display-networ...</a></div><br/></div></div></div></div><div id="38300209" class="c"><input type="checkbox" id="c-38300209" checked=""/><div class="controls bullet"><span class="by">isuckatcoding</span><span>|</span><a href="#38271126">prev</a><span>|</span><a href="#38298989">next</a><span>|</span><label class="collapse" for="c-38300209">[-]</label><label class="expand" for="c-38300209">[5 more]</label></div><br/><div class="children"><div class="content">Ok that seems like magic. How is it able to only read a subset of the data like that? Is this because of the parquet file format?<p>Also can the same thing work for files hosted on s3?</div><br/><div id="38300236" class="c"><input type="checkbox" id="c-38300236" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38300209">parent</a><span>|</span><a href="#38300240">next</a><span>|</span><label class="collapse" for="c-38300236">[-]</label><label class="expand" for="c-38300236">[2 more]</label></div><br/><div class="children"><div class="content">Yes, this works for S3 files as well.<p>The trick it&#x27;s using is the HTTP Range header, which lets a client request eg bytes 4500-4900 of an HTTP file.<p>Most static file hosting platforms - S3, GCS, nginx, Apache etc - support Range headers. They&#x27;re most commonly used for streaming video and audio.<p>The Parquet file format is designed with this in mind. You can read metadata at the start of the file and use it to figure out which ranges to fetch. Columns are grouped together, so sum() against a column can be handled by fetching a subset of the file.</div><br/><div id="38300270" class="c"><input type="checkbox" id="c-38300270" checked=""/><div class="controls bullet"><span class="by">mosselman</span><span>|</span><a href="#38300209">root</a><span>|</span><a href="#38300236">parent</a><span>|</span><a href="#38300240">next</a><span>|</span><label class="collapse" for="c-38300270">[-]</label><label class="expand" for="c-38300270">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for explaining. I never really looked into this to understand it and because of that it felt like magic, which is always in indicator that you just don’t understand something. I was going to add “in tech”, but it is an indicator of that with anything in life.</div><br/></div></div></div></div><div id="38300240" class="c"><input type="checkbox" id="c-38300240" checked=""/><div class="controls bullet"><span class="by">BenoitP</span><span>|</span><a href="#38300209">parent</a><span>|</span><a href="#38300236">prev</a><span>|</span><a href="#38300323">next</a><span>|</span><label class="collapse" for="c-38300240">[-]</label><label class="expand" for="c-38300240">[1 more]</label></div><br/><div class="children"><div class="content">Parquet is columnar, you can read each column independently, and a different compression scheme can be used for each column. You first have to read the beginning of the file to get the layout though (And the end of it as well, according to a library I have been using. Don&#x27;t know why) . The http server must of course support range queries.</div><br/></div></div><div id="38300323" class="c"><input type="checkbox" id="c-38300323" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#38300209">parent</a><span>|</span><a href="#38300240">prev</a><span>|</span><a href="#38298989">next</a><span>|</span><label class="collapse" for="c-38300323">[-]</label><label class="expand" for="c-38300323">[1 more]</label></div><br/><div class="children"><div class="content">Right, there&#x27;s all sorts of metadata and often stats included in any parquet file: <a href="https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;parquet-format#file-format">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;parquet-format#file-format</a><p>The offsets of said metadata are well-defined (i.e. in the footer) so for S3 &#x2F; blob storage so long as you can efficiently request a range of bytes you can pull the metadata without having to read all the data.</div><br/></div></div></div></div><div id="38298989" class="c"><input type="checkbox" id="c-38298989" checked=""/><div class="controls bullet"><span class="by">skadamat</span><span>|</span><a href="#38300209">prev</a><span>|</span><a href="#38298875">next</a><span>|</span><label class="collapse" for="c-38298989">[-]</label><label class="expand" for="c-38298989">[1 more]</label></div><br/><div class="children"><div class="content">This is one of my favorite features of DuckDB and Parquet files!<p>I combined these capabilities with mounting via NFS and documented in the last part of my post here: <a href="https:&#x2F;&#x2F;about.xethub.com&#x2F;blog&#x2F;version-control-machine-learning-workflow-pyxet-part-1" rel="nofollow noreferrer">https:&#x2F;&#x2F;about.xethub.com&#x2F;blog&#x2F;version-control-machine-learni...</a></div><br/></div></div><div id="38298875" class="c"><input type="checkbox" id="c-38298875" checked=""/><div class="controls bullet"><span class="by">didip</span><span>|</span><a href="#38298989">prev</a><span>|</span><a href="#38297999">next</a><span>|</span><label class="collapse" for="c-38298875">[-]</label><label class="expand" for="c-38298875">[10 more]</label></div><br/><div class="children"><div class="content">I still don’t understand DuckDB use case. To handle huge amount of data, you need a large database that’s connected to the network. DuckDB will eventually hit a ceiling.</div><br/><div id="38300870" class="c"><input type="checkbox" id="c-38300870" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#38298875">parent</a><span>|</span><a href="#38298970">next</a><span>|</span><label class="collapse" for="c-38300870">[-]</label><label class="expand" for="c-38300870">[2 more]</label></div><br/><div class="children"><div class="content">DuckDB use case? I remember being in the same spot. Then I realized &quot;the&quot; use case for DuckDB didn&#x27;t really exist. There were dozens of use-cases. No wonder I couldn&#x27;t figure out &quot;the&quot; use case from reading all the articles&#x2F;blogs&#x2F;hype people were creating about what they were doing with DuckDB. It seemed like everyone was using it in different ways. It&#x27;s a data engineering&#x2F;analytics Swiss army knife.<p>ETL&#x2F;ELT, ad-hoc&#x2F;exploratory, munger&#x2F;wrangler, edge-analytics, and so on...<p>&gt; DuckDB will eventually hit a ceiling.<p>Yeah, probably. Just about every database has hit a ceiling in the past. But then someone comes out with some fantastic new idea to overcome the challenges to some degree. Map-reduce, moving the query to the data instead of the data to the query, serverless, separating storage&#x2F;compute...<p>However, what if we start thinking along these lines with DuckDB? 
Reading parquet files addresses separating storage and compute. Parquet also provides columnar&#x2F;row-grouped data giving us push-down predicates (so kinda moving part of the query closer to the data).
We can run a DuckDb instance (EC2&#x2F;S3) closer to the data so that sorta helps too.<p>What I&#x27;m really excited about using DuckDB in a similar way to map-reduce. What if there was a way to take some SQL&#x27;s logical plan and turn it into a physical plan that uses compute resources from a pool of (serverless) DuckDB instances. Starting at the leaves of the graph (physical plan) pulling&#x2F;filtering data from the source (parquet files), and returning their completed work up the branches until it is completed and ready to be served up as results.<p>I&#x27;ve seen a few examples of this already, but nothing that I would consider production ready. I have a hunch that someone is going to drop such a project on us shortly, and it&#x27;s going to change a lot of things we have become use to in the data world.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;BauplanLabs&#x2F;quack-reduce">https:&#x2F;&#x2F;github.com&#x2F;BauplanLabs&#x2F;quack-reduce</a>
<a href="https:&#x2F;&#x2F;www.boilingdata.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.boilingdata.com&#x2F;</a></div><br/><div id="38300935" class="c"><input type="checkbox" id="c-38300935" checked=""/><div class="controls bullet"><span class="by">zX41ZdbW</span><span>|</span><a href="#38298875">root</a><span>|</span><a href="#38300870">parent</a><span>|</span><a href="#38298970">next</a><span>|</span><label class="collapse" for="c-38300935">[-]</label><label class="expand" for="c-38300935">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What if there was a way to take some SQL&#x27;s logical plan and turn it into a physical plan that uses compute resources from a pool<p>This is how ClickHouse works on a cluster.<p>For example, writing the same query as<p><pre><code>    SELECT sum(size)
    FROM urlCluster(&#x27;default&#x27;, &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;0000{01..55}.parquet&#x27;)
</code></pre>
(note the urlCluster usage)<p>will give the result in 0.3 seconds on a cluster: <a href="https:&#x2F;&#x2F;pastila.nl&#x2F;?00ef0aac&#x2F;a54918ef6d3536fad34a5eca0e1157f3#4&#x2F;SenMvm+mngkbiCGnQIXg==" rel="nofollow noreferrer">https:&#x2F;&#x2F;pastila.nl&#x2F;?00ef0aac&#x2F;a54918ef6d3536fad34a5eca0e1157f...</a></div><br/></div></div></div></div><div id="38298970" class="c"><input type="checkbox" id="c-38298970" checked=""/><div class="controls bullet"><span class="by">icyfox</span><span>|</span><a href="#38298875">parent</a><span>|</span><a href="#38300870">prev</a><span>|</span><a href="#38299152">next</a><span>|</span><label class="collapse" for="c-38298970">[-]</label><label class="expand" for="c-38298970">[2 more]</label></div><br/><div class="children"><div class="content">It fits a pretty nice niche between small data (32GB in-memory, pandas level) and truly huge data where the disk files can&#x27;t even fit on one machine (2TB+). Most data that people want to process on a daily cadence fit somewhere in between those ranges.<p>As for whether to use a database or not, I think that&#x27;s a more fundamental question about where your different read and write loads are coming from. Scaling a database to support billions of rows and have high query performance is not trivial; if your use-case fits more in a write-infrequently, read-often then OLAP is a pretty good choice of index structure.</div><br/><div id="38300022" class="c"><input type="checkbox" id="c-38300022" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38298875">root</a><span>|</span><a href="#38298970">parent</a><span>|</span><a href="#38299152">next</a><span>|</span><label class="collapse" for="c-38300022">[-]</label><label class="expand" for="c-38300022">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It fits a pretty nice niche between small data (32GB in-memory, pandas level) and truly huge data where the disk files can&#x27;t even fit on one machine (2TB+)<p>you can build&#x2F;rent server with say 50TB nvme raid and run duckdb on it?..</div><br/></div></div></div></div><div id="38299152" class="c"><input type="checkbox" id="c-38299152" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38298875">parent</a><span>|</span><a href="#38298970">prev</a><span>|</span><a href="#38299703">next</a><span>|</span><label class="collapse" for="c-38299152">[-]</label><label class="expand" for="c-38299152">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s for analytics. It&#x27;s a tool that can run on my laptop that means I can run analytical queries against potentially TBs of data hosted in an S3 bucket, without needing to spin up an expensive data warehouse.</div><br/></div></div><div id="38299703" class="c"><input type="checkbox" id="c-38299703" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#38298875">parent</a><span>|</span><a href="#38299152">prev</a><span>|</span><a href="#38299434">next</a><span>|</span><label class="collapse" for="c-38299703">[-]</label><label class="expand" for="c-38299703">[3 more]</label></div><br/><div class="children"><div class="content">No no it is insanely useful and I wish I had a job where I could use it more! Noone really has big big data, they only think they do.<p>You can do the analytics that Databricks and Snowflake are trying to sell your boss for $100.000 a month, only then on an 80$ vps and a couple of S3 buckets full of parquet files.<p>It&#x27;s a fantastic tool to run analytics. Lean, purposeful. I hope they can manage to stay out of the clutches of corporate enshittification for a while longer.</div><br/><div id="38300617" class="c"><input type="checkbox" id="c-38300617" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#38298875">root</a><span>|</span><a href="#38299703">parent</a><span>|</span><a href="#38299434">next</a><span>|</span><label class="collapse" for="c-38300617">[-]</label><label class="expand" for="c-38300617">[2 more]</label></div><br/><div class="children"><div class="content">You seem to be quite sure of how useful duckdb is even though you haven’t found a use case for it. This is actually a common pattern I’ve seen everywhere - the people super excited about duckdb are people who never use it in their job and at best for toy use cases.<p>If your data is truly small enough to run on a vps with duckdb, your monthly snowflake bill will not break a few hundred dollars by any stretch. A terabyte stored on snowflake costs 23 bucks a month and running a query on it depending on complexity will cost no more than a dollar. And you don’t pay any cost other than these two.</div><br/><div id="38300660" class="c"><input type="checkbox" id="c-38300660" checked=""/><div class="controls bullet"><span class="by">pradeepchhetri</span><span>|</span><a href="#38298875">root</a><span>|</span><a href="#38300617">parent</a><span>|</span><a href="#38299434">next</a><span>|</span><label class="collapse" for="c-38300660">[-]</label><label class="expand" for="c-38300660">[1 more]</label></div><br/><div class="children"><div class="content">Wow I didn&#x27;t know that snowflake charges on query level. Very good business idea to first create dependency and rip them off once their business grows.</div><br/></div></div></div></div></div></div><div id="38299434" class="c"><input type="checkbox" id="c-38299434" checked=""/><div class="controls bullet"><span class="by">swasheck</span><span>|</span><a href="#38298875">parent</a><span>|</span><a href="#38299703">prev</a><span>|</span><a href="#38297999">next</a><span>|</span><label class="collapse" for="c-38299434">[-]</label><label class="expand" for="c-38299434">[1 more]</label></div><br/><div class="children"><div class="content">similar use case to sqlite as an embedded database system with a columnar&#x2F;analytics focus.</div><br/></div></div></div></div><div id="38297999" class="c"><input type="checkbox" id="c-38297999" checked=""/><div class="controls bullet"><span class="by">wizwit999</span><span>|</span><a href="#38298875">prev</a><span>|</span><a href="#38299767">next</a><span>|</span><label class="collapse" for="c-38297999">[-]</label><label class="expand" for="c-38297999">[7 more]</label></div><br/><div class="children"><div class="content">you know there&#x27;s this great abstraction over files we came up with in the data world called &#x27;tables&#x27;..</div><br/><div id="38300579" class="c"><input type="checkbox" id="c-38300579" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#38297999">parent</a><span>|</span><a href="#38298079">next</a><span>|</span><label class="collapse" for="c-38300579">[-]</label><label class="expand" for="c-38300579">[1 more]</label></div><br/><div class="children"><div class="content">and guess what those tables use to store data a lot of the time? Parquet.<p>I assume that when you say &quot;tables&quot; you mean &quot;external tables&quot; since you&#x27;re in the data world.<p>If you didn&#x27;t then I guess when you say &quot;tables&quot;, you mean &quot;tables&quot; with everything including the kitchen sink? Database, compute, etc...? Does the database always have to be running for the table to be accessible? Responsibility for the database hardware, resources or services?<p>Of course there are fantastic new databases like Snowflake and BigQuery that separate compute and storage... but do they, really? Separating storage and compute is just incredible for scaling, suspend&#x2F;resume, etc. But can you query a Snowflake&#x2F;BigQuery table without also having to use their compute? Is there a way that I can just get a &quot;table&quot; and not be forced into using a specific compute-engine and all the other bells and whistles?<p>So when you say &quot;table&quot;, where and how do I get one? And to maintain the theme of the article, a columnar&#x2F;OLAP&#x2F;analytics &quot;table&quot; in particular?<p>As you probably know, there are several (external table) options, Apache Iceberg probably being the most talked about one at the moment. External &quot;tables&quot; are just collections of metadata about your files, or conventions about how to lay your files down. When you query these tables with SQL using athena, redshift, snowflake, duckdb, etc... each and everyone of those query-engines is reading parquet files.<p>(Snowflake, BigQuery and others are working on features to both manage and read Iceberg tables, so i kinda lied earlier)</div><br/></div></div><div id="38298079" class="c"><input type="checkbox" id="c-38298079" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38297999">parent</a><span>|</span><a href="#38300579">prev</a><span>|</span><a href="#38298915">next</a><span>|</span><label class="collapse" for="c-38298079">[-]</label><label class="expand" for="c-38298079">[4 more]</label></div><br/><div class="children"><div class="content">If you want something that looks like a table, while still benefiting from not having to download 8GB of Parquet in order to run the query, you can get one using a CTE:<p><pre><code>    with midjourney_messages as (
        select
            *
        from read_parquet(
            list_transform(
                generate_series(0, 2),
                n -&gt; &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;&#x27; ||
                    format(&#x27;{:06d}&#x27;, n) || &#x27;.parquet&#x27;
            )
        )
    )
    select sum(size) as size from midjourney_messages;
</code></pre>
Or you can create a view:<p><pre><code>    create view midjourney_messages as
    select * from read_parquet(
        list_transform(
            generate_series(0, 2),
            n -&gt; &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;&#x27; ||
                format(&#x27;{:06d}&#x27;, n) || &#x27;.parquet&#x27;
        )
    );
    select sum(size) as size from midjourney_messages;</code></pre></div><br/><div id="38299211" class="c"><input type="checkbox" id="c-38299211" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#38297999">root</a><span>|</span><a href="#38298079">parent</a><span>|</span><a href="#38298915">next</a><span>|</span><label class="collapse" for="c-38299211">[-]</label><label class="expand" for="c-38299211">[3 more]</label></div><br/><div class="children"><div class="content">Are there any pros&#x2F;cons to the raw query, CTE, and view approches?</div><br/><div id="38300304" class="c"><input type="checkbox" id="c-38300304" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#38297999">root</a><span>|</span><a href="#38299211">parent</a><span>|</span><a href="#38299364">next</a><span>|</span><label class="collapse" for="c-38300304">[-]</label><label class="expand" for="c-38300304">[1 more]</label></div><br/><div class="children"><div class="content">Yes.<p>Using a view in this example, you can&#x27;t dynamically change which files are being selected (not even with joins or where clauses). What if new files are generated and suddenly there are more or less files? Therefore you probably wouldn&#x27;t want to encapsulate your SQL into a view. Most of the time you would probably bind the list of files in your SQL as needed:<p><pre><code>   SELECT SUM(size) AS size FROM read_parquet(:files);
</code></pre>
But in this case, a table macro&#x2F;function might also be an option:<p><pre><code>   CREATE MACRO GET_TOTAL_SIZE(num_of_files) AS TABLE (
        SELECT
            SUM(size) AS size
        FROM read_parquet(
            list_transform(generate_series(0, num_of_files), n -&gt; &#x27;https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;vivym&#x2F;midjourney-messages&#x2F;resolve&#x2F;main&#x2F;data&#x2F;&#x27; || format(&#x27;{:06d}&#x27;, n) || &#x27;.parquet&#x27;
            )
        )
    );

    SELECT * FROM GET_MJ_TOTAL_SIZE(55);
</code></pre>
Not necessarily related to this article, but CTEs are useful for breaking down a complex query into more understandable chunks. Moreover, you can do interesting things within the CTE&#x27;s temp-tables like recursion or freezing&#x2F;materializing a temp-table&#x27;s results so that it only gets evaluated one time, instead of every time it gets referenced.<p><a href="http:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;sql&#x2F;query_syntax&#x2F;with#recursive-ctes" rel="nofollow noreferrer">http:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;sql&#x2F;query_syntax&#x2F;with#recursive-ctes</a>
<a href="http:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;sql&#x2F;query_syntax&#x2F;with#materialized-ctes" rel="nofollow noreferrer">http:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;sql&#x2F;query_syntax&#x2F;with#materialized-ct...</a></div><br/></div></div><div id="38299364" class="c"><input type="checkbox" id="c-38299364" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38297999">root</a><span>|</span><a href="#38299211">parent</a><span>|</span><a href="#38300304">prev</a><span>|</span><a href="#38298915">next</a><span>|</span><label class="collapse" for="c-38299364">[-]</label><label class="expand" for="c-38299364">[1 more]</label></div><br/><div class="children"><div class="content">From my experience with other databases my assumption for DuckDB is:<p>- Using a raw query, a CTE or a view will have no impact at all on query performance - they&#x27;ll be optimized down to exactly the same operations (or to a query plan that&#x27;s similar enough that the differences are negligible)<p>- CTEs are mainly useful for breaking down more complicated queries - so not great for this example, but really useful the moment you start doing anything more complicated.<p>- Views are effectively persistent CTEs - they&#x27;re great if you want to permanently &quot;bookmark&quot; more complex pieces of queries to use later.<p>I wrote a bit more about CTEs here: <a href="https:&#x2F;&#x2F;datasette.io&#x2F;tutorials&#x2F;data-analysis#ctes" rel="nofollow noreferrer">https:&#x2F;&#x2F;datasette.io&#x2F;tutorials&#x2F;data-analysis#ctes</a></div><br/></div></div></div></div></div></div></div></div><div id="38299767" class="c"><input type="checkbox" id="c-38299767" checked=""/><div class="controls bullet"><span class="by">jwmoz</span><span>|</span><a href="#38297999">prev</a><span>|</span><a href="#38298309">next</a><span>|</span><label class="collapse" for="c-38299767">[-]</label><label class="expand" for="c-38299767">[4 more]</label></div><br/><div class="children"><div class="content">Use polars it’s faster</div><br/><div id="38300025" class="c"><input type="checkbox" id="c-38300025" checked=""/><div class="controls bullet"><span class="by">pjot</span><span>|</span><a href="#38299767">parent</a><span>|</span><a href="#38299824">next</a><span>|</span><label class="collapse" for="c-38300025">[-]</label><label class="expand" for="c-38300025">[1 more]</label></div><br/><div class="children"><div class="content">DuckDB will work with polars<p><a href="https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;archive&#x2F;0.9.2&#x2F;guides&#x2F;python&#x2F;polars.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;duckdb.org&#x2F;docs&#x2F;archive&#x2F;0.9.2&#x2F;guides&#x2F;python&#x2F;polars.h...</a></div><br/></div></div><div id="38299824" class="c"><input type="checkbox" id="c-38299824" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38299767">parent</a><span>|</span><a href="#38300025">prev</a><span>|</span><a href="#38298309">next</a><span>|</span><label class="collapse" for="c-38299824">[-]</label><label class="expand" for="c-38299824">[2 more]</label></div><br/><div class="children"><div class="content">Can it run aggregate queries against 8 GiB of remotely hosted data while only pulling down the 287 MiB of columns it needs?</div><br/><div id="38300198" class="c"><input type="checkbox" id="c-38300198" checked=""/><div class="controls bullet"><span class="by">chrisjc</span><span>|</span><a href="#38299767">root</a><span>|</span><a href="#38299824">parent</a><span>|</span><a href="#38298309">next</a><span>|</span><label class="collapse" for="c-38300198">[-]</label><label class="expand" for="c-38300198">[1 more]</label></div><br/><div class="children"><div class="content">As much as I love to tout the benefits of duckdb, the ability to only pull down the columns that you need is not really functionality that is enabled by or exclusive to duckdb.<p>It really has to do with the parquet file format. And it gets even more powerful than just being able to download specific columns. To a certain degree, you can also limit rows at the same time.<p><a href="http:&#x2F;&#x2F;peter-hoffmann.com&#x2F;2020&#x2F;understand-predicate-pushdown-on-rowgroup-level-in-parquet-with-pyarrow-and-python.html" rel="nofollow noreferrer">http:&#x2F;&#x2F;peter-hoffmann.com&#x2F;2020&#x2F;understand-predicate-pushdown...</a><p>All that being said... yes, as far as I know, polars can do all of that. Probably because it has the necessary parquet dependencies.<p>Now whether you should use polars instead of duckdb or vise-versa? Is polars &quot;faster&quot;? It depends</div><br/></div></div></div></div></div></div><div id="38298309" class="c"><input type="checkbox" id="c-38298309" checked=""/><div class="controls bullet"><span class="by">smabie</span><span>|</span><a href="#38299767">prev</a><span>|</span><label class="collapse" for="c-38298309">[-]</label><label class="expand" for="c-38298309">[5 more]</label></div><br/><div class="children"><div class="content">He did all this work and wrote a blog post bc he didn&#x27;t want to download... 8gb of data?</div><br/><div id="38298326" class="c"><input type="checkbox" id="c-38298326" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38298309">parent</a><span>|</span><a href="#38298331">next</a><span>|</span><label class="collapse" for="c-38298326">[-]</label><label class="expand" for="c-38298326">[3 more]</label></div><br/><div class="children"><div class="content">Or because I wanted to learn how to use DuckDB to access remote parquet files, and this was a good opportunity to figure that out.</div><br/><div id="38298344" class="c"><input type="checkbox" id="c-38298344" checked=""/><div class="controls bullet"><span class="by">smabie</span><span>|</span><a href="#38298309">root</a><span>|</span><a href="#38298326">parent</a><span>|</span><a href="#38298331">next</a><span>|</span><label class="collapse" for="c-38298344">[-]</label><label class="expand" for="c-38298344">[2 more]</label></div><br/><div class="children"><div class="content">Totally understandable but just say that in the post then :)<p>DuckDB has been a godsend to me for a certain types of stuff (mostly accessing very large parquet files over S3)</div><br/><div id="38298351" class="c"><input type="checkbox" id="c-38298351" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38298309">root</a><span>|</span><a href="#38298344">parent</a><span>|</span><a href="#38298331">next</a><span>|</span><label class="collapse" for="c-38298351">[-]</label><label class="expand" for="c-38298351">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s kind of implied by the whole TIL site! It&#x27;s for writing about things I&#x27;ve learned.</div><br/></div></div></div></div></div></div><div id="38298331" class="c"><input type="checkbox" id="c-38298331" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#38298309">parent</a><span>|</span><a href="#38298326">prev</a><span>|</span><label class="collapse" for="c-38298331">[-]</label><label class="expand" for="c-38298331">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more that DuckDB and Clickhouse make it wicked easy to query data on remote servers or object stores in a fairly efficient manner.</div><br/></div></div></div></div></div></div></div></div></div></body></html>