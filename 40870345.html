<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720083676209" as="style"/><link rel="stylesheet" href="styles.css?v=1720083676209"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://salykova.github.io/matmul-cpu">Beating NumPy matrix multiplication in 150 lines of C</a> <span class="domain">(<a href="https://salykova.github.io">salykova.github.io</a>)</span></div><div class="subtext"><span>p1esk</span> | <span>28 comments</span></div><br/><div><div id="40871935" class="c"><input type="checkbox" id="c-40871935" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#40871909">next</a><span>|</span><label class="collapse" for="c-40871935">[-]</label><label class="expand" for="c-40871935">[7 more]</label></div><br/><div class="children"><div class="content">This looks like a nice write-up and implementation. I&#x27;m left wondering what is the &quot;trick&quot;? How does it manage to beat OpenBLAS, which is assembly+C optimized over decades for this exact problem?  It goes into detail about caching, etc - is BLAS is not taking advantage of these things, or is this more tuned to this specific processor, etc?</div><br/><div id="40872149" class="c"><input type="checkbox" id="c-40872149" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40873438">next</a><span>|</span><label class="collapse" for="c-40872149">[-]</label><label class="expand" for="c-40872149">[2 more]</label></div><br/><div class="children"><div class="content">- OpenBLAS isn&#x27;t _that_ optimized for any specific modern architecture.<p>- The matrices weren&#x27;t that big. Numpy has cffi overhead.<p>- The perf difference was much more noticeable with _peak_ throughput rather than _mean_ throughput, which matters for almost no applications (a few, admittedly, but even where &quot;peak&quot; is close to the right measure you usually want something like the mean of the top-k results or the proportion with under some latency, ...).<p>- The benchmarking code they displayed runs through Python&#x27;s allocator for numpy and is suggestive of not going through any allocator for the C implementation. Everything might be fine, but that&#x27;d be the first place I checked for microbenchmarking errors or discrepancies (most numpy routines allow in-place operations; given that that&#x27;s known to be a bottleneck in some applications of numpy, I&#x27;d be tempted to explicitly examine benchmarks for in-place versions of both).<p>- Numpy has some bounds checking and error handling code which runs regardless of the underlying implementation. That&#x27;s part of why it&#x27;s so bleedingly slow for small matrices compared to even vanilla Python lists (they tested bigger matrices too, so this isn&#x27;t the only effect, but I&#x27;ll mention it anyway). It&#x27;s hard to make something faster when you add a few thousand cycles of pure overhead.<p>- This was a very principled approach to saturating the relevant caches. It&#x27;s &quot;obvious&quot; in some sense, but clear engineering improvements are worth highlighting in discussions like this, in the sense that OpenBLAS, even with many man-hours, likely hasn&#x27;t thought of everything.<p>And so on. Anyone can rattle off differences. A proper explanation requires an actual deep-dive into both chunks of code.</div><br/><div id="40872965" class="c"><input type="checkbox" id="c-40872965" checked=""/><div class="controls bullet"><span class="by">robxorb</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40872149">parent</a><span>|</span><a href="#40873438">next</a><span>|</span><label class="collapse" for="c-40872965">[-]</label><label class="expand" for="c-40872965">[1 more]</label></div><br/><div class="children"><div class="content">To your third point - it looks as if the lines of mean values were averaged, this posts code would still be a clear winner.</div><br/></div></div></div></div><div id="40873438" class="c"><input type="checkbox" id="c-40873438" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40872149">prev</a><span>|</span><a href="#40872100">next</a><span>|</span><label class="collapse" for="c-40873438">[-]</label><label class="expand" for="c-40873438">[1 more]</label></div><br/><div class="children"><div class="content">Comparison with numpy 2.0 should be better for numpy because it integrates Google highway for better simd across different microarchitectures.</div><br/></div></div><div id="40872100" class="c"><input type="checkbox" id="c-40872100" checked=""/><div class="controls bullet"><span class="by">sbstp</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40873438">prev</a><span>|</span><a href="#40871909">next</a><span>|</span><label class="collapse" for="c-40872100">[-]</label><label class="expand" for="c-40872100">[3 more]</label></div><br/><div class="children"><div class="content">Maybe -march=native gives it an edge as it compiles for this exact CPU model whereas numpy is compiled for a more generic (older) x86-64. -march=native would probably get v4 on a Ryzen CPU where numpy is probably targeting v1 or v2.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;X86-64#Microarchitecture_levels" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;X86-64#Microarchitecture_level...</a></div><br/><div id="40872336" class="c"><input type="checkbox" id="c-40872336" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40872100">parent</a><span>|</span><a href="#40871909">next</a><span>|</span><label class="collapse" for="c-40872336">[-]</label><label class="expand" for="c-40872336">[2 more]</label></div><br/><div class="children"><div class="content">Doesn’t numpy have runtime SIMD dispatching and whatnot based on CPU flags?<p>E.g. <a href="https:&#x2F;&#x2F;github.com&#x2F;numpy&#x2F;numpy&#x2F;blob&#x2F;main&#x2F;numpy&#x2F;_core&#x2F;src&#x2F;common&#x2F;npy_cpu_dispatch.h">https:&#x2F;&#x2F;github.com&#x2F;numpy&#x2F;numpy&#x2F;blob&#x2F;main&#x2F;numpy&#x2F;_core&#x2F;src&#x2F;com...</a></div><br/><div id="40873032" class="c"><input type="checkbox" id="c-40873032" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40872336">parent</a><span>|</span><a href="#40871909">next</a><span>|</span><label class="collapse" for="c-40873032">[-]</label><label class="expand" for="c-40873032">[1 more]</label></div><br/><div class="children"><div class="content">np.matmul just uses whatever blas library your NumPy distribution was configured for&#x2F;shipped with.<p>Could be MKL (i believe the conda version comes with it) but it could also be an ancient version of OpenBLAS you already had installed. So yeah, being faster than np.matmul probably just means your NumPy is not installed optimally.</div><br/></div></div></div></div></div></div></div></div><div id="40871909" class="c"><input type="checkbox" id="c-40871909" checked=""/><div class="controls bullet"><span class="by">ssivark</span><span>|</span><a href="#40871935">prev</a><span>|</span><a href="#40872146">next</a><span>|</span><label class="collapse" for="c-40871909">[-]</label><label class="expand" for="c-40871909">[2 more]</label></div><br/><div class="children"><div class="content">Most common coding patterns leave a lot of performance unclaimed, by not fully specializing to the hardware. This article is an interesting example. For another interesting demonstration, see this CS classic &quot;There&#x27;s plenty of room at the top&quot;<p><a href="https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;science.aam9744" rel="nofollow">https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;science.aam9744</a></div><br/><div id="40872444" class="c"><input type="checkbox" id="c-40872444" checked=""/><div class="controls bullet"><span class="by">hmaarrfk</span><span>|</span><a href="#40871909">parent</a><span>|</span><a href="#40872146">next</a><span>|</span><label class="collapse" for="c-40872444">[-]</label><label class="expand" for="c-40872444">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing. That was a great read</div><br/></div></div></div></div><div id="40872146" class="c"><input type="checkbox" id="c-40872146" checked=""/><div class="controls bullet"><span class="by">leeoniya</span><span>|</span><a href="#40871909">prev</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40872146">[-]</label><label class="expand" for="c-40872146">[3 more]</label></div><br/><div class="children"><div class="content">what about jart&#x27;s tinyBLAS?<p><a href="https:&#x2F;&#x2F;hacks.mozilla.org&#x2F;2024&#x2F;04&#x2F;llamafiles-progress-four-months-in&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hacks.mozilla.org&#x2F;2024&#x2F;04&#x2F;llamafiles-progress-four-m...</a><p>also <a href="https:&#x2F;&#x2F;justine.lol&#x2F;matmul&#x2F;" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;matmul&#x2F;</a></div><br/><div id="40872654" class="c"><input type="checkbox" id="c-40872654" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40872146">parent</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40872654">[-]</label><label class="expand" for="c-40872654">[2 more]</label></div><br/><div class="children"><div class="content">We were actively chatting with Justine yesterday, seems like the implementation is at least 2x faster than tinyBLAS on her workstation. The whole discussion is in Mozilla AI discord: <a href="https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;NSnjHmT5xY" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;NSnjHmT5xY</a></div><br/><div id="40873272" class="c"><input type="checkbox" id="c-40873272" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40872146">root</a><span>|</span><a href="#40872654">parent</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40873272">[-]</label><label class="expand" for="c-40873272">[1 more]</label></div><br/><div class="children"><div class="content">&quot;off-topic&quot; channel</div><br/></div></div></div></div></div></div><div id="40872035" class="c"><input type="checkbox" id="c-40872035" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#40872146">prev</a><span>|</span><a href="#40873072">next</a><span>|</span><label class="collapse" for="c-40872035">[-]</label><label class="expand" for="c-40872035">[2 more]</label></div><br/><div class="children"><div class="content">Though not at all part of the hot path, the inefficiency of the mask generation (&#x27;bit_mask&#x27; usage) nags me. Some more efficient methods include creating a global constant array of {-1,-1,-1,-1,-1,-1,-1,-1, -1,-1,-1,-1,-1,-1,-1,-1, 0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0} and loading from it at element offsets 16-m and 8-m, or comparing constant vector {0,1,2,3,4,...} with broadcasted m and m-8.<p>Very moot nitpick though, given that this is for only one column of the matrix, the following loops of maskload&#x2F;maskstore will take significantly more time (esp. store, which is still slow on Zen 4[1] despite the AVX-512 instruction (whose only difference is taking the mask in a mask register) being 6x faster), and clang autovectorizes the shifting anyways (maybe like 2-3x slower than my suggestions).<p>[1]: <a href="https:&#x2F;&#x2F;uops.info&#x2F;table.html?search=vmaskmovps&amp;cb_lat=on&amp;cb_tp=on&amp;cb_ZEN4=on&amp;cb_measurements=on&amp;cb_avx=on&amp;cb_avx2=on" rel="nofollow">https:&#x2F;&#x2F;uops.info&#x2F;table.html?search=vmaskmovps&amp;cb_lat=on&amp;cb_...</a></div><br/><div id="40872620" class="c"><input type="checkbox" id="c-40872620" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40872035">parent</a><span>|</span><a href="#40873072">next</a><span>|</span><label class="collapse" for="c-40872620">[-]</label><label class="expand" for="c-40872620">[1 more]</label></div><br/><div class="children"><div class="content">Hi! I&#x27;m the author of the article. It&#x27;s my really first time optimizing C code and using intrinsics, so I&#x27;m definitely not an expert in this area, but Im willing to learn more! Many thanks for your feedback; I truly appreciate comments that provide new perspectives.<p>Regarding &quot;creating a constant global array and loading from it&quot; - if I recall correctly, I&#x27;ve tested this approach and it was a bit slower than bit mask shifting. But let me re-test this to be 100% sure.<p>&quot;Comparing a constant vector {0, 1, 2, 3, 4, ...} with broadcasted m and m-8&quot; - good idea, I will try it!</div><br/></div></div></div></div><div id="40873072" class="c"><input type="checkbox" id="c-40873072" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#40872035">prev</a><span>|</span><a href="#40873292">next</a><span>|</span><label class="collapse" for="c-40873072">[-]</label><label class="expand" for="c-40873072">[1 more]</label></div><br/><div class="children"><div class="content">Very nice write up.  Those are the kind of matrix sizes that MKL is fairly good at, might be worth a comparison as well?<p>Also, if you were designing for smaller cases, say MNK=16 or 32, how would you approach it differently?  I&#x27;m implementing neural ODEs and this is one point I&#x27;ve been considering.</div><br/></div></div><div id="40873292" class="c"><input type="checkbox" id="c-40873292" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40873072">prev</a><span>|</span><a href="#40871597">next</a><span>|</span><label class="collapse" for="c-40873292">[-]</label><label class="expand" for="c-40873292">[1 more]</label></div><br/><div class="children"><div class="content">In the README, it says:<p>&gt; Important! Please don’t expect peak performance without fine-tuning the hyperparameters, such as the number of threads, kernel and block sizes, unless you are running it on a Ryzen 7700(X). More on this in the tutorial.<p>I think I&#x27;ll need a TL;DR on what to change all these values to.<p>I have a Ryzen 7950X and as a first test I tried to only change NTHREADS to 32 in benchmark.c, but matmul.c performed worse than NumPy on my machine.<p>So I took a look at the other values present in the benchmark.c, but MC and NC are already calculated via the amount of threads (so these are probably already &#x27;fine-tuned&#x27;?), and I couldn&#x27;t really understand how KC = 1000 fits for the 7700(X) (the author&#x27;s CPU) and how I&#x27;d need to adjust it for the 7950X (with the informations from the article).</div><br/></div></div><div id="40871597" class="c"><input type="checkbox" id="c-40871597" checked=""/><div class="controls bullet"><span class="by">azornathogron</span><span>|</span><a href="#40873292">prev</a><span>|</span><a href="#40871892">next</a><span>|</span><label class="collapse" for="c-40871597">[-]</label><label class="expand" for="c-40871597">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only skimmed so far, but this post has a lot of detail and explanation. Looks like a pretty great description of how fast matrix multiplications are implemented to take into account architectural concerns. Goes on my reading list!</div><br/><div id="40871802" class="c"><input type="checkbox" id="c-40871802" checked=""/><div class="controls bullet"><span class="by">ghghgfdfgh</span><span>|</span><a href="#40871597">parent</a><span>|</span><a href="#40871892">next</a><span>|</span><label class="collapse" for="c-40871802">[-]</label><label class="expand" for="c-40871802">[1 more]</label></div><br/><div class="children"><div class="content">I don’t save articles often, but maybe one time in a few months I see something I know I will enjoy reading again even after 1 or 2 years. Keep up the great work OP!</div><br/></div></div></div></div><div id="40871892" class="c"><input type="checkbox" id="c-40871892" checked=""/><div class="controls bullet"><span class="by">jstrong</span><span>|</span><a href="#40871597">prev</a><span>|</span><a href="#40871719">next</a><span>|</span><label class="collapse" for="c-40871892">[-]</label><label class="expand" for="c-40871892">[4 more]</label></div><br/><div class="children"><div class="content">in terms of comparing to numpy, how much overhead would there be from Python (vs. running the numpy C code alone)?</div><br/><div id="40872008" class="c"><input type="checkbox" id="c-40872008" checked=""/><div class="controls bullet"><span class="by">SJC_Hacker</span><span>|</span><a href="#40871892">parent</a><span>|</span><a href="#40871719">next</a><span>|</span><label class="collapse" for="c-40872008">[-]</label><label class="expand" for="c-40872008">[3 more]</label></div><br/><div class="children"><div class="content">Python can efficiently call C libs if you use ctypes and native pointers, which numpy uses.  Of course depends on expected layout.<p>If you want to convert to Python lists its is going to take time.  Not sure about Python arrays.</div><br/><div id="40873000" class="c"><input type="checkbox" id="c-40873000" checked=""/><div class="controls bullet"><span class="by">dgan</span><span>|</span><a href="#40871892">root</a><span>|</span><a href="#40872008">parent</a><span>|</span><a href="#40872300">next</a><span>|</span><label class="collapse" for="c-40873000">[-]</label><label class="expand" for="c-40873000">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t recall the link but there was a github repo with comparisons of CFFI implementations in different languages, and from what i remember Python was &#x27;bout 3 orders of magnitude slower than, say, Lua or Ocaml<p>Edit: ha, found it <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=github.com&#x2F;dyu">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=github.com&#x2F;dyu</a></div><br/></div></div><div id="40872300" class="c"><input type="checkbox" id="c-40872300" checked=""/><div class="controls bullet"><span class="by">brnt</span><span>|</span><a href="#40871892">root</a><span>|</span><a href="#40872008">parent</a><span>|</span><a href="#40873000">prev</a><span>|</span><a href="#40871719">next</a><span>|</span><label class="collapse" for="c-40872300">[-]</label><label class="expand" for="c-40872300">[1 more]</label></div><br/><div class="children"><div class="content">If you use numpy, then you use an ndarray, which you can create from a C array for &#x27;free&#x27; (no copying, just set a pointer).</div><br/></div></div></div></div></div></div><div id="40871719" class="c"><input type="checkbox" id="c-40871719" checked=""/><div class="controls bullet"><span class="by">le-mark</span><span>|</span><a href="#40871892">prev</a><span>|</span><a href="#40871839">next</a><span>|</span><label class="collapse" for="c-40871719">[-]</label><label class="expand" for="c-40871719">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This is my first time writing a blog post. If you enjoy it, please subscribe and share it!<p>Great job! Self publishing things like this were a hallmark of the early internet I for one sorely miss.</div><br/></div></div><div id="40871839" class="c"><input type="checkbox" id="c-40871839" checked=""/><div class="controls bullet"><span class="by">KerrAvon</span><span>|</span><a href="#40871719">prev</a><span>|</span><label class="collapse" for="c-40871839">[-]</label><label class="expand" for="c-40871839">[4 more]</label></div><br/><div class="children"><div class="content">The article claims this is portable C. Given the use of intel intrinsics, what happens if you try to compile it for ARM64?</div><br/><div id="40871867" class="c"><input type="checkbox" id="c-40871867" checked=""/><div class="controls bullet"><span class="by">tempay</span><span>|</span><a href="#40871839">parent</a><span>|</span><a href="#40872958">next</a><span>|</span><label class="collapse" for="c-40871867">[-]</label><label class="expand" for="c-40871867">[2 more]</label></div><br/><div class="children"><div class="content">I think they mean portable between modern x86 CPUs as opposed to truly portable.</div><br/></div></div><div id="40872958" class="c"><input type="checkbox" id="c-40872958" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#40871839">parent</a><span>|</span><a href="#40871867">prev</a><span>|</span><label class="collapse" for="c-40872958">[-]</label><label class="expand" for="c-40872958">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;d need first to convert it to portable SIMD intrinsics. There are several libraries.</div><br/></div></div></div></div></div></div></div></div></div></body></html>