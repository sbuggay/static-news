<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720170065432" as="style"/><link rel="stylesheet" href="styles.css?v=1720170065432"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://salykova.github.io/matmul-cpu">Beating NumPy matrix multiplication in 150 lines of C</a> <span class="domain">(<a href="https://salykova.github.io">salykova.github.io</a>)</span></div><div class="subtext"><span>p1esk</span> | <span>69 comments</span></div><br/><div><div id="40876743" class="c"><input type="checkbox" id="c-40876743" checked=""/><div class="controls bullet"><span class="by">epr</span><span>|</span><a href="#40878254">next</a><span>|</span><label class="collapse" for="c-40876743">[-]</label><label class="expand" for="c-40876743">[5 more]</label></div><br/><div class="children"><div class="content">If the point of this article is that there&#x27;s generally performance left on the table, if anything it&#x27;s understating how much room there generally is for improvement considering how much effort goes into matmul libraries compared to most other software.<p>Getting a 10-1000x or more improvement on existing code is very common without putting in a ton of effort if the code was not already heavily optimized. These are listed roughly in order of importance, but performance is often such a non-consideration from most developers that a little effort goes a long way.<p>1. Most importantly, is the algorithm a good choice? Can we eliminate some work entirely? (this is what algo interviews are testing for)<p>2. Can we eliminate round trips to the kernel and similar heavy operations? The most common huge gain here is replacing tons of malloc calls with a custom allocator.<p>3. Can we vectorize? Explicit vector intrinsics like in the blog post are great, but you can often get the same machine code by reorganizing your data into arrays &#x2F; struct of arrays rather than arrays of structs.<p>4. Can we optimize for cache efficiency? If you already reorganized for vectors this might already be handled, but this can get more complicated with parallel code if you can&#x27;t isolate data to one thread (false sharing, etc.)<p>5. Can we do anything else that&#x27;s hardware specific? This can be anything from using intrinsics to hand-coding assembly.</div><br/><div id="40878750" class="c"><input type="checkbox" id="c-40878750" checked=""/><div class="controls bullet"><span class="by">bartread</span><span>|</span><a href="#40876743">parent</a><span>|</span><a href="#40878021">next</a><span>|</span><label class="collapse" for="c-40878750">[-]</label><label class="expand" for="c-40878750">[3 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget the impact of network. I managed to get a several hundred times performance improvement on one occasion because I found a distributed query that was pulling back roughly 1M rows over the network and then doing a join that dropped all but 5 - 10 of them. I restructured the query so the join occurred on the remote server and only 5 - 10 rows were sent over the network and, boom, suddenly it&#x27;s fast.<p>There&#x27;s always going to be some fixed overhead and latency (and there&#x27;s a great article about the impact of latency on performance called &quot;It&#x27;s the latency, stupid&quot; that&#x27;s worth a read: <a href="http:&#x2F;&#x2F;www.stuartcheshire.org&#x2F;rants&#x2F;latency.html" rel="nofollow">http:&#x2F;&#x2F;www.stuartcheshire.org&#x2F;rants&#x2F;latency.html</a>) but sending far more data than is needed over a network connection will sooner or later kill performance.<p>Overall though, I agree with your considerations, and in roughish terms the order of them.</div><br/><div id="40878933" class="c"><input type="checkbox" id="c-40878933" checked=""/><div class="controls bullet"><span class="by">epr</span><span>|</span><a href="#40876743">root</a><span>|</span><a href="#40878750">parent</a><span>|</span><a href="#40878021">next</a><span>|</span><label class="collapse" for="c-40878933">[-]</label><label class="expand" for="c-40878933">[2 more]</label></div><br/><div class="children"><div class="content">I meant this kind of thing to fall under #1. Don&#x27;t do work that can be avoided includes pulling 1M rows * a bunch of columns you don&#x27;t need over the network.<p>From your description though, it doesn&#x27;t sound like something I&#x27;d classify as a network issue. That&#x27;s just classic orm nonsense. I guess I don&#x27;t know what you mean by &quot;distributed query&quot;, but it sounds terrible.<p>The most classic network performance issue is forgetting to disable nagle&#x27;s algorithm.<p>The most classic sql performance issue is not using an index.</div><br/><div id="40879685" class="c"><input type="checkbox" id="c-40879685" checked=""/><div class="controls bullet"><span class="by">bartread</span><span>|</span><a href="#40876743">root</a><span>|</span><a href="#40878933">parent</a><span>|</span><a href="#40878021">next</a><span>|</span><label class="collapse" for="c-40879685">[-]</label><label class="expand" for="c-40879685">[1 more]</label></div><br/><div class="children"><div class="content">A distributed query is something you execute over multiple instances of your DBMS. I actually would disagree with you that this specific issue was an instance of (1). There wasn&#x27;t anything wrong with the query per se but rather the issue was with where the bulk of the work in the query was being done: move that work to the right place and the query becomes fast.<p>When considering performance issues, in my experience it&#x27;s a mistake not to explicitly consider the network.</div><br/></div></div></div></div></div></div></div></div><div id="40878254" class="c"><input type="checkbox" id="c-40878254" checked=""/><div class="controls bullet"><span class="by">gnufx</span><span>|</span><a href="#40876743">prev</a><span>|</span><a href="#40871909">next</a><span>|</span><label class="collapse" for="c-40878254">[-]</label><label class="expand" for="c-40878254">[1 more]</label></div><br/><div class="children"><div class="content">The papers referenced in the BLIS repo are the authoritative reference to understand this stuff.  I&#x27;ve no idea why people think optimized BLASes aren&#x27;t performant; you should expect &gt;90% of CPU peak for sufficiently large matrices, and serial OpenBLAS was generally on a par with MKL last I saw.  (BLAS implements GEMM, not matmul, as the basic linear algebra building block.)  I don&#x27;t understand using numpy rather than the usual benchmark frameworks, and on Zen surely you should compare with AMD&#x27;s BLAS (based on BLIS).  BLIS had a better parallelization story than OpenBLAS last I knew.  AMD BLIS also has the implementation switch for &quot;small&quot; dimensions, and I don&#x27;t know if current OpenBLAS does.<p>SIMD intrinsics aren&#x27;t needed for micro-kernel vectorization, as a decent C compiler will fully vectorize and unroll it.  BLIS&#x27; pure C micro-kernel gets &gt;80% of the performance of the hand-optimized implementation on Haswell with appropriate block sizes.  The difference is likely to be due to prefecth, but I don&#x27;t properly understand it.</div><br/></div></div><div id="40871909" class="c"><input type="checkbox" id="c-40871909" checked=""/><div class="controls bullet"><span class="by">ssivark</span><span>|</span><a href="#40878254">prev</a><span>|</span><a href="#40875435">next</a><span>|</span><label class="collapse" for="c-40871909">[-]</label><label class="expand" for="c-40871909">[3 more]</label></div><br/><div class="children"><div class="content">Most common coding patterns leave a lot of performance unclaimed, by not fully specializing to the hardware. This article is an interesting example. For another interesting demonstration, see this CS classic &quot;There&#x27;s plenty of room at the top&quot;<p><a href="https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;science.aam9744" rel="nofollow">https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;science.aam9744</a></div><br/><div id="40874090" class="c"><input type="checkbox" id="c-40874090" checked=""/><div class="controls bullet"><span class="by">auselen</span><span>|</span><a href="#40871909">parent</a><span>|</span><a href="#40872444">next</a><span>|</span><label class="collapse" for="c-40874090">[-]</label><label class="expand" for="c-40874090">[1 more]</label></div><br/><div class="children"><div class="content">Title comes from: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;There%27s_Plenty_of_Room_at_the_Bottom" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;There%27s_Plenty_of_Room_at_...</a></div><br/></div></div><div id="40872444" class="c"><input type="checkbox" id="c-40872444" checked=""/><div class="controls bullet"><span class="by">hmaarrfk</span><span>|</span><a href="#40871909">parent</a><span>|</span><a href="#40874090">prev</a><span>|</span><a href="#40875435">next</a><span>|</span><label class="collapse" for="c-40872444">[-]</label><label class="expand" for="c-40872444">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing. That was a great read</div><br/></div></div></div></div><div id="40875435" class="c"><input type="checkbox" id="c-40875435" checked=""/><div class="controls bullet"><span class="by">bjourne</span><span>|</span><a href="#40871909">prev</a><span>|</span><a href="#40871935">next</a><span>|</span><label class="collapse" for="c-40875435">[-]</label><label class="expand" for="c-40875435">[1 more]</label></div><br/><div class="children"><div class="content">Good writeup and commendable of you to make your benchmark so easily repeatable. On my 16-core Xeon(R) W-2245 CPU @ 3.90GHz matmul.c takes about 1.41 seconds to multiply 8192x8192 matrices when compiled with gcc -O3 and 1.47 seconds when compiled with clang -O2, while NumPy does it in 1.07 seconds. I believe an avx512 kernel would be significantly faster. Another reason for the lackluster performance may be omp. IME, you can reduce overhead by managing the thread pool explicitly with pthreads (and use sysconf(_SC_NPROCESSORS_ONLN) instead of hard-coding).</div><br/></div></div><div id="40871935" class="c"><input type="checkbox" id="c-40871935" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#40875435">prev</a><span>|</span><a href="#40873765">next</a><span>|</span><label class="collapse" for="c-40871935">[-]</label><label class="expand" for="c-40871935">[9 more]</label></div><br/><div class="children"><div class="content">This looks like a nice write-up and implementation. I&#x27;m left wondering what is the &quot;trick&quot;? How does it manage to beat OpenBLAS, which is assembly+C optimized over decades for this exact problem?  It goes into detail about caching, etc - is BLAS is not taking advantage of these things, or is this more tuned to this specific processor, etc?</div><br/><div id="40872149" class="c"><input type="checkbox" id="c-40872149" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40878767">next</a><span>|</span><label class="collapse" for="c-40872149">[-]</label><label class="expand" for="c-40872149">[2 more]</label></div><br/><div class="children"><div class="content">- OpenBLAS isn&#x27;t _that_ optimized for any specific modern architecture.<p>- The matrices weren&#x27;t that big. Numpy has cffi overhead.<p>- The perf difference was much more noticeable with _peak_ throughput rather than _mean_ throughput, which matters for almost no applications (a few, admittedly, but even where &quot;peak&quot; is close to the right measure you usually want something like the mean of the top-k results or the proportion with under some latency, ...).<p>- The benchmarking code they displayed runs through Python&#x27;s allocator for numpy and is suggestive of not going through any allocator for the C implementation. Everything might be fine, but that&#x27;d be the first place I checked for microbenchmarking errors or discrepancies (most numpy routines allow in-place operations; given that that&#x27;s known to be a bottleneck in some applications of numpy, I&#x27;d be tempted to explicitly examine benchmarks for in-place versions of both).<p>- Numpy has some bounds checking and error handling code which runs regardless of the underlying implementation. That&#x27;s part of why it&#x27;s so bleedingly slow for small matrices compared to even vanilla Python lists (they tested bigger matrices too, so this isn&#x27;t the only effect, but I&#x27;ll mention it anyway). It&#x27;s hard to make something faster when you add a few thousand cycles of pure overhead.<p>- This was a very principled approach to saturating the relevant caches. It&#x27;s &quot;obvious&quot; in some sense, but clear engineering improvements are worth highlighting in discussions like this, in the sense that OpenBLAS, even with many man-hours, likely hasn&#x27;t thought of everything.<p>And so on. Anyone can rattle off differences. A proper explanation requires an actual deep-dive into both chunks of code.</div><br/><div id="40872965" class="c"><input type="checkbox" id="c-40872965" checked=""/><div class="controls bullet"><span class="by">robxorb</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40872149">parent</a><span>|</span><a href="#40878767">next</a><span>|</span><label class="collapse" for="c-40872965">[-]</label><label class="expand" for="c-40872965">[1 more]</label></div><br/><div class="children"><div class="content">To your third point - it looks as if the lines of mean values were averaged, this posts code would still be a clear winner.</div><br/></div></div></div></div><div id="40878767" class="c"><input type="checkbox" id="c-40878767" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40872149">prev</a><span>|</span><a href="#40872100">next</a><span>|</span><label class="collapse" for="c-40878767">[-]</label><label class="expand" for="c-40878767">[1 more]</label></div><br/><div class="children"><div class="content">To beat OpenBLAS is neither suprising nor it&#x27;s not unheard of, for example D language library  Mir for linear algebra did just that many years ago [1]. For C++ and C implementation please check these metaprogramming approaches [2], [3].<p>What&#x27;s really surprising is that many modern languages are still depending on OpenBLAS for examples Matlab, Julia, Mojo, etc. But to be fair they probably have their own reasons, right?<p>[1] Numeric age for D: Mir GLAS is faster than OpenBLAS and Eigen (2016):<p><a href="http:&#x2F;&#x2F;blog.mir.dlang.io&#x2F;glas&#x2F;benchmark&#x2F;openblas&#x2F;2016&#x2F;09&#x2F;23&#x2F;glas-gemm-benchmark.html" rel="nofollow">http:&#x2F;&#x2F;blog.mir.dlang.io&#x2F;glas&#x2F;benchmark&#x2F;openblas&#x2F;2016&#x2F;09&#x2F;23&#x2F;...</a><p>[2] Vastly outperforming LAPACK with C++ metaprogramming (2018):<p><a href="https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;vastly_outperforming_lapack_with_cpp_metaprogramming.html" rel="nofollow">https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;vastly_outperforming_lapack_w...</a><p>[3] Outperforming LAPACK with C metaprogramming (2018):<p><a href="https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;outperforming_lapack_with_c_metaprogramming.html" rel="nofollow">https:&#x2F;&#x2F;wordsandbuttons.online&#x2F;outperforming_lapack_with_c_m...</a></div><br/></div></div><div id="40872100" class="c"><input type="checkbox" id="c-40872100" checked=""/><div class="controls bullet"><span class="by">sbstp</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40878767">prev</a><span>|</span><a href="#40873438">next</a><span>|</span><label class="collapse" for="c-40872100">[-]</label><label class="expand" for="c-40872100">[3 more]</label></div><br/><div class="children"><div class="content">Maybe -march=native gives it an edge as it compiles for this exact CPU model whereas numpy is compiled for a more generic (older) x86-64. -march=native would probably get v4 on a Ryzen CPU where numpy is probably targeting v1 or v2.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;X86-64#Microarchitecture_levels" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;X86-64#Microarchitecture_level...</a></div><br/><div id="40872336" class="c"><input type="checkbox" id="c-40872336" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40872100">parent</a><span>|</span><a href="#40873438">next</a><span>|</span><label class="collapse" for="c-40872336">[-]</label><label class="expand" for="c-40872336">[2 more]</label></div><br/><div class="children"><div class="content">Doesn’t numpy have runtime SIMD dispatching and whatnot based on CPU flags?<p>E.g. <a href="https:&#x2F;&#x2F;github.com&#x2F;numpy&#x2F;numpy&#x2F;blob&#x2F;main&#x2F;numpy&#x2F;_core&#x2F;src&#x2F;common&#x2F;npy_cpu_dispatch.h">https:&#x2F;&#x2F;github.com&#x2F;numpy&#x2F;numpy&#x2F;blob&#x2F;main&#x2F;numpy&#x2F;_core&#x2F;src&#x2F;com...</a></div><br/><div id="40873032" class="c"><input type="checkbox" id="c-40873032" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40872336">parent</a><span>|</span><a href="#40873438">next</a><span>|</span><label class="collapse" for="c-40873032">[-]</label><label class="expand" for="c-40873032">[1 more]</label></div><br/><div class="children"><div class="content">np.matmul just uses whatever blas library your NumPy distribution was configured for&#x2F;shipped with.<p>Could be MKL (i believe the conda version comes with it) but it could also be an ancient version of OpenBLAS you already had installed. So yeah, being faster than np.matmul probably just means your NumPy is not installed optimally.</div><br/></div></div></div></div></div></div><div id="40873438" class="c"><input type="checkbox" id="c-40873438" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#40871935">parent</a><span>|</span><a href="#40872100">prev</a><span>|</span><a href="#40873765">next</a><span>|</span><label class="collapse" for="c-40873438">[-]</label><label class="expand" for="c-40873438">[2 more]</label></div><br/><div class="children"><div class="content">Comparison with numpy 2.0 should be better for numpy because it integrates Google highway for better simd across different microarchitectures.</div><br/><div id="40877440" class="c"><input type="checkbox" id="c-40877440" checked=""/><div class="controls bullet"><span class="by">janwas</span><span>|</span><a href="#40871935">root</a><span>|</span><a href="#40873438">parent</a><span>|</span><a href="#40873765">next</a><span>|</span><label class="collapse" for="c-40877440">[-]</label><label class="expand" for="c-40877440">[1 more]</label></div><br/><div class="children"><div class="content">Highway TL here. The integration is a work in progress; what has been integrated so far is VQSort :)</div><br/></div></div></div></div></div></div><div id="40873765" class="c"><input type="checkbox" id="c-40873765" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#40871935">prev</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40873765">[-]</label><label class="expand" for="c-40873765">[10 more]</label></div><br/><div class="children"><div class="content">There is no reason to burden one side with Python while the other side is C, when they could have just as easily perform an apples-to-apples comparison where both sides are written in C, one calling a BLAS library while the other calls this other implementation.</div><br/><div id="40873832" class="c"><input type="checkbox" id="c-40873832" checked=""/><div class="controls bullet"><span class="by">moomin</span><span>|</span><a href="#40873765">parent</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40873832">[-]</label><label class="expand" for="c-40873832">[9 more]</label></div><br/><div class="children"><div class="content">Python is the right thing to compare to here, because it is easily the most popular way to perform these computations in the modern day. Specifically using numpy. The overhead isn&#x27;t that high, but as mentioned elsewhere in this thread, calling it correctly is important. Pitting naive numpy code against tuned C code is definitely not a fair comparison.</div><br/><div id="40874088" class="c"><input type="checkbox" id="c-40874088" checked=""/><div class="controls bullet"><span class="by">stinos</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40873832">parent</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40874088">[-]</label><label class="expand" for="c-40874088">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Python is the right thing to compare to here, because it is easily the most popular way to perform these computations in the modern day. Specifically using numpy.<p>By that reasoning, wouldn&#x27;t it make more sense to wrap their C code and maybe even make it operate on numpy&#x27;s array representation, so it can be called from Python?</div><br/><div id="40874236" class="c"><input type="checkbox" id="c-40874236" checked=""/><div class="controls bullet"><span class="by">moomin</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40874088">parent</a><span>|</span><a href="#40874309">next</a><span>|</span><label class="collapse" for="c-40874236">[-]</label><label class="expand" for="c-40874236">[5 more]</label></div><br/><div class="children"><div class="content">I think it’s okay to say “This is the benchmark, now I’m going to compare it against something else.” It’s up to the reader to decide if a 3% (or 300%) improvement is worth the investment if it involves learning a whole other language.</div><br/><div id="40874572" class="c"><input type="checkbox" id="c-40874572" checked=""/><div class="controls bullet"><span class="by">pmeira</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40874236">parent</a><span>|</span><a href="#40874284">next</a><span>|</span><label class="collapse" for="c-40874572">[-]</label><label class="expand" for="c-40874572">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a muddy comparison given that NumPy is commonly used with other BLAS implementations, which the author even lists, but doesn&#x27;t properly address. Anaconda defaults to Intel oneAPI MKL, for example, and that&#x27;s a widely used distribution. Not that I think MKL would do great on AMD hardware, BLIS is probably a better alternative.<p>The author also says &quot;(...) implementation follows the BLIS design&quot;, but then proceeds to compare *only* with OpenBLAS. I&#x27;d love to see a more thorough analysis, and using C directly would make it easier to compare multiple BLAS libs.</div><br/></div></div><div id="40874284" class="c"><input type="checkbox" id="c-40874284" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40874236">parent</a><span>|</span><a href="#40874572">prev</a><span>|</span><a href="#40874309">next</a><span>|</span><label class="collapse" for="c-40874284">[-]</label><label class="expand" for="c-40874284">[3 more]</label></div><br/><div class="children"><div class="content">If that was the goal, they should have compared NumPy to BLAS. What they did was comparing OpenBLAS wrapped in NumPy with their C code. It is not a reasonable comparison to make.<p>Look, I&#x27;m trying to be charitable to the authors, hard as that might be.</div><br/><div id="40875136" class="c"><input type="checkbox" id="c-40875136" checked=""/><div class="controls bullet"><span class="by">cnity</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40874284">parent</a><span>|</span><a href="#40874309">next</a><span>|</span><label class="collapse" for="c-40875136">[-]</label><label class="expand" for="c-40875136">[2 more]</label></div><br/><div class="children"><div class="content">There is some reason in this comparison. You might want to answer the question: &quot;if I pick the common approach to matrix multiplication in the world of Data Science (numpy), how far off is the performance from some potential ideal reference implementation?&quot;<p>I actually do have that question niggling in the back of my mind when I use something like NumPy. I don&#x27;t necessarily care exactly _where_ the overhead comes from, I might just be interested whether it&#x27;s close to ideal or not.</div><br/><div id="40876039" class="c"><input type="checkbox" id="c-40876039" checked=""/><div class="controls bullet"><span class="by">david-gpu</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40875136">parent</a><span>|</span><a href="#40874309">next</a><span>|</span><label class="collapse" for="c-40876039">[-]</label><label class="expand" for="c-40876039">[1 more]</label></div><br/><div class="children"><div class="content">If that was your question, you would compare against a number of BLAS libraries, which are already well optimized.<p>What they are doing here is patting themselves on the back after handicapping the competition. Not to mention that they have given themselves the chance to cherry pick the very best hyperparameters for this particular comparison while BLAS is limited to using heuristics to guess which of their kernels will suit this particular combination of hardware and parameters.<p>The authors need to be called out for this contrived comparison.</div><br/></div></div></div></div></div></div></div></div><div id="40874309" class="c"><input type="checkbox" id="c-40874309" checked=""/><div class="controls bullet"><span class="by">lamcny</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40874088">parent</a><span>|</span><a href="#40874236">prev</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40874309">[-]</label><label class="expand" for="c-40874309">[2 more]</label></div><br/><div class="children"><div class="content">Popular does not mean best.<p>Suppose that this blog post were part of a series that questions the axiom (largely bolstered by academic marketing) that one needs Python to do array computing. Then it is valid to compare C directly to NumPy.<p>It isn&#x27;t even far fetched. The quality of understanding something after having implemented it in C is far greater than the understanding gained by rearranging PyTorch or NumPY snippets.<p>That said, the Python overhead should not be very high if M=1000, N=1000, K=1000 was used. The article is a bit silent on the array sizes, this is somewhere from the middle of the article.</div><br/><div id="40874766" class="c"><input type="checkbox" id="c-40874766" checked=""/><div class="controls bullet"><span class="by">CleanRoomClub</span><span>|</span><a href="#40873765">root</a><span>|</span><a href="#40874309">parent</a><span>|</span><a href="#40872035">next</a><span>|</span><label class="collapse" for="c-40874766">[-]</label><label class="expand" for="c-40874766">[1 more]</label></div><br/><div class="children"><div class="content">Python is popular precisely because non-programmers are able to rearrange snippets and write rudimentary programs without a huge time investment in learning the language or tooling. It’s a very high level language with syntactic sugar that has a lot of data science libraries which can call C code for performance, which makes it great for data scientists.<p>It would be a huge detriment and time sink for these data scientist to take the time to learn to write an equivalent C program if their ultimate goal is to do data science.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40872035" class="c"><input type="checkbox" id="c-40872035" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#40873765">prev</a><span>|</span><a href="#40880384">next</a><span>|</span><label class="collapse" for="c-40872035">[-]</label><label class="expand" for="c-40872035">[4 more]</label></div><br/><div class="children"><div class="content">Though not at all part of the hot path, the inefficiency of the mask generation (&#x27;bit_mask&#x27; usage) nags me. Some more efficient methods include creating a global constant array of {-1,-1,-1,-1,-1,-1,-1,-1, -1,-1,-1,-1,-1,-1,-1,-1, 0,0,0,0,0,0,0,0, 0,0,0,0,0,0,0} and loading from it at element offsets 16-m and 8-m, or comparing constant vector {0,1,2,3,4,...} with broadcasted m and m-8.<p>Very moot nitpick though, given that this is for only one column of the matrix, the following loops of maskload&#x2F;maskstore will take significantly more time (esp. store, which is still slow on Zen 4[1] despite the AVX-512 instruction (whose only difference is taking the mask in a mask register) being 6x faster), and clang autovectorizes the shifting anyways (maybe like 2-3x slower than my suggestions).<p>[1]: <a href="https:&#x2F;&#x2F;uops.info&#x2F;table.html?search=vmaskmovps&amp;cb_lat=on&amp;cb_tp=on&amp;cb_ZEN4=on&amp;cb_measurements=on&amp;cb_avx=on&amp;cb_avx2=on" rel="nofollow">https:&#x2F;&#x2F;uops.info&#x2F;table.html?search=vmaskmovps&amp;cb_lat=on&amp;cb_...</a></div><br/><div id="40872620" class="c"><input type="checkbox" id="c-40872620" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40872035">parent</a><span>|</span><a href="#40878342">next</a><span>|</span><label class="collapse" for="c-40872620">[-]</label><label class="expand" for="c-40872620">[1 more]</label></div><br/><div class="children"><div class="content">Hi! I&#x27;m the author of the article. It&#x27;s my really first time optimizing C code and using intrinsics, so I&#x27;m definitely not an expert in this area, but Im willing to learn more! Many thanks for your feedback; I truly appreciate comments that provide new perspectives.<p>Regarding &quot;creating a constant global array and loading from it&quot; - if I recall correctly, I&#x27;ve tested this approach and it was a bit slower than bit mask shifting. But let me re-test this to be 100% sure.<p>&quot;Comparing a constant vector {0, 1, 2, 3, 4, ...} with broadcasted m and m-8&quot; - good idea, I will try it!</div><br/></div></div><div id="40878342" class="c"><input type="checkbox" id="c-40878342" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#40872035">parent</a><span>|</span><a href="#40872620">prev</a><span>|</span><a href="#40880384">next</a><span>|</span><label class="collapse" for="c-40878342">[-]</label><label class="expand" for="c-40878342">[2 more]</label></div><br/><div class="children"><div class="content">&gt; creating a global constant array<p>Note you can keep int8_t elements in that array, and sign extend bytes into int32_t while loading. The _mm_loadu_si64 &#x2F; _mm256_cvtepi8_epi32 combo should compile into a single vpmovsxbd instruction with a memory operand. This way the entire constant array fits in a single cache line, as long as it’s aligned properly with alignas(32)<p>This is good fit for the OP’s use case because they need two masks, the second vpmovsxbd instruction will be a guaranteed L1D cache hit.</div><br/><div id="40878494" class="c"><input type="checkbox" id="c-40878494" checked=""/><div class="controls bullet"><span class="by">dzaima</span><span>|</span><a href="#40872035">root</a><span>|</span><a href="#40878342">parent</a><span>|</span><a href="#40880384">next</a><span>|</span><label class="collapse" for="c-40878494">[-]</label><label class="expand" for="c-40878494">[1 more]</label></div><br/><div class="children"><div class="content">vpmovsxbd ymm,[…] still presumably decomposes back into two uops (definitely does on intel, but uops.info doesn&#x27;t show memory uops for AMD); still better than broadcast+compare though (which does still have a load for the constant; and, for that matter, the original shifting version also has multiple loads). Additionally, the int8_t elements mean no cacheline-crossing loads. (there&#x27;s the more compressed option of only having a {8×-1, 8×0} array, at the cost of more scalar offset computation)</div><br/></div></div></div></div></div></div><div id="40880384" class="c"><input type="checkbox" id="c-40880384" checked=""/><div class="controls bullet"><span class="by">p0w3n3d</span><span>|</span><a href="#40872035">prev</a><span>|</span><a href="#40872146">next</a><span>|</span><label class="collapse" for="c-40880384">[-]</label><label class="expand" for="c-40880384">[1 more]</label></div><br/><div class="children"><div class="content">This only proves even more how Python is magnificently fast. Only 5% difference from the native code...</div><br/></div></div><div id="40872146" class="c"><input type="checkbox" id="c-40872146" checked=""/><div class="controls bullet"><span class="by">leeoniya</span><span>|</span><a href="#40880384">prev</a><span>|</span><a href="#40877185">next</a><span>|</span><label class="collapse" for="c-40872146">[-]</label><label class="expand" for="c-40872146">[3 more]</label></div><br/><div class="children"><div class="content">what about jart&#x27;s tinyBLAS?<p><a href="https:&#x2F;&#x2F;hacks.mozilla.org&#x2F;2024&#x2F;04&#x2F;llamafiles-progress-four-months-in&#x2F;" rel="nofollow">https:&#x2F;&#x2F;hacks.mozilla.org&#x2F;2024&#x2F;04&#x2F;llamafiles-progress-four-m...</a><p>also <a href="https:&#x2F;&#x2F;justine.lol&#x2F;matmul&#x2F;" rel="nofollow">https:&#x2F;&#x2F;justine.lol&#x2F;matmul&#x2F;</a></div><br/><div id="40872654" class="c"><input type="checkbox" id="c-40872654" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40872146">parent</a><span>|</span><a href="#40877185">next</a><span>|</span><label class="collapse" for="c-40872654">[-]</label><label class="expand" for="c-40872654">[2 more]</label></div><br/><div class="children"><div class="content">We were actively chatting with Justine yesterday, seems like the implementation is at least 2x faster than tinyBLAS on her workstation. The whole discussion is in Mozilla AI discord: <a href="https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;NSnjHmT5xY" rel="nofollow">https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;NSnjHmT5xY</a></div><br/><div id="40873272" class="c"><input type="checkbox" id="c-40873272" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40872146">root</a><span>|</span><a href="#40872654">parent</a><span>|</span><a href="#40877185">next</a><span>|</span><label class="collapse" for="c-40873272">[-]</label><label class="expand" for="c-40873272">[1 more]</label></div><br/><div class="children"><div class="content">&quot;off-topic&quot; channel</div><br/></div></div></div></div></div></div><div id="40877185" class="c"><input type="checkbox" id="c-40877185" checked=""/><div class="controls bullet"><span class="by">throwaway4good</span><span>|</span><a href="#40872146">prev</a><span>|</span><a href="#40871597">next</a><span>|</span><label class="collapse" for="c-40877185">[-]</label><label class="expand" for="c-40877185">[2 more]</label></div><br/><div class="children"><div class="content">What is the point of making the matrix multiplication itself multithreaded (other than benchmarking)? Wouldn&#x27;t it be more beneficial in practice to have the multithreadedness in the algorithm that use the multiplication?</div><br/><div id="40878314" class="c"><input type="checkbox" id="c-40878314" checked=""/><div class="controls bullet"><span class="by">gnufx</span><span>|</span><a href="#40877185">parent</a><span>|</span><a href="#40871597">next</a><span>|</span><label class="collapse" for="c-40878314">[-]</label><label class="expand" for="c-40878314">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s indeed what&#x27;s typically done in HPC.  However, substituting a parallel BLAS can help the right sort of R code simply, for instance, but HPC codes typically aren&#x27;t bottlenacked on GEMM.</div><br/></div></div></div></div><div id="40871597" class="c"><input type="checkbox" id="c-40871597" checked=""/><div class="controls bullet"><span class="by">azornathogron</span><span>|</span><a href="#40877185">prev</a><span>|</span><a href="#40875379">next</a><span>|</span><label class="collapse" for="c-40871597">[-]</label><label class="expand" for="c-40871597">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only skimmed so far, but this post has a lot of detail and explanation. Looks like a pretty great description of how fast matrix multiplications are implemented to take into account architectural concerns. Goes on my reading list!</div><br/><div id="40871802" class="c"><input type="checkbox" id="c-40871802" checked=""/><div class="controls bullet"><span class="by">ghghgfdfgh</span><span>|</span><a href="#40871597">parent</a><span>|</span><a href="#40875379">next</a><span>|</span><label class="collapse" for="c-40871802">[-]</label><label class="expand" for="c-40871802">[1 more]</label></div><br/><div class="children"><div class="content">I don’t save articles often, but maybe one time in a few months I see something I know I will enjoy reading again even after 1 or 2 years. Keep up the great work OP!</div><br/></div></div></div></div><div id="40875379" class="c"><input type="checkbox" id="c-40875379" checked=""/><div class="controls bullet"><span class="by">Bayes7</span><span>|</span><a href="#40871597">prev</a><span>|</span><a href="#40873292">next</a><span>|</span><label class="collapse" for="c-40875379">[-]</label><label class="expand" for="c-40875379">[2 more]</label></div><br/><div class="children"><div class="content">This was a great read, thanks a lot!
One a side note, any one has a good guess what tool&#x2F;software they used to create the visualisations for matrix multiplications or memory outline?</div><br/><div id="40876469" class="c"><input type="checkbox" id="c-40876469" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40875379">parent</a><span>|</span><a href="#40873292">next</a><span>|</span><label class="collapse" for="c-40876469">[-]</label><label class="expand" for="c-40876469">[1 more]</label></div><br/><div class="children"><div class="content">excalidraw &lt;3</div><br/></div></div></div></div><div id="40873292" class="c"><input type="checkbox" id="c-40873292" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40875379">prev</a><span>|</span><a href="#40873072">next</a><span>|</span><label class="collapse" for="c-40873292">[-]</label><label class="expand" for="c-40873292">[4 more]</label></div><br/><div class="children"><div class="content">In the README, it says:<p>&gt; Important! Please don’t expect peak performance without fine-tuning the hyperparameters, such as the number of threads, kernel and block sizes, unless you are running it on a Ryzen 7700(X). More on this in the tutorial.<p>I think I&#x27;ll need a TL;DR on what to change all these values to.<p>I have a Ryzen 7950X and as a first test I tried to only change NTHREADS to 32 in benchmark.c, but matmul.c performed worse than NumPy on my machine.<p>So I took a look at the other values present in the benchmark.c, but MC and NC are already calculated via the amount of threads (so these are probably already &#x27;fine-tuned&#x27;?), and I couldn&#x27;t really understand how KC = 1000 fits for the 7700(X) (the author&#x27;s CPU) and how I&#x27;d need to adjust it for the 7950X (with the informations from the article).</div><br/><div id="40875273" class="c"><input type="checkbox" id="c-40875273" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40873292">parent</a><span>|</span><a href="#40873072">next</a><span>|</span><label class="collapse" for="c-40875273">[-]</label><label class="expand" for="c-40875273">[3 more]</label></div><br/><div class="children"><div class="content">Actually, leaving it on 16 threads performs a bit better than setting it to 32 threads.<p>16 threads: <a href="https:&#x2F;&#x2F;0x0.st&#x2F;XaDB.png" rel="nofollow">https:&#x2F;&#x2F;0x0.st&#x2F;XaDB.png</a><p>32 threads: <a href="https:&#x2F;&#x2F;0x0.st&#x2F;XaDM.png" rel="nofollow">https:&#x2F;&#x2F;0x0.st&#x2F;XaDM.png</a><p>But still not as fast as it ran on your 7700(X) and NumPy is 2-3x faster than matmul.c on my PC.<p>I also changed KC to some other values (500: <a href="https:&#x2F;&#x2F;0x0.st&#x2F;XaD9.png" rel="nofollow">https:&#x2F;&#x2F;0x0.st&#x2F;XaD9.png</a>, 2000: <a href="https:&#x2F;&#x2F;0x0.st&#x2F;XaDp.png" rel="nofollow">https:&#x2F;&#x2F;0x0.st&#x2F;XaDp.png</a>), but it didn&#x27;t change much performance wise.</div><br/><div id="40875968" class="c"><input type="checkbox" id="c-40875968" checked=""/><div class="controls bullet"><span class="by">salykova</span><span>|</span><a href="#40873292">root</a><span>|</span><a href="#40875273">parent</a><span>|</span><a href="#40873072">next</a><span>|</span><label class="collapse" for="c-40875968">[-]</label><label class="expand" for="c-40875968">[2 more]</label></div><br/><div class="children"><div class="content">as we discussed earlier, the code really needs Clang to attain high performance</div><br/><div id="40876129" class="c"><input type="checkbox" id="c-40876129" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#40873292">root</a><span>|</span><a href="#40875968">parent</a><span>|</span><a href="#40873072">next</a><span>|</span><label class="collapse" for="c-40876129">[-]</label><label class="expand" for="c-40876129">[1 more]</label></div><br/><div class="children"><div class="content">Agreed <a href="https:&#x2F;&#x2F;0x0.st&#x2F;XakD.png" rel="nofollow">https:&#x2F;&#x2F;0x0.st&#x2F;XakD.png</a> ;)</div><br/></div></div></div></div></div></div></div></div><div id="40873072" class="c"><input type="checkbox" id="c-40873072" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#40873292">prev</a><span>|</span><a href="#40873682">next</a><span>|</span><label class="collapse" for="c-40873072">[-]</label><label class="expand" for="c-40873072">[3 more]</label></div><br/><div class="children"><div class="content">Very nice write up.  Those are the kind of matrix sizes that MKL is fairly good at, might be worth a comparison as well?<p>Also, if you were designing for smaller cases, say MNK=16 or 32, how would you approach it differently?  I&#x27;m implementing neural ODEs and this is one point I&#x27;ve been considering.</div><br/><div id="40878346" class="c"><input type="checkbox" id="c-40878346" checked=""/><div class="controls bullet"><span class="by">gnufx</span><span>|</span><a href="#40873072">parent</a><span>|</span><a href="#40874621">next</a><span>|</span><label class="collapse" for="c-40878346">[-]</label><label class="expand" for="c-40878346">[1 more]</label></div><br/><div class="children"><div class="content">For very small sizes on amd64, you can, and likely should, use libxsmm.  MKL&#x27;s improved performance in that region was due to libxsmm originally showing it up, but this isn&#x27;t an Intel CPU anyway.</div><br/></div></div><div id="40874621" class="c"><input type="checkbox" id="c-40874621" checked=""/><div class="controls bullet"><span class="by">pmeira</span><span>|</span><a href="#40873072">parent</a><span>|</span><a href="#40878346">prev</a><span>|</span><a href="#40873682">next</a><span>|</span><label class="collapse" for="c-40874621">[-]</label><label class="expand" for="c-40874621">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget BLIS itself!</div><br/></div></div></div></div><div id="40873682" class="c"><input type="checkbox" id="c-40873682" checked=""/><div class="controls bullet"><span class="by">teo_zero</span><span>|</span><a href="#40873072">prev</a><span>|</span><a href="#40878798">next</a><span>|</span><label class="collapse" for="c-40873682">[-]</label><label class="expand" for="c-40873682">[3 more]</label></div><br/><div class="children"><div class="content">Does it make sense to compare a C executable with an interpreted Python program that calls a compiled library? Is the difference due to the algorithm or the call stack?</div><br/><div id="40874330" class="c"><input type="checkbox" id="c-40874330" checked=""/><div class="controls bullet"><span class="by">lamcny</span><span>|</span><a href="#40873682">parent</a><span>|</span><a href="#40873741">next</a><span>|</span><label class="collapse" for="c-40874330">[-]</label><label class="expand" for="c-40874330">[1 more]</label></div><br/><div class="children"><div class="content">Yes, as far as I can tell the array sizes were large enough to make the wrapping overhead negligible.</div><br/></div></div><div id="40873741" class="c"><input type="checkbox" id="c-40873741" checked=""/><div class="controls bullet"><span class="by">rustybolt</span><span>|</span><a href="#40873682">parent</a><span>|</span><a href="#40874330">prev</a><span>|</span><a href="#40878798">next</a><span>|</span><label class="collapse" for="c-40873741">[-]</label><label class="expand" for="c-40873741">[1 more]</label></div><br/><div class="children"><div class="content">Yes</div><br/></div></div></div></div><div id="40878798" class="c"><input type="checkbox" id="c-40878798" checked=""/><div class="controls bullet"><span class="by">tom306</span><span>|</span><a href="#40873682">prev</a><span>|</span><a href="#40871892">next</a><span>|</span><label class="collapse" for="c-40878798">[-]</label><label class="expand" for="c-40878798">[1 more]</label></div><br/><div class="children"><div class="content">Does numpy&#x27;s implementation actually use multithreading? If not, then the comparison is not fair.</div><br/></div></div><div id="40871892" class="c"><input type="checkbox" id="c-40871892" checked=""/><div class="controls bullet"><span class="by">jstrong</span><span>|</span><a href="#40878798">prev</a><span>|</span><a href="#40877224">next</a><span>|</span><label class="collapse" for="c-40871892">[-]</label><label class="expand" for="c-40871892">[4 more]</label></div><br/><div class="children"><div class="content">in terms of comparing to numpy, how much overhead would there be from Python (vs. running the numpy C code alone)?</div><br/><div id="40872008" class="c"><input type="checkbox" id="c-40872008" checked=""/><div class="controls bullet"><span class="by">SJC_Hacker</span><span>|</span><a href="#40871892">parent</a><span>|</span><a href="#40877224">next</a><span>|</span><label class="collapse" for="c-40872008">[-]</label><label class="expand" for="c-40872008">[3 more]</label></div><br/><div class="children"><div class="content">Python can efficiently call C libs if you use ctypes and native pointers, which numpy uses.  Of course depends on expected layout.<p>If you want to convert to Python lists its is going to take time.  Not sure about Python arrays.</div><br/><div id="40873000" class="c"><input type="checkbox" id="c-40873000" checked=""/><div class="controls bullet"><span class="by">dgan</span><span>|</span><a href="#40871892">root</a><span>|</span><a href="#40872008">parent</a><span>|</span><a href="#40872300">next</a><span>|</span><label class="collapse" for="c-40873000">[-]</label><label class="expand" for="c-40873000">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t recall the link but there was a github repo with comparisons of CFFI implementations in different languages, and from what i remember Python was &#x27;bout 3 orders of magnitude slower than, say, Lua or Ocaml<p>Edit: ha, found it <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=github.com&#x2F;dyu">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=github.com&#x2F;dyu</a></div><br/></div></div><div id="40872300" class="c"><input type="checkbox" id="c-40872300" checked=""/><div class="controls bullet"><span class="by">brnt</span><span>|</span><a href="#40871892">root</a><span>|</span><a href="#40872008">parent</a><span>|</span><a href="#40873000">prev</a><span>|</span><a href="#40877224">next</a><span>|</span><label class="collapse" for="c-40872300">[-]</label><label class="expand" for="c-40872300">[1 more]</label></div><br/><div class="children"><div class="content">If you use numpy, then you use an ndarray, which you can create from a C array for &#x27;free&#x27; (no copying, just set a pointer).</div><br/></div></div></div></div></div></div><div id="40877224" class="c"><input type="checkbox" id="c-40877224" checked=""/><div class="controls bullet"><span class="by">softmicro</span><span>|</span><a href="#40871892">prev</a><span>|</span><a href="#40877170">next</a><span>|</span><label class="collapse" for="c-40877224">[-]</label><label class="expand" for="c-40877224">[1 more]</label></div><br/><div class="children"><div class="content">Is there any explaination for the dropping and rising in GFLOP&#x2F;S at certain matrix sizes as shown in the plots.</div><br/></div></div><div id="40877170" class="c"><input type="checkbox" id="c-40877170" checked=""/><div class="controls bullet"><span class="by">38</span><span>|</span><a href="#40877224">prev</a><span>|</span><a href="#40871719">next</a><span>|</span><label class="collapse" for="c-40877170">[-]</label><label class="expand" for="c-40877170">[3 more]</label></div><br/><div class="children"><div class="content">&gt; #define min(x, y) ((x) &lt; (y) ? (x) : (y))<p>cursed code. I checked and every single invocation is just `int`, so why do this? you can just write a function:<p><pre><code>    func min(x, y int) int {
       if x &lt; y { return x }
       return y
    }
</code></pre>
and keep type safety</div><br/><div id="40879345" class="c"><input type="checkbox" id="c-40879345" checked=""/><div class="controls bullet"><span class="by">slashdave</span><span>|</span><a href="#40877170">parent</a><span>|</span><a href="#40871719">next</a><span>|</span><label class="collapse" for="c-40879345">[-]</label><label class="expand" for="c-40879345">[2 more]</label></div><br/><div class="children"><div class="content">Heck, you can also use the min instruction, which avoids a branch.</div><br/><div id="40879659" class="c"><input type="checkbox" id="c-40879659" checked=""/><div class="controls bullet"><span class="by">38</span><span>|</span><a href="#40877170">root</a><span>|</span><a href="#40879345">parent</a><span>|</span><a href="#40871719">next</a><span>|</span><label class="collapse" for="c-40879659">[-]</label><label class="expand" for="c-40879659">[1 more]</label></div><br/><div class="children"><div class="content">no such thing as a min instruction</div><br/></div></div></div></div></div></div><div id="40871719" class="c"><input type="checkbox" id="c-40871719" checked=""/><div class="controls bullet"><span class="by">le-mark</span><span>|</span><a href="#40877170">prev</a><span>|</span><a href="#40871839">next</a><span>|</span><label class="collapse" for="c-40871719">[-]</label><label class="expand" for="c-40871719">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This is my first time writing a blog post. If you enjoy it, please subscribe and share it!<p>Great job! Self publishing things like this were a hallmark of the early internet I for one sorely miss.</div><br/></div></div><div id="40871839" class="c"><input type="checkbox" id="c-40871839" checked=""/><div class="controls bullet"><span class="by">KerrAvon</span><span>|</span><a href="#40871719">prev</a><span>|</span><label class="collapse" for="c-40871839">[-]</label><label class="expand" for="c-40871839">[5 more]</label></div><br/><div class="children"><div class="content">The article claims this is portable C. Given the use of intel intrinsics, what happens if you try to compile it for ARM64?</div><br/><div id="40871867" class="c"><input type="checkbox" id="c-40871867" checked=""/><div class="controls bullet"><span class="by">tempay</span><span>|</span><a href="#40871839">parent</a><span>|</span><a href="#40872958">next</a><span>|</span><label class="collapse" for="c-40871867">[-]</label><label class="expand" for="c-40871867">[2 more]</label></div><br/><div class="children"><div class="content">I think they mean portable between modern x86 CPUs as opposed to truly portable.</div><br/></div></div><div id="40872958" class="c"><input type="checkbox" id="c-40872958" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#40871839">parent</a><span>|</span><a href="#40871867">prev</a><span>|</span><label class="collapse" for="c-40872958">[-]</label><label class="expand" for="c-40872958">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;d need first to convert it to portable SIMD intrinsics. There are several libraries.</div><br/><div id="40877084" class="c"><input type="checkbox" id="c-40877084" checked=""/><div class="controls bullet"><span class="by">pmeira</span><span>|</span><a href="#40871839">root</a><span>|</span><a href="#40872958">parent</a><span>|</span><label class="collapse" for="c-40877084">[-]</label><label class="expand" for="c-40877084">[1 more]</label></div><br/><div class="children"><div class="content">Apparently it also needs Clang to achieve the same performance: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40875968">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40875968</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>