<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1720170065432" as="style"/><link rel="stylesheet" href="styles.css?v=1720170065432"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.nelhage.com/post/fuzzy-dedup/">Finding near-duplicates with Jaccard similarity and MinHash</a> <span class="domain">(<a href="https://blog.nelhage.com">blog.nelhage.com</a>)</span></div><div class="subtext"><span>brianyu8</span> | <span>25 comments</span></div><br/><div><div id="40874909" class="c"><input type="checkbox" id="c-40874909" checked=""/><div class="controls bullet"><span class="by">tpoacher</span><span>|</span><a href="#40873410">next</a><span>|</span><label class="collapse" for="c-40874909">[-]</label><label class="expand" for="c-40874909">[1 more]</label></div><br/><div class="children"><div class="content">One thing many people miss about set based metrics like the jaccard similarity (aka Tanimoto coefficient) and F1 score (aka Dice coefficient), is that they can also be used identically with fuzzy sets.<p>The only complication is that you then need to choose a suitable T-Norm &#x2F; T-Conorm pair, which express the concept of intersection and union for fuzzy sets, and there&#x27;s an infinite family of them. But that&#x27;s a good thing, since you get to pick the pair with your desired semantics.<p>I wrote about this ([0][1]) in the context of validating medical image segmentations when both the segmentation and ground truth are probabilistic&#x2F;fuzzy rather than binary masks.<p>Otherwise what most people do instead is to simply threshold at 0.5 to obtain binary sets for use with the binary variants of the jaccard &#x2F; dice coefficients. Which apparently decreases the precision of your <i>validation</i> operator by 2 orders of magnitude. It&#x27;s like, you publish your algorithm claiming it&#x27;s better than SOTA by 0.001, ignoring the fact that your validation operator has an error margin of 0.1 ...<p>[0] <a href="https:&#x2F;&#x2F;link.springer.com&#x2F;chapter&#x2F;10.1007&#x2F;978-3-319-46723-8_42" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;chapter&#x2F;10.1007&#x2F;978-3-319-46723-8_...</a><p>[1] <a href="https:&#x2F;&#x2F;ora.ox.ac.uk&#x2F;objects&#x2F;uuid:dc352697-c804-4257-8aec-088ea28806c5" rel="nofollow">https:&#x2F;&#x2F;ora.ox.ac.uk&#x2F;objects&#x2F;uuid:dc352697-c804-4257-8aec-08...</a></div><br/></div></div><div id="40873410" class="c"><input type="checkbox" id="c-40873410" checked=""/><div class="controls bullet"><span class="by">BiteCode_dev</span><span>|</span><a href="#40874909">prev</a><span>|</span><a href="#40880090">next</a><span>|</span><label class="collapse" for="c-40873410">[-]</label><label class="expand" for="c-40873410">[9 more]</label></div><br/><div class="children"><div class="content">I worked with a client that implemented their own Python version of this to deduplicate citizen entries in a big french gov database. It worked well.<p>Of course, nowaday I would probably just tell them to use datasketch (<a href="https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;datasketch&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;datasketch&#x2F;</a>).<p>With this trip to memory lane, I looked around a little, and noticed people are still creating new stuff on the topic. E.G:<p><a href="https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;rensa&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;rensa&#x2F;</a><p>Which is basically a more specialized but faster version of datasketch minhash, written in rust, with a little python on top.</div><br/><div id="40874034" class="c"><input type="checkbox" id="c-40874034" checked=""/><div class="controls bullet"><span class="by">RobinL</span><span>|</span><a href="#40873410">parent</a><span>|</span><a href="#40875935">next</a><span>|</span><label class="collapse" for="c-40874034">[-]</label><label class="expand" for="c-40874034">[7 more]</label></div><br/><div class="children"><div class="content">For deduplicating people, the Fellegi Sunter model is also a powerful approach.  Splink[0] is a free Python library that implements this for big datasets.  Probably you could combine parts of both approaches as well.  Full disclosure: I am the lead author.<p>I&#x27;ve also written up some interactive tutorials on how the method works [1] if anyone&#x27;s interested<p>[0]<a href="https:&#x2F;&#x2F;github.com&#x2F;moj-analytical-services&#x2F;splink">https:&#x2F;&#x2F;github.com&#x2F;moj-analytical-services&#x2F;splink</a>
[1]<a href="https:&#x2F;&#x2F;www.robinlinacre.com&#x2F;intro_to_probabilistic_linkage&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.robinlinacre.com&#x2F;intro_to_probabilistic_linkage&#x2F;</a></div><br/><div id="40874114" class="c"><input type="checkbox" id="c-40874114" checked=""/><div class="controls bullet"><span class="by">BiteCode_dev</span><span>|</span><a href="#40873410">root</a><span>|</span><a href="#40874034">parent</a><span>|</span><a href="#40877396">next</a><span>|</span><label class="collapse" for="c-40874114">[-]</label><label class="expand" for="c-40874114">[5 more]</label></div><br/><div class="children"><div class="content">Interesting.<p>What would you say are the main differences with other approaches?</div><br/><div id="40874179" class="c"><input type="checkbox" id="c-40874179" checked=""/><div class="controls bullet"><span class="by">RobinL</span><span>|</span><a href="#40873410">root</a><span>|</span><a href="#40874114">parent</a><span>|</span><a href="#40877396">next</a><span>|</span><label class="collapse" for="c-40874179">[-]</label><label class="expand" for="c-40874179">[4 more]</label></div><br/><div class="children"><div class="content">The Fellegi Sunter model is able to estimate the importance of different types of information from the data itself (i.e. unsupervised learning).<p>For instance, a match on a date of birth column lends a greater weight of evidence in favour of a match than a match on first name (since dob has higher cardinality).<p>The method is also able to estimate weights for fuzzy matches (how much evidence in favour of a match is close match on dob with one character difference), and also how much evidence against a match a mismatch is.<p>For instance, if you have very high data quality on gender, then a match on gender doesn&#x27;t tell you much, but a mismatch on gender is quite strong evidence  against the idea two records match.<p>I have a blog post here that delves into this a bit more:
<a href="https:&#x2F;&#x2F;www.robinlinacre.com&#x2F;fellegi_sunter_accuracy&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.robinlinacre.com&#x2F;fellegi_sunter_accuracy&#x2F;</a></div><br/><div id="40880283" class="c"><input type="checkbox" id="c-40880283" checked=""/><div class="controls bullet"><span class="by">JimmyRuska</span><span>|</span><a href="#40873410">root</a><span>|</span><a href="#40874179">parent</a><span>|</span><a href="#40874242">next</a><span>|</span><label class="collapse" for="c-40880283">[-]</label><label class="expand" for="c-40880283">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t sound like the approaches are incompatible. You can use minhash LSH to search a large set and get a top-k list for any individual, then use a weighted average with penalty rules to decide which of those qualifies as a dupe or not. Weighted minhash can also be used to efficiently add repeats to give some additional weighting.</div><br/></div></div><div id="40874242" class="c"><input type="checkbox" id="c-40874242" checked=""/><div class="controls bullet"><span class="by">BiteCode_dev</span><span>|</span><a href="#40873410">root</a><span>|</span><a href="#40874179">parent</a><span>|</span><a href="#40880283">prev</a><span>|</span><a href="#40877396">next</a><span>|</span><label class="collapse" for="c-40874242">[-]</label><label class="expand" for="c-40874242">[2 more]</label></div><br/><div class="children"><div class="content">Ok, so you get better accuracy if you datasets have obviously weighted fields. Do you pay that in perfs, and if yes how much?</div><br/><div id="40874276" class="c"><input type="checkbox" id="c-40874276" checked=""/><div class="controls bullet"><span class="by">RobinL</span><span>|</span><a href="#40873410">root</a><span>|</span><a href="#40874242">parent</a><span>|</span><a href="#40877396">next</a><span>|</span><label class="collapse" for="c-40874276">[-]</label><label class="expand" for="c-40874276">[1 more]</label></div><br/><div class="children"><div class="content">The performance is pretty good because the prediction is ultimately just adding up match weights.  Much of the performance is dictated by<p>(1) The blocking approach you choose (how wide you cast the net in searching for matches.  This is actually somewhere were minhash can be used in conjunction<p>(2) whether you choose to use  complex fuzzy matching functions and how many - this is chosen by the user.<p>There&#x27;s some benchmarking results here:
<a href="https:&#x2F;&#x2F;www.robinlinacre.com&#x2F;fast_deduplication&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.robinlinacre.com&#x2F;fast_deduplication&#x2F;</a><p>Overall it&#x27;s an approach which makes a pretty good trade off between speed and accuracy.  That&#x27;s why it&#x27;s used by many national stats institutes (UK, US, Aus, Germany etc.) - because it&#x27;s capable of working on country-population sized datasets.</div><br/></div></div></div></div></div></div></div></div><div id="40877396" class="c"><input type="checkbox" id="c-40877396" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#40873410">root</a><span>|</span><a href="#40874034">parent</a><span>|</span><a href="#40874114">prev</a><span>|</span><a href="#40875935">next</a><span>|</span><label class="collapse" for="c-40877396">[-]</label><label class="expand" for="c-40877396">[1 more]</label></div><br/><div class="children"><div class="content">Wonderful library and a really great set of explainers! Thanks for sharing this. Wish I had this for quite a few past projects…</div><br/></div></div></div></div><div id="40875935" class="c"><input type="checkbox" id="c-40875935" checked=""/><div class="controls bullet"><span class="by">molodec</span><span>|</span><a href="#40873410">parent</a><span>|</span><a href="#40874034">prev</a><span>|</span><a href="#40880090">next</a><span>|</span><label class="collapse" for="c-40875935">[-]</label><label class="expand" for="c-40875935">[1 more]</label></div><br/><div class="children"><div class="content">There is also gaoya ( I am the author ), which is also written in Rust and has python bindings. datasketch is great, but it is not performant enough for my use case. gaoya is used in a production system for large scale clustering
<a href="https:&#x2F;&#x2F;github.com&#x2F;serega&#x2F;gaoya">https:&#x2F;&#x2F;github.com&#x2F;serega&#x2F;gaoya</a></div><br/></div></div></div></div><div id="40880090" class="c"><input type="checkbox" id="c-40880090" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#40873410">prev</a><span>|</span><a href="#40880089">next</a><span>|</span><label class="collapse" for="c-40880090">[-]</label><label class="expand" for="c-40880090">[1 more]</label></div><br/><div class="children"><div class="content">a little backstory (that seems to be missing from the blog post). this is a technique that was invented in the early days of google for deduplicating the crawl set (turns out that building an llm and building a plain ol&#x27; web text index look awfully similar; i wonder why that is?!).  you can read about it in depth in jeffrey ullman&#x27;s free book &quot;mining massive datasets&quot; which describes many of the really cool and impressive techniques that were used to prepare an index build for the entire internet when such a thing was really hard.<p>you can find the relevant material for free by searching &quot;chapter 3 pdf mmds ullman&quot;<p>enjoy!<p>edit: oh no!  i&#x27;m wrong!  according to wikipedia it was invented at dec for altavista. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MinHash" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MinHash</a> either way there&#x27;s a nice description in the ullman book and they do describe how it was used at google as well.</div><br/></div></div><div id="40880089" class="c"><input type="checkbox" id="c-40880089" checked=""/><div class="controls bullet"><span class="by">carnewal</span><span>|</span><a href="#40880090">prev</a><span>|</span><a href="#40874347">next</a><span>|</span><label class="collapse" for="c-40880089">[-]</label><label class="expand" for="c-40880089">[1 more]</label></div><br/><div class="children"><div class="content">As I got into Minhash &amp; its variants I found it hard to wrap my head around this, so I&#x27;m building an online visualizer<p><a href="https:&#x2F;&#x2F;websla.sh&#x2F;tools&#x2F;minhash" rel="nofollow">https:&#x2F;&#x2F;websla.sh&#x2F;tools&#x2F;minhash</a><p>It&#x27;s not really complete, I&#x27;d like to show Jaccard Similarity calculations among other things, but this already allows you to enter multiple strings and see with your own eyes what a &quot;minhash&quot; actually is.</div><br/></div></div><div id="40874347" class="c"><input type="checkbox" id="c-40874347" checked=""/><div class="controls bullet"><span class="by">pkeenan11</span><span>|</span><a href="#40880089">prev</a><span>|</span><a href="#40878897">next</a><span>|</span><label class="collapse" for="c-40874347">[-]</label><label class="expand" for="c-40874347">[1 more]</label></div><br/><div class="children"><div class="content">Holy synchronicity Batman! I just implemented a minhash system that some may find interesting.<p>The problem is to find (pseudo) inverses of many different proper submatrices of a big square matrix.<p>Certain matrix identities (Woodbury, banachiewicz) allow you to update the inverse of a “close” submatrix to cheaply calculate a new one.<p>So you store the inverses you’ve calculated already, with their row&#x2F;column indices as keys. Then for each new submatrix you want to find a close already-computed inverse to update from.<p>This is solved with minhashing. You minwise hash the indices so that close matrices are likely to have the same hash.<p>In my particular implementation I did a “multi-resolution” hash so that I can change the selectiveness of the search as the number of prior computed inverses grows</div><br/></div></div><div id="40878897" class="c"><input type="checkbox" id="c-40878897" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#40874347">prev</a><span>|</span><a href="#40874070">next</a><span>|</span><label class="collapse" for="c-40878897">[-]</label><label class="expand" for="c-40878897">[1 more]</label></div><br/><div class="children"><div class="content">Hashing or tiny neural nets combined with a Vector Search engine with Tanimoto&#x2F;Jaccard is a very common deduplication strategy for large datasets. It might be wiser than using linear-complexity MapReduce operations.<p>There is a nice Google project using 0.5 M parameter RETSim model and the USearch engine for that: <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;unisim">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;unisim</a></div><br/></div></div><div id="40874070" class="c"><input type="checkbox" id="c-40874070" checked=""/><div class="controls bullet"><span class="by">vivzkestrel</span><span>|</span><a href="#40878897">prev</a><span>|</span><a href="#40877644">next</a><span>|</span><label class="collapse" for="c-40874070">[-]</label><label class="expand" for="c-40874070">[7 more]</label></div><br/><div class="children"><div class="content">I have this problem right now in postgres. I have 600000 feed_items with the schema (feed_item_id uuid, author varchar, content text, guid varchar, link varchar, title varchar, summary text, feed_id integer) The content and summary columns especially for some of the news items are highly similar but not equal.  For any given 2 such news items, I am trying to cut them down to 1. Any ideas?</div><br/><div id="40874652" class="c"><input type="checkbox" id="c-40874652" checked=""/><div class="controls bullet"><span class="by">dleeftink</span><span>|</span><a href="#40874070">parent</a><span>|</span><a href="#40874094">next</a><span>|</span><label class="collapse" for="c-40874652">[-]</label><label class="expand" for="c-40874652">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve implemented a minhash-like system in Bigquery, and was able to calculate cosine similarities (within a certain similarity threshold) between all Stack Overflow in reasonable time. To give you a broad idea of the procedure:<p>1. Concatenate and split all text fields into an array of ngrams (2...n chars)<p>2. Declare two global arrays A &amp; B of length-n and fill them with random 32-64 bit integers<p>3. Hash each ngram to a 32-64 bit integer, multiply the resulting hash by each random value in array A, and modulo the resulting output with each random value in array B. Take the minimum value. Per row, your aim is to end up with an array of &#x27;minhashed&#x27; integers of equal length to the arrays in step 2. If you declare the global arrays to be length of 64, the minhashed array for each row will also be of length 64.<p>4. Bucketise the resulting hash arrays by summing N consecutive minhash values using a window function (e.g. sum each 4 consecutive rows)<p>If all went well, you can now unnest these arrays (call them &#x27;source rows&#x27;) and self join the dataset on each bucketed minhash value (resulting in an additional column of &#x27;target rows&#x27;). Group by source, target columns and count the occurances to get an indication of how likely two rows are similar.<p>In essence, the more often two items hash to a similar bucket, the more similar they are, and it&#x27;s up to you to define the cut-off from where to calculate actual pairwise Jaccard (or cosine) similarities between items.</div><br/></div></div><div id="40874094" class="c"><input type="checkbox" id="c-40874094" checked=""/><div class="controls bullet"><span class="by">RobinL</span><span>|</span><a href="#40874070">parent</a><span>|</span><a href="#40874652">prev</a><span>|</span><a href="#40874362">next</a><span>|</span><label class="collapse" for="c-40874094">[-]</label><label class="expand" for="c-40874094">[2 more]</label></div><br/><div class="children"><div class="content">One useful technique here could be to use text embeddings and cosine similarity:
<a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;</a></div><br/><div id="40875187" class="c"><input type="checkbox" id="c-40875187" checked=""/><div class="controls bullet"><span class="by">swasheck</span><span>|</span><a href="#40874070">root</a><span>|</span><a href="#40874094">parent</a><span>|</span><a href="#40874362">next</a><span>|</span><label class="collapse" for="c-40875187">[-]</label><label class="expand" for="c-40875187">[1 more]</label></div><br/><div class="children"><div class="content">love this and have been using tf&#x2F;idf for embeddings and various measures of similarity for some personal pet projects. one thing i came across in my research is that cosine similarity was more useful for vectors of different lengths and that euclidean distance was useful for vectors of similar length but simon alludes to a same-length requirement. i’m not formally trained in this area so i was hoping someone could shed some light on this for me.</div><br/></div></div></div></div><div id="40877129" class="c"><input type="checkbox" id="c-40877129" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#40874070">parent</a><span>|</span><a href="#40874362">prev</a><span>|</span><a href="#40875203">next</a><span>|</span><label class="collapse" for="c-40877129">[-]</label><label class="expand" for="c-40877129">[1 more]</label></div><br/><div class="children"><div class="content">Have an LLM generate a reverse index for the entries, but force it to keep the cardinality low. Then you can use a Jaccard similarity.</div><br/></div></div><div id="40875203" class="c"><input type="checkbox" id="c-40875203" checked=""/><div class="controls bullet"><span class="by">probably_wrong</span><span>|</span><a href="#40874070">parent</a><span>|</span><a href="#40877129">prev</a><span>|</span><a href="#40877644">next</a><span>|</span><label class="collapse" for="c-40875203">[-]</label><label class="expand" for="c-40875203">[1 more]</label></div><br/><div class="children"><div class="content">If you think two items cover the same highly similar _keywords_ then Jaccard distance should work.<p>If you think two items share highly similar _text_ then give Levenshtein distance a try.</div><br/></div></div></div></div><div id="40877644" class="c"><input type="checkbox" id="c-40877644" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#40874070">prev</a><span>|</span><a href="#40877675">next</a><span>|</span><label class="collapse" for="c-40877644">[-]</label><label class="expand" for="c-40877644">[1 more]</label></div><br/><div class="children"><div class="content">As a document clustering &#x2F; dataset deduplication technique, how well does &quot;throwing ML at the problem&quot; (e.g. using a pre-trained LLM encoder to generate vector embeddings of your documents, and then sticking those vectors into a vector DB and doing k-means to cluster the vectors) compare, quality-wise and performance-wise, to these simpler discrete-algorithm methods?</div><br/></div></div><div id="40877675" class="c"><input type="checkbox" id="c-40877675" checked=""/><div class="controls bullet"><span class="by">motohagiography</span><span>|</span><a href="#40877644">prev</a><span>|</span><a href="#40873987">next</a><span>|</span><label class="collapse" for="c-40877675">[-]</label><label class="expand" for="c-40877675">[1 more]</label></div><br/><div class="children"><div class="content">there was a really simple string sort by similarity oneliner I can&#x27;t find in my history, but the basic idea was to take a wordlist, split each word in half, reverse the halves, concatenate them back together, sort the resulting strings, then flip the strings back to their original words.<p>this simple shuffle was a poor man&#x27;s similarity sort that I used to find sound-alike words. the thing about the quality of similarity is that it&#x27;s hard to know for sure if it worked, but this was sufficient.</div><br/></div></div><div id="40873987" class="c"><input type="checkbox" id="c-40873987" checked=""/><div class="controls bullet"><span class="by">is_true</span><span>|</span><a href="#40877675">prev</a><span>|</span><label class="collapse" for="c-40873987">[-]</label><label class="expand" for="c-40873987">[1 more]</label></div><br/><div class="children"><div class="content">I had to do this with sport teams but I used levenshtein for the names.  I ended up creating a vector with other data (country, gender) and using that vector to calculate the distance (similarity).</div><br/></div></div></div></div></div></div></div></body></html>