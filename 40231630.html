<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1714640483936" as="style"/><link rel="stylesheet" href="styles.css?v=1714640483936"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/espeak-ng/espeak-ng">ESpeak-ng: speech synthesizer with more than one hundred languages and accents</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>nateb2022</span> | <span>41 comments</span></div><br/><div><div id="40232384" class="c"><input type="checkbox" id="c-40232384" checked=""/><div class="controls bullet"><span class="by">retrac</span><span>|</span><a href="#40233104">next</a><span>|</span><label class="collapse" for="c-40232384">[-]</label><label class="expand" for="c-40232384">[16 more]</label></div><br/><div class="children"><div class="content">Classic speech synthesis is interesting, in that relatively simple approaches, produce useful results.  Formant synthesis takes relatively simple sounds, and modifies them according to the various distinctions the human speech tract can make.  The basic vowel quality can be modelled as two sine waves that change over time.  (Nothing more complex than what&#x27;s needed to generate touch tone dialing tones, basically.)    Add a few types of buzzing or clicking noises before or after that for consonants, and you&#x27;re halfway there.  The technique predates computers; it&#x27;s basically the same technique used by the original voder [1] just under computer control.<p>Join that with algorithms which can translate English into phonetic tokens with relatively high accuracy, and you have speech synthesis.  Make the dictionary big enough, add enough finesse, and a few hundred rules about transitioning from phoneme to phoneme, and it&#x27;s produces relatively understandable speech.<p>Part of me feels that we are losing something, moving away from these classic approaches to AI.  It used to be that, to teach a machine how to speak, or translate, the designer of the system had to understand how language worked.  Sometimes these models percolated back into broader thinking about language.  Formant synthesis ended up being an inspiration to some ideas for how the brain recognizes phonemes.  (Or maybe that worked in both directions.)  It was thought, further advances would come from better theories about language, better abstractions.  Deep learning has produced far better systems than the classic approach, but they also offer little in terms of understanding or simplifying.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Voder" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Voder</a></div><br/><div id="40232918" class="c"><input type="checkbox" id="c-40232918" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40232384">parent</a><span>|</span><a href="#40232490">next</a><span>|</span><label class="collapse" for="c-40232918">[-]</label><label class="expand" for="c-40232918">[3 more]</label></div><br/><div class="children"><div class="content">I feel like you. Relatively simple formant models gets &quot;close enough&quot; that it feels like you should be able to do well with very little.<p>One of the things I&#x27;ve long wanted to do but not found time for, is to take a few different variants of formant synths, and try to train a simple TTS model to control one instead of producing &quot;raw&quot; output. It&#x27;s amazing what TTS models can do with &quot;raw&quot; output, but we know our brains aren&#x27;t producing raw, unconstrained digital audio, and so I think there&#x27;s a lot of potential to understanding more and simplifying if you train models constrained to produce outputs we know ought to be sufficient, and push their size as far down as we can.</div><br/><div id="40233857" class="c"><input type="checkbox" id="c-40233857" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232918">parent</a><span>|</span><a href="#40232490">next</a><span>|</span><label class="collapse" for="c-40233857">[-]</label><label class="expand" for="c-40233857">[2 more]</label></div><br/><div class="children"><div class="content">Too late to edit, but to any one who needs &quot;convincing&quot; of the flexibility of a formant synthesizer, you should 1) play with Pink Trombone[1], a Javascript formant synthesizer with a UI that lets you graphically manipulate a vocal tract, and 2) have a look at this programmable version of it[2]<p>[1] <a href="https:&#x2F;&#x2F;dood.al&#x2F;pinktrombone&#x2F;" rel="nofollow">https:&#x2F;&#x2F;dood.al&#x2F;pinktrombone&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;zakaton&#x2F;Pink-Trombone">https:&#x2F;&#x2F;github.com&#x2F;zakaton&#x2F;Pink-Trombone</a></div><br/><div id="40234129" class="c"><input type="checkbox" id="c-40234129" checked=""/><div class="controls bullet"><span class="by">drcongo</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40233857">parent</a><span>|</span><a href="#40232490">next</a><span>|</span><label class="collapse" for="c-40234129">[-]</label><label class="expand" for="c-40234129">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for those links, that&#x27;s superb. Sounds surprisingly like the &quot;Oh long Johnson&quot; cat - <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kkwiQmGWK4c" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=kkwiQmGWK4c</a></div><br/></div></div></div></div></div></div><div id="40232490" class="c"><input type="checkbox" id="c-40232490" checked=""/><div class="controls bullet"><span class="by">bhaney</span><span>|</span><a href="#40232384">parent</a><span>|</span><a href="#40232918">prev</a><span>|</span><a href="#40232461">next</a><span>|</span><label class="collapse" for="c-40232490">[-]</label><label class="expand" for="c-40232490">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It used to be that, to teach a machine how to [X], the designer of the system had to understand how [X] worked.<p>It does feel like we&#x27;re rapidly losing this relationship in general. I think it&#x27;s going to be a good thing overall for productivity and the advancement of mankind, but it definitely takes a lot of the humanity out of our collective accomplishments. I feel warm and fuzzy when a person goes on a quest to deeply understand a subject and then shares the fruits of their efforts with everyone else, but I don&#x27;t feel like that when someone points at a subject and says &quot;hey computer, become good at that&quot; with similar end results.</div><br/><div id="40232541" class="c"><input type="checkbox" id="c-40232541" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232490">parent</a><span>|</span><a href="#40232461">next</a><span>|</span><label class="collapse" for="c-40232541">[-]</label><label class="expand" for="c-40232541">[2 more]</label></div><br/><div class="children"><div class="content"><i>I think it&#x27;s going to be a good thing overall for productivity and the advancement of mankind, but it definitely takes a lot of the humanity out of our collective accomplishments.</i><p>I think AI will only cause us to become stuck in another local maximum, since not understanding how something works can only lead to imitation at best, and not inspiration.</div><br/><div id="40233038" class="c"><input type="checkbox" id="c-40233038" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232541">parent</a><span>|</span><a href="#40232461">next</a><span>|</span><label class="collapse" for="c-40233038">[-]</label><label class="expand" for="c-40233038">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not convinced, because I think there will be a drive to distill down models and constrain them, and try to train models with access to &quot;premade blocks&quot; of functionality we know should help.<p>E.g. we know human voices can be produced well with formant synthesis because we know how the human vocal tract is shaped. So you can &quot;give&quot; a model a formant synth, and try to train smaller models outputting to it.<p>I think there&#x27;s going to be a whole lot of research possibilities in placing constraints and training smaller models, and even training ensembles of models constrained in how they&#x27;re interacting and their relative sizes to try to &quot;force&quot; extraction of functionality.<p>E.g. we have reasonable estimates at the lowest bitrate raw audio that produces passable voice. Now consider training two models A, B, where A =&gt; B =&gt; audio, and the &quot;channel&quot; between A and B is constrained to a small fraction of the bitrate that&#x27;d let A do all the work, and where the size of B is set at a level you&#x27;ve first struggled to get passable TTS output from.<p>Try to squeeze the bitrate and&#x2F;or the size of B down and see if you can get something to emerge where analysing what happens in B is doable.</div><br/></div></div></div></div></div></div><div id="40232461" class="c"><input type="checkbox" id="c-40232461" checked=""/><div class="controls bullet"><span class="by">senkora</span><span>|</span><a href="#40232384">parent</a><span>|</span><a href="#40232490">prev</a><span>|</span><a href="#40233022">next</a><span>|</span><label class="collapse" for="c-40232461">[-]</label><label class="expand" for="c-40232461">[8 more]</label></div><br/><div class="children"><div class="content">It is possible to synthesize an English voice with a 1.5MB model using <a href="http:&#x2F;&#x2F;cmuflite.org&#x2F;" rel="nofollow">http:&#x2F;&#x2F;cmuflite.org&#x2F;</a> or some of the Apple VoiceOver voices, which is just crazy to me. Most of the model is diphone samples for pairs of phonemes.<p>I don&#x27;t know of any way to go smaller than that with software. I tried, but it seems like a fundamental limit for English.</div><br/><div id="40233078" class="c"><input type="checkbox" id="c-40233078" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232461">parent</a><span>|</span><a href="#40232518">next</a><span>|</span><label class="collapse" for="c-40233078">[-]</label><label class="expand" for="c-40233078">[2 more]</label></div><br/><div class="children"><div class="content">You probably could by what I call the &quot;eastern-european method.&quot; Record one wave period of each phoneme, perhaps two for plosives, downsample to 8 or 11 kHz 8 bit, and repeat that recording on-the-fly enough times to make the right sound. If you&#x27;re thinking &quot;mod file&quot;, you&#x27;re on the right track.<p>For phonetically simple languages, such a system can easily fit on a microcontroller with kilobytes of RAM and a slow CPU. English might require a little bit more on the text-to-phoneme stage, but you can definitely go far below 1MB.</div><br/><div id="40233819" class="c"><input type="checkbox" id="c-40233819" checked=""/><div class="controls bullet"><span class="by">rhdunn</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40233078">parent</a><span>|</span><a href="#40232518">next</a><span>|</span><label class="collapse" for="c-40233819">[-]</label><label class="expand" for="c-40233819">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s effectively what eSpeak-ng is doing.<p>For the CMU flite voices they represent the data as LPC (linear predictive coding) data with residual remainder (residual excited LPC). The HTS models use simple neural networks to predict the waveforms -- IIRC, these are similar to RNNs.<p>The MBROLA models use OLA (overlapped add) to overlap small waveform samples. They also use diphone samples taken from midpoint to midpoint in order to create better phoneme transitions.</div><br/></div></div></div></div><div id="40232518" class="c"><input type="checkbox" id="c-40232518" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232461">parent</a><span>|</span><a href="#40233078">prev</a><span>|</span><a href="#40233087">next</a><span>|</span><label class="collapse" for="c-40232518">[-]</label><label class="expand" for="c-40232518">[2 more]</label></div><br/><div class="children"><div class="content"><i>I don&#x27;t know of any way to go smaller than that with software. I tried, but it seems like a fundamental limit for English.</i><p>If you include &quot;robotic&quot; speech, then there&#x27;s <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Software_Automatic_Mouth" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Software_Automatic_Mouth</a> in a few tens of KB, and the demoscene has done similar in around 1&#x2F;10th that. All formant synths, of course, not the sample-based ones that you&#x27;re referring to.</div><br/><div id="40232958" class="c"><input type="checkbox" id="c-40232958" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232518">parent</a><span>|</span><a href="#40233087">next</a><span>|</span><label class="collapse" for="c-40232958">[-]</label><label class="expand" for="c-40232958">[1 more]</label></div><br/><div class="children"><div class="content">SAM is awful but at the same time tantalisingly <i>close</i>  (one of the demos of it apparently draws on a great reverse engineered version by Sebastian Macke and refactoring efforts by a couple of others including me - I spent too many hours listening to SAM output...) - especially when comparing to the still awful Festival&#x2F;Flite models -, that I keep wanting to see what a better generic formant synth used as constraint on an ML model would produce.<p>That is, instead of allowing a generic machine learning model to output unconstrained audio, train it on the basis of letting it produce low bitrate input&#x2F;control values for a formant synth instead, and see just how small you can push the model.</div><br/></div></div></div></div><div id="40233087" class="c"><input type="checkbox" id="c-40233087" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232461">parent</a><span>|</span><a href="#40232518">prev</a><span>|</span><a href="#40233368">next</a><span>|</span><label class="collapse" for="c-40233087">[-]</label><label class="expand" for="c-40233087">[2 more]</label></div><br/><div class="children"><div class="content">Most of the links there are dead.<p>That&#x27;s a descendant of Festival Singer, which was well respected in its day.<p>What&#x27;s a current practical text-to-speech system that&#x27;s open source, local, and not huge?</div><br/><div id="40233793" class="c"><input type="checkbox" id="c-40233793" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40233087">parent</a><span>|</span><a href="#40233368">next</a><span>|</span><label class="collapse" for="c-40233793">[-]</label><label class="expand" for="c-40233793">[1 more]</label></div><br/><div class="children"><div class="content">Flite.</div><br/></div></div></div></div><div id="40233368" class="c"><input type="checkbox" id="c-40233368" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#40232384">root</a><span>|</span><a href="#40232461">parent</a><span>|</span><a href="#40233087">prev</a><span>|</span><a href="#40233022">next</a><span>|</span><label class="collapse" for="c-40233368">[-]</label><label class="expand" for="c-40233368">[1 more]</label></div><br/><div class="children"><div class="content">I always wanted something like Flite but for Spanish.</div><br/></div></div></div></div><div id="40233022" class="c"><input type="checkbox" id="c-40233022" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#40232384">parent</a><span>|</span><a href="#40232461">prev</a><span>|</span><a href="#40233104">next</a><span>|</span><label class="collapse" for="c-40233022">[-]</label><label class="expand" for="c-40233022">[1 more]</label></div><br/><div class="children"><div class="content">Do you have any good resources on this?<p>I took a few stabs at understanding Klatt, but I feel like I had far too little DSP, math and linguistic intuitions back then to fully comprehend it, perhaps I should take another one now.</div><br/></div></div></div></div><div id="40233104" class="c"><input type="checkbox" id="c-40233104" checked=""/><div class="controls bullet"><span class="by">miki123211</span><span>|</span><a href="#40232384">prev</a><span>|</span><a href="#40232263">next</a><span>|</span><label class="collapse" for="c-40233104">[-]</label><label class="expand" for="c-40233104">[3 more]</label></div><br/><div class="children"><div class="content">Blind person here, ESpeak-ng is literally what I use on all of my devices for most of my day, every day.<p>I switched to it in early childhood, at a time where human-sounding synthesizers were notoriously slow and noticeably unresponsive, and just haven&#x27;t found anything better ever since. I&#x27;ve used Vocalizer for a while, which is what iOS and Mac OS ship with, but then third-party synthesizer support was added and I switched right back.</div><br/><div id="40233232" class="c"><input type="checkbox" id="c-40233232" checked=""/><div class="controls bullet"><span class="by">maxglute</span><span>|</span><a href="#40233104">parent</a><span>|</span><a href="#40233750">next</a><span>|</span><label class="collapse" for="c-40233232">[-]</label><label class="expand" for="c-40233232">[1 more]</label></div><br/><div class="children"><div class="content">How fast do you set speech playback speed&#x2F;rate?<p>I tried a bunch of speech synthesis, with speed and intelligibility in mind.<p>ESpeakng-ng barely intelligible past ~500 words per minute, and just generally unpleasant to listen to. Maybe my brain just can&#x27;t acclimatize to it.<p>Microsoft Zira Mobile (unlock on win11 desktop via regex) sounds much more natural and intelligible at max windows SAPI speech rate, which I estimate is around ~600 and equivalent to most conversation&#x2F;casual spoken word at 2x speed. I wish windows could increase playback even further, my brain can process 900-1200 words per minute or 3x-4x normal playback speed.<p>On Android, Google&#x27;s &quot;United States - 1&quot; sounds a little awkward but also intelligible at 3x-4x speed.</div><br/></div></div><div id="40233750" class="c"><input type="checkbox" id="c-40233750" checked=""/><div class="controls bullet"><span class="by">agumonkey</span><span>|</span><a href="#40233104">parent</a><span>|</span><a href="#40233232">prev</a><span>|</span><a href="#40232263">next</a><span>|</span><label class="collapse" for="c-40233750">[-]</label><label class="expand" for="c-40233750">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the heads-up. May I ask you if you know websites &#x2F; articles that explain daily setups for blind people ? I had issues that required me not to rely on sight and I couldn&#x27;t find much.</div><br/></div></div></div></div><div id="40232263" class="c"><input type="checkbox" id="c-40232263" checked=""/><div class="controls bullet"><span class="by">mewse-hn</span><span>|</span><a href="#40233104">prev</a><span>|</span><a href="#40233054">next</a><span>|</span><label class="collapse" for="c-40232263">[-]</label><label class="expand" for="c-40232263">[1 more]</label></div><br/><div class="children"><div class="content">No example output? Here&#x27;s a youtube video where he plays with this software<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=493xbPIQBSU" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=493xbPIQBSU</a></div><br/></div></div><div id="40233054" class="c"><input type="checkbox" id="c-40233054" checked=""/><div class="controls bullet"><span class="by">fisian</span><span>|</span><a href="#40232263">prev</a><span>|</span><a href="#40232527">next</a><span>|</span><label class="collapse" for="c-40233054">[-]</label><label class="expand" for="c-40233054">[1 more]</label></div><br/><div class="children"><div class="content">I used it on Android and it seems to be one of very few apps that can replace the default Google services text-to-speech engine.<p>However, I wasn&#x27;t satisfied with the speech quality so now I&#x27;m using RHVoice.
RHVoice seems to produce more natural&#x2F;human-sounding output yo me.</div><br/></div></div><div id="40232527" class="c"><input type="checkbox" id="c-40232527" checked=""/><div class="controls bullet"><span class="by">SoftTalker</span><span>|</span><a href="#40233054">prev</a><span>|</span><a href="#40233249">next</a><span>|</span><label class="collapse" for="c-40232527">[-]</label><label class="expand" for="c-40232527">[3 more]</label></div><br/><div class="children"><div class="content">Can I get my map navigation prompts in the voice of Yoda please?<p>&quot;At the roundabout, the second exit take.&quot;<p>&quot;At your destination, arrived have you.&quot;</div><br/><div id="40233032" class="c"><input type="checkbox" id="c-40233032" checked=""/><div class="controls bullet"><span class="by">082349872349872</span><span>|</span><a href="#40232527">parent</a><span>|</span><a href="#40233774">next</a><span>|</span><label class="collapse" for="c-40233032">[-]</label><label class="expand" for="c-40233032">[1 more]</label></div><br/><div class="children"><div class="content">Quenya is an option, so (assuming you speak it) you <i>could</i> get your map navigation prompts in the voice of Galadriel...<p>&quot;A star shall shine on the hour of our taking the second exit.&quot;<p>&quot;You have reached your Destination, fair as the Sea and the Sun and the Snow upon the Mountain!&quot;</div><br/></div></div><div id="40233774" class="c"><input type="checkbox" id="c-40233774" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#40232527">parent</a><span>|</span><a href="#40233032">prev</a><span>|</span><a href="#40233249">next</a><span>|</span><label class="collapse" for="c-40233774">[-]</label><label class="expand" for="c-40233774">[1 more]</label></div><br/><div class="children"><div class="content">You mean the voice, or the grammar? The grammar part is outside of the scope of a synthesizer. That&#x27;s completely up to the user.<p>Or you want a model which translates normal English into Yoda-English (on text level) and then attach a speech synthesizer on that?<p>Or I guess an end-to-end speech synthesizer, a big neural network which operates on the whole sentence at once, could also internally learn to do that grammar transformation.</div><br/></div></div></div></div><div id="40233249" class="c"><input type="checkbox" id="c-40233249" checked=""/><div class="controls bullet"><span class="by">deknos</span><span>|</span><a href="#40232527">prev</a><span>|</span><a href="#40232309">next</a><span>|</span><label class="collapse" for="c-40233249">[-]</label><label class="expand" for="c-40233249">[2 more]</label></div><br/><div class="children"><div class="content">Is this better than the classical espeak which is available in opensource repositories?<p>I would be very glad if there&#x27;s a truly open source local hosted text to speech software which brings good human sounding speech in woman&#x2F;man german&#x2F;english&#x2F;french&#x2F;spanish&#x2F;russian&#x2F;arabic language...</div><br/><div id="40233362" class="c"><input type="checkbox" id="c-40233362" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#40233249">parent</a><span>|</span><a href="#40232309">next</a><span>|</span><label class="collapse" for="c-40233362">[-]</label><label class="expand" for="c-40233362">[1 more]</label></div><br/><div class="children"><div class="content">When you install espeak with a distro package manager, you&#x27;re quite likely to get espeak-ng.</div><br/></div></div></div></div><div id="40232309" class="c"><input type="checkbox" id="c-40232309" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#40233249">prev</a><span>|</span><a href="#40233103">next</a><span>|</span><label class="collapse" for="c-40232309">[-]</label><label class="expand" for="c-40232309">[2 more]</label></div><br/><div class="children"><div class="content">Anyone know why the default voice is set to be so bad?</div><br/><div id="40233234" class="c"><input type="checkbox" id="c-40233234" checked=""/><div class="controls bullet"><span class="by">mrob</span><span>|</span><a href="#40232309">parent</a><span>|</span><a href="#40233103">next</a><span>|</span><label class="collapse" for="c-40233234">[-]</label><label class="expand" for="c-40233234">[1 more]</label></div><br/><div class="children"><div class="content">Why specifically do you consider it to be bad? Espeak-ng is primarily an accessibility tool, used as the voice synthesizer for screen readers. Clarity at high speed is more important than realism.</div><br/></div></div></div></div><div id="40233103" class="c"><input type="checkbox" id="c-40233103" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#40232309">prev</a><span>|</span><a href="#40232320">next</a><span>|</span><label class="collapse" for="c-40233103">[-]</label><label class="expand" for="c-40233103">[4 more]</label></div><br/><div class="children"><div class="content">Why is the quality of open source TTS so horribly, horribly, horribly behind the commercial neural ones? This is nowhere near the quality of Google, Microsoft, or Amazon TTS, yet for image generation and LLMs almost everything outside of OpenAI seems to be open-sourced.</div><br/><div id="40233808" class="c"><input type="checkbox" id="c-40233808" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#40233103">parent</a><span>|</span><a href="#40233191">next</a><span>|</span><label class="collapse" for="c-40233808">[-]</label><label class="expand" for="c-40233808">[1 more]</label></div><br/><div class="children"><div class="content">The quality also depends on the type of model. I&#x27;m not really sure what ESpeak-ng actually uses? The classical TTS approaches often use some statistical model (e.g. HMM) + some vocoder. You can get to intelligible speech pretty easily but the quality is bad (w.r.t. how natural it sounds).<p>There are better open source TTS models. E.g. check <a href="https:&#x2F;&#x2F;github.com&#x2F;neonbjb&#x2F;tortoise-tts">https:&#x2F;&#x2F;github.com&#x2F;neonbjb&#x2F;tortoise-tts</a> or <a href="https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;tacotron2">https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;tacotron2</a>. Or here for more: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;12kjof5&#x2F;d_what_is_the_best_open_source_text_to_speech&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;12kjof5&#x2F;d_...</a></div><br/></div></div><div id="40233191" class="c"><input type="checkbox" id="c-40233191" checked=""/><div class="controls bullet"><span class="by">str3wer</span><span>|</span><a href="#40233103">parent</a><span>|</span><a href="#40233808">prev</a><span>|</span><a href="#40233403">next</a><span>|</span><label class="collapse" for="c-40233191">[-]</label><label class="expand" for="c-40233191">[1 more]</label></div><br/><div class="children"><div class="content">almost like there&#x27;s a few bilion dollars difference in their budgets</div><br/></div></div><div id="40233403" class="c"><input type="checkbox" id="c-40233403" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#40233103">parent</a><span>|</span><a href="#40233191">prev</a><span>|</span><a href="#40232320">next</a><span>|</span><label class="collapse" for="c-40233403">[-]</label><label class="expand" for="c-40233403">[1 more]</label></div><br/><div class="children"><div class="content">Festival it&#x27;s nicer; Flite would run on a toaster and Mbrola can work with Espeak but the data it&#x27;s restricted for commercial usage.</div><br/></div></div></div></div><div id="40232320" class="c"><input type="checkbox" id="c-40232320" checked=""/><div class="controls bullet"><span class="by">droopyEyelids</span><span>|</span><a href="#40233103">prev</a><span>|</span><a href="#40232626">next</a><span>|</span><label class="collapse" for="c-40232320">[-]</label><label class="expand" for="c-40232320">[3 more]</label></div><br/><div class="children"><div class="content">Another project falls victim to the tragic “ng” relative naming, leaving it without options for future generations</div><br/><div id="40232391" class="c"><input type="checkbox" id="c-40232391" checked=""/><div class="controls bullet"><span class="by">nialv7</span><span>|</span><a href="#40232320">parent</a><span>|</span><a href="#40232626">next</a><span>|</span><label class="collapse" for="c-40232391">[-]</label><label class="expand" for="c-40232391">[2 more]</label></div><br/><div class="children"><div class="content">They can name the next iteration ESpeak-DS9 ;)</div><br/><div id="40232739" class="c"><input type="checkbox" id="c-40232739" checked=""/><div class="controls bullet"><span class="by">hibikir</span><span>|</span><a href="#40232320">root</a><span>|</span><a href="#40232391">parent</a><span>|</span><a href="#40232626">next</a><span>|</span><label class="collapse" for="c-40232739">[-]</label><label class="expand" for="c-40232739">[1 more]</label></div><br/><div class="children"><div class="content">I actually have seen that done at a former employer, a very large agribusiness. I bet there are more examples of that very specific, not so intended versioning system out there.</div><br/></div></div></div></div></div></div><div id="40232626" class="c"><input type="checkbox" id="c-40232626" checked=""/><div class="controls bullet"><span class="by">zambonidriver</span><span>|</span><a href="#40232320">prev</a><span>|</span><a href="#40232672">next</a><span>|</span><label class="collapse" for="c-40232626">[-]</label><label class="expand" for="c-40232626">[2 more]</label></div><br/><div class="children"><div class="content">Is it an LLM? What base model does it use?</div><br/><div id="40232790" class="c"><input type="checkbox" id="c-40232790" checked=""/><div class="controls bullet"><span class="by">celestache</span><span>|</span><a href="#40232626">parent</a><span>|</span><a href="#40232672">next</a><span>|</span><label class="collapse" for="c-40232790">[-]</label><label class="expand" for="c-40232790">[1 more]</label></div><br/><div class="children"><div class="content">eSpeak uses what is known as formant synthesis, and no LLM as far as I know.</div><br/></div></div></div></div><div id="40232672" class="c"><input type="checkbox" id="c-40232672" checked=""/><div class="controls bullet"><span class="by">webprofusion</span><span>|</span><a href="#40232626">prev</a><span>|</span><label class="collapse" for="c-40232672">[-]</label><label class="expand" for="c-40232672">[3 more]</label></div><br/><div class="children"><div class="content">&quot;More than hundred&quot;</div><br/><div id="40233105" class="c"><input type="checkbox" id="c-40233105" checked=""/><div class="controls bullet"><span class="by">Aachen</span><span>|</span><a href="#40232672">parent</a><span>|</span><label class="collapse" for="c-40233105">[-]</label><label class="expand" for="c-40233105">[2 more]</label></div><br/><div class="children"><div class="content">Fwiw, in many languages that&#x27;s correct. Coming from Dutch&#x27; &quot;meer dan honderd&quot;, being taught to say <i>one</i> hundred is like teaching an English person to say &quot;more than one ten&quot; for values &gt;10</div><br/></div></div></div></div></div></div></div></div></div></body></html>