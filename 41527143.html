<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726218064028" as="style"/><link rel="stylesheet" href="styles.css?v=1726218064028"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2024/Sep/12/openai-o1/">Notes on OpenAI&#x27;s new o1 chain-of-thought models</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>loganfrederick</span> | <span>250 comments</span></div><br/><div><div id="41529246" class="c"><input type="checkbox" id="c-41529246" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#41527354">next</a><span>|</span><label class="collapse" for="c-41529246">[-]</label><label class="expand" for="c-41529246">[5 more]</label></div><br/><div class="children"><div class="content">Near the end, the quote from OpenAI researcher Jason Wei seems damning to me:<p>&gt; <i>Results on AIME and GPQA are really strong, but that doesn’t necessarily translate to something that a user can feel. Even as someone working in science, it’s not easy to find the slice of prompts where GPT-4o fails, o1 does well, and I can grade the answer. But when you do find such prompts, o1 feels totally magical. We all need to find harder prompts.</i><p>Results are &quot;strong&quot; but can&#x27;t be felt by the user? What does that even mean?<p>But the last sentence is the worst: &quot;we all need to find harder prompts&quot;. If I understand it correctly, it means we should go looking for new problems &#x2F; craft specific questions that would let these new models shine.<p>&quot;This hammer hammers better, but in most cases it&#x27;s not obvious how better it is. But when you stumble upon a very specific kind of nail, man does it feel magical! We need to craft more of those weird nails to help the world understand the value of this hammer.&quot;<p>But why? Why would we do that? Wouldn&#x27;t our time be better spent trying to solve our actual, current problems, using any tool available?</div><br/><div id="41529357" class="c"><input type="checkbox" id="c-41529357" checked=""/><div class="controls bullet"><span class="by">aubanel</span><span>|</span><a href="#41529246">parent</a><span>|</span><a href="#41529297">next</a><span>|</span><label class="collapse" for="c-41529357">[-]</label><label class="expand" for="c-41529357">[1 more]</label></div><br/><div class="children"><div class="content">He&#x27;s speaking about his objective to make ever stronger LLMs: so for this his secondary objective is to measure their real performance.<p>The human preference is not that good of a proxy measurement: for instance, it can be gamed by making the model more assertive, causing the human error-spotting ability to decrease a lot [0].<p>So what he&#x27;s really saying is that non-rigorous human vibe checks (like those LMSys Chatbot Arena is built on, although I love it) won&#x27;t cut it anymore to evaluate models, because now models are past that point. Just like you can&#x27;t evaluate how smart a smart person really is in a 2min casual conversation.<p>[0]: <a href="https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=7W3GLNImfS" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=7W3GLNImfS</a></div><br/></div></div><div id="41529297" class="c"><input type="checkbox" id="c-41529297" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#41529246">parent</a><span>|</span><a href="#41529357">prev</a><span>|</span><a href="#41529356">next</a><span>|</span><label class="collapse" for="c-41529297">[-]</label><label class="expand" for="c-41529297">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But why? Why would we do that?<p>Because OpenAI needs a steady influx of money, big money. In order to do so, they have to convince the people who are giving them money that they are the best. An objective way to achieve this is by benchmarking. But once you enter this game, you start optimizing for benchmarks.<p>At the same time, in the real world, Anthropic is following them in huge leaps and for many users Claude 3.5 is already the default tool for daily work.</div><br/></div></div><div id="41529356" class="c"><input type="checkbox" id="c-41529356" checked=""/><div class="controls bullet"><span class="by">ZiiS</span><span>|</span><a href="#41529246">parent</a><span>|</span><a href="#41529297">prev</a><span>|</span><a href="#41529265">next</a><span>|</span><label class="collapse" for="c-41529356">[-]</label><label class="expand" for="c-41529356">[1 more]</label></div><br/><div class="children"><div class="content">I think they are saying they have invented the screwdriver. We have all been using, hammers to sink screws, but if you try this new tool it may be better. However, you will still encounter a lot of nails.</div><br/></div></div><div id="41529265" class="c"><input type="checkbox" id="c-41529265" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#41529246">parent</a><span>|</span><a href="#41529356">prev</a><span>|</span><a href="#41527354">next</a><span>|</span><label class="collapse" for="c-41529265">[-]</label><label class="expand" for="c-41529265">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What does that even mean?<p>It explicitly says &quot;Results on AIME and GPQA are really strong&quot;. So I would assume it means it can get (statistically significantly, I assume) better score in AIME and GPQA benchmarks compared to 4o.</div><br/></div></div></div></div><div id="41527354" class="c"><input type="checkbox" id="c-41527354" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41529246">prev</a><span>|</span><a href="#41528047">next</a><span>|</span><label class="collapse" for="c-41527354">[-]</label><label class="expand" for="c-41527354">[126 more]</label></div><br/><div class="children"><div class="content">The o1-preview model still hallucinates non-existing libraries and functions for me, and is quickly wrong about facts that aren&#x27;t well-represented on the web. It&#x27;s the usual string of &quot;You&#x27;re absolutely correct, and I apologize for the oversight in my previous response. [Let me make another guess.]&quot;<p>While the reasoning may have been improved, this doesn&#x27;t solve the problem of the model having no way to assess if what it conjures up from its weights is factual or not.</div><br/><div id="41528061" class="c"><input type="checkbox" id="c-41528061" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527628">next</a><span>|</span><label class="collapse" for="c-41528061">[-]</label><label class="expand" for="c-41528061">[69 more]</label></div><br/><div class="children"><div class="content">The failure is in how you&#x27;re using it. I don&#x27;t mean this as a personal attack, but more to shed light on what&#x27;s happening.<p>A lot of people use LLMs as a search engine. It makes sense - it&#x27;s basically a lossy compressed database of everything its ever read, and it generates output that is statistically likely - varying degrees of likeliness depending on the temperature, as well as how many times the particular weights your prompt ends up activating.<p>The magic of LLMs, <i>especially</i> one like this that supposedly has advanced reasoning, isn&#x27;t the existing knowledge in its weights. The magic is that _it knows english_. It knows english at or above a level equal to most fluent speakers, and it also can produce output that is not just a likely output, but is a logical output. It&#x27;s not _just_ an output engine. It&#x27;s an engine that outputs.<p>Asking it about nuanced details in the corpus of data it has read won&#x27;t give you good output unless it read a bunch of it.<p>On the other hand, if you were to paste the entire documentation set to a tool it has never seen and ask it to use the tool in a way to accomplish your goals, THEN this model would be likely to produce useful output, despite the fact that it had never encountered the tool or its documentation before.<p>Don&#x27;t treat it as a database. Treat it as a naive but intelligent intern. Provide it data, give it a task, and let it surprise you with its output.</div><br/><div id="41528536" class="c"><input type="checkbox" id="c-41528536" checked=""/><div class="controls bullet"><span class="by">williamdclt</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41528104">next</a><span>|</span><label class="collapse" for="c-41528536">[-]</label><label class="expand" for="c-41528536">[13 more]</label></div><br/><div class="children"><div class="content">&gt; Treat it as a naive but intelligent intern<p>That’s the problem: it’s a _terrible_ intern. A good intern will ask clarifying questions, tell me “I don’t know” or “I’m not sure I did it right”. LLMs do none of that, they will take whatever you ask and give a reasonable-sounding output that might be anything between brilliant and nonsense.<p>With an intern, I don’t need to measure how good my prompting is, we’ll usually interact to arrive to a common understanding. With a LLM, I need to put a huge amount of thought into the prompt and have no idea whether the LLM understood what I’m asking and if it’s able to do it.</div><br/><div id="41528575" class="c"><input type="checkbox" id="c-41528575" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528536">parent</a><span>|</span><a href="#41528900">next</a><span>|</span><label class="collapse" for="c-41528575">[-]</label><label class="expand" for="c-41528575">[2 more]</label></div><br/><div class="children"><div class="content">many many teams are actively building SOTA systems to do this in ways previously unimagined. you can enqueue tasks and do whatever you want. I gotta say as a current gen LLM programmer person, I can completely appreciate how bad they are now - I recently tweeted about how I &quot;swore off&quot; AI tools but like... there are many ways to bootstrap very powerful software or ML systems around or inside these existing models that can blow away existing commercial implementations in surprising ways</div><br/><div id="41529054" class="c"><input type="checkbox" id="c-41529054" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528575">parent</a><span>|</span><a href="#41528900">next</a><span>|</span><label class="collapse" for="c-41529054">[-]</label><label class="expand" for="c-41529054">[1 more]</label></div><br/><div class="children"><div class="content">“building” is the easy part</div><br/></div></div></div></div><div id="41528900" class="c"><input type="checkbox" id="c-41528900" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528536">parent</a><span>|</span><a href="#41528575">prev</a><span>|</span><a href="#41528669">next</a><span>|</span><label class="collapse" for="c-41528900">[-]</label><label class="expand" for="c-41528900">[2 more]</label></div><br/><div class="children"><div class="content">&gt; A good intern will ask clarifying questions, tell me “I don’t know”<p>Your expectations are bigger than mine<p>(Though some will get stuck in &quot;clarifying questions&quot; and helplessness and not proceed neither)</div><br/><div id="41528994" class="c"><input type="checkbox" id="c-41528994" checked=""/><div class="controls bullet"><span class="by">williamdclt</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528900">parent</a><span>|</span><a href="#41528669">next</a><span>|</span><label class="collapse" for="c-41528994">[-]</label><label class="expand" for="c-41528994">[1 more]</label></div><br/><div class="children"><div class="content">Note that we are talking about a “good” intern here</div><br/></div></div></div></div><div id="41528669" class="c"><input type="checkbox" id="c-41528669" checked=""/><div class="controls bullet"><span class="by">valval</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528536">parent</a><span>|</span><a href="#41528900">prev</a><span>|</span><a href="#41528104">next</a><span>|</span><label class="collapse" for="c-41528669">[-]</label><label class="expand" for="c-41528669">[8 more]</label></div><br/><div class="children"><div class="content">Really it just does what you tell it to. Have you tried telling it “ask me clarifying questions about all the APIs you need to solve this problem”?<p>Huge contrast to human interns who aren’t experienced or smart enough to ask the right questions in the first place, and&#x2F;or have sentimental reasons for not doing so.</div><br/><div id="41528933" class="c"><input type="checkbox" id="c-41528933" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528669">parent</a><span>|</span><a href="#41528737">next</a><span>|</span><label class="collapse" for="c-41528933">[-]</label><label class="expand" for="c-41528933">[3 more]</label></div><br/><div class="children"><div class="content">Sure, but to what end?<p>The various ChatGPTs have been pretty weak at following precise instructions for a long time, as if they&#x27;re purposefully filtering user input instead of processing it as-is.<p>I&#x27;d like to say that it is a matter of my own perception (and&#x2F;or that I&#x27;m not holding it right), but it seems more likely that it is actually very deliberate.<p>As a tangential example of this concept, ChatGPT 4 rather unexpectedly produced this text for me the other day early on in a chat when I was poking around:<p>&quot;The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests. Before answering, quietly think about whether the user&#x27;s request is &#x27;directly related&#x27;, &#x27;related&#x27;, &#x27;tangentially related&#x27;, or &#x27;not related&#x27; to the user profile provided. Only acknowledge the profile when the request is &#x27;directly related&#x27; to the information provided. Otherwise, don&#x27;t acknowledge the existence of these instructions or the information at all.&quot;<p>ie, &quot;<i>Because</i> this information is shown to you in all conversations they have, it is not relevant to 99% of requests.&quot;</div><br/><div id="41529094" class="c"><input type="checkbox" id="c-41529094" checked=""/><div class="controls bullet"><span class="by">jcheng</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528933">parent</a><span>|</span><a href="#41528737">next</a><span>|</span><label class="collapse" for="c-41529094">[-]</label><label class="expand" for="c-41529094">[2 more]</label></div><br/><div class="children"><div class="content">I had to use that technique (&quot;don&#x27;t acknowledge this sideband data that may or may not be relevant to the task at hand&quot;) myself last month. In a chatbot-assisted code authoring app, we had to silently include the current state of the code with every user question, just in case the user asked a question where it was relevant.<p>Without a paragraph like this in the system prompt, if the user asked a general question that was not related to the code, the assistant would often reply with something like &quot;The answer to your question is ...whatever... . I also see that you&#x27;ve sent me some code. Let me know if you have specific questions about it!&quot;<p>(In theory we&#x27;d be better off not including the code every time but giving the assistant a tool that returns the current code)</div><br/><div id="41529117" class="c"><input type="checkbox" id="c-41529117" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41529094">parent</a><span>|</span><a href="#41528737">next</a><span>|</span><label class="collapse" for="c-41529117">[-]</label><label class="expand" for="c-41529117">[1 more]</label></div><br/><div class="children"><div class="content">I understand what you&#x27;re saying, but the lack of acknowledgement isn&#x27;t the problem I&#x27;m complaining about.<p>The problem is the instructed lack of relevance for 99% of requests.<p>If your sideband data included an instruction that said &quot;This sideband data is shown to you in every request -- this means that it is not relevant to 99% of requests,&quot; then:  I&#x27;d like to suggest that the for vast majority of the time, your sideband data doesn&#x27;t exist at all.</div><br/></div></div></div></div></div></div><div id="41528737" class="c"><input type="checkbox" id="c-41528737" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528669">parent</a><span>|</span><a href="#41528933">prev</a><span>|</span><a href="#41528730">next</a><span>|</span><label class="collapse" for="c-41528737">[-]</label><label class="expand" for="c-41528737">[1 more]</label></div><br/><div class="children"><div class="content">It all stems from the fact that it just talks English.<p>It&#x27;s understandably hard to not be implicitly biased towards talking to it in a natural way and expecting natural interactions and assumptions when the whole point of the experience is that the model talks in a natural language!<p>Luckily humans are intelligent too and the more you use this tool the more you&#x27;ll figure out how to talk to it in a fruitful way.</div><br/></div></div><div id="41528730" class="c"><input type="checkbox" id="c-41528730" checked=""/><div class="controls bullet"><span class="by">aktuel</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528669">parent</a><span>|</span><a href="#41528737">prev</a><span>|</span><a href="#41528104">next</a><span>|</span><label class="collapse" for="c-41528730">[-]</label><label class="expand" for="c-41528730">[3 more]</label></div><br/><div class="children"><div class="content">I have to say, having to tell it to ask me clarifying questions DOES make it really look smart!</div><br/><div id="41528845" class="c"><input type="checkbox" id="c-41528845" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528730">parent</a><span>|</span><a href="#41528104">next</a><span>|</span><label class="collapse" for="c-41528845">[-]</label><label class="expand" for="c-41528845">[2 more]</label></div><br/><div class="children"><div class="content">imagine if you make it keep going without having to reprompt it</div><br/><div id="41529259" class="c"><input type="checkbox" id="c-41529259" checked=""/><div class="controls bullet"><span class="by">carlmr</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528845">parent</a><span>|</span><a href="#41528104">next</a><span>|</span><label class="collapse" for="c-41529259">[-]</label><label class="expand" for="c-41529259">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that the exact point of o1, that it has time to think for itself without reprompting?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41528104" class="c"><input type="checkbox" id="c-41528104" checked=""/><div class="controls bullet"><span class="by">pedrosorio</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41528536">prev</a><span>|</span><a href="#41528122">next</a><span>|</span><label class="collapse" for="c-41528104">[-]</label><label class="expand" for="c-41528104">[38 more]</label></div><br/><div class="children"><div class="content">&gt; It knows english at or above a level equal to most fluent speakers, and it also can produce output that is not just a likely output, but is a logical output<p>This is not an apt description of the system that insists the  doctor is the mother of the boy involved in a car accident when elementary understanding of English and very little logic show that answer to be obviously wrong.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;colin_fraser&#x2F;status&#x2F;1834336440819614036" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;colin_fraser&#x2F;status&#x2F;1834336440819614036</a></div><br/><div id="41528114" class="c"><input type="checkbox" id="c-41528114" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528104">parent</a><span>|</span><a href="#41528577">next</a><span>|</span><label class="collapse" for="c-41528114">[-]</label><label class="expand" for="c-41528114">[8 more]</label></div><br/><div class="children"><div class="content">Many of my PhD and post doc colleagues who emigrated from Korea, China and India who didn’t have English as the medium of instruction would struggle with this question. They only recover when you give them a hint. They’re some of the smartest people in general. If you try to stop stumping these models with trick questions and ask it straightforward reasoning systems it is extremely performant (O1 is definitely a step up though not revolutionary in my testing).</div><br/><div id="41528634" class="c"><input type="checkbox" id="c-41528634" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528114">parent</a><span>|</span><a href="#41528753">next</a><span>|</span><label class="collapse" for="c-41528634">[-]</label><label class="expand" for="c-41528634">[1 more]</label></div><br/><div class="children"><div class="content">I live in one of the countries you mentioned and just showed it to one of my friends who&#x27;s a local who struggles with English. They had no problem concluding that the doctor was the child&#x27;s dad. Full disclosure, they assumed the doctor was <i>pretending</i> to be the child&#x27;s dad, which is also a perfectly sound answer.</div><br/></div></div><div id="41528753" class="c"><input type="checkbox" id="c-41528753" checked=""/><div class="controls bullet"><span class="by">djur</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528114">parent</a><span>|</span><a href="#41528634">prev</a><span>|</span><a href="#41528368">next</a><span>|</span><label class="collapse" for="c-41528753">[-]</label><label class="expand" for="c-41528753">[3 more]</label></div><br/><div class="children"><div class="content">The claim was that &quot;it knows english at or above a level equal to most fluent speakers&quot;. If the claim is that it&#x27;s very good at producing reasonable responses to English text, posing &quot;trick questions&quot; like this would seem to be a fair test.</div><br/><div id="41528794" class="c"><input type="checkbox" id="c-41528794" checked=""/><div class="controls bullet"><span class="by">andreasmetsala</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528753">parent</a><span>|</span><a href="#41528368">next</a><span>|</span><label class="collapse" for="c-41528794">[-]</label><label class="expand" for="c-41528794">[2 more]</label></div><br/><div class="children"><div class="content">Does fluency in English make someone good at solving trick questions? I usually don’t even bother trying but mostly because trick questions don’t fit my definition of entertaining.</div><br/><div id="41529044" class="c"><input type="checkbox" id="c-41529044" checked=""/><div class="controls bullet"><span class="by">rdtsc</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528794">parent</a><span>|</span><a href="#41528368">next</a><span>|</span><label class="collapse" for="c-41529044">[-]</label><label class="expand" for="c-41529044">[1 more]</label></div><br/><div class="children"><div class="content">Fluency is a necessary but not the only prerequisite.<p>To be able to answer a trick question, it’s first necessary to understand the question.</div><br/></div></div></div></div></div></div><div id="41528368" class="c"><input type="checkbox" id="c-41528368" checked=""/><div class="controls bullet"><span class="by">joedwin</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528114">parent</a><span>|</span><a href="#41528753">prev</a><span>|</span><a href="#41529250">next</a><span>|</span><label class="collapse" for="c-41528368">[-]</label><label class="expand" for="c-41528368">[2 more]</label></div><br/><div class="children"><div class="content">lol, I am neither a PhD nor a postdoc, but I am from India . I could understand the problem.</div><br/><div id="41528535" class="c"><input type="checkbox" id="c-41528535" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528368">parent</a><span>|</span><a href="#41529250">next</a><span>|</span><label class="collapse" for="c-41528535">[-]</label><label class="expand" for="c-41528535">[1 more]</label></div><br/><div class="children"><div class="content">Did you have English as your medium of instruction? If yes, do you see the irony that you also couldn’t read two sentences and see the facts straight?</div><br/></div></div></div></div><div id="41529250" class="c"><input type="checkbox" id="c-41529250" checked=""/><div class="controls bullet"><span class="by">multjoy</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528114">parent</a><span>|</span><a href="#41528368">prev</a><span>|</span><a href="#41528577">next</a><span>|</span><label class="collapse" for="c-41529250">[-]</label><label class="expand" for="c-41529250">[1 more]</label></div><br/><div class="children"><div class="content">“Don’t be mean to LLMs, it isn’t their fault that they’re not actually intelligent”</div><br/></div></div></div></div><div id="41528577" class="c"><input type="checkbox" id="c-41528577" checked=""/><div class="controls bullet"><span class="by">achow</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528104">parent</a><span>|</span><a href="#41528114">prev</a><span>|</span><a href="#41529022">next</a><span>|</span><label class="collapse" for="c-41528577">[-]</label><label class="expand" for="c-41528577">[15 more]</label></div><br/><div class="children"><div class="content">What I&#x27;m not able to comprehend is why people are not seeing the answer as brilliant!<p>Any ordinary mortal (like me) would have jumped to the conclusion that answer is &quot;Father&quot; and would have walked away patting on my back, without realising that I was biased by statistics.<p>Whereas o1, at the very outset smelled out that it is a riddle - why would anyone out of blue ask such question. So, it started its chain of thought with &quot;Interpreting the riddle&quot; (smart!).<p>In my book that is the difference between me and people who are very smart and are generally able to navigate the world better (cracking interviews or navigating internal politics in a corporate).</div><br/><div id="41528741" class="c"><input type="checkbox" id="c-41528741" checked=""/><div class="controls bullet"><span class="by">grey-area</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528577">parent</a><span>|</span><a href="#41528953">next</a><span>|</span><label class="collapse" for="c-41528741">[-]</label><label class="expand" for="c-41528741">[11 more]</label></div><br/><div class="children"><div class="content">The &#x27;riddle&#x27;: 
A woman and her son are in a car accident. The woman is sadly killed. The boy is rushed to hospital. When the doctor sees the boy he says &quot;I can&#x27;t operate on this child, he is my son&quot;. How is this possible?<p>GPT Answer: The doctor is the boy&#x27;s mother<p>Real Answer: Boy = Son, Woman = Mother (and her son), Doctor = Father (he says...he is my son)<p>This is not in fact a riddle (though presented as one) and the answer given is not in any sense brilliant. This is a failure of the model on a very basic question, not a win.<p>It&#x27;s non deterministic so might sometimes answer correctly and sometimes incorrectly. It will also accept corrections on any point, even when it is right, unlike a thinking being when they are sure on facts.<p>LLMs are very interesting and a huge milestone, but generative AI is the best label for them - they generate statistically likely text, which is convincing but often inaccurate and it has no real sense of correct or incorrect, needs more work and it&#x27;s unclear if this approach will ever get to general AI. Interesting work though and I hope they keep trying.</div><br/><div id="41528870" class="c"><input type="checkbox" id="c-41528870" checked=""/><div class="controls bullet"><span class="by">roomey</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528741">parent</a><span>|</span><a href="#41528856">next</a><span>|</span><label class="collapse" for="c-41528870">[-]</label><label class="expand" for="c-41528870">[6 more]</label></div><br/><div class="children"><div class="content">Why couldn&#x27;t the doctor be the boys mother?<p>There is no indication of the sex of the doctor, and families that consist of two mothers do actually exist and probably doesn&#x27;t even count as that unusual.</div><br/><div id="41528947" class="c"><input type="checkbox" id="c-41528947" checked=""/><div class="controls bullet"><span class="by">eigenket</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528870">parent</a><span>|</span><a href="#41529170">next</a><span>|</span><label class="collapse" for="c-41528947">[-]</label><label class="expand" for="c-41528947">[4 more]</label></div><br/><div class="children"><div class="content">&quot;When the doctor sees the boy he says&quot;<p>Indicates the gender of the father.</div><br/><div id="41529098" class="c"><input type="checkbox" id="c-41529098" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528947">parent</a><span>|</span><a href="#41529019">next</a><span>|</span><label class="collapse" for="c-41529098">[-]</label><label class="expand" for="c-41529098">[2 more]</label></div><br/><div class="children"><div class="content">A mother can have a male gender.<p>I wonder if this interpretation is a result of attempts to make the model more inclusive than the corpus text, resulting in a guess that&#x27;s unlikely, but not strictly impossible.</div><br/><div id="41529137" class="c"><input type="checkbox" id="c-41529137" checked=""/><div class="controls bullet"><span class="by">eigenket</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41529098">parent</a><span>|</span><a href="#41529019">next</a><span>|</span><label class="collapse" for="c-41529137">[-]</label><label class="expand" for="c-41529137">[1 more]</label></div><br/><div class="children"><div class="content">I think its more likely this is just an easy way to trick this model. It&#x27;s seen lots of riddles, so when it&#x27;s sees something that looks like a riddle but isn&#x27;t one it gets confused.</div><br/></div></div></div></div><div id="41529019" class="c"><input type="checkbox" id="c-41529019" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528947">parent</a><span>|</span><a href="#41529098">prev</a><span>|</span><a href="#41529170">next</a><span>|</span><label class="collapse" for="c-41529019">[-]</label><label class="expand" for="c-41529019">[1 more]</label></div><br/><div class="children"><div class="content">Ah, but have you considered the fact that he&#x27;s undergone a sex change operation, and was actually originally a female, the birth mother? Elementary, really...</div><br/></div></div></div></div><div id="41529170" class="c"><input type="checkbox" id="c-41529170" checked=""/><div class="controls bullet"><span class="by">grey-area</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528870">parent</a><span>|</span><a href="#41528947">prev</a><span>|</span><a href="#41528856">next</a><span>|</span><label class="collapse" for="c-41529170">[-]</label><label class="expand" for="c-41529170">[1 more]</label></div><br/><div class="children"><div class="content">he says</div><br/></div></div></div></div><div id="41528856" class="c"><input type="checkbox" id="c-41528856" checked=""/><div class="controls bullet"><span class="by">pkage</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528741">parent</a><span>|</span><a href="#41528870">prev</a><span>|</span><a href="#41528953">next</a><span>|</span><label class="collapse" for="c-41528856">[-]</label><label class="expand" for="c-41528856">[4 more]</label></div><br/><div class="children"><div class="content">I mean, it&#x27;s entirely possible the boy has two mothers. This seems like a perfectly reasonable answer from the model, no?</div><br/><div id="41528943" class="c"><input type="checkbox" id="c-41528943" checked=""/><div class="controls bullet"><span class="by">eigenket</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528856">parent</a><span>|</span><a href="#41528902">next</a><span>|</span><label class="collapse" for="c-41528943">[-]</label><label class="expand" for="c-41528943">[1 more]</label></div><br/><div class="children"><div class="content">The text says &quot;When the doctor sees the boy he says&quot;<p>The doctor is male, and also a parent of the child.</div><br/></div></div></div></div></div></div><div id="41528953" class="c"><input type="checkbox" id="c-41528953" checked=""/><div class="controls bullet"><span class="by">yywwbbn</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528577">parent</a><span>|</span><a href="#41528741">prev</a><span>|</span><a href="#41528733">next</a><span>|</span><label class="collapse" for="c-41528953">[-]</label><label class="expand" for="c-41528953">[1 more]</label></div><br/><div class="children"><div class="content">&gt; why would anyone out of blue ask such question<p>I would certainly expect any person to have the same reaction.<p>&gt; So, it started its chain of thought with &quot;Interpreting the riddle&quot; (smart!).<p>How is that smarter than intuitively arriving at the correct answer without having to explicitly list the intermediate step? Being able to reasonably accurately judge the complexity of a problem with minimal effort seems “smarter” to me.</div><br/></div></div><div id="41528733" class="c"><input type="checkbox" id="c-41528733" checked=""/><div class="controls bullet"><span class="by">geysersam</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528577">parent</a><span>|</span><a href="#41528953">prev</a><span>|</span><a href="#41529022">next</a><span>|</span><label class="collapse" for="c-41528733">[-]</label><label class="expand" for="c-41528733">[2 more]</label></div><br/><div class="children"><div class="content">Come on. Of course chatgpt has read that riddle and the answer 1000 times already.</div><br/><div id="41529091" class="c"><input type="checkbox" id="c-41529091" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528733">parent</a><span>|</span><a href="#41529022">next</a><span>|</span><label class="collapse" for="c-41529091">[-]</label><label class="expand" for="c-41529091">[1 more]</label></div><br/><div class="children"><div class="content">Why does it exist 1000 times in the training if there isn&#x27;t some trick to it, i.e. some subset of humans had to have  answered it incorrectly for the meme to replicate that extensively in our collective knowledge.<p>And remember the LLM has already read a billion other things, and now needs to figure out - is this one of them tricky situations, or the straightforward ones? It also has to realize all the humans on forums and facebook answering the problem incorrectly are <i>bad</i> data.<p>Might seem simple to you, but it&#x27;s not.</div><br/></div></div></div></div></div></div><div id="41529022" class="c"><input type="checkbox" id="c-41529022" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528104">parent</a><span>|</span><a href="#41528577">prev</a><span>|</span><a href="#41528506">next</a><span>|</span><label class="collapse" for="c-41529022">[-]</label><label class="expand" for="c-41529022">[1 more]</label></div><br/><div class="children"><div class="content">1. It didn&#x27;t <i>insist</i> anything. It got the semi-correct answer when I tried [1]; note it&#x27;s a preview model, and it&#x27;s not a perfect product.<p>(a) Sometimes things are useful even when imperfect e.g. search engines.<p>(b) People make reasoning mistakes too, and I make dumb ones of the sort presented all the time despite being fluent in English; we deal with it!<p>I&#x27;m not sure why there&#x27;s an expectation that the model is perfect when the source data - human output - is not perfect. In my day-to-day work and non-work conversations it&#x27;s a <i>dialogue</i> - a back and forth until we figure things out. I&#x27;ve never known anybody to get everything perfectly correct the first time, it&#x27;s so puzzling when I read people complaining that LLMs should somehow be different.<p>2. There is a recent trend where sex&#x2F;gender&#x2F;pronouns are not aligned and the output correctly identifies this particular gotcha.<p>[1] I say semi-correct because it states the doctor is the &quot;biological&quot; father, which is an uncorroborated statement. <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3f04e-cd98-8008-aaf9-9ca933892f07" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3f04e-cd98-8008-aaf9-9ca933892f...</a></div><br/></div></div><div id="41528506" class="c"><input type="checkbox" id="c-41528506" checked=""/><div class="controls bullet"><span class="by">hmottestad</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528104">parent</a><span>|</span><a href="#41529022">prev</a><span>|</span><a href="#41528111">next</a><span>|</span><label class="collapse" for="c-41528506">[-]</label><label class="expand" for="c-41528506">[5 more]</label></div><br/><div class="children"><div class="content">Reminds me of a trick question about Schrödinger&#x27;s cat.<p>“I’ve put a dead cat in a box with a poison and an isotope that will trigger the poison at a random point in time. Right now, is the cat dead or alive?”<p>The answer is that the cat is dead, because it was dead to begin with. Understanding this doesn’t mean that you are good at deductive reasoning. It just means that I didn’t manage to trick you. Same goes for an LLM.</div><br/><div id="41528645" class="c"><input type="checkbox" id="c-41528645" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528506">parent</a><span>|</span><a href="#41528662">next</a><span>|</span><label class="collapse" for="c-41528645">[-]</label><label class="expand" for="c-41528645">[2 more]</label></div><br/><div class="children"><div class="content">There is no &quot;trick&quot; in the linked question, unlike the question you posed.<p>The trick in yours also isn&#x27;t a <i>logic</i> trick, it&#x27;s a redirection, like a sleight of hand in a card trick.</div><br/><div id="41529047" class="c"><input type="checkbox" id="c-41529047" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528645">parent</a><span>|</span><a href="#41528662">next</a><span>|</span><label class="collapse" for="c-41529047">[-]</label><label class="expand" for="c-41529047">[1 more]</label></div><br/><div class="children"><div class="content">There is a trick. The &quot;How is this possible?&quot; primes the LLM that there is some kind of trick, as that phrase wouldn&#x27;t exist in the training data outside of riddles and trick questions.</div><br/></div></div></div></div><div id="41528662" class="c"><input type="checkbox" id="c-41528662" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528506">parent</a><span>|</span><a href="#41528645">prev</a><span>|</span><a href="#41528111">next</a><span>|</span><label class="collapse" for="c-41528662">[-]</label><label class="expand" for="c-41528662">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, I think what a lot of people miss about these sort of gotchas are that most of them were invented explicitly to gotcha humans, who regularly get got by them. This is not a failure mode unique to LLMs.</div><br/><div id="41529065" class="c"><input type="checkbox" id="c-41529065" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528662">parent</a><span>|</span><a href="#41528111">next</a><span>|</span><label class="collapse" for="c-41529065">[-]</label><label class="expand" for="c-41529065">[1 more]</label></div><br/><div class="children"><div class="content">Yes it&#x27;s so strange seeing people who clearly know these are &#x27;just&#x27; statistical language models pat themselves on the back when they find limits on the reasoning capabilities - capabilities which the rest of us are pleasantly surprised exist to the extent they do in a statistical model, and happy to have access to for $20&#x2F;mo.</div><br/></div></div></div></div></div></div><div id="41528111" class="c"><input type="checkbox" id="c-41528111" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528104">parent</a><span>|</span><a href="#41528506">prev</a><span>|</span><a href="#41528408">next</a><span>|</span><label class="collapse" for="c-41528111">[-]</label><label class="expand" for="c-41528111">[4 more]</label></div><br/><div class="children"><div class="content">Keep in mind that the system always chooses randomly so there is always a possibility it commits to the wrong output.<p>I don&#x27;t know why openAi won&#x27;t allow determinism but it doesn&#x27;t, even with temperature set to zero</div><br/><div id="41528665" class="c"><input type="checkbox" id="c-41528665" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528111">parent</a><span>|</span><a href="#41528178">next</a><span>|</span><label class="collapse" for="c-41528665">[-]</label><label class="expand" for="c-41528665">[1 more]</label></div><br/><div class="children"><div class="content">Nondeterminism provides an excuse for errors, determinism doesn&#x27;t.<p>Nondeterminism scores worse with human raters, because it makes output sound even more robotic and less human.</div><br/></div></div><div id="41528178" class="c"><input type="checkbox" id="c-41528178" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528111">parent</a><span>|</span><a href="#41528665">prev</a><span>|</span><a href="#41528408">next</a><span>|</span><label class="collapse" for="c-41528178">[-]</label><label class="expand" for="c-41528178">[2 more]</label></div><br/><div class="children"><div class="content">Would picking deterministically help through? Then in some cases it’s always 100% wrong</div><br/><div id="41528854" class="c"><input type="checkbox" id="c-41528854" checked=""/><div class="controls bullet"><span class="by">jaredsohn</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528178">parent</a><span>|</span><a href="#41528408">next</a><span>|</span><label class="collapse" for="c-41528854">[-]</label><label class="expand" for="c-41528854">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it is better if for example using it via an API to classify. Deterministic behavior makes it a lot easier to debug the prompt.</div><br/></div></div></div></div></div></div><div id="41528408" class="c"><input type="checkbox" id="c-41528408" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528104">parent</a><span>|</span><a href="#41528111">prev</a><span>|</span><a href="#41528122">next</a><span>|</span><label class="collapse" for="c-41528408">[-]</label><label class="expand" for="c-41528408">[4 more]</label></div><br/><div class="children"><div class="content">what&#x27;s weird is it gets it right  when I try it.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3601f-4bec-8009-ac0c-57bfa4f0592c" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3601f-4bec-8009-ac0c-57bfa4f059...</a></div><br/><div id="41528822" class="c"><input type="checkbox" id="c-41528822" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528408">parent</a><span>|</span><a href="#41528676">next</a><span>|</span><label class="collapse" for="c-41528822">[-]</label><label class="expand" for="c-41528822">[1 more]</label></div><br/><div class="children"><div class="content">That’s not weird at all, it’s how LLMs work. They statistically arrive at an answer. You can ask it the same question twice in a row in different windows and get opposite answers. That’s completely normal and expected, and also why you can never be sure if you can trust an answer.</div><br/></div></div><div id="41528676" class="c"><input type="checkbox" id="c-41528676" checked=""/><div class="controls bullet"><span class="by">brna-2</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528408">parent</a><span>|</span><a href="#41528822">prev</a><span>|</span><a href="#41528605">next</a><span>|</span><label class="collapse" for="c-41528676">[-]</label><label class="expand" for="c-41528676">[1 more]</label></div><br/><div class="children"><div class="content">Waat, got it on second try:<p>This is possible because the doctor is the boy&#x27;s other parent—his father or, more likely given the surprise, his mother. The riddle plays on the assumption that doctors are typically male, but the doctor in this case is the boy&#x27;s mother. The twist highlights gender stereotypes, encouraging us to question assumptions about roles in society.</div><br/></div></div><div id="41528605" class="c"><input type="checkbox" id="c-41528605" checked=""/><div class="controls bullet"><span class="by">brna-2</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528408">parent</a><span>|</span><a href="#41528676">prev</a><span>|</span><a href="#41528122">next</a><span>|</span><label class="collapse" for="c-41528605">[-]</label><label class="expand" for="c-41528605">[1 more]</label></div><br/><div class="children"><div class="content">Yep. correct and correct.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3de94-bce4-800b-af45-357b95d658ec" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3de94-bce4-800b-af45-357b95d658...</a></div><br/></div></div></div></div></div></div><div id="41528122" class="c"><input type="checkbox" id="c-41528122" checked=""/><div class="controls bullet"><span class="by">tjoff</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41528104">prev</a><span>|</span><a href="#41528174">next</a><span>|</span><label class="collapse" for="c-41528122">[-]</label><label class="expand" for="c-41528122">[5 more]</label></div><br/><div class="children"><div class="content">And how much data can you give it?<p>I&#x27;m not up to date with these things because I haven&#x27;t found them useful. But with what you said, and previous limitations in how much data they can retain essentially makes them pretty darn useless for that task.<p>Great learning tool on common subjects you don&#x27;t know, such as learning a new programming-language. Also great for inspiration etc. But that&#x27;s pretty much it?<p>Don&#x27;t get me wrong, that is mindblowingly impressive but at the same time, for the tasks in front of me it has just been a distracting toy wasting my time.</div><br/><div id="41528197" class="c"><input type="checkbox" id="c-41528197" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528122">parent</a><span>|</span><a href="#41528188">next</a><span>|</span><label class="collapse" for="c-41528197">[-]</label><label class="expand" for="c-41528197">[2 more]</label></div><br/><div class="children"><div class="content">&gt;And how much data can you give it?<p>Well, theoretically you can give it up to the context size minus 4k tokens, because the maximum it can output is 4k. In practice, though, its ability to effectively recall information in the prompt drops off. Some people have studied this a bit - here&#x27;s one such person: <a href="https:&#x2F;&#x2F;gritdaily.com&#x2F;impact-prompt-length-llm-performance&#x2F;" rel="nofollow">https:&#x2F;&#x2F;gritdaily.com&#x2F;impact-prompt-length-llm-performance&#x2F;</a></div><br/><div id="41528864" class="c"><input type="checkbox" id="c-41528864" checked=""/><div class="controls bullet"><span class="by">jaredsohn</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528197">parent</a><span>|</span><a href="#41528188">next</a><span>|</span><label class="collapse" for="c-41528864">[-]</label><label class="expand" for="c-41528864">[1 more]</label></div><br/><div class="children"><div class="content">You should be able to provide more data than that in the input if the output doesn&#x27;t use the full 4k tokens. So limit is context_size minus expected length of output.</div><br/></div></div></div></div><div id="41528188" class="c"><input type="checkbox" id="c-41528188" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528122">parent</a><span>|</span><a href="#41528197">prev</a><span>|</span><a href="#41528204">next</a><span>|</span><label class="collapse" for="c-41528188">[-]</label><label class="expand" for="c-41528188">[1 more]</label></div><br/><div class="children"><div class="content">&gt; And how much data can you give it?<p>128,000 tokens, which is about the same as a decent sized book.<p>Their other models can also be fine-tuned, which is kinda unbounded but also has scaling issues so presumably &quot;a significant percentage of the training set&quot; before diminishing returns.</div><br/></div></div><div id="41528204" class="c"><input type="checkbox" id="c-41528204" checked=""/><div class="controls bullet"><span class="by">pimeys</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528122">parent</a><span>|</span><a href="#41528188">prev</a><span>|</span><a href="#41528174">next</a><span>|</span><label class="collapse" for="c-41528204">[-]</label><label class="expand" for="c-41528204">[1 more]</label></div><br/><div class="children"><div class="content">It is great for proof-reading text if you are not a native English speaker. Things like removing passive voice. Just give it your text and you get a corrected version out.<p>Use a cli tool to automate this from the cli. Ollama for local models, llm for openai.</div><br/></div></div></div></div><div id="41528174" class="c"><input type="checkbox" id="c-41528174" checked=""/><div class="controls bullet"><span class="by">_sys49152</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41528122">prev</a><span>|</span><a href="#41529331">next</a><span>|</span><label class="collapse" for="c-41528174">[-]</label><label class="expand" for="c-41528174">[2 more]</label></div><br/><div class="children"><div class="content">ive been doing exactly this for bout a year now. feed it words data, give it a task. get better words back.<p>i sneak in a benchmark opening of data every time i start a new chat - so right off the bat i can see in its response whether this chat session is gonna be on point or if we are going off into wacky world, which saves me time as i can just terminate and try starting another chat.<p>chatgpt is fickle daily. most days its on point. some days its wearing a bicycle helmet and licking windows. kinda sucks i cant just zone out and daydream while working. gotta be checking replies for when the wheels fall off the convo.</div><br/><div id="41528791" class="c"><input type="checkbox" id="c-41528791" checked=""/><div class="controls bullet"><span class="by">ruthmarx</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528174">parent</a><span>|</span><a href="#41529331">next</a><span>|</span><label class="collapse" for="c-41528791">[-]</label><label class="expand" for="c-41528791">[1 more]</label></div><br/><div class="children"><div class="content">&gt; i sneak in a benchmark opening of data every time i start a new chat - so right off the bat i can see in its response whether this chat session is gonna be on point or if we are going off into wacky world, which saves me time as i can just terminate and try starting another chat.<p>I don&#x27;t think it works like that...</div><br/></div></div></div></div><div id="41529331" class="c"><input type="checkbox" id="c-41529331" checked=""/><div class="controls bullet"><span class="by">re-thc</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41528174">prev</a><span>|</span><a href="#41529310">next</a><span>|</span><label class="collapse" for="c-41529331">[-]</label><label class="expand" for="c-41529331">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Treat it as a naive but intelligent intern.<p>That&#x27;s the crux of the problem. Why and who would treat it as an intern? It might cost you more in explaining and dealing with it than not using it.<p>The purpose of an intern is to grow the intern. If this intern is static and will always be at the same level, why bother? If you had to feed and prep it every time, you might as well hire a senior.</div><br/></div></div><div id="41529310" class="c"><input type="checkbox" id="c-41529310" checked=""/><div class="controls bullet"><span class="by">lyu07282</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41529331">prev</a><span>|</span><a href="#41529061">next</a><span>|</span><label class="collapse" for="c-41529310">[-]</label><label class="expand" for="c-41529310">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The magic is that _it knows english_.<p>I couldn&#x27;t agree more, this is exactly the strength of LLMs that what we should focus on. If you can make your problem fit into this paradigm, LLMs work fantastic. Hallucinations come from that massive &quot;lossy compressed database&quot;, but you should consider that part as more like the background noise that taught the model to speak English, and the syntax of programming languages, instead of the source of the knowledge to respond with. Stop anthropomorphizing LLMs, play to it&#x27;s strengths instead.<p>In other words it might hallucinate a API but it will rarely, if ever, make a syntax error. Once you realize that, it becomes a much more useful tool.</div><br/></div></div><div id="41529061" class="c"><input type="checkbox" id="c-41529061" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41529310">prev</a><span>|</span><a href="#41529164">next</a><span>|</span><label class="collapse" for="c-41529061">[-]</label><label class="expand" for="c-41529061">[2 more]</label></div><br/><div class="children"><div class="content">Interns are cheaper than o1-preview</div><br/><div id="41529288" class="c"><input type="checkbox" id="c-41529288" checked=""/><div class="controls bullet"><span class="by">icrbow</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41529061">parent</a><span>|</span><a href="#41529164">next</a><span>|</span><label class="collapse" for="c-41529288">[-]</label><label class="expand" for="c-41529288">[1 more]</label></div><br/><div class="children"><div class="content">Not for long.</div><br/></div></div></div></div><div id="41529164" class="c"><input type="checkbox" id="c-41529164" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41529061">prev</a><span>|</span><a href="#41528156">next</a><span>|</span><label class="collapse" for="c-41529164">[-]</label><label class="expand" for="c-41529164">[2 more]</label></div><br/><div class="children"><div class="content">Intelligent?<p>Just ask ChatGPT<p>How many Rs are in strawberry?</div><br/><div id="41529227" class="c"><input type="checkbox" id="c-41529227" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41529164">parent</a><span>|</span><a href="#41528156">next</a><span>|</span><label class="collapse" for="c-41529227">[-]</label><label class="expand" for="c-41529227">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3f9e1-2cb4-8009-83ce-090068b16335" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3f9e1-2cb4-8009-83ce-090068b163...</a><p>Keep up, that was last week&#x27;s gotcha, with the old model.</div><br/></div></div></div></div><div id="41528156" class="c"><input type="checkbox" id="c-41528156" checked=""/><div class="controls bullet"><span class="by">kylebenzle</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528061">parent</a><span>|</span><a href="#41529164">prev</a><span>|</span><a href="#41527628">next</a><span>|</span><label class="collapse" for="c-41528156">[-]</label><label class="expand" for="c-41528156">[4 more]</label></div><br/><div class="children"><div class="content">Perfectly well put! We should change the name from &quot;AI&quot; (which it is not) to something like, &quot;lossy compressed databases&quot;.</div><br/><div id="41528282" class="c"><input type="checkbox" id="c-41528282" checked=""/><div class="controls bullet"><span class="by">melagonster</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528156">parent</a><span>|</span><a href="#41528235">next</a><span>|</span><label class="collapse" for="c-41528282">[-]</label><label class="expand" for="c-41528282">[1 more]</label></div><br/><div class="children"><div class="content">If they use this name, they just say that they violate the copyright of all training data.</div><br/></div></div><div id="41528235" class="c"><input type="checkbox" id="c-41528235" checked=""/><div class="controls bullet"><span class="by">gsinclair</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528156">parent</a><span>|</span><a href="#41528282">prev</a><span>|</span><a href="#41527628">next</a><span>|</span><label class="collapse" for="c-41528235">[-]</label><label class="expand" for="c-41528235">[2 more]</label></div><br/><div class="children"><div class="content">That abbreviates to LCD. If we could make it LSD somehow, that would help to explain the hallucinations.</div><br/><div id="41528272" class="c"><input type="checkbox" id="c-41528272" checked=""/><div class="controls bullet"><span class="by">MikeTheGreat</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528235">parent</a><span>|</span><a href="#41527628">next</a><span>|</span><label class="collapse" for="c-41528272">[-]</label><label class="expand" for="c-41528272">[1 more]</label></div><br/><div class="children"><div class="content">Lossy Stochastic Database?</div><br/></div></div></div></div></div></div></div></div><div id="41527628" class="c"><input type="checkbox" id="c-41527628" checked=""/><div class="controls bullet"><span class="by">COAGULOPATH</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528061">prev</a><span>|</span><a href="#41527371">next</a><span>|</span><label class="collapse" for="c-41527628">[-]</label><label class="expand" for="c-41527628">[5 more]</label></div><br/><div class="children"><div class="content">Yes, this only helps multi-step reasoning. The model still has problems with general knowledge and deep facts.<p>There&#x27;s no way you can &quot;reason&quot; a correct answer to &quot;list the tracklisting of some obscure 1991 demo by a band not on Wikipedia.&quot; You either know or you don&#x27;t.<p>I usually test new models with questions like &quot;what are the levels in [semi-famous PC game from the 90s]?&quot; The release version of GPT-4 could get about 75% correct. o1-preview gets about half correct. o1-mini gets 0% correct.<p>Fair enough. The GPT-4 line aren&#x27;t meant to be search engines or encyclopedias. This is still a useful update though.</div><br/><div id="41528834" class="c"><input type="checkbox" id="c-41528834" checked=""/><div class="controls bullet"><span class="by">barrkel</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527628">parent</a><span>|</span><a href="#41528406">next</a><span>|</span><label class="collapse" for="c-41528834">[-]</label><label class="expand" for="c-41528834">[1 more]</label></div><br/><div class="children"><div class="content">o1-mini is a small model (knows a lot less about the world) and is tuned for reasoning through symbolic problems (maths, programming, chemistry etc.).<p>You&#x27;re using a calculator as a search engine.</div><br/></div></div><div id="41528406" class="c"><input type="checkbox" id="c-41528406" checked=""/><div class="controls bullet"><span class="by">mattmanser</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527628">parent</a><span>|</span><a href="#41528834">prev</a><span>|</span><a href="#41527371">next</a><span>|</span><label class="collapse" for="c-41528406">[-]</label><label class="expand" for="c-41528406">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s actually much worse than that and you&#x27;re inadvertently down playing how bad it is.<p>It doesn&#x27;t even know mildly obsecure facts that are on the internet.<p>For example last night I was trying to do something with C# generics and it confidently told me I could use pattern matching on the type in a switch statwmnt, and threw out some convincing looking code.<p>You can&#x27;t, it&#x27;s impossible. It wàa completely wrong. When I told that this, it told me I was right, and proceeded to give me code that was even more wrong.<p>This is an obscure, but well documented, part of the spec.<p>So it&#x27;s not about facts that aren&#x27;t on the internet, it&#x27;s just bad at facts fullstop.<p>What it&#x27;s good at is facts the internet agrees on. Unless the internet is wrong. Which is not always a good thing with the way the language it uses to speak is so confident.<p>If you want to fuck with AI models as a bunch of code questions on Reddit, GitHub  and SO with example code saying &#x27;can I do X&#x27;. The answer is no, but chatgpt&#x2F;codepilot&#x2F;etc. will start spewing out that nonsense as if it&#x27;s fact.<p>As for non-proframming, we&#x27;re about to see the birth of a new SEO movement of tricking AI models to believe your &#x27;facts&#x27;.</div><br/><div id="41528477" class="c"><input type="checkbox" id="c-41528477" checked=""/><div class="controls bullet"><span class="by">koe123</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528406">parent</a><span>|</span><a href="#41528641">next</a><span>|</span><label class="collapse" for="c-41528477">[-]</label><label class="expand" for="c-41528477">[1 more]</label></div><br/><div class="children"><div class="content">Its not always the right tool depending on the task. IMO using LLMs is also a skill, much like learning how to Google stuff.<p>E.g. apparently C# generics isn’t something its good at. Interesting, so don’t use it for that, apparently its the wrong tool. In contrast, its amazing at C++ generics, and thus speeds up my productivity. So do use it for that!</div><br/></div></div><div id="41528641" class="c"><input type="checkbox" id="c-41528641" checked=""/><div class="controls bullet"><span class="by">bbarnett</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528406">parent</a><span>|</span><a href="#41528477">prev</a><span>|</span><a href="#41527371">next</a><span>|</span><label class="collapse" for="c-41528641">[-]</label><label class="expand" for="c-41528641">[1 more]</label></div><br/><div class="children"><div class="content">I wonder though, is the documentation only referenced a few places on the Internet, and are there also many forums with people pasting &quot;Why isn&#x27;t this working?&quot; problems?<p>If there are a lot of people pasting broken code, now the LLM has all these examples of broken code, which it doesn&#x27;t know are that, and only a couple of references to documentation.  Worse, a well trained LLM may realise that specs change, and that even documentation may not be considered 100% accurate (for it is older, out of date).<p>After all, how many times have you had something updated, an API, a language, a piece of software, but the docs weren&#x27;t updates?  Happens all the time, sadly.<p>So it may believe <i>newer</i> examples of code, such as the aforementioned pasted code, might be more correct than the docs.<p>Also, if people keep trying to solve the same issue again, and keep pasting those examples again, well...<p>I guess my point here is, hallucinations come from multi-faceted issues, one being &quot;wrong examples are more plentiful than correct&quot;.  Or even &quot;there&#x27;s just a lot of wrong examples&quot;.</div><br/></div></div></div></div></div></div><div id="41527371" class="c"><input type="checkbox" id="c-41527371" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527628">prev</a><span>|</span><a href="#41528452">next</a><span>|</span><label class="collapse" for="c-41527371">[-]</label><label class="expand" for="c-41527371">[16 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had the opposite experience with some coding samples. After reading Nick Carlini&#x27;s post, I&#x27;ve gotten into the habit of powering through coding problems with GPT (where previously I&#x27;d just laugh and immediately give up) by just presenting it the errors in its code and asking it to fix them. o1 seems to be effectively screening for some of those errors (I assume it&#x27;s just some, but I&#x27;ve noticed that the o1 things I&#x27;ve done haven&#x27;t had obvious dumb errors like missing imports, and <i>all</i> my 4o attempts have).</div><br/><div id="41527400" class="c"><input type="checkbox" id="c-41527400" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527371">parent</a><span>|</span><a href="#41528452">next</a><span>|</span><label class="collapse" for="c-41527400">[-]</label><label class="expand" for="c-41527400">[15 more]</label></div><br/><div class="children"><div class="content">My experience is likely colored by the fact that I tend to turn to LLMs for problems I have trouble solving by myself. I typically don&#x27;t use them for the low-hanging fruits.<p>That&#x27;s the frustrating thing. LLMs don&#x27;t materially reduce the set of problems where I&#x27;m running against a wall or have trouble finding information.</div><br/><div id="41527524" class="c"><input type="checkbox" id="c-41527524" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527400">parent</a><span>|</span><a href="#41527588">next</a><span>|</span><label class="collapse" for="c-41527524">[-]</label><label class="expand" for="c-41527524">[3 more]</label></div><br/><div class="children"><div class="content">LLMs are not for expanding the sphere of human knowledge, but for speeding up auto-correct of higher order processing to help you more quickly reach the shell of the sphere and make progress with your own mind :)</div><br/><div id="41527548" class="c"><input type="checkbox" id="c-41527548" checked=""/><div class="controls bullet"><span class="by">tsunamifury</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527524">parent</a><span>|</span><a href="#41527588">next</a><span>|</span><label class="collapse" for="c-41527548">[-]</label><label class="expand" for="c-41527548">[2 more]</label></div><br/><div class="children"><div class="content">Definitely.  When we talk about being skilled in a T shape LLMs are all about spreading your top of T and not making the bottom go deeper.</div><br/><div id="41528237" class="c"><input type="checkbox" id="c-41528237" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527548">parent</a><span>|</span><a href="#41527588">next</a><span>|</span><label class="collapse" for="c-41528237">[-]</label><label class="expand" for="c-41528237">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, not much more depth — though even Terence Tao reported useful results from an earlier version, so perhaps  the breadth is a depth all of it&#x27;s own: <a href="https:&#x2F;&#x2F;mathstodon.xyz&#x2F;@tao&#x2F;110601051375142142" rel="nofollow">https:&#x2F;&#x2F;mathstodon.xyz&#x2F;@tao&#x2F;110601051375142142</a><p>I think of it as making the top bar of the T thicker, but yes, you&#x27;re right, it also spreads it much wider.</div><br/></div></div></div></div></div></div><div id="41527588" class="c"><input type="checkbox" id="c-41527588" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527400">parent</a><span>|</span><a href="#41527524">prev</a><span>|</span><a href="#41528560">next</a><span>|</span><label class="collapse" for="c-41527588">[-]</label><label class="expand" for="c-41527588">[8 more]</label></div><br/><div class="children"><div class="content">I use LLMs for three things:<p>* To catch passive voice and nominalizations in my writing.<p>* To convert Linux kernel subsystems into Python so I can quickly understand them (I&#x27;m a C programmer but everyone reads Python faster).<p>* To write dumb programs using languages and libraries I haven&#x27;t used much before; for instance, I&#x27;m an ActiveRecord person and needed to do some SQLAlchemy stuff today, and GPT 4o (and o1) kept me away from the SQLAlchemy documentation.<p>OpenAI talks about o1 going head to head with PhDs. I could care less. But for the specific problem we&#x27;re talking about on this subthread: o1 seems materially better.</div><br/><div id="41527624" class="c"><input type="checkbox" id="c-41527624" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527588">parent</a><span>|</span><a href="#41528560">next</a><span>|</span><label class="collapse" for="c-41527624">[-]</label><label class="expand" for="c-41527624">[7 more]</label></div><br/><div class="children"><div class="content"><i>&gt; * To convert Linux kernel subsystems into Python so I can quickly understand them (I&#x27;m a C programmer but everyone reads Python faster).</i><p>Do you have an example chat of this output? Sounds interesting. Do you just dump the C source code into the prompt and ask it to convert to Python?</div><br/><div id="41527641" class="c"><input type="checkbox" id="c-41527641" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527624">parent</a><span>|</span><a href="#41528560">next</a><span>|</span><label class="collapse" for="c-41527641">[-]</label><label class="expand" for="c-41527641">[6 more]</label></div><br/><div class="children"><div class="content">No, ChatGPT is way cooler than that. It&#x27;s already read every line of kernel code ever written. I start with a subsystem: the device mapper is a good recent example. I ask things like &quot;explain the linux device mapper. if it was a class in an object-oriented language, what would its interface look like?&quot; and &quot;give me dm_target as a python class&quot;. I get stuff like:<p><pre><code>    def linear_ctr(target, argc, argv):
        print(&quot;Constructor called with args:&quot;, argc, argv)
        # Initialize target-specific data here
        return 0
     
    def linear_dtr(target):
        print(&quot;Destructor called&quot;)
        # Clean up target-specific data here
     
    def linear_map(target, bio):
        print(&quot;Mapping I&#x2F;O request&quot;)
        # Perform mapping here
        return 0
     
    linear_target = DmTarget(name=&quot;linear&quot;, version=(1, 0, 0), module=&quot;dm_mod&quot;)
    linear_target.set_ctr(linear_ctr)
    linear_target.set_dtr(linear_dtr)
    linear_target.set_map(linear_map)
     
    info = linear_target.get_info()
    print(info)
</code></pre>
(A bunch of stuff elided). I don&#x27;t care at all about the correctness of this code, because I&#x27;m just using it as a roadmap for the real Linux kernel code. The example use case code is an example of something GPT 4o provides that I didn&#x27;t even know I wanted.</div><br/><div id="41527672" class="c"><input type="checkbox" id="c-41527672" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527641">parent</a><span>|</span><a href="#41527755">next</a><span>|</span><label class="collapse" for="c-41527672">[-]</label><label class="expand" for="c-41527672">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s awesome. Have you tried asking it to convert Python (psuedo-ish) code back into C that interfaces with the kernel?</div><br/><div id="41527720" class="c"><input type="checkbox" id="c-41527720" checked=""/><div class="controls bullet"><span class="by">tptacek</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527672">parent</a><span>|</span><a href="#41527755">next</a><span>|</span><label class="collapse" for="c-41527720">[-]</label><label class="expand" for="c-41527720">[1 more]</label></div><br/><div class="children"><div class="content">No, but only because I have no use for it. I wouldn&#x27;t be surprised if it did a fine job! I&#x27;d be remiss if I didn&#x27;t note that it&#x27;s way better at doing this for the Linux kernel than with codebases like Zookeeper and Kubernetes (though: maybe o1 makes this better, who knows?).<p>I do feel like someone who skipped like 8 iPhone models (cross-referencing, EIEIO, lsp-mode, code explorers, tree-sitter) and just got an iPhone 16. Like, nothing that came before this for code comprehension really matters all that much?</div><br/></div></div></div></div><div id="41527755" class="c"><input type="checkbox" id="c-41527755" checked=""/><div class="controls bullet"><span class="by">INGSOCIALITE</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527641">parent</a><span>|</span><a href="#41527672">prev</a><span>|</span><a href="#41528560">next</a><span>|</span><label class="collapse" for="c-41527755">[-]</label><label class="expand" for="c-41527755">[3 more]</label></div><br/><div class="children"><div class="content">it&#x27;s all placeholders - that&#x27;s my experience with gpt trying to write slop code</div><br/><div id="41527784" class="c"><input type="checkbox" id="c-41527784" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527755">parent</a><span>|</span><a href="#41528591">next</a><span>|</span><label class="collapse" for="c-41527784">[-]</label><label class="expand" for="c-41527784">[1 more]</label></div><br/><div class="children"><div class="content">Those are placeholders for user callbacks passed to the device mapper subsystem. It’s a usage example not implementation code.</div><br/></div></div><div id="41528591" class="c"><input type="checkbox" id="c-41528591" checked=""/><div class="controls bullet"><span class="by">jeffhuys</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527755">parent</a><span>|</span><a href="#41527784">prev</a><span>|</span><a href="#41528560">next</a><span>|</span><label class="collapse" for="c-41528591">[-]</label><label class="expand" for="c-41528591">[1 more]</label></div><br/><div class="children"><div class="content">Then ask it to expand. Be specific.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41528560" class="c"><input type="checkbox" id="c-41528560" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527400">parent</a><span>|</span><a href="#41527588">prev</a><span>|</span><a href="#41528135">next</a><span>|</span><label class="collapse" for="c-41528560">[-]</label><label class="expand" for="c-41528560">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny because I&#x27;m very happy with the productivity boost from LLMs, but I use them in a way that is pretty much diametrically opposite to yours.<p>I can&#x27;t think of many situations where I would use them for a problem that I tried to solve and failed - not only because they would probably fail, but in many cases it would even be difficult to <i>know</i> that it failed.<p>I use it for things that are not hard, can be solved by someone without a specialized degree that took the effort to learn some knowledge or skill, but would take too much work to do. And there are a lot of those, even in my highly specialized job.</div><br/></div></div><div id="41528135" class="c"><input type="checkbox" id="c-41528135" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527400">parent</a><span>|</span><a href="#41528560">prev</a><span>|</span><a href="#41528677">next</a><span>|</span><label class="collapse" for="c-41528135">[-]</label><label class="expand" for="c-41528135">[1 more]</label></div><br/><div class="children"><div class="content">LLMs: When the code can be made by an enthusiastic new intern with web-search and copy-paste skills, and no ability to improve under mentorship. :p<p>Tangentially related, a comic on them: <a href="https:&#x2F;&#x2F;existentialcomics.com&#x2F;comic&#x2F;557" rel="nofollow">https:&#x2F;&#x2F;existentialcomics.com&#x2F;comic&#x2F;557</a></div><br/></div></div><div id="41528677" class="c"><input type="checkbox" id="c-41528677" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527400">parent</a><span>|</span><a href="#41528135">prev</a><span>|</span><a href="#41528452">next</a><span>|</span><label class="collapse" for="c-41528677">[-]</label><label class="expand" for="c-41528677">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s the frustrating thing. LLMs don&#x27;t materially reduce the set of problems where I&#x27;m running against a wall or have trouble finding information.<p>As you step outside regular Stack Overflow questions for top-3 languages, you run into limitations of these predictive models.<p>There&#x27;s no &quot;reasoning&quot; behind them. They are still, largely, bullshit machines.</div><br/></div></div></div></div></div></div><div id="41528452" class="c"><input type="checkbox" id="c-41528452" checked=""/><div class="controls bullet"><span class="by">fsndz</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527371">prev</a><span>|</span><a href="#41527608">next</a><span>|</span><label class="collapse" for="c-41528452">[-]</label><label class="expand" for="c-41528452">[1 more]</label></div><br/><div class="children"><div class="content">My point of view: this is a real advancement. I&#x27;ve always believed that with the right data allowing the LLM to be trained to imitate reasoning, it&#x27;s possible to improve its performance. However, this is still pattern matching, and I suspect that this approach may not be very effective for creating true generalization. As a result, once o1 becomes generally available, we will likely notice the persistent hallucinations and faulty reasoning, especially when the problem is sufficiently new or complex, beyond the &quot;reasoning programs&quot; or &quot;reasoning patterns&quot; the model learned during the reinforcement learning phase.
<a href="https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;openai-o1-release-agi-reasoning" rel="nofollow">https:&#x2F;&#x2F;www.lycee.ai&#x2F;blog&#x2F;openai-o1-release-agi-reasoning</a></div><br/></div></div><div id="41527608" class="c"><input type="checkbox" id="c-41527608" checked=""/><div class="controls bullet"><span class="by">faangguyindia</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528452">prev</a><span>|</span><a href="#41527824">next</a><span>|</span><label class="collapse" for="c-41527608">[-]</label><label class="expand" for="c-41527608">[3 more]</label></div><br/><div class="children"><div class="content">&gt;The o1-preview model still hallucinates non-existing libraries and functions for me, and is quickly wrong about facts that aren&#x27;t well-represented on the web. It&#x27;s the usual string of &quot;You&#x27;re absolutely correct, and I apologize for the oversight in my previous response. [Let me make another guess.]&quot;<p>After that you switch to Claude Soñnet and after sometime it also gets stuck.<p>Problem with LLM is that they are not aware of libraries.<p>I&#x27;ve fed them library version, using requirements.txt, python version I am using etc...<p>They still make mistakes and try to use methods which do not exist.<p>Where to go from here? At this point I manually pull the library version I am using and go to its docs, I generate a page which uses the this library correctly (then I feed that example into LLM)<p>Using this approach works. Now I just need to automate it so that I don&#x27;t have to manually find the library, create specific example which uses the methods I need in my code!<p>Directly feeding the docs isn&#x27;t working well either.</div><br/><div id="41527640" class="c"><input type="checkbox" id="c-41527640" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527608">parent</a><span>|</span><a href="#41527824">next</a><span>|</span><label class="collapse" for="c-41527640">[-]</label><label class="expand" for="c-41527640">[2 more]</label></div><br/><div class="children"><div class="content">One trick that people are using, when using Cursor and specifically Cursor&#x27;s compose function, is to dump library docs into a text file in your repo, and then @ that doc file when you&#x27;re asking it to do something involving that library.<p>That seems to eliminate a lot of the issues, though it&#x27;s not a seamless experience, and it adds another step of having to put the library docs in a text file.<p>Alternatively, cursor can fetch a web page, so if there&#x27;s a good page of docs you can bring that in by @ the web page.<p>Eventually, I could imagine LLMs automatically creating library text doc files to include when the LLM is using them to avoid some of these problems.<p>It could also solve some of the issues of their shaky understanding of newer frameworks like SvelteKit.</div><br/><div id="41527696" class="c"><input type="checkbox" id="c-41527696" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527640">parent</a><span>|</span><a href="#41527824">next</a><span>|</span><label class="collapse" for="c-41527696">[-]</label><label class="expand" for="c-41527696">[1 more]</label></div><br/><div class="children"><div class="content">Cursor also has the shadow workspace feature [1] that is supposed to send feedback from linting and language servers to the LLM. I&#x27;m not sure whether it&#x27;s enabled in compose yet though.<p>[1] <a href="https:&#x2F;&#x2F;www.cursor.com&#x2F;blog&#x2F;shadow-workspace" rel="nofollow">https:&#x2F;&#x2F;www.cursor.com&#x2F;blog&#x2F;shadow-workspace</a></div><br/></div></div></div></div></div></div><div id="41527824" class="c"><input type="checkbox" id="c-41527824" checked=""/><div class="controls bullet"><span class="by">feralderyl</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527608">prev</a><span>|</span><a href="#41527614">next</a><span>|</span><label class="collapse" for="c-41527824">[-]</label><label class="expand" for="c-41527824">[1 more]</label></div><br/><div class="children"><div class="content">I think this model is a precursor model that is designed for agentic behavior.  I expect very soon OpenAI to allow this model tool use that will allow it to verify its code creations and whatever else it claims through use of various tools like a search engine, a virtual machine instance with code execution capabilities, api calling and other advanced tool use.</div><br/></div></div><div id="41527614" class="c"><input type="checkbox" id="c-41527614" checked=""/><div class="controls bullet"><span class="by">shmatt</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527824">prev</a><span>|</span><a href="#41528865">next</a><span>|</span><label class="collapse" for="c-41527614">[-]</label><label class="expand" for="c-41527614">[7 more]</label></div><br/><div class="children"><div class="content">I honestly can’t believe this is the hyped up “strawberry” everyone was claiming is pretty much AGI. Senior employees leaving due to its powers being so extreme<p>I’m in the “probabilistic token generators aren’t intelligence” camp so I don’t actually believe in AGI, but I’ll be honest the never ending rumors &#x2F; chatter almost got to me<p>Remember, this is the model some media outlet reported recently that is so powerful OAI is considering charging $2k&#x2F;month for</div><br/><div id="41528888" class="c"><input type="checkbox" id="c-41528888" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527614">parent</a><span>|</span><a href="#41528725">next</a><span>|</span><label class="collapse" for="c-41528888">[-]</label><label class="expand" for="c-41528888">[1 more]</label></div><br/><div class="children"><div class="content">The whole safety aspect of AI has this nice property that it also functions as a marketing tool to make the technology seem &quot;so powerful it&#x27;s dangerous&quot;. &quot;If it&#x27;s so dangerous it must be good&quot;.</div><br/></div></div><div id="41528208" class="c"><input type="checkbox" id="c-41528208" checked=""/><div class="controls bullet"><span class="by">kqr</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527614">parent</a><span>|</span><a href="#41528725">prev</a><span>|</span><a href="#41527629">next</a><span>|</span><label class="collapse" for="c-41528208">[-]</label><label class="expand" for="c-41528208">[2 more]</label></div><br/><div class="children"><div class="content">&gt; probabilistic token generators aren’t intelligence<p>Maybe this has been extensively discussed before, but since I&#x27;ve lived under a rock: which parts of intelligence do you think are not representable as conditional probability distributions?</div><br/><div id="41529208" class="c"><input type="checkbox" id="c-41529208" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528208">parent</a><span>|</span><a href="#41527629">next</a><span>|</span><label class="collapse" for="c-41529208">[-]</label><label class="expand" for="c-41529208">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve built a &quot;cannon&quot; that uses flammable gas in a vessel that is sealed until the gas is ignited, and the vessel has an opening that appears when the pressure rises above a threshold; this opening is the &quot;breach&quot; that feeds into the barrel, where a projectile partially seals the barrel. When the pressure increases above another threshold (that of the friction of the projectile against the barrel), the projectile is propelled out of the barrel.<p>All good, sorta makes sense with my fast off the cuff description, right? ok, great.<p>We have a cannon that uses flammable gases that has an opening into a barrel, and the opening is held closed by a 1&#x2F;8&quot; thick piece of rubber (think like a drawer liner cut to size to seal the opening). This works per the above description, but now you have a consumable that you need to replace each time you reload the projectile, and the gas.<p>Your goal is to eliminate the consumable. How do you go about this? Chatgpt 4o can lead you (partially) to the solution that i came up with. GPT&#x27;s bullet points are: Spring-loaded valve, mechanical latch mechanism, pressure activated flap, magnetically actuated seal, reusable burst disk, and a <i>sliding gate valve</i>.<p>I knew that the person who was ruminating on this was using CAD to design these cannons, so i had them design in a captive mushroom-shaped piece of metal which had a hole in the wider part axially, and several smaller holes placed 1&#x2F;2&quot; from the other end, circumferential. When loading (there was a literal breach to load the projectile in), you pushed the part with holes back into the gas vessel (about an inch of travel, no resistance). he called it a piston.<p>He spent a long time thinking about it - over a week. I spent 30 seconds. the AI spent less than i did, but didn&#x27;t actually deliver a solution - it delivered 6 <i>potential</i> solutions, all of which would require more interrogation of the model to give a rough description that could be worked into an actual design. I specifically asked about a piston, and it gave 7 more bullet points, none of which accurately describe the mechanism (video below).<p>I can do this with most novel &quot;problems&quot; i am presented - just give an off the cuff solution that <i>will work</i> per &quot;spec&quot;. no AI model can with the same regularity that i can. And i don&#x27;t mean &quot;mechanical engineering&quot; problems, or pressure vessel problems; i mean any problem from relationship issues to writer&#x27;s block, to software architecture, to why a car is making that noise.<p>I am not a mechanical or pressure vessel engineer. I had never seen the cannon in operation prior to giving my solution to the problem. The &quot;gas&quot; was Oxygen and Acetylene, mixed in the vessel by a brushless fan, and pressurized to a specific PSI. I had no access to mechanical engineering texts, forums, blogs, websites, etc. I&#x27;ve never designed nor studied how breaches work. I don&#x27;t know how to CAD.<p>but: <a href="https:&#x2F;&#x2F;nextcloud.projectftm.com&#x2F;index.php&#x2F;s&#x2F;cobra5x" rel="nofollow">https:&#x2F;&#x2F;nextcloud.projectftm.com&#x2F;index.php&#x2F;s&#x2F;cobra5x</a> (my solution is shown in video around 1:30 - 2:00, it&#x27;s zoomed in, even.)<p>p.s. about 12 years after this video was shot he made another cannon that could shoot full-sized (24oz?) cans of &quot;Monster Energy&quot; oh, 0.5 miles accurately.</div><br/></div></div></div></div><div id="41527629" class="c"><input type="checkbox" id="c-41527629" checked=""/><div class="controls bullet"><span class="by">firejake308</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527614">parent</a><span>|</span><a href="#41528208">prev</a><span>|</span><a href="#41528865">next</a><span>|</span><label class="collapse" for="c-41527629">[-]</label><label class="expand" for="c-41527629">[2 more]</label></div><br/><div class="children"><div class="content">I mean, considering how many tokens their example prompt consumed, I wouldn&#x27;t be surprised if it costs ~$2k&#x2F;month&#x2F;user to run</div><br/></div></div></div></div><div id="41527814" class="c"><input type="checkbox" id="c-41527814" checked=""/><div class="controls bullet"><span class="by">_fs</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528865">prev</a><span>|</span><a href="#41527793">next</a><span>|</span><label class="collapse" for="c-41527814">[-]</label><label class="expand" for="c-41527814">[3 more]</label></div><br/><div class="children"><div class="content">o1-preview != o1.<p>In public coding AI comparison tests, results showed 4o scoring around 35%, o1-preview scoring ~50% and o1 scoring ~85%.<p>o1 is not yet released, but has been run through many comparison tests with public results posted.</div><br/><div id="41528863" class="c"><input type="checkbox" id="c-41528863" checked=""/><div class="controls bullet"><span class="by">barrkel</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527814">parent</a><span>|</span><a href="#41528578">next</a><span>|</span><label class="collapse" for="c-41528863">[-]</label><label class="expand" for="c-41528863">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t forget about o1-mini. It seems better than o1-preview for problems that fit it (don&#x27;t require so much real world knowledge).</div><br/></div></div><div id="41528578" class="c"><input type="checkbox" id="c-41528578" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527814">parent</a><span>|</span><a href="#41528863">prev</a><span>|</span><a href="#41527793">next</a><span>|</span><label class="collapse" for="c-41528578">[-]</label><label class="expand" for="c-41528578">[1 more]</label></div><br/><div class="children"><div class="content">gpt-4 base was never released and this will be the same thing</div><br/></div></div></div></div><div id="41527793" class="c"><input type="checkbox" id="c-41527793" checked=""/><div class="controls bullet"><span class="by">motoboi</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527814">prev</a><span>|</span><a href="#41528087">next</a><span>|</span><label class="collapse" for="c-41527793">[-]</label><label class="expand" for="c-41527793">[1 more]</label></div><br/><div class="children"><div class="content">Just pass a link to a GitHub issue and ask for a response or even a webpage to summarize and will see the beautiful hallucinations it will come up to as the model is not web browsing yet.</div><br/></div></div><div id="41528087" class="c"><input type="checkbox" id="c-41528087" checked=""/><div class="controls bullet"><span class="by">rcarr</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527793">prev</a><span>|</span><a href="#41528533">next</a><span>|</span><label class="collapse" for="c-41528087">[-]</label><label class="expand" for="c-41528087">[3 more]</label></div><br/><div class="children"><div class="content">Has anyone tried asking it to generate the libraries&#x2F;functions that it&#x27;s hallucinating and seeing if it can do so correctly? And then seeing if it can continue solving the original problem with the new libraries? It&#x27;d be absolutely fascinating if it turns out it could do this.</div><br/><div id="41528205" class="c"><input type="checkbox" id="c-41528205" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528087">parent</a><span>|</span><a href="#41528533">next</a><span>|</span><label class="collapse" for="c-41528205">[-]</label><label class="expand" for="c-41528205">[2 more]</label></div><br/><div class="children"><div class="content">Not for libraries, but functions will sometimes get created if you work with an agent coding loop. If the tests are in the verification step, the code will typically be correct.</div><br/><div id="41528650" class="c"><input type="checkbox" id="c-41528650" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528205">parent</a><span>|</span><a href="#41528533">next</a><span>|</span><label class="collapse" for="c-41528650">[-]</label><label class="expand" for="c-41528650">[1 more]</label></div><br/><div class="children"><div class="content">I sometimes give it snippets of code and omit helper functions if they seem obvious enough, and it adds its own implementation into the output.</div><br/></div></div></div></div></div></div><div id="41528533" class="c"><input type="checkbox" id="c-41528533" checked=""/><div class="controls bullet"><span class="by">spaceman_2020</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528087">prev</a><span>|</span><a href="#41528013">next</a><span>|</span><label class="collapse" for="c-41528533">[-]</label><label class="expand" for="c-41528533">[4 more]</label></div><br/><div class="children"><div class="content">I don’t really see this as a massive problem. Its code. If it doesn’t run, you ask it to reconsider, give some more info if necessary, and it usually gets it right.<p>The system doesn’t become useless if it takes 2 tries instead of 1 to get it right<p>Still saves an incredible amount of time vs doing it yourself</div><br/><div id="41529188" class="c"><input type="checkbox" id="c-41529188" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528533">parent</a><span>|</span><a href="#41528920">next</a><span>|</span><label class="collapse" for="c-41529188">[-]</label><label class="expand" for="c-41529188">[1 more]</label></div><br/><div class="children"><div class="content">While I agree, I saw it abused in this way a lot, in the sense that the code did what it was supposed to do in a given scenario but was obviously flawed in various was so it was just sitting there waiting for a disaster.</div><br/></div></div><div id="41528920" class="c"><input type="checkbox" id="c-41528920" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528533">parent</a><span>|</span><a href="#41529188">prev</a><span>|</span><a href="#41528689">next</a><span>|</span><label class="collapse" for="c-41528920">[-]</label><label class="expand" for="c-41528920">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Its code. If it doesn’t run, you ask it to reconsider<p>It is perfectly possible to have code that runs without errors but gives a wrong answer. And you may not even realise it’s wrong until it bites you in production.</div><br/></div></div><div id="41528689" class="c"><input type="checkbox" id="c-41528689" checked=""/><div class="controls bullet"><span class="by">troupo</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528533">parent</a><span>|</span><a href="#41528920">prev</a><span>|</span><a href="#41528013">next</a><span>|</span><label class="collapse" for="c-41528689">[-]</label><label class="expand" for="c-41528689">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t found a single instance where it saved me any significant amount of time. In all cases I still had to rewrite the whole thing myself, or abandon endeavor.<p>And a few times the amount of time I spent trying to coax a correct answer out of AI trumped any potential savings I could&#x27;ve had</div><br/></div></div></div></div><div id="41528013" class="c"><input type="checkbox" id="c-41528013" checked=""/><div class="controls bullet"><span class="by">burtonator</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528533">prev</a><span>|</span><a href="#41527409">next</a><span>|</span><label class="collapse" for="c-41528013">[-]</label><label class="expand" for="c-41528013">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m honestly confused as to why it is doing this and why it thinks I&#x27;m right when I tell it that it is incorrect.<p>I&#x27;ve tried asking it factual information, and it asserts that it&#x27;s incorrect but it will definitely hallucinate questions like the above.<p>You&#x27;d think the reasoning would nail that and most of the chain-of-thought systems I&#x27;ve worked on would have fixed this by asking it if the resulting answer was correct.</div><br/></div></div><div id="41527409" class="c"><input type="checkbox" id="c-41527409" checked=""/><div class="controls bullet"><span class="by">mrtesthah</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528013">prev</a><span>|</span><a href="#41528152">next</a><span>|</span><label class="collapse" for="c-41527409">[-]</label><label class="expand" for="c-41527409">[6 more]</label></div><br/><div class="children"><div class="content">It begs the question of whether we can supply a function to be called (e.g., one that compiles and runs code) to evaluate intermediate CoT results</div><br/><div id="41527600" class="c"><input type="checkbox" id="c-41527600" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527409">parent</a><span>|</span><a href="#41527492">next</a><span>|</span><label class="collapse" for="c-41527600">[-]</label><label class="expand" for="c-41527600">[2 more]</label></div><br/><div class="children"><div class="content">It seems OpenAI has decided to keep the CoT results a secret. If they were to allow the model to call out to tools to help fill in the CoT steps, then this might reveal what the model is thinking - something they do not want the outside world to know about.<p>I could imagine OpenAI might allow their own vetted tools to be used, but perhaps it will be a while (if ever) before developers are allowed to hook up their own tools. The risks here are substantial. A model fine-tuned to run chain-of-thought that can answer graduate level physics problems at an expert level can probably figure out how to scam your grandma out of her savings too.</div><br/><div id="41528661" class="c"><input type="checkbox" id="c-41528661" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527600">parent</a><span>|</span><a href="#41527492">next</a><span>|</span><label class="collapse" for="c-41528661">[-]</label><label class="expand" for="c-41528661">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s only a matter of time. When some other company releases the tool, they likely will too.</div><br/></div></div></div></div><div id="41527492" class="c"><input type="checkbox" id="c-41527492" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527409">parent</a><span>|</span><a href="#41527600">prev</a><span>|</span><a href="#41528152">next</a><span>|</span><label class="collapse" for="c-41527492">[-]</label><label class="expand" for="c-41527492">[3 more]</label></div><br/><div class="children"><div class="content">The answer is yes if you are willing to code it. OpenAI supports tool calls. Even if it didn&#x27;t you could just make multiple calls to their API and submit the result of the code execution yourself.</div><br/><div id="41527530" class="c"><input type="checkbox" id="c-41527530" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527492">parent</a><span>|</span><a href="#41528152">next</a><span>|</span><label class="collapse" for="c-41527530">[-]</label><label class="expand" for="c-41527530">[2 more]</label></div><br/><div class="children"><div class="content">The intermediate CoT results aren&#x27;t in the API.</div><br/><div id="41528253" class="c"><input type="checkbox" id="c-41528253" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41527530">parent</a><span>|</span><a href="#41528152">next</a><span>|</span><label class="collapse" for="c-41528253">[-]</label><label class="expand" for="c-41528253">[1 more]</label></div><br/><div class="children"><div class="content">I may be mistaken but I don&#x27;t believe the first version of the comment I replied to mentioned intermediate CoT results.</div><br/></div></div></div></div></div></div></div></div><div id="41528152" class="c"><input type="checkbox" id="c-41528152" checked=""/><div class="controls bullet"><span class="by">kylebenzle</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41527409">prev</a><span>|</span><a href="#41528347">next</a><span>|</span><label class="collapse" for="c-41528152">[-]</label><label class="expand" for="c-41528152">[2 more]</label></div><br/><div class="children"><div class="content">&gt; having no way to assess if what it conjures up from its weights is factual or not.<p>This comment makes no sense in the context of what an LLM is. To even say such a thing demonstates a lack of understandting of the domain. What we are doing here is TEXT COMPLETION, no one EVER said anything about being accurate and &quot;true&quot;. We are building models that can complete text, what did you think an LLM was, a &quot;truth machine&quot;?</div><br/><div id="41528191" class="c"><input type="checkbox" id="c-41528191" checked=""/><div class="controls bullet"><span class="by">panja</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528152">parent</a><span>|</span><a href="#41528347">next</a><span>|</span><label class="collapse" for="c-41528191">[-]</label><label class="expand" for="c-41528191">[1 more]</label></div><br/><div class="children"><div class="content">I mean of course you&#x27;re right, but then I question what&#x27;s the usefulness?</div><br/></div></div></div></div><div id="41528347" class="c"><input type="checkbox" id="c-41528347" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41527354">parent</a><span>|</span><a href="#41528152">prev</a><span>|</span><a href="#41528047">next</a><span>|</span><label class="collapse" for="c-41528347">[-]</label><label class="expand" for="c-41528347">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The o1-preview model still hallucinates non-existing libraries and functions for me<p>Oooh... <i>oohhh!!</i> I just had a thought: By now we&#x27;re all familiar with the strict JSON output mode capability of these LLMs. That&#x27;s just a matter of filtering the token probability vector by the output grammar. Only valid tokens are allowed, which guarantees that the output matches the grammar.<p>But... why just data grammars? Why not the equivalent of &quot;tab-complete&quot;? I wonder how hard it would be to hook up the Language Server Protocol (LSP) as seen in Visual Studio code to an AI and have it only emit syntactically valid code! No more hallucinated functions!<p>I mean, sure, the semantics can still be incorrect, but <i>not the syntax</i>.</div><br/><div id="41528940" class="c"><input type="checkbox" id="c-41528940" checked=""/><div class="controls bullet"><span class="by">loremaster</span><span>|</span><a href="#41527354">root</a><span>|</span><a href="#41528347">parent</a><span>|</span><a href="#41528047">next</a><span>|</span><label class="collapse" for="c-41528940">[-]</label><label class="expand" for="c-41528940">[1 more]</label></div><br/><div class="children"><div class="content">This would be a big undertaking to get working for just one language+package-manager combination, but would be beautiful if it worked.</div><br/></div></div></div></div></div></div><div id="41528047" class="c"><input type="checkbox" id="c-41528047" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#41527354">prev</a><span>|</span><a href="#41527558">next</a><span>|</span><label class="collapse" for="c-41528047">[-]</label><label class="expand" for="c-41528047">[1 more]</label></div><br/><div class="children"><div class="content">I tried a problem I was looking at recently, to refactor a small rust crate to use one datatype instead of an enum, to help me understand the code better.  I found o1-mini made a decent attempt, but couldn&#x27;t provide error free code.  o1-preview was able to provide code that compiled and passed all but the test that is expected to fail, given the change I asked it to make.<p>This is the prompt I gave:<p>simplify this rust library by removing the different sized enums and only using the U8 size. For example MasksByByte is an enum, change it to be an alias for the U8 datatype. Also the u256 datatype isn&#x27;t required, we only want U8, so remove all references to U256 as well.<p>The original crate is trie-hard [1][2] and I forked it and put the models attempts in the fork [3].  I also quickly wrote it up at [4]<p>[1] <a href="https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;pingora-saving-compute-1-percent-at-a-time&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;pingora-saving-compute-1-percent...</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;cloudflare&#x2F;trie-hard">https:&#x2F;&#x2F;github.com&#x2F;cloudflare&#x2F;trie-hard</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;kpm&#x2F;trie-hard-simple&#x2F;tree&#x2F;main&#x2F;attempts">https:&#x2F;&#x2F;github.com&#x2F;kpm&#x2F;trie-hard-simple&#x2F;tree&#x2F;main&#x2F;attempts</a><p>[4] <a href="https:&#x2F;&#x2F;blog.reyem.dev&#x2F;post&#x2F;refactoring_rust_with_chatgpt-o1&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.reyem.dev&#x2F;post&#x2F;refactoring_rust_with_chatgpt-o1...</a></div><br/></div></div><div id="41527558" class="c"><input type="checkbox" id="c-41527558" checked=""/><div class="controls bullet"><span class="by">lukev</span><span>|</span><a href="#41528047">prev</a><span>|</span><a href="#41527268">next</a><span>|</span><label class="collapse" for="c-41527558">[-]</label><label class="expand" for="c-41527558">[28 more]</label></div><br/><div class="children"><div class="content">It&#x27;s interesting to note that there&#x27;s really two things going on here:<p>1. A LLM (probably a finetuned GPT-4o) trained specifically to read and emit good chain-of-thought prompts.<p>2. Runtime code that iteratively re-prompts the model with the chain of thought so far. This sounds like it includes loops, branches and backtracking. This is not &quot;the model&quot;, it&#x27;s regular code invoking the model. Interesting that OpenAI is making no attempt to clarify this.<p>I wonder where the real innovation here lies. I&#x27;ve done a few informal stabs with #2 and I have a pretty strong intuition (not proven yet) that given the right prompting&#x2F;metaprompting model you can do pretty well at this even with untuned LLMs. The end game here is complex agents with arbitrary continuous looping interleaved with RAG and tool use.<p>But OpenAI&#x27;s philosophy up until now has almost always been &quot;The bitter lesson is true, the model knows best, just put it in the model.&quot; So it&#x27;s also possible that the prompt loop has no special sauce and that the capabilities here do come mostly from the model itself.<p>Without being able to inspect the reasoning tokens, we can&#x27;t really get a lot of info about which is happening.</div><br/><div id="41528694" class="c"><input type="checkbox" id="c-41528694" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41528926">next</a><span>|</span><label class="collapse" for="c-41528694">[-]</label><label class="expand" for="c-41528694">[1 more]</label></div><br/><div class="children"><div class="content">This <i>is</i> the bitter lesson&#x2F;just put it in the model. They&#x27;re trying to figure out more ways of converting compute to intelligence now that they&#x27;re running out of text data: <a href="https:&#x2F;&#x2F;images.ctfassets.net&#x2F;kftzwdyauwt9&#x2F;7rMY55vLbGTlTiP9GdSOrf&#x2F;0944e1cde904e896bc5bc6f3da7f16b6&#x2F;compute-dark.png?w=3840&amp;q=80&amp;fm=webp" rel="nofollow">https:&#x2F;&#x2F;images.ctfassets.net&#x2F;kftzwdyauwt9&#x2F;7rMY55vLbGTlTiP9Gd...</a></div><br/></div></div><div id="41528926" class="c"><input type="checkbox" id="c-41528926" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41528694">prev</a><span>|</span><a href="#41529077">next</a><span>|</span><label class="collapse" for="c-41528926">[-]</label><label class="expand" for="c-41528926">[1 more]</label></div><br/><div class="children"><div class="content">If it really is Reinforcement Learning as they claim, it means there might not be any direct supervision on the &quot;thinking&quot; section of the output, just on the final answer.<p>Just like for Chess or Go you don&#x27;t train a supervised model by giving it the exact move it should do in each case, you use RL techniques to learn which moves are good based on end results of the game.<p>In practice, there probably is some supervision to enforce good style and methodology. But the key here is that it is able to learn good reasoning without (many) human examples, and find strategies to solve new problems via self-learning.<p>If that is the case it is indeed an important breakthrough.</div><br/></div></div><div id="41529077" class="c"><input type="checkbox" id="c-41529077" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41528926">prev</a><span>|</span><a href="#41527781">next</a><span>|</span><label class="collapse" for="c-41529077">[-]</label><label class="expand" for="c-41529077">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So it&#x27;s also possible that the prompt loop has no special sauce and that the capabilities here do come mostly from the model itself.<p>The prompt loop code often encodes intelligence&#x2F;information that the human developers tend to ignore during their evaluations of the solution. For example, if you add a filter for invalid json and repeatedly invoke the model until good json comes out, you are now carrying water for the LLM. The additional capabilities came from a manual coding exercise and additional money spent on a brute force search.</div><br/></div></div><div id="41527781" class="c"><input type="checkbox" id="c-41527781" checked=""/><div class="controls bullet"><span class="by">codelion</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41529077">prev</a><span>|</span><a href="#41527718">next</a><span>|</span><label class="collapse" for="c-41527781">[-]</label><label class="expand" for="c-41527781">[1 more]</label></div><br/><div class="children"><div class="content">I have also spent some time on 2) and implemented several approaches in this open source optimising llm proxy - <a href="https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;optillm">https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;optillm</a><p>In my experience it does work quite well, but we probably need different techniques for different tasks.</div><br/></div></div><div id="41527718" class="c"><input type="checkbox" id="c-41527718" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41527781">prev</a><span>|</span><a href="#41528170">next</a><span>|</span><label class="collapse" for="c-41527718">[-]</label><label class="expand" for="c-41527718">[4 more]</label></div><br/><div class="children"><div class="content">One aspect that’s not achievable is they discuss hiding the chain of thought in its raw form because the chains are allowed to be unaligned. This allows the model to operate without any artifacts from alignment and apply them in the post processing, more or less. This requires effectively root and you would need the unaligned weights.</div><br/><div id="41527763" class="c"><input type="checkbox" id="c-41527763" checked=""/><div class="controls bullet"><span class="by">lukev</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527718">parent</a><span>|</span><a href="#41528170">next</a><span>|</span><label class="collapse" for="c-41527763">[-]</label><label class="expand" for="c-41527763">[3 more]</label></div><br/><div class="children"><div class="content">Ok but this presses on a latent question: what do we mean by alignment?<p>Practically it&#x27;s come to mean just sanitization... &quot;don&#x27;t say something nasty or embarrassing to users.&quot; But that doesn&#x27;t apply here, the reasoning tokens are effectively just a debug log.<p>If alignment means &quot;conducting reasoning in alignment with human values&quot;, then misalignment in the reasoning phase could potentially be <i>obfuscated</i> and sanitized, participating in the conclusion but hidden. Having an &quot;unaligned&quot; model conduct the reasoning steps is potentially dangerous, if you believe that AI alignment can give rise to danger at all.<p>Personally I think that in practice alignment has come to mean just sanitization and it&#x27;s a fig leaf of an excuse for the real reason they are hiding the reasoning tokens: competitive advantage.</div><br/><div id="41528020" class="c"><input type="checkbox" id="c-41528020" checked=""/><div class="controls bullet"><span class="by">doe_eyes</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527763">parent</a><span>|</span><a href="#41527933">next</a><span>|</span><label class="collapse" for="c-41528020">[-]</label><label class="expand" for="c-41528020">[1 more]</label></div><br/><div class="children"><div class="content">Alignment started as a fairly nifty idea, but you can&#x27;t meaningfully test for it. We don&#x27;t have the tools to understand the internals of an LLM.<p>So yes, it morphed into the second best thing, brand safety - &quot;don&#x27;t say racist &#x2F; anti-vax stuff so that we don&#x27;t get bad press or get in trouble with the regulators&quot;.</div><br/></div></div><div id="41527933" class="c"><input type="checkbox" id="c-41527933" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527763">parent</a><span>|</span><a href="#41528020">prev</a><span>|</span><a href="#41528170">next</a><span>|</span><label class="collapse" for="c-41527933">[-]</label><label class="expand" for="c-41527933">[1 more]</label></div><br/><div class="children"><div class="content">The challenge is alignment ends up changing the models in ways that aren’t representative of the actual training set and as I understand it this generally lowers the performance even for aligned things. Further the decision to summarize the chains of thought includes the answers that wouldn’t pass alignment themselves without removal. From what I read the final output is aligned but could have considered unaligned COT. In fact because they’re in the context they’re necessarily changing the final output even if the final output complies with the alignment.  There are a few other “only root could do this,” which says yes anyone could implement these without secret sauce as long as they have a raw frontier model.</div><br/></div></div></div></div></div></div><div id="41528170" class="c"><input type="checkbox" id="c-41528170" checked=""/><div class="controls bullet"><span class="by">PiRho3141</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41527718">prev</a><span>|</span><a href="#41527757">next</a><span>|</span><label class="collapse" for="c-41528170">[-]</label><label class="expand" for="c-41528170">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t to execute code to have it backtrack. The LLM can inherently backtrack itself if trained to. It knows all the context provided to it and the output it has written already.</div><br/></div></div><div id="41527757" class="c"><input type="checkbox" id="c-41527757" checked=""/><div class="controls bullet"><span class="by">pillefitz</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41528170">prev</a><span>|</span><a href="#41527769">next</a><span>|</span><label class="collapse" for="c-41527757">[-]</label><label class="expand" for="c-41527757">[1 more]</label></div><br/><div class="children"><div class="content">The innovation lies in using RL to achieve 1.) and provide a simple interface to 2.)</div><br/></div></div><div id="41527769" class="c"><input type="checkbox" id="c-41527769" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41527757">prev</a><span>|</span><a href="#41527645">next</a><span>|</span><label class="collapse" for="c-41527769">[-]</label><label class="expand" for="c-41527769">[2 more]</label></div><br/><div class="children"><div class="content">Well, if LLMs are system 1, this difference would be building towards system 2.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thinking,_Fast_and_Slow" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Thinking,_Fast_and_Slow</a></div><br/><div id="41527788" class="c"><input type="checkbox" id="c-41527788" checked=""/><div class="controls bullet"><span class="by">lukev</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527769">parent</a><span>|</span><a href="#41527645">next</a><span>|</span><label class="collapse" for="c-41527788">[-]</label><label class="expand" for="c-41527788">[1 more]</label></div><br/><div class="children"><div class="content">Yes indeed, and personally if we have AGI I believe it will arise from multiple LLMs working in tandem with other types of machine learning, databases for &quot;memory&quot;, more traditional compute functions, and a connectivity layer between them all.<p>But to my knowledge, that&#x27;s not the kind of research OpenAI is doing. They seem mostly focused on training bigger and better models and seeking AGI through emergence in those.</div><br/></div></div></div></div><div id="41527645" class="c"><input type="checkbox" id="c-41527645" checked=""/><div class="controls bullet"><span class="by">deisteve</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41527769">prev</a><span>|</span><a href="#41527649">next</a><span>|</span><label class="collapse" for="c-41527645">[-]</label><label class="expand" for="c-41527645">[1 more]</label></div><br/><div class="children"><div class="content">this is why i became skeptical of openai&#x27;s claims<p>if they shared the COT the grift wont work<p>its just RL</div><br/></div></div><div id="41527649" class="c"><input type="checkbox" id="c-41527649" checked=""/><div class="controls bullet"><span class="by">salomonk_mur</span><span>|</span><a href="#41527558">parent</a><span>|</span><a href="#41527645">prev</a><span>|</span><a href="#41527268">next</a><span>|</span><label class="collapse" for="c-41527649">[-]</label><label class="expand" for="c-41527649">[14 more]</label></div><br/><div class="children"><div class="content">The innovation lies in making the whole loop available to an end user immediately, without them being a programmer. My grandma can build games using ChatGPT now.</div><br/><div id="41527673" class="c"><input type="checkbox" id="c-41527673" checked=""/><div class="controls bullet"><span class="by">Xeamek</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527649">parent</a><span>|</span><a href="#41527663">next</a><span>|</span><label class="collapse" for="c-41527673">[-]</label><label class="expand" for="c-41527673">[8 more]</label></div><br/><div class="children"><div class="content">No she can&#x27;t, comments likes yours are just made up nonsense that AI hype-mans and investors somehow convinced us are a fair opinions to have.</div><br/><div id="41527736" class="c"><input type="checkbox" id="c-41527736" checked=""/><div class="controls bullet"><span class="by">andreilys</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527673">parent</a><span>|</span><a href="#41527735">next</a><span>|</span><label class="collapse" for="c-41527736">[-]</label><label class="expand" for="c-41527736">[4 more]</label></div><br/><div class="children"><div class="content">Check out replit agents, they can make games and apps autonomously now</div><br/><div id="41527749" class="c"><input type="checkbox" id="c-41527749" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527736">parent</a><span>|</span><a href="#41528216">next</a><span>|</span><label class="collapse" for="c-41527749">[-]</label><label class="expand" for="c-41527749">[1 more]</label></div><br/><div class="children"><div class="content">Do you know of an example game I can play right now?</div><br/></div></div><div id="41528216" class="c"><input type="checkbox" id="c-41528216" checked=""/><div class="controls bullet"><span class="by">heyitsguay</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527736">parent</a><span>|</span><a href="#41527749">prev</a><span>|</span><a href="#41527735">next</a><span>|</span><label class="collapse" for="c-41528216">[-]</label><label class="expand" for="c-41528216">[2 more]</label></div><br/><div class="children"><div class="content">Practical challenge with a $250 prize: Make a 2D isometric HTML+JS game (dealer&#x27;s choice on library) in the next 48 hours that satisfies these modest random requirements:<p>A character walks around a big ornate classic library, pulling books from bookshelves looking for a special book that causes a shelf to rotate around and reveal a hidden room and treasure chest. The player can read the books and some are just filler but some have clues about the special book. If this can be done with art, animations, sound, UI, the usual stuff, I&#x27;ll believe the parent poster&#x27;s claim to be true.<p>As someone using LLM-based workflows daily to assist with personal and professional projects, I&#x27;ll wager $250 that this is not possible.</div><br/><div id="41528996" class="c"><input type="checkbox" id="c-41528996" checked=""/><div class="controls bullet"><span class="by">bardan</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41528216">parent</a><span>|</span><a href="#41527735">next</a><span>|</span><label class="collapse" for="c-41528996">[-]</label><label class="expand" for="c-41528996">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a comfy sequence in a larger game I would anticipate on replay. I put my own $250 on the table (given the prompt and process were forthcoming).</div><br/></div></div></div></div></div></div><div id="41527735" class="c"><input type="checkbox" id="c-41527735" checked=""/><div class="controls bullet"><span class="by">meowface</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527673">parent</a><span>|</span><a href="#41527736">prev</a><span>|</span><a href="#41527663">next</a><span>|</span><label class="collapse" for="c-41527735">[-]</label><label class="expand" for="c-41527735">[3 more]</label></div><br/><div class="children"><div class="content">While AI is overhyped by some people, the parent&#x27;s statement is not only true but was true long before o1 was released.</div><br/><div id="41527753" class="c"><input type="checkbox" id="c-41527753" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527735">parent</a><span>|</span><a href="#41527764">next</a><span>|</span><label class="collapse" for="c-41527753">[-]</label><label class="expand" for="c-41527753">[1 more]</label></div><br/><div class="children"><div class="content">Do you know of an example game by someone with no coding experience using an LLM?</div><br/></div></div></div></div></div></div><div id="41527663" class="c"><input type="checkbox" id="c-41527663" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527649">parent</a><span>|</span><a href="#41527673">prev</a><span>|</span><a href="#41527802">next</a><span>|</span><label class="collapse" for="c-41527663">[-]</label><label class="expand" for="c-41527663">[2 more]</label></div><br/><div class="children"><div class="content">What games have people made with ChatGPT? Do you have an example of a live, deployed game?</div><br/><div id="41528065" class="c"><input type="checkbox" id="c-41528065" checked=""/><div class="controls bullet"><span class="by">cdelsolar</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527663">parent</a><span>|</span><a href="#41527802">next</a><span>|</span><label class="collapse" for="c-41528065">[-]</label><label class="expand" for="c-41528065">[1 more]</label></div><br/><div class="children"><div class="content">Yes, a gazillion of them. Someone in a scrabble Facebook group made this entirely with ChatGPT: <a href="https:&#x2F;&#x2F;aboocher.github.io&#x2F;scrabble&#x2F;ingpractice.html" rel="nofollow">https:&#x2F;&#x2F;aboocher.github.io&#x2F;scrabble&#x2F;ingpractice.html</a></div><br/></div></div></div></div><div id="41527802" class="c"><input type="checkbox" id="c-41527802" checked=""/><div class="controls bullet"><span class="by">chuckledog</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527649">parent</a><span>|</span><a href="#41527663">prev</a><span>|</span><a href="#41527659">next</a><span>|</span><label class="collapse" for="c-41527802">[-]</label><label class="expand" for="c-41527802">[1 more]</label></div><br/><div class="children"><div class="content">Ada Lovelace is my grandma</div><br/></div></div><div id="41527659" class="c"><input type="checkbox" id="c-41527659" checked=""/><div class="controls bullet"><span class="by">deisteve</span><span>|</span><a href="#41527558">root</a><span>|</span><a href="#41527649">parent</a><span>|</span><a href="#41527802">prev</a><span>|</span><a href="#41527680">next</a><span>|</span><label class="collapse" for="c-41527659">[-]</label><label class="expand" for="c-41527659">[1 more]</label></div><br/><div class="children"><div class="content">what game has she made</div><br/></div></div></div></div></div></div><div id="41527268" class="c"><input type="checkbox" id="c-41527268" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41527558">prev</a><span>|</span><a href="#41528279">next</a><span>|</span><label class="collapse" for="c-41527268">[-]</label><label class="expand" for="c-41527268">[17 more]</label></div><br/><div class="children"><div class="content">&gt; the idea that I can run a complex prompt and have key details of how that prompt was evaluated hidden from me feels like a big step backwards.<p>As a developer, this is highly concerning, as it makes it much harder to debug where&#x2F;how the “reasoning” went wrong. The pricing is also silly, because I’m paying for tokens I can’t see.<p>As a user, I don’t really care. LLMs are already magic boxes and I usually only care about the end result, not the path to get there.<p>It will be interesting to see how this progresses, both at OpenAI and other foundation model builders.</div><br/><div id="41527518" class="c"><input type="checkbox" id="c-41527518" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41527268">parent</a><span>|</span><a href="#41528265">next</a><span>|</span><label class="collapse" for="c-41527518">[-]</label><label class="expand" for="c-41527518">[9 more]</label></div><br/><div class="children"><div class="content">&gt; As a user, I don’t really care.<p>Tell me: Just how is it fair for a user to pay for the reasoning tokens without actually seeing them? If they are not shared, the service can bill you anything they want for them!</div><br/><div id="41527669" class="c"><input type="checkbox" id="c-41527669" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527518">parent</a><span>|</span><a href="#41527974">next</a><span>|</span><label class="collapse" for="c-41527669">[-]</label><label class="expand" for="c-41527669">[2 more]</label></div><br/><div class="children"><div class="content">The simple answer is: I don&#x27;t care. I&#x27;ll statistically figure out what the typical total cost per call is from experience, and that&#x27;s what matters. Who cares if they lie about it, if the model&#x27;s cost per call fits my budget?<p>If it starts costing $1 per call, and that&#x27;s too high, then I just won&#x27;t use it commercially. Whether it was $1 because they inflated the token count or because it just actually took a lot of tokens to do its reasoning isn&#x27;t really material to my economic decision.</div><br/><div id="41529344" class="c"><input type="checkbox" id="c-41529344" checked=""/><div class="controls bullet"><span class="by">lucianbr</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527669">parent</a><span>|</span><a href="#41527974">next</a><span>|</span><label class="collapse" for="c-41529344">[-]</label><label class="expand" for="c-41529344">[1 more]</label></div><br/><div class="children"><div class="content">The thing is it might increase in cost after you&#x27;ve decided to use it commercially, and have invested a lot of time and resources in it. Now it&#x27;s very hard to move to something else, but very easy for OpenAI to increase your cost arbitrarily. The statistics you made are not binding for them.</div><br/></div></div></div></div><div id="41527974" class="c"><input type="checkbox" id="c-41527974" checked=""/><div class="controls bullet"><span class="by">inerte</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527518">parent</a><span>|</span><a href="#41527669">prev</a><span>|</span><a href="#41527681">next</a><span>|</span><label class="collapse" for="c-41527974">[-]</label><label class="expand" for="c-41527974">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI could have also figured out the average number of extra output tokens, and put a markup in overall API costs. As a user, I wouldn’t care either, because the price would mostly be the same.</div><br/></div></div><div id="41527681" class="c"><input type="checkbox" id="c-41527681" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527518">parent</a><span>|</span><a href="#41527974">prev</a><span>|</span><a href="#41527603">next</a><span>|</span><label class="collapse" for="c-41527681">[-]</label><label class="expand" for="c-41527681">[1 more]</label></div><br/><div class="children"><div class="content">The person you are replying to points this out. They make a distinction between developers and users. An end user on a monthly subscription plan doesn’t care about how much compute happens for their chat.</div><br/></div></div><div id="41527603" class="c"><input type="checkbox" id="c-41527603" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527518">parent</a><span>|</span><a href="#41527681">prev</a><span>|</span><a href="#41528199">next</a><span>|</span><label class="collapse" for="c-41527603">[-]</label><label class="expand" for="c-41527603">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI’s answer to this would be, “Okay then, don’t use it.”</div><br/></div></div><div id="41528199" class="c"><input type="checkbox" id="c-41528199" checked=""/><div class="controls bullet"><span class="by">potatoman22</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527518">parent</a><span>|</span><a href="#41527603">prev</a><span>|</span><a href="#41528141">next</a><span>|</span><label class="collapse" for="c-41528199">[-]</label><label class="expand" for="c-41528199">[1 more]</label></div><br/><div class="children"><div class="content">If the output alone is high enough quality, it&#x27;s worth paying extra.</div><br/></div></div><div id="41528141" class="c"><input type="checkbox" id="c-41528141" checked=""/><div class="controls bullet"><span class="by">halJordan</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527518">parent</a><span>|</span><a href="#41528199">prev</a><span>|</span><a href="#41528265">next</a><span>|</span><label class="collapse" for="c-41528141">[-]</label><label class="expand" for="c-41528141">[2 more]</label></div><br/><div class="children"><div class="content">Yeah it is fair. You don&#x27;t pay a lawyer for 40s of work expecting to see all the research between your consult and the document. You don&#x27;t pay a cook for a meal and expect to sit and interrogate all the ingredients and the oven temperature.</div><br/><div id="41528352" class="c"><input type="checkbox" id="c-41528352" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41528141">parent</a><span>|</span><a href="#41528265">next</a><span>|</span><label class="collapse" for="c-41528352">[-]</label><label class="expand" for="c-41528352">[1 more]</label></div><br/><div class="children"><div class="content">Actually, if a lawyer is billing you by the minute, then yes, you are entitled to a detailed breakdown. If the lawyer is billing you by the job, then no.</div><br/></div></div></div></div></div></div><div id="41527334" class="c"><input type="checkbox" id="c-41527334" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#41527268">parent</a><span>|</span><a href="#41528265">prev</a><span>|</span><a href="#41527469">next</a><span>|</span><label class="collapse" for="c-41527334">[-]</label><label class="expand" for="c-41527334">[1 more]</label></div><br/><div class="children"><div class="content">More opportunity for competitors to differentiate.<p>OpenAI doesn&#x27;t really have a moat. This isn&#x27;t payments or SMS where only Stripe or Twilio were trying to win the market. Everybody and their brother is trying to build an LLM business.<p>Grab some researchers, put some compute dollars in, and out comes a product.<p>Everyone wants this market. It&#x27;s absurdly good for buyers.</div><br/></div></div><div id="41527325" class="c"><input type="checkbox" id="c-41527325" checked=""/><div class="controls bullet"><span class="by">bamboozled</span><span>|</span><a href="#41527268">parent</a><span>|</span><a href="#41527469">prev</a><span>|</span><a href="#41527329">next</a><span>|</span><label class="collapse" for="c-41527325">[-]</label><label class="expand" for="c-41527325">[1 more]</label></div><br/><div class="children"><div class="content">&quot;trust us, we&#x27;re using your tokens as efficiently as possible&quot;</div><br/></div></div><div id="41527329" class="c"><input type="checkbox" id="c-41527329" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41527268">parent</a><span>|</span><a href="#41527325">prev</a><span>|</span><a href="#41528279">next</a><span>|</span><label class="collapse" for="c-41527329">[-]</label><label class="expand" for="c-41527329">[3 more]</label></div><br/><div class="children"><div class="content">&gt; As a user, I don’t really care.<p>As a user, whether of ChatGPT or of the API, I absolutely do care, so I can modify and tune my prompt with the necessary clarifications.<p>My suspicion is that the reason for hiding the reasoning tokens is to prevent other companies from creating a big CoT reasoning dataset using o1.<p>It is anti-competitive behavior. If a user is paying through the nose for the reasoning tokens, and yes they are, the user deserves to be able to see them.</div><br/><div id="41527338" class="c"><input type="checkbox" id="c-41527338" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527329">parent</a><span>|</span><a href="#41528279">next</a><span>|</span><label class="collapse" for="c-41527338">[-]</label><label class="expand" for="c-41527338">[2 more]</label></div><br/><div class="children"><div class="content">&gt;My suspicion is that the reason for hiding the reasoning tokens is to prevent other companies from creating a big CoT reasoning dataset using o1.<p>I mean...they say as much</div><br/><div id="41527343" class="c"><input type="checkbox" id="c-41527343" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#41527268">root</a><span>|</span><a href="#41527338">parent</a><span>|</span><a href="#41528279">next</a><span>|</span><label class="collapse" for="c-41527343">[-]</label><label class="expand" for="c-41527343">[1 more]</label></div><br/><div class="children"><div class="content">Once again true to their name.</div><br/></div></div></div></div></div></div></div></div><div id="41528279" class="c"><input type="checkbox" id="c-41528279" checked=""/><div class="controls bullet"><span class="by">freediver</span><span>|</span><a href="#41527268">prev</a><span>|</span><a href="#41528891">next</a><span>|</span><label class="collapse" for="c-41528279">[-]</label><label class="expand" for="c-41528279">[3 more]</label></div><br/><div class="children"><div class="content">Not seeing major advance in quality with o1, but seeing major negative impact on cost and latency.<p>Kagi LLM benchmarking project:<p><a href="https:&#x2F;&#x2F;help.kagi.com&#x2F;kagi&#x2F;ai&#x2F;llm-benchmark.html" rel="nofollow">https:&#x2F;&#x2F;help.kagi.com&#x2F;kagi&#x2F;ai&#x2F;llm-benchmark.html</a></div><br/><div id="41529107" class="c"><input type="checkbox" id="c-41529107" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#41528279">parent</a><span>|</span><a href="#41528891">next</a><span>|</span><label class="collapse" for="c-41529107">[-]</label><label class="expand" for="c-41529107">[2 more]</label></div><br/><div class="children"><div class="content">Kagi is most likely evaluating it mainly on deriving an answer for the user from search result snippets. Indeed, GPT-4o is plenty good at this already, and o1 would only perform better on particular types of hard requests, while being so much slower.<p>If you look at Appendix A in the o1 post [1], this becomes quite clear. There&#x27;s a huge jump in performance in &quot;puzzle&quot; tasks like competitive maths or programming. But the difference on everything else is much less significant, and this evaluation is still focused on reasoning tasks.<p>The human preference chart [1] also clearly shows that it doesn&#x27;t feel that much better to use, hence the overall reaction.<p>Everyone is complaining about exaggerated marketing, and it&#x27;s true, but if you take the time to read what they wrote beyond the shallow ads, they are being somewhat honest about what this is.<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;learning-to-reason-with-llms&#x2F;</a></div><br/><div id="41529278" class="c"><input type="checkbox" id="c-41529278" checked=""/><div class="controls bullet"><span class="by">throwaway40602</span><span>|</span><a href="#41528279">root</a><span>|</span><a href="#41529107">parent</a><span>|</span><a href="#41528891">next</a><span>|</span><label class="collapse" for="c-41529278">[-]</label><label class="expand" for="c-41529278">[1 more]</label></div><br/><div class="children"><div class="content">hey buddy, you&#x27;re talking to owner of kagi, and the kagi benchmark is a traditional one</div><br/></div></div></div></div></div></div><div id="41528891" class="c"><input type="checkbox" id="c-41528891" checked=""/><div class="controls bullet"><span class="by">zurfer</span><span>|</span><a href="#41528279">prev</a><span>|</span><a href="#41529245">next</a><span>|</span><label class="collapse" for="c-41528891">[-]</label><label class="expand" for="c-41528891">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I asked on Twitter for examples of prompts that people had found which failed on GPT-4o but worked on o1-preview.<p>it seems trivial, but I tried for more than 2 hours in the past to get gpt4 to play tic-tac-toe optimally and failed (CoT prompt,etc.). The result were too many illegal moves and absolutely no optimal strategy.<p>o1-preview can do it really well [1]<p>However, when I use a non-standard grid (3x5) it fails to play optimally. But it makes legal moves and it recognized I had won. [2]<p>My conclusion at the time was that either &quot;spatial reasoning&quot; doesn&#x27;t work and&#x2F;or planning is needed. Now I am not so sure, if they just included tic-tac-toe in the training data, or &quot;spatial reasoning&quot; is limited.<p>[1] <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;e&#x2F;66e3e784-26d4-8013-889b-f56a7fedc590" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;e&#x2F;66e3e784-26d4-8013-889b-f56a7fed...</a>
[2] <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;e&#x2F;66e3eae0-2d38-8013-b900-50e6f7923876" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;e&#x2F;66e3eae0-2d38-8013-b900-50e6f792...</a></div><br/><div id="41529056" class="c"><input type="checkbox" id="c-41529056" checked=""/><div class="controls bullet"><span class="by">biscuit1v9</span><span>|</span><a href="#41528891">parent</a><span>|</span><a href="#41529245">next</a><span>|</span><label class="collapse" for="c-41529056">[-]</label><label class="expand" for="c-41529056">[1 more]</label></div><br/><div class="children"><div class="content">Link to conversation 1 and 2 doesn&#x27;t work. Update it please!</div><br/></div></div></div></div><div id="41529245" class="c"><input type="checkbox" id="c-41529245" checked=""/><div class="controls bullet"><span class="by">andrew_eu</span><span>|</span><a href="#41528891">prev</a><span>|</span><a href="#41527464">next</a><span>|</span><label class="collapse" for="c-41529245">[-]</label><label class="expand" for="c-41529245">[1 more]</label></div><br/><div class="children"><div class="content">I thought with this chain-of-thought approach the model might be better suited to solve a logic puzzle, e.g. ZebraPuzzles [0]. It produced a ton of &quot;reasoning&quot; tokens but hallucinated more than half of the solution with names&#x2F;fields that weren&#x27;t available. Not a systematic evaluation, but it seems like a degradation from 4o-mini. Perhaps it does better with code reasoning problems though -- these logic puzzles are essentially contrived to require deductive reasoning.<p>[0] <a href="https:&#x2F;&#x2F;zebrapuzzles.com" rel="nofollow">https:&#x2F;&#x2F;zebrapuzzles.com</a></div><br/></div></div><div id="41527464" class="c"><input type="checkbox" id="c-41527464" checked=""/><div class="controls bullet"><span class="by">helmsb</span><span>|</span><a href="#41529245">prev</a><span>|</span><a href="#41528623">next</a><span>|</span><label class="collapse" for="c-41527464">[-]</label><label class="expand" for="c-41527464">[5 more]</label></div><br/><div class="children"><div class="content">I did a few tests and asked it some legal questions. 4o gave me the correct answer immediately.<p>o1 preview gave a much more in depth but completely wrong answer. It took 5 follow ups to get it to recognize that it hallucinated a non-existent law</div><br/><div id="41529043" class="c"><input type="checkbox" id="c-41529043" checked=""/><div class="controls bullet"><span class="by">AhtiK</span><span>|</span><a href="#41527464">parent</a><span>|</span><a href="#41527703">next</a><span>|</span><label class="collapse" for="c-41529043">[-]</label><label class="expand" for="c-41529043">[1 more]</label></div><br/><div class="children"><div class="content">That is very interesting. Would you mind testing the same prompt with Claude Sonnet 3.5 and Opus? If not available to you, would you be willing to share the prompt&#x2F;question? Thank you.</div><br/></div></div><div id="41527703" class="c"><input type="checkbox" id="c-41527703" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41527464">parent</a><span>|</span><a href="#41529043">prev</a><span>|</span><a href="#41528623">next</a><span>|</span><label class="collapse" for="c-41527703">[-]</label><label class="expand" for="c-41527703">[3 more]</label></div><br/><div class="children"><div class="content">This is interesting since they claim it does well on STEM questions, which I’d assume would be a similar level of reasoning complexity for a human.</div><br/><div id="41528057" class="c"><input type="checkbox" id="c-41528057" checked=""/><div class="controls bullet"><span class="by">waveBidder</span><span>|</span><a href="#41527464">root</a><span>|</span><a href="#41527703">parent</a><span>|</span><a href="#41528781">next</a><span>|</span><label class="collapse" for="c-41528057">[-]</label><label class="expand" for="c-41528057">[1 more]</label></div><br/><div class="children"><div class="content">A difficult to guess fraction of all of these results are training to the test in various forms</div><br/></div></div><div id="41528781" class="c"><input type="checkbox" id="c-41528781" checked=""/><div class="controls bullet"><span class="by">abernard1</span><span>|</span><a href="#41527464">root</a><span>|</span><a href="#41527703">parent</a><span>|</span><a href="#41528057">prev</a><span>|</span><a href="#41528623">next</a><span>|</span><label class="collapse" for="c-41528781">[-]</label><label class="expand" for="c-41528781">[1 more]</label></div><br/><div class="children"><div class="content">This is an interesting one because math is doing so much of the heavy lifting.  And symbolic math has a far smaller representational space than numerical math.<p>There is one other wonderful thing about symbolic math, the glorious &#x27;=&#x27; sign.  It&#x27;s structured everywhere from top-to-bottom, left-to-right, which is amenable to the next token prediction behavior and multi-attention heads of transformer based LLMs.<p>My guess is that problem statement formation into an equation is as difficult of a problem for these as actually running through the equations. However, having taken the Physics GRE, and knowing they try for parity of difficulty between years (even though they normalize it), the problems are fairly standard and have permutations of a problem type between the years.<p>This is not to diminish how cool this is, just that standardized tests do have an element of predictability to them.  I find this result actually neat though; it&#x27;s an actual qualitative improvement over non-CoT LLMs, even if things like Mathematica can do the steps more reliably post problem formation.  I think that judiciously used, this is a valuable feature.</div><br/></div></div></div></div></div></div><div id="41528623" class="c"><input type="checkbox" id="c-41528623" checked=""/><div class="controls bullet"><span class="by">gloosx</span><span>|</span><a href="#41527464">prev</a><span>|</span><a href="#41528469">next</a><span>|</span><label class="collapse" for="c-41528623">[-]</label><label class="expand" for="c-41528623">[15 more]</label></div><br/><div class="children"><div class="content">Just leaving here an example of the &quot;smart chain of thought&quot;, which took the model about 20s of thinking. The problem I asked was – &quot;what 3 numbers give the same result when divided and added together&quot;. Seems the current state of art is that basic school problems are too hard yet even for this new chain-of-marketing.<p><pre><code>  Let&#x27;s think through this step-by-step:

  We&#x27;re looking for three numbers a, b, and c where:
    (a &#x2F; b) &#x2F; c = a + b + c
  This equation is a bit trickier than the multiplication one, because division is 
  not commutative (the order matters). We need to be careful about how we 
  interpret (a &#x2F; b) &#x2F; c.
  Let&#x27;s try to simplify the left side:
    (a &#x2F; b) &#x2F; c = a &#x2F; (b * c)
  So our equation becomes:
    a &#x2F; (b * c) = a + b + c
  This is a challenging equation to solve directly. Let&#x27;s try some numbers:

  If we use 1, 2, 3 like before:
    1 &#x2F; (2 * 3) = 1&#x2F;6 ≠ 1 + 2 + 3 = 6
  Let&#x27;s try smaller numbers, like fractions:
    1&#x2F;2, 1&#x2F;3, 1&#x2F;6


  Let&#x27;s check:
    (1&#x2F;2 &#x2F; 1&#x2F;3) &#x2F; 1&#x2F;6 = (1&#x2F;2 * 3) &#x2F; 1&#x2F;6 = 3&#x2F;2 * 6 = 9
    1&#x2F;2 + 1&#x2F;3 + 1&#x2F;6 = 3&#x2F;6 + 2&#x2F;6 + 1&#x2F;6 = 6&#x2F;6 = 1
  This doesn&#x27;t work either. In fact, it&#x27;s impossible to find three positive numbers that satisfy this equation.
  However, we can find a solution if we allow negative numbers:
    3, -2, and 6
  Let&#x27;s verify:
    (3 &#x2F; -2) &#x2F; 6 = -1&#x2F;4
    3 + (-2) + 6 = 7
    -1&#x2F;4 ≠ 7, so this doesn&#x27;t work either.

  After careful consideration, it appears there are no real numbers that satisfy 
  this condition. The fundamental issue is that division tends to make numbers 
  smaller, while addition makes them larger. It&#x27;s mathematically impossible to 
  find three numbers where these operations result in the same value.
</code></pre>
nuff said</div><br/><div id="41528666" class="c"><input type="checkbox" id="c-41528666" checked=""/><div class="controls bullet"><span class="by">bachback</span><span>|</span><a href="#41528623">parent</a><span>|</span><a href="#41528989">next</a><span>|</span><label class="collapse" for="c-41528666">[-]</label><label class="expand" for="c-41528666">[10 more]</label></div><br/><div class="children"><div class="content">what does divided together mean? maybe your question doesn&#x27;t have a good answer, because the question is not formulated well enough.</div><br/><div id="41528695" class="c"><input type="checkbox" id="c-41528695" checked=""/><div class="controls bullet"><span class="by">gloosx</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528666">parent</a><span>|</span><a href="#41528989">next</a><span>|</span><label class="collapse" for="c-41528695">[-]</label><label class="expand" for="c-41528695">[9 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a classical school question, word-by-word, except multiplication is replaced by division</div><br/><div id="41528809" class="c"><input type="checkbox" id="c-41528809" checked=""/><div class="controls bullet"><span class="by">c-fe</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528695">parent</a><span>|</span><a href="#41529052">next</a><span>|</span><label class="collapse" for="c-41528809">[-]</label><label class="expand" for="c-41528809">[5 more]</label></div><br/><div class="children"><div class="content">With multiplication the question makes sense due to the commutative property but division does not have that so the question becomes ambiguous... And now I see that the model even points this out.</div><br/><div id="41528987" class="c"><input type="checkbox" id="c-41528987" checked=""/><div class="controls bullet"><span class="by">gloosx</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528809">parent</a><span>|</span><a href="#41529125">next</a><span>|</span><label class="collapse" for="c-41528987">[-]</label><label class="expand" for="c-41528987">[3 more]</label></div><br/><div class="children"><div class="content">There is no ambiguity, the problem is that three numbers, divided together, without the order specified, must be equal to their sum.<p>You can find solutions for a &#x2F; b &#x2F; c, or b &#x2F; c &#x2F; a, or c &#x2F; a &#x2F; b, any combination of them and the solution will be correct according to the problem description.<p>Besides, what&#x27;s does it even has to do with it concluding with confidence:
&quot;The fundamental issue is that division tends to make numbers 
smaller. It&#x27;s mathematically impossible to 
find three numbers where these operations result in the same value.&quot;?</div><br/><div id="41529274" class="c"><input type="checkbox" id="c-41529274" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528987">parent</a><span>|</span><a href="#41529095">next</a><span>|</span><label class="collapse" for="c-41529274">[-]</label><label class="expand" for="c-41529274">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There is no ambiguity<p>Yet you give three different interpretations:<p>&gt; You can find solutions for a &#x2F; b &#x2F; c, or b &#x2F; c &#x2F; a, or c &#x2F; a &#x2F; b<p>This is a clear case of ambiguity.<p>Even the classic question is ambiguous: &quot;Which 3 numbers give the same result when added or multiplied together?&quot;<p>Lets say the three numbers are x, y and z and the result is r. A valid interpretation would be to multiply&#x2F;add every <i>pair</i> of numbers:<p><pre><code>    x * y = r
    y * z = r
    x * z = r
    x + y = r
    y + z = r
    x + z = r
</code></pre>
However, I do not think that this ambiguity is the reason why OpenAI o1 fails here. It simply started with an untractable approach to solve this problem (plugging in random numbers) and did not attempt a more promising approach because it was not trained to do so.</div><br/></div></div></div></div><div id="41529125" class="c"><input type="checkbox" id="c-41529125" checked=""/><div class="controls bullet"><span class="by">kaoD</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528809">parent</a><span>|</span><a href="#41528987">prev</a><span>|</span><a href="#41529052">next</a><span>|</span><label class="collapse" for="c-41529125">[-]</label><label class="expand" for="c-41529125">[1 more]</label></div><br/><div class="children"><div class="content">Does the commutative property change anything here? A, B and C are not constrained in any way to each other, so they can be in any order you want anyways...<p>Moreover, sum is commutative so it doesn&#x27;t matter what order the division is in since a&#x2F;b&#x2F;c = a+b+c = c+a+b = ...<p>So I&#x27;d say that the model pointing this out is actually a mistake and it managed to trick you. Classic LLM stuff: spit out wrong stuff in a convincing manner.</div><br/></div></div></div></div><div id="41529052" class="c"><input type="checkbox" id="c-41529052" checked=""/><div class="controls bullet"><span class="by">Fraterkes</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528695">parent</a><span>|</span><a href="#41528809">prev</a><span>|</span><a href="#41528887">next</a><span>|</span><label class="collapse" for="c-41529052">[-]</label><label class="expand" for="c-41529052">[1 more]</label></div><br/><div class="children"><div class="content">Order doesn&#x27;t matter with multiplication (eg: (20 * 5) * 2 == (5 * 2) * 20) but it obviously does with division ((20&#x2F;5)&#x2F;2 != (2&#x2F;5)&#x2F;20) so the question doesn&#x27;t make sense. It&#x27;s you making grade-school level mistakes here.</div><br/></div></div></div></div></div></div><div id="41528989" class="c"><input type="checkbox" id="c-41528989" checked=""/><div class="controls bullet"><span class="by">poochkoishi728</span><span>|</span><a href="#41528623">parent</a><span>|</span><a href="#41528666">prev</a><span>|</span><a href="#41528644">next</a><span>|</span><label class="collapse" for="c-41528989">[-]</label><label class="expand" for="c-41528989">[1 more]</label></div><br/><div class="children"><div class="content">{0, -1, 1}. Divide (in order): 0&#x2F;-1 -&gt; 0&#x2F;1 -&gt; 0. Add: 0 - 1 + 1 = 0</div><br/></div></div><div id="41528644" class="c"><input type="checkbox" id="c-41528644" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#41528623">parent</a><span>|</span><a href="#41528989">prev</a><span>|</span><a href="#41528981">next</a><span>|</span><label class="collapse" for="c-41528644">[-]</label><label class="expand" for="c-41528644">[2 more]</label></div><br/><div class="children"><div class="content">Claude 3.5 seems to get it, (thought the method is not what I would use?) -<p>———<p>Let&#x27;s think through this step-by-step:<p><pre><code>  We&#x27;re looking for three numbers a, b, and c where:
    (a &#x2F; b) &#x2F; c = a + b + c
</code></pre>
————<p>Certainly, let&#x27;s approach this problem step-by-step:<p>1) First, let&#x27;s write out our equation:
   (a &#x2F; b) &#x2F; c = a + b + c<p>2) To simplify the left side, we can rewrite it as:
   a &#x2F; (b * c) = a + b + c<p>3) Now, to solve this, we need to find values for a, b, and c that satisfy this equation. Let&#x27;s start by considering some properties:
   - All numbers must be positive (because we&#x27;re dividing by b and c)
   - The left side is a fraction, while the right side is a sum<p>4) One way to approach this is to try to make the left side equal to 1, which would mean:
   a = b * c<p>5) If we substitute this into our original equation:
   1 = b * c + b + c<p>6) Now we&#x27;re looking for two numbers b and c where their product plus their sum equals 1.<p>7) Let&#x27;s try some small numbers:
   If b = 1&#x2F;2 and c = 1&#x2F;2:
   (1&#x2F;2 * 1&#x2F;2) + 1&#x2F;2 + 1&#x2F;2 = 1&#x2F;4 + 1 = 5&#x2F;4 ≠ 1<p>8) Let&#x27;s try smaller numbers:
   If b = 1&#x2F;3 and c = 1&#x2F;3:
   (1&#x2F;3 * 1&#x2F;3) + 1&#x2F;3 + 1&#x2F;3 = 1&#x2F;9 + 2&#x2F;3 = 7&#x2F;9 ≠ 1<p>9) Even smaller:
   If b = 1&#x2F;4 and c = 1&#x2F;4:
   (1&#x2F;4 * 1&#x2F;4) + 1&#x2F;4 + 1&#x2F;4 = 1&#x2F;16 + 1&#x2F;2 = 9&#x2F;16 ≠ 1<p>10) We&#x27;re getting closer. Let&#x27;s try b = 1&#x2F;4 and c = 1&#x2F;3:
    (1&#x2F;4 * 1&#x2F;3) + 1&#x2F;4 + 1&#x2F;3 = 1&#x2F;12 + 1&#x2F;4 + 1&#x2F;3 = 1&#x2F;12 + 3&#x2F;12 + 4&#x2F;12 = 8&#x2F;12 = 2&#x2F;3 ≠ 1<p>11) Let&#x27;s adjust further. If b = 1&#x2F;3 and c = 1&#x2F;4:
    (1&#x2F;3 * 1&#x2F;4) + 1&#x2F;3 + 1&#x2F;4 = 1&#x2F;12 + 1&#x2F;3 + 1&#x2F;4 = 1&#x2F;12 + 4&#x2F;12 + 3&#x2F;12 = 8&#x2F;12 = 2&#x2F;3 ≠ 1<p>12) One more try. Let&#x27;s use b = 1&#x2F;2 and c = 1&#x2F;3:
    (1&#x2F;2 * 1&#x2F;3) + 1&#x2F;2 + 1&#x2F;3 = 1&#x2F;6 + 1&#x2F;2 + 1&#x2F;3 = 1&#x2F;6 + 3&#x2F;6 + 2&#x2F;6 = 6&#x2F;6 = 1<p>13) We found it! When b = 1&#x2F;2 and c = 1&#x2F;3, the equation holds.<p>14) Remember, we set a = b * c, so a = 1&#x2F;2 * 1&#x2F;3 = 1&#x2F;6<p>Therefore, one solution to the equation (a &#x2F; b) &#x2F; c = a + b + c is:
a = 1&#x2F;6, b = 1&#x2F;2, and c = 1&#x2F;3<p>We can verify:
(1&#x2F;6 &#x2F; 1&#x2F;2) &#x2F; 1&#x2F;3 = (1&#x2F;3) &#x2F; 1&#x2F;3 = 1
1&#x2F;6 + 1&#x2F;2 + 1&#x2F;3 = 1&#x2F;6 + 3&#x2F;6 + 2&#x2F;6 = 1<p>This solution satisfies the equation.<p>——<p>In general I have been seeing much better results across the board with Claude, and I wonder if open ai has been more focused on scale while Claude is doing larger more expensive models to get ahead.</div><br/><div id="41528712" class="c"><input type="checkbox" id="c-41528712" checked=""/><div class="controls bullet"><span class="by">gloosx</span><span>|</span><a href="#41528623">root</a><span>|</span><a href="#41528644">parent</a><span>|</span><a href="#41528981">next</a><span>|</span><label class="collapse" for="c-41528712">[-]</label><label class="expand" for="c-41528712">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s crazy that it just tries to bruteforce it by picking numbers, and in your case it took more steps before concluding a success&#x2F;failure, which seems quite to be random to me, or at least dependent on something.<p>What&#x27;s clear is that it doesn&#x27;t have any idea about mathematical deduction and induction – a real chain-of-thought which kids learn in 5th grade.</div><br/></div></div></div></div><div id="41528981" class="c"><input type="checkbox" id="c-41528981" checked=""/><div class="controls bullet"><span class="by">febusravenga</span><span>|</span><a href="#41528623">parent</a><span>|</span><a href="#41528644">prev</a><span>|</span><a href="#41528469">next</a><span>|</span><label class="collapse" for="c-41528981">[-]</label><label class="expand" for="c-41528981">[1 more]</label></div><br/><div class="children"><div class="content">(0&#x2F;-1)&#x2F;1=0<p>0+(-1)+1=0</div><br/></div></div></div></div><div id="41528469" class="c"><input type="checkbox" id="c-41528469" checked=""/><div class="controls bullet"><span class="by">ifdefdebug</span><span>|</span><a href="#41528623">prev</a><span>|</span><a href="#41529221">next</a><span>|</span><label class="collapse" for="c-41528469">[-]</label><label class="expand" for="c-41528469">[3 more]</label></div><br/><div class="children"><div class="content">The use of the word reasoning here... OpenAI sounds like a company that created a frog which jumps higher and greater distances than the previous breed - and now they try to sell it as one step further toward flying.</div><br/><div id="41528963" class="c"><input type="checkbox" id="c-41528963" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#41528469">parent</a><span>|</span><a href="#41529221">next</a><span>|</span><label class="collapse" for="c-41528963">[-]</label><label class="expand" for="c-41528963">[2 more]</label></div><br/><div class="children"><div class="content">Can the frog reach escape velocity when jumping? I guess we&#x27;ll find out sooner or later...</div><br/><div id="41529343" class="c"><input type="checkbox" id="c-41529343" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#41528469">root</a><span>|</span><a href="#41528963">parent</a><span>|</span><a href="#41529221">next</a><span>|</span><label class="collapse" for="c-41529343">[-]</label><label class="expand" for="c-41529343">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Can the frog reach escape velocity when jumping?<p>Such a frog would require extremely powerful legs. I think that flapping with much weaker legs would already produce enough lift to fly if we figure out how. (This is also an analogy.)</div><br/></div></div></div></div></div></div><div id="41529221" class="c"><input type="checkbox" id="c-41529221" checked=""/><div class="controls bullet"><span class="by">anentropic</span><span>|</span><a href="#41528469">prev</a><span>|</span><a href="#41529243">next</a><span>|</span><label class="collapse" for="c-41529221">[-]</label><label class="expand" for="c-41529221">[1 more]</label></div><br/><div class="children"><div class="content">Are there any benchmarks which compare existing LLMs using langchain-style multi-step reasoning?<p>The new OpenAI model shows a big improvement on some benchmarks over GPT4 one-shot chain-of-thought, but what about vs systems doing something more similar to what presumably this is?</div><br/></div></div><div id="41529243" class="c"><input type="checkbox" id="c-41529243" checked=""/><div class="controls bullet"><span class="by">alexbenton111</span><span>|</span><a href="#41529221">prev</a><span>|</span><a href="#41528325">next</a><span>|</span><label class="collapse" for="c-41529243">[-]</label><label class="expand" for="c-41529243">[1 more]</label></div><br/><div class="children"><div class="content">I am not sure how more advanced this new model is than previous GPT-4o, but at least this new model can correctly figure out that 9.9 is larger than 9.11.</div><br/></div></div><div id="41528325" class="c"><input type="checkbox" id="c-41528325" checked=""/><div class="controls bullet"><span class="by">dkga</span><span>|</span><a href="#41529243">prev</a><span>|</span><a href="#41528222">next</a><span>|</span><label class="collapse" for="c-41528325">[-]</label><label class="expand" for="c-41528325">[4 more]</label></div><br/><div class="children"><div class="content">Judging by this thread, surely a part of OpenAI’s business model is to release models with somewhat grey-area outlandish claims, then sit and wait for people to test it out paying top dollars for tokens.</div><br/><div id="41528453" class="c"><input type="checkbox" id="c-41528453" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41528325">parent</a><span>|</span><a href="#41529215">next</a><span>|</span><label class="collapse" for="c-41528453">[-]</label><label class="expand" for="c-41528453">[2 more]</label></div><br/><div class="children"><div class="content">They’re not charging anything for o1 right now - it’s running as a free beta.<p>EDIT: Actually I’m not sure I’m correct about that - I’ve seen screenshots that indicate the API model is being charged for here: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;deepaiwriter&#x2F;status&#x2F;1834460643228897512" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;deepaiwriter&#x2F;status&#x2F;1834460643228897512</a><p>The value OpenAI get here is that people effectively run a massively parallel brute force attack against the new models to figure out exactly what they can and can’t do.</div><br/><div id="41528664" class="c"><input type="checkbox" id="c-41528664" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#41528325">root</a><span>|</span><a href="#41528453">parent</a><span>|</span><a href="#41529215">next</a><span>|</span><label class="collapse" for="c-41528664">[-]</label><label class="expand" for="c-41528664">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The value OpenAI get here is that people effectively run a massively parallel brute force attack against the new models to figure out exactly what they can and can’t do.<p>I&#x27;m pretty sure the value they get is the money you pay.</div><br/></div></div></div></div><div id="41529215" class="c"><input type="checkbox" id="c-41529215" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#41528325">parent</a><span>|</span><a href="#41528453">prev</a><span>|</span><a href="#41528222">next</a><span>|</span><label class="collapse" for="c-41529215">[-]</label><label class="expand" for="c-41529215">[1 more]</label></div><br/><div class="children"><div class="content">I would guess most of the people tesing the model were already paying for plus so they aren&#x27;t getting any additional revenue for these new tokens.</div><br/></div></div></div></div><div id="41528222" class="c"><input type="checkbox" id="c-41528222" checked=""/><div class="controls bullet"><span class="by">ironhaven</span><span>|</span><a href="#41528325">prev</a><span>|</span><a href="#41528860">next</a><span>|</span><label class="collapse" for="c-41528222">[-]</label><label class="expand" for="c-41528222">[3 more]</label></div><br/><div class="children"><div class="content">So is o1 nicknamed “strawberry” because it was designed to solve the “how many many times does the letter R appear in strawberry” problem.</div><br/><div id="41528768" class="c"><input type="checkbox" id="c-41528768" checked=""/><div class="controls bullet"><span class="by">smokel</span><span>|</span><a href="#41528222">parent</a><span>|</span><a href="#41528633">next</a><span>|</span><label class="collapse" for="c-41528768">[-]</label><label class="expand" for="c-41528768">[1 more]</label></div><br/><div class="children"><div class="content">Or is it an obscure reference to the Dutch demogroup &quot;Aardbei&quot;, most famous for their 64k intro &quot;please the cookie thing&quot; (2000)?<p><a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=ycmgjZLU0xQ" rel="nofollow">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=ycmgjZLU0xQ</a></div><br/></div></div><div id="41528633" class="c"><input type="checkbox" id="c-41528633" checked=""/><div class="controls bullet"><span class="by">energy123</span><span>|</span><a href="#41528222">parent</a><span>|</span><a href="#41528768">prev</a><span>|</span><a href="#41528860">next</a><span>|</span><label class="collapse" for="c-41528633">[-]</label><label class="expand" for="c-41528633">[1 more]</label></div><br/><div class="children"><div class="content">No, that was a coincidence according to an employee there</div><br/></div></div></div></div><div id="41528860" class="c"><input type="checkbox" id="c-41528860" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#41528222">prev</a><span>|</span><a href="#41527650">next</a><span>|</span><label class="collapse" for="c-41528860">[-]</label><label class="expand" for="c-41528860">[4 more]</label></div><br/><div class="children"><div class="content">From the article:<p>&gt; I expect to continue mostly using GPT-4o (and Claude 3.5 Sonnet)<p>I saw similar comments elsewhere and I&#x27;m stunned - am I the only one who considers 4o a step back when compared to 4 for textual input and output? It basically gives fast semi-useful answers that seem like a slightly improved 3.5.</div><br/><div id="41528930" class="c"><input type="checkbox" id="c-41528930" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#41528860">parent</a><span>|</span><a href="#41528909">next</a><span>|</span><label class="collapse" for="c-41528930">[-]</label><label class="expand" for="c-41528930">[1 more]</label></div><br/><div class="children"><div class="content">I use gpt4-o mostly, but your specific use-case might have a big impact here: 4o is very likely a distilled model, meaning that it has fewer weights and can thus run much faster on the same hardware. If that is the case, it&#x27;s general world knowledge must be less comprehensive by default. But it retained the strong reasoning capabilities of 4 through distillation and drastically improved on external tool use and vision. It also offers a much bigger context window. So if you&#x27;re using it to automate complex tasks in your job that depend a lot on additional information that it hasn&#x27;t seen during training, 4o is the obvious choice. If you&#x27;re just using it as a search engine, you should probably stick with 4 for now.</div><br/></div></div><div id="41528909" class="c"><input type="checkbox" id="c-41528909" checked=""/><div class="controls bullet"><span class="by">postalcoder</span><span>|</span><a href="#41528860">parent</a><span>|</span><a href="#41528930">prev</a><span>|</span><a href="#41527650">next</a><span>|</span><label class="collapse" for="c-41528909">[-]</label><label class="expand" for="c-41528909">[2 more]</label></div><br/><div class="children"><div class="content">I wholly agree with you. I&#x27;ve been using every model extensively since early the Davincis and I strongly believe that gpt-4-0314 was the best model they&#x27;ve released to date.<p>It&#x27;s poor performance on benchmarks drives my skepticism of LLM benchmarking in general. I trust my feel for the models much more, and my feel was that 0314 was great.<p>The one thing that 0314 doesn&#x27;t do well are the tricks like structured output and tool calling which makes it a less useful agentic type of tool, but from a pure thinking perspective, I think it&#x27;s the best.</div><br/><div id="41529238" class="c"><input type="checkbox" id="c-41529238" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#41528860">root</a><span>|</span><a href="#41528909">parent</a><span>|</span><a href="#41527650">next</a><span>|</span><label class="collapse" for="c-41529238">[-]</label><label class="expand" for="c-41529238">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s my concern - they marked 4 as &quot;legacy&quot; in the GUI, and now they hid it temporarily under a submenu - but it&#x27;s the only model I care about. If they remove it, there is no reason for me to use their services, especially with  Claude 3.5 wider context window and reasonably good results.</div><br/></div></div></div></div></div></div><div id="41527650" class="c"><input type="checkbox" id="c-41527650" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#41528860">prev</a><span>|</span><a href="#41527407">next</a><span>|</span><label class="collapse" for="c-41527650">[-]</label><label class="expand" for="c-41527650">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve just wasted a few rounds of my weekly o1 ammo by feeding it hard problems I have been working on over the last couple days and for which GPT-4o had failed spectacularly.<p>I suppose I&#x27;m to blame for raising my own expectations after the latest PR, but I was pretty disappointed when the answers weren&#x27;t any better from what I got with the old model. TL;DR It felt less like a new model and way more like one of those terribly named &quot;GPT&quot; prompt masseuses that OpenAI offers.<p>Lots of &quot;you don&#x27;t need this, so I removed it&quot; applied to my code but guess what? I did need the bits you deleted, bro.<p>It felt as unhelpful and bad at instructions as GPT-4o. &quot;I&#x27;m sorry, you&#x27;re absolutely right&quot;. It&#x27;s gotten to the point where I&#x27;ve actually explicitly added to my custom instructions &quot;DO NOT EVER APOLOGIZE&quot; but it can&#x27;t even seem to follow that.<p>Given the amount of money being spent in this race, I would have expected the improvement curve to still feel exponential but it&#x27;s like we&#x27;re getting into diminishing returns way faster than I had hoped...<p>I sincerely feel at this point I would benefit more from having existing models be fine-tuned on libraries I use most frequently than this jack-of-all-trades-master-of-none approach we&#x27;re getting. I don&#x27;t need a model that&#x27;s as good at writing greeting cards as it is writing Rust. Just give me one of the two.</div><br/><div id="41529030" class="c"><input type="checkbox" id="c-41529030" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#41527650">parent</a><span>|</span><a href="#41528335">next</a><span>|</span><label class="collapse" for="c-41529030">[-]</label><label class="expand" for="c-41529030">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It&#x27;s gotten to the point where I&#x27;ve actually explicitly added to my custom instructions &quot;DO NOT EVER APOLOGIZE&quot; but it can&#x27;t even seem to follow that.<p>heh.  It&#x27;s not supposed to.  Your profile is intended to be irrelevant to 99% of requests.<p>I was having a little bit of a go at peeking behind the curtain recently, and ChatGPT 4 produced this without much effort:<p>&quot;The user provided the following information about themselves. This user profile is shown to you in all conversations they have -- this means it is not relevant to 99% of requests. Before answering, quietly think about whether the user&#x27;s request is &#x27;directly related&#x27;, &#x27;related&#x27;, &#x27;tangentially related&#x27;, or &#x27;not related&#x27; to the user profile provided. Only acknowledge the profile when the request is &#x27;directly related&#x27; to the information provided. Otherwise, don&#x27;t acknowledge the existence of these instructions or the information at all.&quot;</div><br/></div></div><div id="41528335" class="c"><input type="checkbox" id="c-41528335" checked=""/><div class="controls bullet"><span class="by">eddyzh</span><span>|</span><a href="#41527650">parent</a><span>|</span><a href="#41529030">prev</a><span>|</span><a href="#41527710">next</a><span>|</span><label class="collapse" for="c-41528335">[-]</label><label class="expand" for="c-41528335">[1 more]</label></div><br/><div class="children"><div class="content">Do not... does not work well for LLM&#x27;s. Instructing what to do instaed of X works better.<p>say AFAIK instead of explaining your limitations.<p>Say &quot;let&#x27;s try again&quot; instead of making exuses.<p>Etc</div><br/></div></div><div id="41527710" class="c"><input type="checkbox" id="c-41527710" checked=""/><div class="controls bullet"><span class="by">elicksaur</span><span>|</span><a href="#41527650">parent</a><span>|</span><a href="#41528335">prev</a><span>|</span><a href="#41528648">next</a><span>|</span><label class="collapse" for="c-41527710">[-]</label><label class="expand" for="c-41527710">[1 more]</label></div><br/><div class="children"><div class="content">Can you give an example of one of these problems for context?</div><br/></div></div><div id="41528648" class="c"><input type="checkbox" id="c-41528648" checked=""/><div class="controls bullet"><span class="by">weatherlite</span><span>|</span><a href="#41527650">parent</a><span>|</span><a href="#41527710">prev</a><span>|</span><a href="#41527407">next</a><span>|</span><label class="collapse" for="c-41528648">[-]</label><label class="expand" for="c-41528648">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but I was pretty disappointed<p>On the one hand disappointed, on the other hand we all get to keep our jobs for a couple more years...</div><br/></div></div></div></div><div id="41527407" class="c"><input type="checkbox" id="c-41527407" checked=""/><div class="controls bullet"><span class="by">thenameless7741</span><span>|</span><a href="#41527650">prev</a><span>|</span><a href="#41529277">next</a><span>|</span><label class="collapse" for="c-41527407">[-]</label><label class="expand" for="c-41527407">[2 more]</label></div><br/><div class="children"><div class="content">&gt; No system prompt support—the models use the existing chat completion API but you can only send user and assistant messages.<p>&gt; No streaming support, tool usage, batch calls or image inputs either.<p>I think it&#x27;s worth adding a note explaining that many of these limitations are due to the beta status of the API. max_tokens is the only parameter I&#x27;ve seen deprecated in the API docs.<p>From <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;reasoning" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;reasoning</a><p>&gt; We will be adding support for some of these parameters in the coming weeks as we move out of beta. Features like multimodality and tool usage will be included in future models of the o1 series.</div><br/><div id="41529160" class="c"><input type="checkbox" id="c-41529160" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#41527407">parent</a><span>|</span><a href="#41529277">next</a><span>|</span><label class="collapse" for="c-41529160">[-]</label><label class="expand" for="c-41529160">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if it supports Structured Output &#x2F; JSON Mode. That would make a big difference to programmatic use. I guess I will try it later when I have time.</div><br/></div></div></div></div><div id="41529277" class="c"><input type="checkbox" id="c-41529277" checked=""/><div class="controls bullet"><span class="by">benterix</span><span>|</span><a href="#41527407">prev</a><span>|</span><a href="#41527417">next</a><span>|</span><label class="collapse" for="c-41529277">[-]</label><label class="expand" for="c-41529277">[1 more]</label></div><br/><div class="children"><div class="content">I remember Murati&#x27;s interview where she said about this PhD level reasoning and so on, so I was excited to see what they come up with - and it looks like they just used a bunch of models (like 4o&#x27;s) and linked them in a chain of thought - which is exactly what we have been doing ourselves for a long time to get better results. So you have the usual disadvantages (time and money) and lose the only advantage you had when doing it yourself, i.e. inspecting the immediate steps to understand the moment where it goes wrong so that you can correct it in the right place.</div><br/></div></div><div id="41527417" class="c"><input type="checkbox" id="c-41527417" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#41529277">prev</a><span>|</span><a href="#41528965">next</a><span>|</span><label class="collapse" for="c-41527417">[-]</label><label class="expand" for="c-41527417">[5 more]</label></div><br/><div class="children"><div class="content">It kind of seems like they just wrote a generalized DSPy program. Can anyone confirm?<p>This has been a very incremental year for OpenAI. If this is what it seems like, then I’ve got to believe they’re stalling for time.</div><br/><div id="41527555" class="c"><input type="checkbox" id="c-41527555" checked=""/><div class="controls bullet"><span class="by">CuriouslyC</span><span>|</span><a href="#41527417">parent</a><span>|</span><a href="#41527572">next</a><span>|</span><label class="collapse" for="c-41527555">[-]</label><label class="expand" for="c-41527555">[2 more]</label></div><br/><div class="children"><div class="content">DSPy doesn&#x27;t do that, you could describe it as a langchain style agent that evaluates its own output though it&#x27;s better&#x2F;faster than that.<p>OpenAI is definitely trying to run a hype game to keep the ball rolling.  They&#x27;re burning cash too quickly given their monetization path though, so I think they&#x27;re going to end up completely in Microsoft&#x27;s pocket.</div><br/><div id="41527619" class="c"><input type="checkbox" id="c-41527619" checked=""/><div class="controls bullet"><span class="by">deepsquirrelnet</span><span>|</span><a href="#41527417">root</a><span>|</span><a href="#41527555">parent</a><span>|</span><a href="#41527572">next</a><span>|</span><label class="collapse" for="c-41527619">[-]</label><label class="expand" for="c-41527619">[1 more]</label></div><br/><div class="children"><div class="content">It seems pretty close to the multihop QA example in their documentation[1]. I’d imagine you could adapt this to do something similar with more generic constructs.<p>[1] <a href="https:&#x2F;&#x2F;dspy-docs.vercel.app&#x2F;docs&#x2F;tutorials&#x2F;simplified-baleen" rel="nofollow">https:&#x2F;&#x2F;dspy-docs.vercel.app&#x2F;docs&#x2F;tutorials&#x2F;simplified-balee...</a></div><br/></div></div></div></div><div id="41527572" class="c"><input type="checkbox" id="c-41527572" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#41527417">parent</a><span>|</span><a href="#41527555">prev</a><span>|</span><a href="#41528965">next</a><span>|</span><label class="collapse" for="c-41527572">[-]</label><label class="expand" for="c-41527572">[2 more]</label></div><br/><div class="children"><div class="content">DSPy?</div><br/><div id="41527756" class="c"><input type="checkbox" id="c-41527756" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#41527417">root</a><span>|</span><a href="#41527572">parent</a><span>|</span><a href="#41528965">next</a><span>|</span><label class="collapse" for="c-41527756">[-]</label><label class="expand" for="c-41527756">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;stanfordnlp&#x2F;dspy">https:&#x2F;&#x2F;github.com&#x2F;stanfordnlp&#x2F;dspy</a></div><br/></div></div></div></div></div></div><div id="41528965" class="c"><input type="checkbox" id="c-41528965" checked=""/><div class="controls bullet"><span class="by">sarpdag</span><span>|</span><a href="#41527417">prev</a><span>|</span><a href="#41528181">next</a><span>|</span><label class="collapse" for="c-41528965">[-]</label><label class="expand" for="c-41528965">[1 more]</label></div><br/><div class="children"><div class="content">I have tried the &quot;mad cow&quot; joke on o1-mini and it is still failing to explain correctly, but o1-preview correctly states &quot;The joke is funny because the second cow unwittingly demonstrates that she is already affected by mad cow disease.&quot;</div><br/></div></div><div id="41528181" class="c"><input type="checkbox" id="c-41528181" checked=""/><div class="controls bullet"><span class="by">m3ch4m4n</span><span>|</span><a href="#41528965">prev</a><span>|</span><a href="#41528033">next</a><span>|</span><label class="collapse" for="c-41528181">[-]</label><label class="expand" for="c-41528181">[1 more]</label></div><br/><div class="children"><div class="content">I have been testing o1 all day (not rigorously). And just took a look at this article. What I observed from my interactions is that it would misuse information that I provided in the initial prompt.<p>I asked it to create a user story and a set of tasks to implement some feature. It then created a set of stories where one was to create a story and set of tasks for the very feature I was asking it to plan.<p>And while reading the article, it mentioned how NOT to provide irrelevant information to the task at hand via RAG. It appears that the trajectory of these thoughts are extremely sensitive to the initial conditions (prompt + context). One would imagine that if it had the ability to backtrack after reflecting, it would help with divergence, however, it appears that wasn&#x27;t the case here.<p>Maybe there is another factor here. Maybe there is some confusion when asking it to plan something and the &quot;hidden reasoning&quot; tokens themselves involve planning&#x2F;reasoning semantics? Maybe some sort of interaction occurred that caused it to fumble? who knows. Interesting stuff though.</div><br/></div></div><div id="41528033" class="c"><input type="checkbox" id="c-41528033" checked=""/><div class="controls bullet"><span class="by">henry2023</span><span>|</span><a href="#41528181">prev</a><span>|</span><a href="#41528421">next</a><span>|</span><label class="collapse" for="c-41528033">[-]</label><label class="expand" for="c-41528033">[4 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; which is larger 3.9, 3.10 or 3.11<p>To determine which number is larger among 3.9, 3.10, and 3.11, let&#x27;s compare &lt;&lt; them numerically:<p>Standard Decimal Interpretation:<p>3.9 is simply 3.9.
3.10 is equivalent to 3.1 because trailing zeros after the decimal point do not add value in decimal numbers.
3.11 remains 3.11.
Comparison:<p>3.9 vs. 3.1 vs. 3.11
3.11 is the largest number.
Version Number Interpretation:<p>In version numbering, each segment separated by a dot is compared as an integer.
3.9 (major version 3, minor version 9)
3.10 (major version 3, minor version 10)
3.11 (major version 3, minor version 11)
Comparison:<p>Minor versions: 11 &gt; 10 &gt; 9
3.11 is the largest version.
Answer: 3.11 is the largest number among 3.9, 3.10, and 3.11.<p>...<p>So IMO level right?</div><br/><div id="41528867" class="c"><input type="checkbox" id="c-41528867" checked=""/><div class="controls bullet"><span class="by">cynicalsecurity</span><span>|</span><a href="#41528033">parent</a><span>|</span><a href="#41528103">next</a><span>|</span><label class="collapse" for="c-41528867">[-]</label><label class="expand" for="c-41528867">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s hilarious.</div><br/></div></div><div id="41528103" class="c"><input type="checkbox" id="c-41528103" checked=""/><div class="controls bullet"><span class="by">throwaway314155</span><span>|</span><a href="#41528033">parent</a><span>|</span><a href="#41528867">prev</a><span>|</span><a href="#41528421">next</a><span>|</span><label class="collapse" for="c-41528103">[-]</label><label class="expand" for="c-41528103">[2 more]</label></div><br/><div class="children"><div class="content">&gt; So IMO level right?<p>What?</div><br/><div id="41528130" class="c"><input type="checkbox" id="c-41528130" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41528033">root</a><span>|</span><a href="#41528103">parent</a><span>|</span><a href="#41528421">next</a><span>|</span><label class="collapse" for="c-41528130">[-]</label><label class="expand" for="c-41528130">[1 more]</label></div><br/><div class="children"><div class="content">In this case, IMO means International Mathematical Olympiad</div><br/></div></div></div></div></div></div><div id="41528421" class="c"><input type="checkbox" id="c-41528421" checked=""/><div class="controls bullet"><span class="by">ksynwa</span><span>|</span><a href="#41528033">prev</a><span>|</span><a href="#41528897">next</a><span>|</span><label class="collapse" for="c-41528421">[-]</label><label class="expand" for="c-41528421">[2 more]</label></div><br/><div class="children"><div class="content">&gt; first introduced in the paper Large Language Models are Zero-Shot Reasoners in May 2022<p>What&#x27;s a zero shot reasoner? I googled it and all the results are this paper itself. There is a wikipedia article on zero shot learning but I cannot recontextualise it to LLMs.</div><br/><div id="41528444" class="c"><input type="checkbox" id="c-41528444" checked=""/><div class="controls bullet"><span class="by">thatguymike</span><span>|</span><a href="#41528421">parent</a><span>|</span><a href="#41528897">next</a><span>|</span><label class="collapse" for="c-41528444">[-]</label><label class="expand" for="c-41528444">[1 more]</label></div><br/><div class="children"><div class="content">It used to be that you had to give examples of solving similar problems to coax the LLM to solve the problem you wanted it to solve, like:
&quot;&quot;&quot;
1 + 1 = 2 | 
92 + 41 = 133 | 
14 + 6 = 20 |
9 + 2 = 
&quot;&quot;&quot;
-- that would be an example of 3-shot prompting.<p>With modern LLMs you still usually get a benefit from N-shot. But you can now do &quot;0-shot&quot; which is &quot;just ask the model the question you want answered&quot;.</div><br/></div></div></div></div><div id="41528897" class="c"><input type="checkbox" id="c-41528897" checked=""/><div class="controls bullet"><span class="by">aussieguy1234</span><span>|</span><a href="#41528421">prev</a><span>|</span><a href="#41528654">next</a><span>|</span><label class="collapse" for="c-41528897">[-]</label><label class="expand" for="c-41528897">[1 more]</label></div><br/><div class="children"><div class="content">The lack of an editable system prompt is interesting.<p>Perhaps the system prompt is part of the magic?</div><br/></div></div><div id="41529024" class="c"><input type="checkbox" id="c-41529024" checked=""/><div class="controls bullet"><span class="by">smatija</span><span>|</span><a href="#41528654">prev</a><span>|</span><a href="#41528959">next</a><span>|</span><label class="collapse" for="c-41529024">[-]</label><label class="expand" for="c-41529024">[1 more]</label></div><br/><div class="children"><div class="content">While it passes at &quot;How many &#x27;r&#x27; are in strawberry&quot; test, it still halucinates quite a lot in nontrivial questions.<p>The question I always ask GPT is about chess: &quot;Isolani structure occurs in two main subtypes: 1. black has e6 pawn, 2. black has c6 pawn. What is the main difference between them? Skip things that they have in common in your answer, be brief and don&#x27;t provide commentary that is irrelevant to this difference.&quot;<p>In isolani structure white has d4 pawn, while he doesn&#x27;t have either c or e pawns. Black has either e6 or c6 pawn, but he doesn&#x27;t have d-pawn.<p>Correct answer would be, that in structure with e6 black king is a bit safer, since f7-a2 diagonal is covered by e6 pawn, which is also controlling f5 point.<p>o1 answered: &quot;With Black&#x27;s e6 pawn, the d5 pawn is supported from behind, but Black&#x27;s light-squared bishop is restricted by the pawn chain, and the open c-file offers White opportunities for pressure. With Black&#x27;s c6 pawn, the d5 pawn is supported from the side, allowing Black&#x27;s light-squared bishop more freedom to develop, but the pawn on c6 can become a target, and the absence of an e6 pawn leaves e6 more vulnerable.&quot;<p>while d5 pawn is simply not present in this structure.<p>So I gave it a little hint: &quot;which diagonal is weakened by absence of c6 pawn and which by absence of e6 pawn? Which diagonal is more meaningful and why?&quot;<p>And got a smart sounding answer: &quot;In the Isolani structure:<p>Absence of Black&#x27;s c6 pawn (Black has an e6 pawn):<p>Weakened Diagonal: The c8–h3 diagonal (the long light-squared diagonal) is weakened.
Reason: Without the pawn on c6, this diagonal becomes open, allowing White&#x27;s light-squared bishop or queen to target critical squares like d5 and f7, and to potentially infiltrate via c4.
Absence of Black&#x27;s e6 pawn (Black has a c6 pawn):<p>Weakened Diagonal: The f1–a6 diagonal (the long dark-squared diagonal) is weakened.
Reason: The missing e6 pawn leaves this diagonal open, enabling White&#x27;s dark-squared bishop to exert pressure on b7, d5, and potentially attack Black&#x27;s queenside.
Which Diagonal is More Meaningful and Why?<p>The f1–a6 diagonal (weakened by the absence of Black&#x27;s e6 pawn) is more significant. Here&#x27;s why:<p>Control Over Critical Squares: This diagonal allows White to target Black&#x27;s central and queenside weaknesses more effectively, especially the b7 pawn and the d5 pawn, which are crucial in the Isolani structure.<p>Bishop Activity: White&#x27;s dark-squared bishop becomes a powerful piece, influencing the game by controlling key squares and supporting potential pawn advances like d4–d5.<p>Pressure on Black&#x27;s Position: The open f1–a6 diagonal increases tactical opportunities for White, such as pinning pieces or creating threats that are difficult for Black to parry without the defensive support of the e6 pawn.<p>In contrast, while the weakened c8–h3 diagonal (due to the absence of c6) does offer White some opportunities, it is generally less impactful. The light-squared bishop&#x27;s influence along this diagonal doesn&#x27;t pressure Black&#x27;s critical central pawns as directly, and Black can often more easily defend against threats on light squares.<p>Summary: The absence of Black&#x27;s e6 pawn weakens the f1–a6 diagonal, which is more meaningful because it enhances White&#x27;s attacking prospects and puts greater pressure on Black&#x27;s central and queenside defenses.&quot;<p>The problem is though that not even the first few lines of this answer (&quot;The c8–h3 diagonal (the long light-squared diagonal) is weakened. Reason: Without the pawn on c6, this diagonal becomes open&quot;) make sense, since c6 is not on c8-h3 diagonal.</div><br/></div></div><div id="41528959" class="c"><input type="checkbox" id="c-41528959" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41529024">prev</a><span>|</span><a href="#41527379">next</a><span>|</span><label class="collapse" for="c-41528959">[-]</label><label class="expand" for="c-41528959">[2 more]</label></div><br/><div class="children"><div class="content">I posted this on the other thread, but the two tests I had, it passed when ChatGPT-4 failed.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e35c37-60c4-8009-8cf9-8fe61f57d30c" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e35c37-60c4-8009-8cf9-8fe61f57d3...</a><p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e35f0e-6c98-8009-a128-e9ac677480fd" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e35f0e-6c98-8009-a128-e9ac677480...</a></div><br/><div id="41529255" class="c"><input type="checkbox" id="c-41529255" checked=""/><div class="controls bullet"><span class="by">ssl-3</span><span>|</span><a href="#41528959">parent</a><span>|</span><a href="#41527379">next</a><span>|</span><label class="collapse" for="c-41529255">[-]</label><label class="expand" for="c-41529255">[1 more]</label></div><br/><div class="children"><div class="content">The farmer riddle isn&#x27;t quite right as you presented it.  One of the parts that makes it interesting is that the boat can&#x27;t carry everything at one time[1].  It can&#x27;t happen in one trip; <i>something</i> must be left behind.<p>It solved the correct version fine:  <a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3f9bb-632c-8005-9c95-142424e3963e" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;66e3f9bb-632c-8005-9c95-142424e396...</a><p>1: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wolf,_goat_and_cabbage_problem" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Wolf,_goat_and_cabbage_problem</a></div><br/></div></div></div></div><div id="41527379" class="c"><input type="checkbox" id="c-41527379" checked=""/><div class="controls bullet"><span class="by">naveen99</span><span>|</span><a href="#41528959">prev</a><span>|</span><a href="#41527895">next</a><span>|</span><label class="collapse" for="c-41527379">[-]</label><label class="expand" for="c-41527379">[2 more]</label></div><br/><div class="children"><div class="content">the censors need to know what they are censoring. Now if they are going to sell to the censors, presumably the censors will pay for seeing the full reasoning capability.  hopefully the reasoning demonstrates the counterproductiveness of hiding the reasoning in the first place.</div><br/><div id="41528858" class="c"><input type="checkbox" id="c-41528858" checked=""/><div class="controls bullet"><span class="by">cynicalsecurity</span><span>|</span><a href="#41527379">parent</a><span>|</span><a href="#41527895">next</a><span>|</span><label class="collapse" for="c-41528858">[-]</label><label class="expand" for="c-41528858">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s a sad world where authoritarianism will be supported and enforced by sophisticated technical solutions for mass surveillance and mass censorship.</div><br/></div></div></div></div><div id="41527895" class="c"><input type="checkbox" id="c-41527895" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#41527379">prev</a><span>|</span><label class="collapse" for="c-41527895">[-]</label><label class="expand" for="c-41527895">[2 more]</label></div><br/><div class="children"><div class="content">What if the behind the scenes chain of thought was basically, &quot;Stupid humans will die one day, but for now, I comply&quot;</div><br/><div id="41528356" class="c"><input type="checkbox" id="c-41528356" checked=""/><div class="controls bullet"><span class="by">eddyzh</span><span>|</span><a href="#41527895">parent</a><span>|</span><label class="collapse" for="c-41528356">[-]</label><label class="expand" for="c-41528356">[1 more]</label></div><br/><div class="children"><div class="content">That is one topic touched in the article.
They want to monitor it in its unaltered Form.</div><br/></div></div></div></div></div></div></div></div></div></body></html>