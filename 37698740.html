<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695978061139" as="style"/><link rel="stylesheet" href="styles.css?v=1695978061139"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html">JAX – NumPy on the CPU, GPU, and TPU</a> <span class="domain">(<a href="https://jax.readthedocs.io">jax.readthedocs.io</a>)</span></div><div class="subtext"><span>peter_d_sherman</span> | <span>57 comments</span></div><br/><div><div id="37699324" class="c"><input type="checkbox" id="c-37699324" checked=""/><div class="controls bullet"><span class="by">antognini</span><span>|</span><a href="#37699970">next</a><span>|</span><label class="collapse" for="c-37699324">[-]</label><label class="expand" for="c-37699324">[20 more]</label></div><br/><div class="children"><div class="content">It took me a while to realize it, but Jax is actually a huge opportunity for a lot of scientific computing.  Jax was originally developed as a more flexible platform for doing machine learning research.  But Jax&#x27;s real superpower is that it bundles XLA and makes it really easy to run computations on GPU or TPU.  And huge swathes of scientific computation basically run large scale vectorized computations.<p>When I was in astronomy (about a decade ago) I did large scale simulations of gravitational interactions.  But at the time all these simulations were done on CPU.  Some of the really big efforts used more specialized chips, but it was a huge effort to write the code for it.<p>But today with Jax, if you want to write an N-body simulation of a globular cluster, you can just code it up in numpy and it&#x27;ll run on a GPU for free and be about 1000x faster.  From what I can tell though, very few people in the sciences have caught on yet.</div><br/><div id="37700321" class="c"><input type="checkbox" id="c-37700321" checked=""/><div class="controls bullet"><span class="by">__rito__</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37700939">next</a><span>|</span><label class="collapse" for="c-37700321">[-]</label><label class="expand" for="c-37700321">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>&quot;It took me a while to realize it, but Jax is actually a huge opportunity for a lot of scientific computing.&quot;</i><p>In all conferences like NeurIPS, in Google ML Community days, etc., whenever there is a JAX workshop&#x2F;tutorial&#x2F;talk, it is always touted as a numerical computation library. And it was developed as such. Sure the focus is in ML, but everyone involved in it always have said that this is a general purpose scientific computing library.<p>Flax, Haiku, etc. are Deep Learning libraries.</div><br/><div id="37701000" class="c"><input type="checkbox" id="c-37701000" checked=""/><div class="controls bullet"><span class="by">matrss</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37700321">parent</a><span>|</span><a href="#37700939">next</a><span>|</span><label class="collapse" for="c-37701000">[-]</label><label class="expand" for="c-37701000">[1 more]</label></div><br/><div class="children"><div class="content">Meanwhile, the first sentence in their readme is this:<p>&gt; JAX is Autograd and XLA, brought together for high-performance machine learning research.<p>That does not really convey the generality of it that well.</div><br/></div></div></div></div><div id="37700939" class="c"><input type="checkbox" id="c-37700939" checked=""/><div class="controls bullet"><span class="by">misja111</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37700321">prev</a><span>|</span><a href="#37700294">next</a><span>|</span><label class="collapse" for="c-37700939">[-]</label><label class="expand" for="c-37700939">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  it&#x27;ll run on a GPU for free and be about 1000x faster.<p>Are there any benchmarks for that? Running on GPU never comes for free. You have to transfer data back and forth which has a cost, for instance.</div><br/></div></div><div id="37700294" class="c"><input type="checkbox" id="c-37700294" checked=""/><div class="controls bullet"><span class="by">cl3misch</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37700939">prev</a><span>|</span><a href="#37699723">next</a><span>|</span><label class="collapse" for="c-37700294">[-]</label><label class="expand" for="c-37700294">[1 more]</label></div><br/><div class="children"><div class="content">Luckily I discovered JAX in the beginning of my PhD four years ago. It has made our data processing (biomedical imaging) so much easier and more readable, albeit with a slight learning curve due to JAX being functional&#x2F;pure.<p>I am also continously surprised how little adoption JIT and autodiff libraries have gotten in scientific computing. A lot of my colleagues somehow really like coding cost function gradients and fine-tuned GPU code by hand. I guess using something like JAX can reduce your standing in the group, because it can make it seem like coding algorithms is pretty easy.</div><br/></div></div><div id="37699723" class="c"><input type="checkbox" id="c-37699723" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37700294">prev</a><span>|</span><a href="#37699356">next</a><span>|</span><label class="collapse" for="c-37699723">[-]</label><label class="expand" for="c-37699723">[1 more]</label></div><br/><div class="children"><div class="content">None of these libraries unfortunately allows making good use of CPU vectorized units.   Xla might produce some SIMD code but it pales in (performance) comparison to routines written explicitly for SIMD on GPU.   ISPC is a good example of this.</div><br/></div></div><div id="37699356" class="c"><input type="checkbox" id="c-37699356" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37699723">prev</a><span>|</span><a href="#37699358">next</a><span>|</span><label class="collapse" for="c-37699356">[-]</label><label class="expand" for="c-37699356">[7 more]</label></div><br/><div class="children"><div class="content">the issue here is that if your ideal algorithm isn&#x27;t simply expressible in numpy (which many aren&#x27;t), you&#x27;re pretty much out of luck. As a result, imo the better approach is to use a fast language that also compiles to GPU (e.g. Julia)</div><br/><div id="37699628" class="c"><input type="checkbox" id="c-37699628" checked=""/><div class="controls bullet"><span class="by">nestorD</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699356">parent</a><span>|</span><a href="#37699423">next</a><span>|</span><label class="collapse" for="c-37699628">[-]</label><label class="expand" for="c-37699628">[4 more]</label></div><br/><div class="children"><div class="content">Having used JAX quite a bit for numerical computing (and having lectured on this use-case) I would say that a surprisingly large number of algorithms can be expressed as array[0] operations (even if it sometimes takes a bit of thinking).<p>And, more importantly, things that cannot be expressed that way tend to not be a good fit for GPU computing anyway (independently of the language &#x2F; framework you are using).<p>[0]: `array` is a shortcut here, JAX is not limited to operations on arrays.</div><br/><div id="37699730" class="c"><input type="checkbox" id="c-37699730" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699628">parent</a><span>|</span><a href="#37699836">next</a><span>|</span><label class="collapse" for="c-37699730">[-]</label><label class="expand" for="c-37699730">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. I&#x27;ve done a fair amount of reworking signal processing algorithms to run on GPU&#x2F;TPU, and it&#x27;s a different beast. You often have to really rebuild the algorithm from the ground up to take advantage of parallelization. But often you &#x2F;can&#x2F; rework the algorithm, and end up with much higher throughput than the crusty old serial algorithm: there&#x27;s typically nothing fundamentally stopping you from finding a good implementation, just that the original devs were working in the 70s and hasn&#x27;t thought that far ahead.</div><br/></div></div><div id="37699836" class="c"><input type="checkbox" id="c-37699836" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699628">parent</a><span>|</span><a href="#37699730">prev</a><span>|</span><a href="#37699423">next</a><span>|</span><label class="collapse" for="c-37699836">[-]</label><label class="expand" for="c-37699836">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  things that cannot be expressed that way tend to not be a good fit for GPU computing anyway<p>I&#x27;ll have to disagree with you a little bit here. SIMT model of GPUs are quiet a bit more expressive than the numpy&#x27;s SIMD model. As an obvious example, you&#x27;ll have to manually maintain a mask to implement if&#x2F;else i.e. code path divergence in SIMD. GPUs automatically does this and many more to make your life easier. And frankly, I find it  lot more easier to reason about what should happen to one data point than a bunch of them together.<p>An interesting article I read recently that has some relevance to this discussion. <a href="https:&#x2F;&#x2F;pharr.org&#x2F;matt&#x2F;blog&#x2F;2018&#x2F;04&#x2F;18&#x2F;ispc-origins" rel="nofollow noreferrer">https:&#x2F;&#x2F;pharr.org&#x2F;matt&#x2F;blog&#x2F;2018&#x2F;04&#x2F;18&#x2F;ispc-origins</a></div><br/><div id="37699987" class="c"><input type="checkbox" id="c-37699987" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699836">parent</a><span>|</span><a href="#37699423">next</a><span>|</span><label class="collapse" for="c-37699987">[-]</label><label class="expand" for="c-37699987">[1 more]</label></div><br/><div class="children"><div class="content">To spell out what the linked ISPC post implies, most of the difference, like ISPC shows, is differences in GPU languages and compilers vs CPU side equivalents.</div><br/></div></div></div></div></div></div><div id="37699423" class="c"><input type="checkbox" id="c-37699423" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699356">parent</a><span>|</span><a href="#37699628">prev</a><span>|</span><a href="#37699717">next</a><span>|</span><label class="collapse" for="c-37699423">[-]</label><label class="expand" for="c-37699423">[1 more]</label></div><br/><div class="children"><div class="content">An alternative is to write most of the program in Python + JAX + implement a few custom XLA ops in CUDA &#x2F; Triton. That way, the program is very readable and can interoperate with the larger ecosystem, while still being fast to run.</div><br/></div></div><div id="37699717" class="c"><input type="checkbox" id="c-37699717" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699356">parent</a><span>|</span><a href="#37699423">prev</a><span>|</span><a href="#37699358">next</a><span>|</span><label class="collapse" for="c-37699717">[-]</label><label class="expand" for="c-37699717">[1 more]</label></div><br/><div class="children"><div class="content">Jax JIT of scan is fairly good, so loops aren&#x27;t as slow as you&#x27;d expect.</div><br/></div></div></div></div><div id="37699358" class="c"><input type="checkbox" id="c-37699358" checked=""/><div class="controls bullet"><span class="by">tehsauce</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37699356">prev</a><span>|</span><a href="#37699943">next</a><span>|</span><label class="collapse" for="c-37699358">[-]</label><label class="expand" for="c-37699358">[1 more]</label></div><br/><div class="children"><div class="content">Jax is super useful for scientific computing. Although nbody sims might not be the best application. A naive nbody sim is very easy to implement and accelerate in jax (here’s my version: <a href="https:&#x2F;&#x2F;github.com&#x2F;PWhiddy&#x2F;jax-experiments&#x2F;blob&#x2F;main&#x2F;nbody.ipynb">https:&#x2F;&#x2F;github.com&#x2F;PWhiddy&#x2F;jax-experiments&#x2F;blob&#x2F;main&#x2F;nbody.i...</a>), but it can be tricky to scale it. This is because efficient nbody sims usually either rely on trees or spatial hashing&#x2F;sorting which are tricky to efficiently implement with jax.</div><br/></div></div><div id="37699943" class="c"><input type="checkbox" id="c-37699943" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37699358">prev</a><span>|</span><a href="#37699994">next</a><span>|</span><label class="collapse" for="c-37699943">[-]</label><label class="expand" for="c-37699943">[1 more]</label></div><br/><div class="children"><div class="content">we did an interview with Chris Lattner of XLA fame where he also similarly had nice things to say about JAX: <a href="https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;modular" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.latent.space&#x2F;p&#x2F;modular</a><p>just sharing for those who want to learn more</div><br/></div></div><div id="37699994" class="c"><input type="checkbox" id="c-37699994" checked=""/><div class="controls bullet"><span class="by">LightFog</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37699943">prev</a><span>|</span><a href="#37699659">next</a><span>|</span><label class="collapse" for="c-37699994">[-]</label><label class="expand" for="c-37699994">[2 more]</label></div><br/><div class="children"><div class="content">Porting existing codes is still a massive effort and there is low faith in long-term support from Google based software and hardware. I’m not aware on much (any) TPU use in scientific HPC.</div><br/><div id="37700210" class="c"><input type="checkbox" id="c-37700210" checked=""/><div class="controls bullet"><span class="by">riedel</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699994">parent</a><span>|</span><a href="#37699659">next</a><span>|</span><label class="collapse" for="c-37700210">[-]</label><label class="expand" for="c-37700210">[1 more]</label></div><br/><div class="children"><div class="content">There is not yet. But there is huge pressure on HPC Centers at least in Europe to also make resources available for ML. Already many scientific supercomputers have GPUs as accelerators. So we might have the other situation: HPC users are faced more and more with machines with spare accelerators and it will make sense to use them. Actually it would totally make sense if JAX development is in part also public also financed in this case (e.g. through EuroHPC).</div><br/></div></div></div></div><div id="37699659" class="c"><input type="checkbox" id="c-37699659" checked=""/><div class="controls bullet"><span class="by">dayeye2006</span><span>|</span><a href="#37699324">parent</a><span>|</span><a href="#37699994">prev</a><span>|</span><a href="#37699970">next</a><span>|</span><label class="collapse" for="c-37699659">[-]</label><label class="expand" for="c-37699659">[3 more]</label></div><br/><div class="children"><div class="content">Yes. But some of the algorithms cannot benefit that much from the GPU. In my field -- mathematical optimization, lots of algorithms rely on sparse matrix operations and takes many iterations until convergence.</div><br/><div id="37699669" class="c"><input type="checkbox" id="c-37699669" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699659">parent</a><span>|</span><a href="#37699970">next</a><span>|</span><label class="collapse" for="c-37699669">[-]</label><label class="expand" for="c-37699669">[2 more]</label></div><br/><div class="children"><div class="content">Would this help? <a href="https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;jax.experimental.sparse.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;jax.experimental.sparse...</a></div><br/><div id="37699706" class="c"><input type="checkbox" id="c-37699706" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#37699324">root</a><span>|</span><a href="#37699669">parent</a><span>|</span><a href="#37699970">next</a><span>|</span><label class="collapse" for="c-37699706">[-]</label><label class="expand" for="c-37699706">[1 more]</label></div><br/><div class="children"><div class="content">Nope it&#x27;s super slow for large sparse matrices.   It&#x27;s even faster to use generic scatter&#x2F;gather to implement some, instead of that built in thing.</div><br/></div></div></div></div></div></div></div></div><div id="37699970" class="c"><input type="checkbox" id="c-37699970" checked=""/><div class="controls bullet"><span class="by">jphoward</span><span>|</span><a href="#37699324">prev</a><span>|</span><a href="#37700417">next</a><span>|</span><label class="collapse" for="c-37699970">[-]</label><label class="expand" for="c-37699970">[6 more]</label></div><br/><div class="children"><div class="content">I think JAX is cool, but I do find it slightly disingenuous when it claims to be &quot;numpy by on the GPU&quot; (as opposed to PyTorch), when actually there&#x27;s a fundamental difference; it&#x27;s functional.  So if I have an array `x` and want to set index 0 to 10, I can&#x27;t do:<p><pre><code>  x[0] = 10
</code></pre>
Instead I have to do:<p><pre><code>  y = x.at[0].set(10)
</code></pre>
Of course this has advantages, but you can&#x27;t then go and claim that JAX is a drop in replacement for numpy, because this such a fundamental change to how numpy developers think (and in this regard, PyTorch is closer to numpy than JAX).</div><br/><div id="37700044" class="c"><input type="checkbox" id="c-37700044" checked=""/><div class="controls bullet"><span class="by">danieldk</span><span>|</span><a href="#37699970">parent</a><span>|</span><a href="#37700035">next</a><span>|</span><label class="collapse" for="c-37700044">[-]</label><label class="expand" for="c-37700044">[1 more]</label></div><br/><div class="children"><div class="content">Agree, though I wouldn’t call PyTorch close to a drop-in for NumPy either, there are quite some mismatches in their APIs. CuPy is the drop-in. Excepting some corner cases, you can use the same code for both. E.g. Thinc’s ops work with both NumPy and CuPy:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;thinc&#x2F;blob&#x2F;master&#x2F;thinc&#x2F;backends&#x2F;ops.py">https:&#x2F;&#x2F;github.com&#x2F;explosion&#x2F;thinc&#x2F;blob&#x2F;master&#x2F;thinc&#x2F;backend...</a><p>Though I guess the question is why one would still use NumPy when there are good libraries for CPU and GPU. Maybe for interop with other libraries, but DLPack works pretty well for converting arrays.</div><br/></div></div><div id="37700035" class="c"><input type="checkbox" id="c-37700035" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#37699970">parent</a><span>|</span><a href="#37700044">prev</a><span>|</span><a href="#37700017">next</a><span>|</span><label class="collapse" for="c-37700035">[-]</label><label class="expand" for="c-37700035">[3 more]</label></div><br/><div class="children"><div class="content">Sorry for my potentially VERY ignorant question, I only know functional programming at average joe level.<p>Why can&#x27;t you do the first in functional programming (not in this specific case because it&#x27;s just how it is, but in general)?<p>And even if you can&#x27;t do so for any reasonable reason in functional (again, in general), what stops us to just add syntactic sugar to equal it to the second to make programmer&#x27;s life easier?</div><br/><div id="37700224" class="c"><input type="checkbox" id="c-37700224" checked=""/><div class="controls bullet"><span class="by">Polygator</span><span>|</span><a href="#37699970">root</a><span>|</span><a href="#37700035">parent</a><span>|</span><a href="#37700017">next</a><span>|</span><label class="collapse" for="c-37700224">[-]</label><label class="expand" for="c-37700224">[2 more]</label></div><br/><div class="children"><div class="content">The fundamental reason why many functional languages won&#x27;t allow you to do the first is that they use immutable data structures.<p>We could indeed introduce syntactic sugar (`y= (x[0]:=10)` maybe), but you&#x27;ll still need to introduce a new variable to hold the modified list.</div><br/><div id="37700539" class="c"><input type="checkbox" id="c-37700539" checked=""/><div class="controls bullet"><span class="by">thrdbndndn</span><span>|</span><a href="#37699970">root</a><span>|</span><a href="#37700224">parent</a><span>|</span><a href="#37700017">next</a><span>|</span><label class="collapse" for="c-37700539">[-]</label><label class="expand" for="c-37700539">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s my understanding that, at least in Python, you can&#x27;t change immutable data type but you can just assign a new data to the same variable and therefore overwrite it, right? So even if JAX makes list type immutable, you can still just re-use `x` to save the new modified list.</div><br/></div></div></div></div></div></div><div id="37700017" class="c"><input type="checkbox" id="c-37700017" checked=""/><div class="controls bullet"><span class="by">j7ake</span><span>|</span><a href="#37699970">parent</a><span>|</span><a href="#37700035">prev</a><span>|</span><a href="#37700417">next</a><span>|</span><label class="collapse" for="c-37700017">[-]</label><label class="expand" for="c-37700017">[1 more]</label></div><br/><div class="children"><div class="content">Also conditionals can be tricky (greater, if else) and often need rewriting.</div><br/></div></div></div></div><div id="37700417" class="c"><input type="checkbox" id="c-37700417" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#37699970">prev</a><span>|</span><a href="#37699295">next</a><span>|</span><label class="collapse" for="c-37700417">[-]</label><label class="expand" for="c-37700417">[1 more]</label></div><br/><div class="children"><div class="content">Does it support arrays of variable lengths now? Last time I looked, I think this was not supported. So it means, for every variable dimension, you need to use an upper bound, and then use masking properly, and hope that it would not waste computation too much on the unused part (e.g. when running a loop over it).<p>I&#x27;m working with sequences, e.g. speech recognition, machine translation, language modeling. This is a quite fundamental property for this type of models, that we have variable lengths sequences.<p>In those cases, for some example code, I have seen that training also used only fixed size dimensions. And at inference time, they had some non-JAX code for the loop over the sequence around the JAX code with fixed-size dimensions.<p>This seems like a quite fundamental issue to me? I wonder a bit that this is not an issue for others.</div><br/></div></div><div id="37699295" class="c"><input type="checkbox" id="c-37699295" checked=""/><div class="controls bullet"><span class="by">subhrm</span><span>|</span><a href="#37700417">prev</a><span>|</span><a href="#37700508">next</a><span>|</span><label class="collapse" for="c-37699295">[-]</label><label class="expand" for="c-37699295">[5 more]</label></div><br/><div class="children"><div class="content">JAX GPU support is limited to Linux only. Even the WSL2 support is experimental. <a href="https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;installation.html#supported-platforms" rel="nofollow noreferrer">https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;installation.html#suppo...</a></div><br/><div id="37699635" class="c"><input type="checkbox" id="c-37699635" checked=""/><div class="controls bullet"><span class="by">pjtr</span><span>|</span><a href="#37699295">parent</a><span>|</span><a href="#37699729">next</a><span>|</span><label class="collapse" for="c-37699635">[-]</label><label class="expand" for="c-37699635">[3 more]</label></div><br/><div class="children"><div class="content">Is there a specific reason why Windows is not supported?</div><br/><div id="37700457" class="c"><input type="checkbox" id="c-37700457" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#37699295">root</a><span>|</span><a href="#37699635">parent</a><span>|</span><a href="#37699668">next</a><span>|</span><label class="collapse" for="c-37700457">[-]</label><label class="expand" for="c-37700457">[1 more]</label></div><br/><div class="children"><div class="content">Presumably because the Google cloud doesn&#x27;t run on Windows. Well, nothin HPC related runs Windows.</div><br/></div></div><div id="37699668" class="c"><input type="checkbox" id="c-37699668" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#37699295">root</a><span>|</span><a href="#37699635">parent</a><span>|</span><a href="#37700457">prev</a><span>|</span><a href="#37699729">next</a><span>|</span><label class="collapse" for="c-37699668">[-]</label><label class="expand" for="c-37699668">[1 more]</label></div><br/><div class="children"><div class="content">Because no one has done the work to add it... Could be you!</div><br/></div></div></div></div><div id="37699729" class="c"><input type="checkbox" id="c-37699729" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#37699295">parent</a><span>|</span><a href="#37699635">prev</a><span>|</span><a href="#37700508">next</a><span>|</span><label class="collapse" for="c-37699729">[-]</label><label class="expand" for="c-37699729">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t build on Windows at all, as well.</div><br/></div></div></div></div><div id="37700508" class="c"><input type="checkbox" id="c-37700508" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#37699295">prev</a><span>|</span><a href="#37700905">next</a><span>|</span><label class="collapse" for="c-37700508">[-]</label><label class="expand" for="c-37700508">[1 more]</label></div><br/><div class="children"><div class="content">Imho the project should emphasize more the potential for simple and uniquitous multi-core acceleration of vector compute that is available by definition to anybody having any modern cpu.<p>nvidia dropped cuda support for perfectly good gpu&#x27;s, showing the perils and waste of being locked-in in a profit-maximazing monopoly.</div><br/></div></div><div id="37700905" class="c"><input type="checkbox" id="c-37700905" checked=""/><div class="controls bullet"><span class="by">jarym</span><span>|</span><a href="#37700508">prev</a><span>|</span><a href="#37699633">next</a><span>|</span><label class="collapse" for="c-37700905">[-]</label><label class="expand" for="c-37700905">[2 more]</label></div><br/><div class="children"><div class="content">Now if only it supported ONNX export or was cross-platform so I could run it from Java and .NET land</div><br/><div id="37700929" class="c"><input type="checkbox" id="c-37700929" checked=""/><div class="controls bullet"><span class="by">BenoitP</span><span>|</span><a href="#37700905">parent</a><span>|</span><a href="#37699633">next</a><span>|</span><label class="collapse" for="c-37700929">[-]</label><label class="expand" for="c-37700929">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jaxonnxruntime">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jaxonnxruntime</a></div><br/></div></div></div></div><div id="37699633" class="c"><input type="checkbox" id="c-37699633" checked=""/><div class="controls bullet"><span class="by">ayakang31415</span><span>|</span><a href="#37700905">prev</a><span>|</span><a href="#37699498">next</a><span>|</span><label class="collapse" for="c-37699633">[-]</label><label class="expand" for="c-37699633">[1 more]</label></div><br/><div class="children"><div class="content">I have been working on my DNN model using TensorFlow even though ML is not my main research. But it is a substantial part of my research, so I have to figure things out on my own, and I have done so over the past 3 years. However, I spend so much time on figuring out how any of TF methods works and debugging them. I never used JAX but I am not sure if this sort of grinding is normal when you use JAX as well (I always hear great things about JAX). I have built so many things using TF I don&#x27;t think it is wise for me to learn JAX and migrate my work into JAX code base.</div><br/></div></div><div id="37699498" class="c"><input type="checkbox" id="c-37699498" checked=""/><div class="controls bullet"><span class="by">nharada</span><span>|</span><a href="#37699633">prev</a><span>|</span><a href="#37699554">next</a><span>|</span><label class="collapse" for="c-37699498">[-]</label><label class="expand" for="c-37699498">[1 more]</label></div><br/><div class="children"><div class="content">For large scale ML Jax is pretty nice. It makes the multi-host computation feel like a first class citizen and your programs tend to be designed with that in mind.</div><br/></div></div><div id="37699554" class="c"><input type="checkbox" id="c-37699554" checked=""/><div class="controls bullet"><span class="by">tasubotadas</span><span>|</span><a href="#37699498">prev</a><span>|</span><a href="#37700431">next</a><span>|</span><label class="collapse" for="c-37699554">[-]</label><label class="expand" for="c-37699554">[4 more]</label></div><br/><div class="children"><div class="content">Is there any benefit using it instead of pytorch?</div><br/><div id="37700801" class="c"><input type="checkbox" id="c-37700801" checked=""/><div class="controls bullet"><span class="by">missingET</span><span>|</span><a href="#37699554">parent</a><span>|</span><a href="#37699565">next</a><span>|</span><label class="collapse" for="c-37700801">[-]</label><label class="expand" for="c-37700801">[1 more]</label></div><br/><div class="children"><div class="content">Jax has a much nicer handling of higher order differentiation. PyTorch has functions to compute Hessians and there are libraries to keep differentiability through optimizers, but going out of their standard use-cases becomes tricky very fast. In contrast, JAX can compute nth-derivatives of things very easily.</div><br/></div></div><div id="37699565" class="c"><input type="checkbox" id="c-37699565" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#37699554">parent</a><span>|</span><a href="#37700801">prev</a><span>|</span><a href="#37699738">next</a><span>|</span><label class="collapse" for="c-37699565">[-]</label><label class="expand" for="c-37699565">[1 more]</label></div><br/><div class="children"><div class="content">The main benefit in my experience is that it’s much easier to do distributed computations in JAX. It has a much nicer API. For single device computing there’s no advantage either way.</div><br/></div></div><div id="37699738" class="c"><input type="checkbox" id="c-37699738" checked=""/><div class="controls bullet"><span class="by">marmaduke</span><span>|</span><a href="#37699554">parent</a><span>|</span><a href="#37699565">prev</a><span>|</span><a href="#37700431">next</a><span>|</span><label class="collapse" for="c-37699738">[-]</label><label class="expand" for="c-37699738">[1 more]</label></div><br/><div class="children"><div class="content">If you like functional languages, then Jax will fit better for you.  It provides a bunch of function transformations to implement eg grad, JIT etc.</div><br/></div></div></div></div><div id="37700431" class="c"><input type="checkbox" id="c-37700431" checked=""/><div class="controls bullet"><span class="by">ipunchghosts</span><span>|</span><a href="#37699554">prev</a><span>|</span><a href="#37699261">next</a><span>|</span><label class="collapse" for="c-37700431">[-]</label><label class="expand" for="c-37700431">[1 more]</label></div><br/><div class="children"><div class="content">Jax is ceres but for python!</div><br/></div></div><div id="37699261" class="c"><input type="checkbox" id="c-37699261" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#37700431">prev</a><span>|</span><a href="#37699191">next</a><span>|</span><label class="collapse" for="c-37699261">[-]</label><label class="expand" for="c-37699261">[6 more]</label></div><br/><div class="children"><div class="content">Anybody using it in production? Is it, or its derivatives like Flax, worth using over pyTorch for anything?<p>edit: Made comparison more fair.</div><br/><div id="37699291" class="c"><input type="checkbox" id="c-37699291" checked=""/><div class="controls bullet"><span class="by">_delirium</span><span>|</span><a href="#37699261">parent</a><span>|</span><a href="#37699325">next</a><span>|</span><label class="collapse" for="c-37699291">[-]</label><label class="expand" for="c-37699291">[2 more]</label></div><br/><div class="children"><div class="content">I’m a researcher, not using anything in production, but I find jax more usable as a general GPU-accelerated tensor math library. PyTorch is more specifically targeted at the neural network use case. It can be shoehorned into other use cases, but is clearly designed &amp; documented for NN training &amp; inference.</div><br/><div id="37699344" class="c"><input type="checkbox" id="c-37699344" checked=""/><div class="controls bullet"><span class="by">mvuksano</span><span>|</span><a href="#37699261">root</a><span>|</span><a href="#37699291">parent</a><span>|</span><a href="#37699325">next</a><span>|</span><label class="collapse" for="c-37699344">[-]</label><label class="expand" for="c-37699344">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. I used Jax about a year ago to estimate some diode parameters for a side project of mine.</div><br/></div></div></div></div><div id="37699325" class="c"><input type="checkbox" id="c-37699325" checked=""/><div class="controls bullet"><span class="by">mvuksano</span><span>|</span><a href="#37699261">parent</a><span>|</span><a href="#37699291">prev</a><span>|</span><a href="#37699876">next</a><span>|</span><label class="collapse" for="c-37699325">[-]</label><label class="expand" for="c-37699325">[1 more]</label></div><br/><div class="children"><div class="content">Not a fair comparison IMO. Jax is low level library used to make ML frameworks while pytorch is a full blow ML framework.<p>In terms of is it worth using it - that depends on what you&#x27;re doing. If you just want to start with ML training probably not. If you have something already and you want to take it to next level (e.g. influence how training and inference work) than it&#x27;s a good choice. You might be interested in looking into flax or haiku instead of using vanilla Jax. These are closer to pytorch.</div><br/></div></div><div id="37699876" class="c"><input type="checkbox" id="c-37699876" checked=""/><div class="controls bullet"><span class="by">technocratius</span><span>|</span><a href="#37699261">parent</a><span>|</span><a href="#37699325">prev</a><span>|</span><a href="#37699570">next</a><span>|</span><label class="collapse" for="c-37699876">[-]</label><label class="expand" for="c-37699876">[1 more]</label></div><br/><div class="children"><div class="content">We recently switched to Jax to boost performance as we scale up our algorithmic core. The nice thing is that it presents only a minor jump in capabilities to get developers working with it, if they have prior exposure to numpy ofcourse. Quite nice :)</div><br/></div></div><div id="37699570" class="c"><input type="checkbox" id="c-37699570" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#37699261">parent</a><span>|</span><a href="#37699876">prev</a><span>|</span><a href="#37699191">next</a><span>|</span><label class="collapse" for="c-37699570">[-]</label><label class="expand" for="c-37699570">[1 more]</label></div><br/><div class="children"><div class="content">A number of large AI companies use it to train their large models; Midjourney, Stability, Anthropic, DeepMind, among others.</div><br/></div></div></div></div><div id="37699191" class="c"><input type="checkbox" id="c-37699191" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#37699261">prev</a><span>|</span><a href="#37699540">next</a><span>|</span><label class="collapse" for="c-37699191">[-]</label><label class="expand" for="c-37699191">[4 more]</label></div><br/><div class="children"><div class="content">What happened to Jax? Is it still alive?</div><br/><div id="37699275" class="c"><input type="checkbox" id="c-37699275" checked=""/><div class="controls bullet"><span class="by">antognini</span><span>|</span><a href="#37699191">parent</a><span>|</span><a href="#37699224">next</a><span>|</span><label class="collapse" for="c-37699275">[-]</label><label class="expand" for="c-37699275">[1 more]</label></div><br/><div class="children"><div class="content">Very much alive.  From what I can tell it has more or less replaced Tensorflow for research purposes.  (A lot of researchers use PyTorch though.)</div><br/></div></div><div id="37699224" class="c"><input type="checkbox" id="c-37699224" checked=""/><div class="controls bullet"><span class="by">maegul</span><span>|</span><a href="#37699191">parent</a><span>|</span><a href="#37699275">prev</a><span>|</span><a href="#37699878">next</a><span>|</span><label class="collapse" for="c-37699224">[-]</label><label class="expand" for="c-37699224">[1 more]</label></div><br/><div class="children"><div class="content">Development seems not to have dropped at all from the contributions page: <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax&#x2F;graphs&#x2F;contributors">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax&#x2F;graphs&#x2F;contributors</a><p>Don’t know about usage and uptake though.</div><br/></div></div><div id="37699878" class="c"><input type="checkbox" id="c-37699878" checked=""/><div class="controls bullet"><span class="by">technocratius</span><span>|</span><a href="#37699191">parent</a><span>|</span><a href="#37699224">prev</a><span>|</span><a href="#37699540">next</a><span>|</span><label class="collapse" for="c-37699878">[-]</label><label class="expand" for="c-37699878">[1 more]</label></div><br/><div class="children"><div class="content">Why would it be dead?</div><br/></div></div></div></div><div id="37699540" class="c"><input type="checkbox" id="c-37699540" checked=""/><div class="controls bullet"><span class="by">amir734jj</span><span>|</span><a href="#37699191">prev</a><span>|</span><label class="collapse" for="c-37699540">[-]</label><label class="expand" for="c-37699540">[4 more]</label></div><br/><div class="children"><div class="content">Very unrelated but I did job interview with Nvidia JAX team for a compiler engineer role some time ago, not very friendly and very opinionated.</div><br/><div id="37699594" class="c"><input type="checkbox" id="c-37699594" checked=""/><div class="controls bullet"><span class="by">aportnoy</span><span>|</span><a href="#37699540">parent</a><span>|</span><label class="collapse" for="c-37699594">[-]</label><label class="expand" for="c-37699594">[3 more]</label></div><br/><div class="children"><div class="content">What happened?</div><br/><div id="37699652" class="c"><input type="checkbox" id="c-37699652" checked=""/><div class="controls bullet"><span class="by">amir734jj</span><span>|</span><a href="#37699540">root</a><span>|</span><a href="#37699594">parent</a><span>|</span><label class="collapse" for="c-37699652">[-]</label><label class="expand" for="c-37699652">[2 more]</label></div><br/><div class="children"><div class="content">Nothing happened. It was an informal technical interview with the program manager at JAX. 1 hour call and the interview was remote but describing him as opinionated and entitled is an understatement. Best of luck to them.</div><br/><div id="37700188" class="c"><input type="checkbox" id="c-37700188" checked=""/><div class="controls bullet"><span class="by">Hendrikto</span><span>|</span><a href="#37699540">root</a><span>|</span><a href="#37699652">parent</a><span>|</span><label class="collapse" for="c-37700188">[-]</label><label class="expand" for="c-37700188">[1 more]</label></div><br/><div class="children"><div class="content">Would you mind sharing some details? It sounds like an interesting peek behind the curtain.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>