<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704877265414" as="style"/><link rel="stylesheet" href="styles.css?v=1704877265414"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2401.04081">MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>jonbaer</span> | <span>30 comments</span></div><br/><div><div id="38934640" class="c"><input type="checkbox" id="c-38934640" checked=""/><div class="controls bullet"><span class="by">mgreg</span><span>|</span><a href="#38936269">next</a><span>|</span><label class="collapse" for="c-38934640">[-]</label><label class="expand" for="c-38934640">[10 more]</label></div><br/><div class="children"><div class="content">I very much appreciate that the authors not only published their code (<a href="https:&#x2F;&#x2F;github.com&#x2F;llm-random&#x2F;llm-random">https:&#x2F;&#x2F;github.com&#x2F;llm-random&#x2F;llm-random</a>) but included the dataset they used (available on Huggingface - <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;c4" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;c4</a>) as well as the training process and hyperparameters they used so others can replicate and build on their work.  The only thing really missing is the weights which would be nice to have on huggingface as well.</div><br/><div id="38934716" class="c"><input type="checkbox" id="c-38934716" checked=""/><div class="controls bullet"><span class="by">swells34</span><span>|</span><a href="#38934640">parent</a><span>|</span><a href="#38936269">next</a><span>|</span><label class="collapse" for="c-38934716">[-]</label><label class="expand" for="c-38934716">[9 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very confusing to me that you are praising the authors of a published scientific paper for <i>almost</i> making their work reproduceable.</div><br/><div id="38935102" class="c"><input type="checkbox" id="c-38935102" checked=""/><div class="controls bullet"><span class="by">mgreg</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38934716">parent</a><span>|</span><a href="#38936837">next</a><span>|</span><label class="collapse" for="c-38935102">[-]</label><label class="expand" for="c-38935102">[2 more]</label></div><br/><div class="children"><div class="content">I understand where you&#x27;re coming from but what they provided DOES make their work reproducible.  You can use the data, source code, and recipe to train the model and get the weights.<p>It would be nice if they provided the weights so it could be USABLE without the effort or knowledge required.<p>We (I think) would all like to see more _truly_ open models (not just the source code) that enable collaboration in the community.</div><br/><div id="38936696" class="c"><input type="checkbox" id="c-38936696" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38935102">parent</a><span>|</span><a href="#38936837">next</a><span>|</span><label class="collapse" for="c-38936696">[-]</label><label class="expand" for="c-38936696">[1 more]</label></div><br/><div class="children"><div class="content">Only if they also include the random seed they used for the initial weights, otherwise you may be able to reproduce similar performance but will not likely obtain their same weights.</div><br/></div></div></div></div><div id="38936837" class="c"><input type="checkbox" id="c-38936837" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38934716">parent</a><span>|</span><a href="#38935102">prev</a><span>|</span><a href="#38936232">next</a><span>|</span><label class="collapse" for="c-38936837">[-]</label><label class="expand" for="c-38936837">[1 more]</label></div><br/><div class="children"><div class="content">If we had a proper data version control, wherein the git commit hash was tied directly to the output data hash and hosted on IPFS (and the make system checked ipfs like it does local files for the cache) then it would be absolutely reproducible.<p>And the wonderful thing is, every person that used git clone on this repo and ran it would be serving the NN weights.<p>But alas, this unfortunately hasn&#x27;t been done yet.</div><br/></div></div><div id="38936232" class="c"><input type="checkbox" id="c-38936232" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38934716">parent</a><span>|</span><a href="#38936837">prev</a><span>|</span><a href="#38934735">next</a><span>|</span><label class="collapse" for="c-38936232">[-]</label><label class="expand" for="c-38936232">[2 more]</label></div><br/><div class="children"><div class="content">The weights aren&#x27;t needed to make it reproducable. The code and training data are needed. Hopefully if you used those, you&#x27;d ultimately reach the same result.</div><br/><div id="38936543" class="c"><input type="checkbox" id="c-38936543" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38936232">parent</a><span>|</span><a href="#38934735">next</a><span>|</span><label class="collapse" for="c-38936543">[-]</label><label class="expand" for="c-38936543">[1 more]</label></div><br/><div class="children"><div class="content">Even in the days where this was standard, that is not the case entirely.<p>There is a whole other world between &quot;released code&quot; and &quot;getting the results as seen in the paper&quot;.<p>Unfortunately. The reproducibility crisis is very much well and alive! :&#x27;( Much more to go into but it is a deep rabbit hole, indeedy. :&#x27;((((</div><br/></div></div></div></div><div id="38934735" class="c"><input type="checkbox" id="c-38934735" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38934716">parent</a><span>|</span><a href="#38936232">prev</a><span>|</span><a href="#38934777">next</a><span>|</span><label class="collapse" for="c-38934735">[-]</label><label class="expand" for="c-38934735">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what confusing means.</div><br/></div></div><div id="38934777" class="c"><input type="checkbox" id="c-38934777" checked=""/><div class="controls bullet"><span class="by">jakderrida</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38934716">parent</a><span>|</span><a href="#38934735">prev</a><span>|</span><a href="#38936269">next</a><span>|</span><label class="collapse" for="c-38934777">[-]</label><label class="expand" for="c-38934777">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a sad world where our standards are that low. But they are that low for good reasons.</div><br/><div id="38936396" class="c"><input type="checkbox" id="c-38936396" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#38934640">root</a><span>|</span><a href="#38934777">parent</a><span>|</span><a href="#38936269">next</a><span>|</span><label class="collapse" for="c-38936396">[-]</label><label class="expand" for="c-38936396">[1 more]</label></div><br/><div class="children"><div class="content">If anything CS papers are far more reproducible than most papers. Maybe that is sad, but I think most scientists and researchers are trying their best.</div><br/></div></div></div></div></div></div></div></div><div id="38936269" class="c"><input type="checkbox" id="c-38936269" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#38934640">prev</a><span>|</span><a href="#38934918">next</a><span>|</span><label class="collapse" for="c-38936269">[-]</label><label class="expand" for="c-38936269">[2 more]</label></div><br/><div class="children"><div class="content">Interesting and expected core idea.  The MoE models in table 1 have drastically larger total parameter count than the baseline transformer, which in turn is super tiny for the purpose of NLP (25M parameters). I suppose inference is of similar speed for the large MoE and the simple mamba model, so for some applications that extra parameter count is OK, but I don’t know how these performance benefits would scale when the model reaches practical sizes at which point the 32x larger MoE may be unrealistic for training purposes.  I’d be very interested to see a real life application of traditional mamba scaling to the 70B or more parameters, or any attempts to get meaningful parallelizing of the original (or the MoE) model.</div><br/><div id="38936803" class="c"><input type="checkbox" id="c-38936803" checked=""/><div class="controls bullet"><span class="by">webappguy</span><span>|</span><a href="#38936269">parent</a><span>|</span><a href="#38934918">next</a><span>|</span><label class="collapse" for="c-38936803">[-]</label><label class="expand" for="c-38936803">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d put my money OpenAI has tried internally</div><br/></div></div></div></div><div id="38934918" class="c"><input type="checkbox" id="c-38934918" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38936269">prev</a><span>|</span><a href="#38934334">next</a><span>|</span><label class="collapse" for="c-38934918">[-]</label><label class="expand" for="c-38934918">[3 more]</label></div><br/><div class="children"><div class="content">This is cool. And interesting, in that MoE+Transformers gets you a little bit of something for free on the training side, and a medium amount of something for free on the inference side -- seeing that MoE still gives those benefits with State space architectures is useful information; it looks like it&#x27;s about twice as efficient in training time over non-MoE Mamba, according to the paper.<p>I don&#x27;t understand Mamba&#x27;s architecture well enough to predict much about it, but I do think we&#x27;ll see significant exploration of MoE architectural ideas this year, and that will be cool!</div><br/><div id="38935070" class="c"><input type="checkbox" id="c-38935070" checked=""/><div class="controls bullet"><span class="by">theLiminator</span><span>|</span><a href="#38934918">parent</a><span>|</span><a href="#38934334">next</a><span>|</span><label class="collapse" for="c-38935070">[-]</label><label class="expand" for="c-38935070">[2 more]</label></div><br/><div class="children"><div class="content">I thought training time&#x2F;compute scales linearly with number of experts?</div><br/></div></div></div></div><div id="38934334" class="c"><input type="checkbox" id="c-38934334" checked=""/><div class="controls bullet"><span class="by">marviel</span><span>|</span><a href="#38934918">prev</a><span>|</span><a href="#38935606">next</a><span>|</span><label class="collapse" for="c-38934334">[-]</label><label class="expand" for="c-38934334">[11 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll struggle through the mamba papers, but does anyone have a good post or article that gives an intuition about what is fundamentally different about Mamba compared to transformers?</div><br/><div id="38935601" class="c"><input type="checkbox" id="c-38935601" checked=""/><div class="controls bullet"><span class="by">mochidusk</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38934545">next</a><span>|</span><label class="collapse" for="c-38935601">[-]</label><label class="expand" for="c-38935601">[2 more]</label></div><br/><div class="children"><div class="content">I struggled learning about Mamba&#x27;s architecture but realized it&#x27;s because I had some gaps in knowledge. In no particular order, they were:<p>- a refresher on differential equations<p>- legendre polynomials<p>- state spaced models; you need to grok the essence of<p>x&#x27; = Ax + Bu<p>y  = Cx<p>- discretization of S4<p>- HiPPO matrix<p>- GPU architecture (SRAM, HBM)<p>Basically, transformers is an architecture that uses attention. Mamba is the same architecture that replaces attention with S4 - but this S4 is modified to overcome its shortcomings, allowing it to act like a CNN during training and an RNN during inference.<p>I found this video very helpful: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8Q_tqwpTpVU" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8Q_tqwpTpVU</a><p>His other videos are really good too.</div><br/><div id="38935933" class="c"><input type="checkbox" id="c-38935933" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#38934334">root</a><span>|</span><a href="#38935601">parent</a><span>|</span><a href="#38934545">next</a><span>|</span><label class="collapse" for="c-38935933">[-]</label><label class="expand" for="c-38935933">[1 more]</label></div><br/><div class="children"><div class="content">I was about to post that video too. Highly recommended.</div><br/></div></div></div></div><div id="38934545" class="c"><input type="checkbox" id="c-38934545" checked=""/><div class="controls bullet"><span class="by">DarmokJalad1701</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38935601">prev</a><span>|</span><a href="#38936162">next</a><span>|</span><label class="collapse" for="c-38934545">[-]</label><label class="expand" for="c-38934545">[1 more]</label></div><br/><div class="children"><div class="content">This is an article about &quot;S4&quot; that uses &quot;Structured State Spaces&quot;.<p><a href="https:&#x2F;&#x2F;srush.github.io&#x2F;annotated-s4&#x2F;" rel="nofollow">https:&#x2F;&#x2F;srush.github.io&#x2F;annotated-s4&#x2F;</a><p>The matrices that make up the state space (A, B and C) are constant in S4. This allowed them to represent some of the math operations as a convolution (which can be parallelized).<p>The difference between S4 and Mamba is that these matrices are input-dependent in Mamba. Plus they add in some CUDA stuff (&quot;parallel scan&quot;) to make it faster to compute on a GPU even if these matrices are not constant.<p>Yannic Kilcher&#x27;s video on Mamba might also be a good resource: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;9dSkvxS2EB0" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;9dSkvxS2EB0</a></div><br/></div></div><div id="38936162" class="c"><input type="checkbox" id="c-38936162" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38934545">prev</a><span>|</span><a href="#38937034">next</a><span>|</span><label class="collapse" for="c-38936162">[-]</label><label class="expand" for="c-38936162">[1 more]</label></div><br/><div class="children"><div class="content">I’ve spent a few hours reading the Mamba paper, examining the code, and watching Yannick’s YouTube dive into the paper. If you took discrete time signal processing in school, you will have what you need to work through the math.<p>Here is my intuition about Mamba. It’s a linear discrete time signal filter where the filter blocks are conditioned on the input at each time step as well as receiving the input as a normal filter would. Think of the predecessor to Mamba as a discrete time filter without this conditioning. That’s the first innovation.<p>The other innovation is some lovely algebra allowing them to reorganize things so they can be computed much more efficiently on current GPU hardware, with respect to slow and fast memory.<p>State space models are very simple conceptually. It’s kind of amazing that Mamba does so well on long sequence modeling because the architecture is ridiculously simple. But when you consider that the state matrices are conditioned on the input, it all makes sense. Somehow, they learn how to adapt the “filter” based on the input. That’s where the intelligence lies.</div><br/></div></div><div id="38937034" class="c"><input type="checkbox" id="c-38937034" checked=""/><div class="controls bullet"><span class="by">gschoeni</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38936162">prev</a><span>|</span><a href="#38935434">next</a><span>|</span><label class="collapse" for="c-38937034">[-]</label><label class="expand" for="c-38937034">[1 more]</label></div><br/><div class="children"><div class="content">We went over it in our Friday paper club before the holidays which helped me gain an intuition.<p><a href="https:&#x2F;&#x2F;blog.oxen.ai&#x2F;mamba-linear-time-sequence-modeling-with-selective-state-spaces-arxiv-dives&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.oxen.ai&#x2F;mamba-linear-time-sequence-modeling-wit...</a><p>I&#x27;m still not convinced on Mamba&#x27;s performance on Natural Language tasks, but maybe it&#x27;s just because they haven&#x27;t trained a large enough model on enough data yet.</div><br/></div></div><div id="38935434" class="c"><input type="checkbox" id="c-38935434" checked=""/><div class="controls bullet"><span class="by">anomaly72</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38937034">prev</a><span>|</span><a href="#38934737">next</a><span>|</span><label class="collapse" for="c-38935434">[-]</label><label class="expand" for="c-38935434">[1 more]</label></div><br/><div class="children"><div class="content">Would recommend Sasha Rush&#x27;s lecture <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dKJEpOtVgXc" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dKJEpOtVgXc</a>, it provides the intuition for state space models via linear RNNs</div><br/></div></div><div id="38934737" class="c"><input type="checkbox" id="c-38934737" checked=""/><div class="controls bullet"><span class="by">RandyRanderson</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38935434">prev</a><span>|</span><a href="#38934523">next</a><span>|</span><label class="collapse" for="c-38934737">[-]</label><label class="expand" for="c-38934737">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9dSkvxS2EB0" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=9dSkvxS2EB0</a> via<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;@YannicKilcher" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;@YannicKilcher</a></div><br/></div></div><div id="38934523" class="c"><input type="checkbox" id="c-38934523" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38934737">prev</a><span>|</span><a href="#38934679">next</a><span>|</span><label class="collapse" for="c-38934523">[-]</label><label class="expand" for="c-38934523">[1 more]</label></div><br/><div class="children"><div class="content">I read this today: <a href="https:&#x2F;&#x2F;www.unite.ai&#x2F;mamba-redefining-sequence-modeling-and-outforming-transformers-architecture&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.unite.ai&#x2F;mamba-redefining-sequence-modeling-and-...</a></div><br/></div></div><div id="38934679" class="c"><input type="checkbox" id="c-38934679" checked=""/><div class="controls bullet"><span class="by">markrogersjr</span><span>|</span><a href="#38934334">parent</a><span>|</span><a href="#38934523">prev</a><span>|</span><a href="#38934555">next</a><span>|</span><label class="collapse" for="c-38934679">[-]</label><label class="expand" for="c-38934679">[1 more]</label></div><br/><div class="children"><div class="content">I’d recommend starting with the appendix of the original hippo paper.</div><br/></div></div></div></div><div id="38935606" class="c"><input type="checkbox" id="c-38935606" checked=""/><div class="controls bullet"><span class="by">deyiao</span><span>|</span><a href="#38934334">prev</a><span>|</span><a href="#38935855">next</a><span>|</span><label class="collapse" for="c-38935606">[-]</label><label class="expand" for="c-38935606">[2 more]</label></div><br/><div class="children"><div class="content">&quot;The MOE architecture uses 20 times the parameters, is this comparison fair? Can it be compared with a single model that also uses 20 times the parameters?&quot;</div><br/><div id="38935981" class="c"><input type="checkbox" id="c-38935981" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38935606">parent</a><span>|</span><a href="#38935855">next</a><span>|</span><label class="collapse" for="c-38935981">[-]</label><label class="expand" for="c-38935981">[1 more]</label></div><br/><div class="children"><div class="content">It has more parameters, but not all of them are used during inference. They compared models that use equal numbers of parameters.</div><br/></div></div></div></div><div id="38935855" class="c"><input type="checkbox" id="c-38935855" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#38935606">prev</a><span>|</span><label class="collapse" for="c-38935855">[-]</label><label class="expand" for="c-38935855">[1 more]</label></div><br/><div class="children"><div class="content">I wonder whether there is a way to use existing model weights (e.g. from open llms) to populate other types of models with a set of weights closer to final state than where&#x27;d they would have started from scratch. I am very much speaking from a position of ignorance, but being mechanistically derived.....</div><br/></div></div></div></div></div></div></div></body></html>