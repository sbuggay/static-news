<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697446867915" as="style"/><link rel="stylesheet" href="styles.css?v=1697446867915"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2310.09199">PaLI-3 Vision Language Models</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>maccaw</span> | <span>15 comments</span></div><br/><div><div id="37896092" class="c"><input type="checkbox" id="c-37896092" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#37897030">next</a><span>|</span><label class="collapse" for="c-37896092">[-]</label><label class="expand" for="c-37896092">[1 more]</label></div><br/><div class="children"><div class="content">Something that stood out to me skimming the paper - that was somewhat buried - they finetune the model on each benchmark.<p>&quot;Finally, for each individual task (benchmark), we fine-tune the PaLI-3 model with frozen ViT image encoder on the task’s training data as described in the cor- responding section. For most tasks, we fine-tune the 812×812 resolution checkpoint, but for two document understanding tasks, we go up to 1064×1064 resolution&quot;<p>S̶o̶ ̶t̶h̶i̶s̶ ̶i̶s̶ ̶c̶o̶m̶p̶a̶r̶i̶n̶g̶ ̶a̶ ̶s̶m̶a̶l̶l̶e̶r̶ ̶m̶o̶d̶e̶l̶ ̶f̶i̶n̶e̶t̶u̶n̶e̶d̶ ̶p̶e̶r̶ ̶b̶e̶n̶c̶h̶m̶a̶r̶k̶ ̶t̶o̶ ̶l̶a̶r̶g̶e̶r̶ ̶m̶o̶d̶e̶l̶s̶ ̶t̶h̶a̶t̶ ̶I̶ ̶p̶r̶e̶s̶u̶m̶e̶ ̶a̶r̶e̶ ̶n̶o̶t̶,̶ ̶t̶h̶o̶u̶g̶h̶ ̶I̶ ̶h̶a̶v̶e̶ ̶n̶o̶t̶ ̶r̶e̶a̶d̶ ̶t̶h̶e̶ ̶P̶a̶l̶i̶-̶X̶ ̶p̶a̶p̶e̶r̶.̶<p>Edit - No, I was wrong, Palm-X is also fine-tuned before each task&#x2F;set of tasks.<p>Impressive improvement!!!</div><br/></div></div><div id="37897030" class="c"><input type="checkbox" id="c-37897030" checked=""/><div class="controls bullet"><span class="by">Technotroll</span><span>|</span><a href="#37896092">prev</a><span>|</span><a href="#37896065">next</a><span>|</span><label class="collapse" for="c-37897030">[-]</label><label class="expand" for="c-37897030">[1 more]</label></div><br/><div class="children"><div class="content">Does the vision-language-model process raw image data, or does it process OCR character output?</div><br/></div></div><div id="37896065" class="c"><input type="checkbox" id="c-37896065" checked=""/><div class="controls bullet"><span class="by">tracyhenry</span><span>|</span><a href="#37897030">prev</a><span>|</span><a href="#37896628">next</a><span>|</span><label class="collapse" for="c-37896065">[-]</label><label class="expand" for="c-37896065">[4 more]</label></div><br/><div class="children"><div class="content">maybe someone more informed can help me understand why they didn&#x27;t compared to Llava (<a href="https:&#x2F;&#x2F;llava-vl.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llava-vl.github.io&#x2F;</a>)?</div><br/><div id="37896288" class="c"><input type="checkbox" id="c-37896288" checked=""/><div class="controls bullet"><span class="by">kolja005</span><span>|</span><a href="#37896065">parent</a><span>|</span><a href="#37896191">next</a><span>|</span><label class="collapse" for="c-37896288">[-]</label><label class="expand" for="c-37896288">[1 more]</label></div><br/><div class="children"><div class="content">The purpose of this research is to compare large vision-language models where the vision component is pre-trained using different techniques, namely on image classification versus unsupervised contrastive pre-training (see OpenAI&#x27;s CLIP). PaLI-3 also isn&#x27;t an instruction-tuned model, so comparing it to Llava would be a little apples-to-oranges.</div><br/></div></div><div id="37896191" class="c"><input type="checkbox" id="c-37896191" checked=""/><div class="controls bullet"><span class="by">dartos</span><span>|</span><a href="#37896065">parent</a><span>|</span><a href="#37896288">prev</a><span>|</span><a href="#37896628">next</a><span>|</span><label class="collapse" for="c-37896191">[-]</label><label class="expand" for="c-37896191">[2 more]</label></div><br/><div class="children"><div class="content">Maybe they just didn’t know about llava while conducting their research. It can take days to train a model sometimes.</div><br/><div id="37896211" class="c"><input type="checkbox" id="c-37896211" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#37896065">root</a><span>|</span><a href="#37896191">parent</a><span>|</span><a href="#37896628">next</a><span>|</span><label class="collapse" for="c-37896211">[-]</label><label class="expand" for="c-37896211">[1 more]</label></div><br/><div class="children"><div class="content">Weeks to months at larger scales even.</div><br/></div></div></div></div></div></div><div id="37896628" class="c"><input type="checkbox" id="c-37896628" checked=""/><div class="controls bullet"><span class="by">telegpt</span><span>|</span><a href="#37896065">prev</a><span>|</span><a href="#37896602">next</a><span>|</span><label class="collapse" for="c-37896628">[-]</label><label class="expand" for="c-37896628">[1 more]</label></div><br/><div class="children"><div class="content">Pali-X: Vision+Language By Google
<a href="https:&#x2F;&#x2F;cliprecaps.com&#x2F;read&#x2F;?v=wP0T1nbW9-w" rel="nofollow noreferrer">https:&#x2F;&#x2F;cliprecaps.com&#x2F;read&#x2F;?v=wP0T1nbW9-w</a></div><br/></div></div><div id="37896349" class="c"><input type="checkbox" id="c-37896349" checked=""/><div class="controls bullet"><span class="by">kolja005</span><span>|</span><a href="#37896602">prev</a><span>|</span><a href="#37896195">next</a><span>|</span><label class="collapse" for="c-37896349">[-]</label><label class="expand" for="c-37896349">[2 more]</label></div><br/><div class="children"><div class="content">I probably could have guessed that contrastive pre-training works better for downstream vision-language tasks than image classification pre-training, but it&#x27;s nice to see this hypothesis thoroughly tested here.<p>Also, hacks to get LLMs to generate structured output seem to be mostly getting the job done. I&#x27;m less optimistic about this approach for traditional vision tasks where language is the interface, however. Are we going to get models to output a pixel-wise segmentation mask as text? I want to doubt it but seeing how LLMs are about to output long sequences of structured text leaves my mind open.</div><br/><div id="37896554" class="c"><input type="checkbox" id="c-37896554" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#37896349">parent</a><span>|</span><a href="#37896195">next</a><span>|</span><label class="collapse" for="c-37896554">[-]</label><label class="expand" for="c-37896554">[1 more]</label></div><br/><div class="children"><div class="content">I don’t see why not- “segment anything” from meta seems to handle labeled pixel-wise segmentation masks fairly well. You can also get rough masks today by looking at where the text part of the model attends to in the image part.</div><br/></div></div></div></div><div id="37896657" class="c"><input type="checkbox" id="c-37896657" checked=""/><div class="controls bullet"><span class="by">facu17y</span><span>|</span><a href="#37896195">prev</a><span>|</span><a href="#37896731">next</a><span>|</span><label class="collapse" for="c-37896657">[-]</label><label class="expand" for="c-37896657">[1 more]</label></div><br/><div class="children"><div class="content">no github?</div><br/></div></div></div></div></div></div></div></body></html>