<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733389256624" as="style"/><link rel="stylesheet" href="styles.css?v=1733389256624"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.helix.ml/p/building-reliable-genai-applications">Test Driven Development (TDD) for your LLMs? Yes please, more of that please</a> <span class="domain">(<a href="https://blog.helix.ml">blog.helix.ml</a>)</span></div><div class="subtext"><span>lewq</span> | <span>22 comments</span></div><br/><div><div id="42322264" class="c"><input type="checkbox" id="c-42322264" checked=""/><div class="controls bullet"><span class="by">heavyarms</span><span>|</span><a href="#42324813">next</a><span>|</span><label class="collapse" for="c-42322264">[-]</label><label class="expand" for="c-42322264">[8 more]</label></div><br/><div class="children"><div class="content">Whenever I see one of these posts, I click just to see if the proposed solution to testing the output of an LLM is to use the output of an LLM... and in almost all cases it is. It doesn&#x27;t matter how many buzzwords and acronyms you use to describe what you&#x27;re doing, at the end of the day it&#x27;s turtles all the way down.<p>The issue is not the technology. When it comes to natural language (LLM responses that are sentences, prose, etc.) there is no actual standard by which you can even judge the output. There is no gold standard for natural language. Otherwise language would be boring. There is also no simple method for determining truth... philosophers have been discussing this for thousands of years and after all that effort we now know that... ¯\_(ツ)_&#x2F;¯... and also, Earth is Flat and Birds Are Not Real.<p>Take, for example, the first sentence of my comment: &quot;Whenever I see one of these posts, I click just to see if the proposed solution to testing the output of an LLM is to use the output of an LLM... and in almost all cases it is.&quot; This is absolutely true, in my own head, as my selective memory is choosing to remember that one time I clicked on a similar post on HN. But beyond the simple question of if it is true or not, even an army of human fact checkers and literature majors could probably not come up with a definitive and logical analysis regarding the quality and veracity of my prose. Is it even a grammatically correct sentence structure... with the run-on ellipsis and what not... ??? Is it meant to be funny? Or snarky? Who knows ¯\_(ツ)_&#x2F;¯ WFT is that random pile of punctuation  marks in the middle of that sentence... does the LLM even have a token for that?</div><br/><div id="42322794" class="c"><input type="checkbox" id="c-42322794" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42322264">parent</a><span>|</span><a href="#42325633">next</a><span>|</span><label class="collapse" for="c-42322794">[-]</label><label class="expand" for="c-42322794">[2 more]</label></div><br/><div class="children"><div class="content">The output of an LLM is often qualitative, not quantitative, and to test that, you <i>need</i> something that can judge the quality.<p>You&#x27;re not debating philosophy with the LLM, you&#x27;re just asking it if the answer matches (semantically) to the expected one.<p>I usually test LLM output quality with the following prompt (simplified):<p>&quot;An AI assistant was tasked with {task}. The relevant information for their task was {context}. Their answer is {answer}. The correct answer should be something like {ground truth}. Is their answer correct?&quot;<p>Then you can spice it up with chain of thought, asking it to judge alongside preferred criteria&#x2F;dimensions and output a score, etc... you can go as wild as you&#x27;d like. But even this simple approach tends to work really well.<p>&gt; turtles all the way down.<p>Saying &quot;LLM testing LLM&quot; is bad is like saying &quot;computer testing computer&quot; is bad. Yet, automated tests have value. And just as the unit tests will not prove your program is bug free, LLM evals won&#x27;t guarantee 100% correctness. But they&#x27;re <i>incredibly</i> useful tool.<p>In my experience working on pretty complex multi-agent multi-step systems, trying to get those to work <i>without</i> an eval framework in place is like playing whack-a-mole, only way less fun.</div><br/><div id="42326277" class="c"><input type="checkbox" id="c-42326277" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#42322264">root</a><span>|</span><a href="#42322794">parent</a><span>|</span><a href="#42325633">next</a><span>|</span><label class="collapse" for="c-42326277">[-]</label><label class="expand" for="c-42326277">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;An AI assistant was tasked with {task}. The relevant information for their task was {context}. Their answer is {answer}. The correct answer should be something like {ground truth}. Is their answer correct?&quot;<p>If you have a ground truth, what was the purpose of asking the AI assistant for an answer in the first place?</div><br/></div></div></div></div><div id="42325633" class="c"><input type="checkbox" id="c-42325633" checked=""/><div class="controls bullet"><span class="by">chrismorgan</span><span>|</span><a href="#42322264">parent</a><span>|</span><a href="#42322794">prev</a><span>|</span><a href="#42324141">next</a><span>|</span><label class="collapse" for="c-42325633">[-]</label><label class="expand" for="c-42325633">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Who knows ¯\_(ツ)_&#x2F;¯ WFT is that random pile of punctuation marks in the middle of that sentence... does the LLM even have a token for that?</i><p><pre><code>  $ ollama run llama3.2
  &gt;&gt;&gt; What does ¯\_(ツ)_&#x2F;¯ mean?
  A popular internet meme!

  ¯\_(ツ)_&#x2F;¯ is a Unicode character that represents a face with a
  surprised or confused expression. The three parts of the character are
  often interpreted as:

  1. `¯` - a horizontal bar, indicating skepticism or &quot;not buying it&quot;
  2. `(ツ)` - a smiling face with a tongue-out gesture, showing amusement
  or frustration
  3. `&#x2F;¯` - an arrow pointing downwards, suggesting &quot;that&#x27;s just not
  right&quot; or &quot;good riddance&quot;

  Together, the character is often used to express exasperation,
  disappointment, or sarcasm in response to a situation that seems
  ridiculous, unfair, or annoying. It&#x27;s commonly used in text messages,
  social media posts, and online forums.

  Example: &quot;You think you can eat an entire pizza by yourself?¯\_(ツ)_&#x2F;¯&quot;

  &gt;&gt;&gt; &#x2F;clear
  Cleared session context
  &gt;&gt;&gt; ¯\_(ツ)_&#x2F;¯
  ¯\_(ツ)_&#x2F;¯

  &gt;&gt;&gt; ¯\_(ツ)_&#x2F;¯
  I see you&#x27;re trying to express a mix of sass and Japanese flair! Keep
  &#x27;em coming, I&#x27;ll keep responding in kind!

  &gt;&gt;&gt; ¯\_(ツ)_&#x2F;¯
  The eternal &quot;shrug emoji&quot; of the internet. You win, you&#x27;ve conveyed an
  entire emotional spectrum with just three characters. Well played,
  friend!

  &gt;&gt;&gt; ¯\_(ツ)_&#x2F;¯
  I think we&#x27;ve reached a new level of emoji sophistication. You&#x27;re not
  even trying to convey anything anymore; it&#x27;s just a masterclass in
  subtlety and understatement. Bravo!
</code></pre>
Sometimes it’s nonsense, sometimes it’s approximately right (“shrug emoji”), sometimes it barks up the wrong tree (“Japanese flair”), sometimes it’s just vapid drivel.</div><br/><div id="42325669" class="c"><input type="checkbox" id="c-42325669" checked=""/><div class="controls bullet"><span class="by">jusssi</span><span>|</span><a href="#42322264">root</a><span>|</span><a href="#42325633">parent</a><span>|</span><a href="#42324141">next</a><span>|</span><label class="collapse" for="c-42325669">[-]</label><label class="expand" for="c-42325669">[2 more]</label></div><br/><div class="children"><div class="content">&gt; barks up the wrong tree (“Japanese flair”)<p>It just knows more than you. Google says:<p>katakana letter tu (U+30C4) - ツ</div><br/><div id="42325985" class="c"><input type="checkbox" id="c-42325985" checked=""/><div class="controls bullet"><span class="by">chrismorgan</span><span>|</span><a href="#42322264">root</a><span>|</span><a href="#42325669">parent</a><span>|</span><a href="#42324141">next</a><span>|</span><label class="collapse" for="c-42325985">[-]</label><label class="expand" for="c-42325985">[1 more]</label></div><br/><div class="children"><div class="content">I used the expression “barks up the wrong tree” on purpose. I know the character is Japanese, but that’s not how it’s being used.</div><br/></div></div></div></div></div></div><div id="42324141" class="c"><input type="checkbox" id="c-42324141" checked=""/><div class="controls bullet"><span class="by">hansonkd</span><span>|</span><a href="#42322264">parent</a><span>|</span><a href="#42325633">prev</a><span>|</span><a href="#42324813">next</a><span>|</span><label class="collapse" for="c-42324141">[-]</label><label class="expand" for="c-42324141">[2 more]</label></div><br/><div class="children"><div class="content">See von neumanns work on reliable machines from unreliable processes.<p>I wouldn&#x27;t doubt that if each layer of an LLM added some additional check for an unreliable process that eventually you can make something reliable from the unreliable.</div><br/><div id="42326186" class="c"><input type="checkbox" id="c-42326186" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#42322264">root</a><span>|</span><a href="#42324141">parent</a><span>|</span><a href="#42324813">next</a><span>|</span><label class="collapse" for="c-42326186">[-]</label><label class="expand" for="c-42326186">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=10177396">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=10177396</a><p><a href="https:&#x2F;&#x2F;www.cs.unm.edu&#x2F;~saia&#x2F;talks&#x2F;reliable-from-unreliable.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.unm.edu&#x2F;~saia&#x2F;talks&#x2F;reliable-from-unreliable....</a></div><br/></div></div></div></div></div></div><div id="42324813" class="c"><input type="checkbox" id="c-42324813" checked=""/><div class="controls bullet"><span class="by">jmathai</span><span>|</span><a href="#42322264">prev</a><span>|</span><a href="#42323748">next</a><span>|</span><label class="collapse" for="c-42324813">[-]</label><label class="expand" for="c-42324813">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working on a prompt to application product[1] and one of the approaches we tried was test driven development. We would have the LLM write tests based on a detailed description of the application. Then give the LLM the tests and the requirements and ask it to write the application.<p>The thinking is we could run the tests to verify that the requirements are functional (assuming it wrote the tests correctly in the first place - in many cases it did, fyi).<p>The problem was that it was too fickle. Sometimes the failing tests caught application bugs. But too often the LLM just couldn&#x27;t get the tests to pass even though sometimes the application was working fine.<p>It resulted in a terrible user experience (they only see latency of getting the application correctly written or a failure if it gives up).<p>That being said, I think a lot of the issues folks like us find with LLMs are because we haven&#x27;t figured out how and what to ask.<p>Ultimately, we found an alternative approach which gets at least 95% of the application working 100% of the time. And this is actually a MUCH better user experience than waiting forever to sometimes just get &quot;Sorry, we couldn&#x27;t create your application.&quot;.<p>[1] <a href="https:&#x2F;&#x2F;withlattice.com" rel="nofollow">https:&#x2F;&#x2F;withlattice.com</a></div><br/><div id="42324918" class="c"><input type="checkbox" id="c-42324918" checked=""/><div class="controls bullet"><span class="by">barapa</span><span>|</span><a href="#42324813">parent</a><span>|</span><a href="#42323748">next</a><span>|</span><label class="collapse" for="c-42324918">[-]</label><label class="expand" for="c-42324918">[4 more]</label></div><br/><div class="children"><div class="content">Ok you&#x27;ve got us at the edge of our seats. What&#x27;s the alternative approach?</div><br/><div id="42324946" class="c"><input type="checkbox" id="c-42324946" checked=""/><div class="controls bullet"><span class="by">jmathai</span><span>|</span><a href="#42324813">root</a><span>|</span><a href="#42324918">parent</a><span>|</span><a href="#42323748">next</a><span>|</span><label class="collapse" for="c-42324946">[-]</label><label class="expand" for="c-42324946">[3 more]</label></div><br/><div class="children"><div class="content">What is working better for us is to spin the application up and poke at it with HTTP requests and look for non 200 response codes.<p>The tradeoff is it&#x27;s not as comprehensive. But giving users something that is 95% working and making it easy for them to fix those issues appears to be the best user experience we&#x27;ve found so far.</div><br/><div id="42325085" class="c"><input type="checkbox" id="c-42325085" checked=""/><div class="controls bullet"><span class="by">solidasparagus</span><span>|</span><a href="#42324813">root</a><span>|</span><a href="#42324946">parent</a><span>|</span><a href="#42323748">next</a><span>|</span><label class="collapse" for="c-42325085">[-]</label><label class="expand" for="c-42325085">[2 more]</label></div><br/><div class="children"><div class="content">How is that different than tests? Or is it just that the tests aren&#x27;t used as context when creating the app?</div><br/><div id="42325116" class="c"><input type="checkbox" id="c-42325116" checked=""/><div class="controls bullet"><span class="by">jmathai</span><span>|</span><a href="#42324813">root</a><span>|</span><a href="#42325085">parent</a><span>|</span><a href="#42323748">next</a><span>|</span><label class="collapse" for="c-42325116">[-]</label><label class="expand" for="c-42325116">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a test in the sense that it&#x27;s meant to validate functionality. You&#x27;re correct there.<p>The endpoints we poke at <i>are</i> provided as context when creating the application.<p>Our approach evolved to be more liberal in what was required to pass. So instead of looking for an HTML element with id=&quot;foo&quot; we accept a 200 HTTP response code. It&#x27;s subtle but had a huge improvement in the end user experience.<p>Citing the article and helix tests:<p><pre><code>      tests:
        - name: addition_test
          steps:
            - prompt: &quot;What is 2 + 2?&quot;
              expected_output: &quot;4&quot;
</code></pre>
We would change expected_output to be &quot;integer&quot; or &quot;not an error&quot;.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42323748" class="c"><input type="checkbox" id="c-42323748" checked=""/><div class="controls bullet"><span class="by">bdangubic</span><span>|</span><a href="#42324813">prev</a><span>|</span><a href="#42325480">next</a><span>|</span><label class="collapse" for="c-42323748">[-]</label><label class="expand" for="c-42323748">[1 more]</label></div><br/><div class="children"><div class="content">I just mock the answers and assert on the mock of the answer - never fails!</div><br/></div></div><div id="42325480" class="c"><input type="checkbox" id="c-42325480" checked=""/><div class="controls bullet"><span class="by">jasfi</span><span>|</span><a href="#42323748">prev</a><span>|</span><a href="#42322234">next</a><span>|</span><label class="collapse" for="c-42325480">[-]</label><label class="expand" for="c-42325480">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m working on an AI agents platform that intends to reduce the amount of code you need to write to get high performing prompts working correctly.<p>The wait-list is at <a href="https:&#x2F;&#x2F;aiconstrux.com" rel="nofollow">https:&#x2F;&#x2F;aiconstrux.com</a></div><br/></div></div><div id="42322234" class="c"><input type="checkbox" id="c-42322234" checked=""/><div class="controls bullet"><span class="by">benatkin</span><span>|</span><a href="#42325480">prev</a><span>|</span><a href="#42322516">next</a><span>|</span><label class="collapse" for="c-42322234">[-]</label><label class="expand" for="c-42322234">[1 more]</label></div><br/><div class="children"><div class="content">I read this blog post on my iPhone and when I went to the top to try and find out more about Helix, it had a giant link to install the Substack iOS app, which detracted from the experience. It might be a good idea to use a real CMS.<p>Here’s the website: <a href="https:&#x2F;&#x2F;tryhelix.ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;tryhelix.ai&#x2F;</a></div><br/></div></div><div id="42322516" class="c"><input type="checkbox" id="c-42322516" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#42322234">prev</a><span>|</span><a href="#42321020">next</a><span>|</span><label class="collapse" for="c-42322516">[-]</label><label class="expand" for="c-42322516">[1 more]</label></div><br/><div class="children"><div class="content">Its inherently difficult because LLMs are necessarily probabilistic and even worse, for any practical use the key step irreversibly discards most of the probabilities</div><br/></div></div><div id="42321020" class="c"><input type="checkbox" id="c-42321020" checked=""/><div class="controls bullet"><span class="by">justanotheratom</span><span>|</span><a href="#42322516">prev</a><span>|</span><label class="collapse" for="c-42321020">[-]</label><label class="expand" for="c-42321020">[4 more]</label></div><br/><div class="children"><div class="content">There is a real need for this. I have to admit most of my testing right now is vibes based. Problem is, these LLM evaluation platforms get in-between me and my LLM.</div><br/><div id="42321750" class="c"><input type="checkbox" id="c-42321750" checked=""/><div class="controls bullet"><span class="by">Onawa</span><span>|</span><a href="#42321020">parent</a><span>|</span><a href="#42322989">next</a><span>|</span><label class="collapse" for="c-42321750">[-]</label><label class="expand" for="c-42321750">[1 more]</label></div><br/><div class="children"><div class="content">I use promptfoo for our internal evaluations. There are a ton of different &quot;assertions&quot; (tests) that you can write, including model-graded evaluations using rubrics.<p>This is far from a solved problem, but there are options out there for systematic testing of LLMs.</div><br/></div></div><div id="42322989" class="c"><input type="checkbox" id="c-42322989" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#42321020">parent</a><span>|</span><a href="#42321750">prev</a><span>|</span><a href="#42324040">next</a><span>|</span><label class="collapse" for="c-42322989">[-]</label><label class="expand" for="c-42322989">[1 more]</label></div><br/><div class="children"><div class="content">Probably non-optimal long term, but you can easily whip up rudimentary tests with your LLM&#x2F;lib of choice (see my other comment at <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42322794">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42322794</a>) without using any of the eval platforms.</div><br/></div></div><div id="42324040" class="c"><input type="checkbox" id="c-42324040" checked=""/><div class="controls bullet"><span class="by">vinnymac</span><span>|</span><a href="#42321020">parent</a><span>|</span><a href="#42322989">prev</a><span>|</span><label class="collapse" for="c-42324040">[-]</label><label class="expand" for="c-42324040">[1 more]</label></div><br/><div class="children"><div class="content">Offline evaluators such as laminar and (evalite wip) stay out of the way fortunately. Evals can be very useful when they do.</div><br/></div></div></div></div></div></div></div></div></div></body></html>