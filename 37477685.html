<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694682063342" as="style"/><link rel="stylesheet" href="styles.css?v=1694682063342"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://alexeymk.com/2023/09/11/statistical-significance-on-a-shoestring-budget.html">Statistical significance on a shoestring budget</a> <span class="domain">(<a href="https://alexeymk.com">alexeymk.com</a>)</span></div><div class="subtext"><span>AlexeyMK</span> | <span>10 comments</span></div><br/><div><div id="37492323" class="c"><input type="checkbox" id="c-37492323" checked=""/><div class="controls bullet"><span class="by">grega5</span><span>|</span><a href="#37492140">next</a><span>|</span><label class="collapse" for="c-37492323">[-]</label><label class="expand" for="c-37492323">[1 more]</label></div><br/><div class="children"><div class="content">First, you really should move away from frequentist statistical testing and use Bayesian statistics instead. It is perfect for such occasions where you want to adjust your beliefs in what UX is best based on empirical data to support your decision. With collecting data you are increasing confidence in your decision rather than trying to meet an arbitrary criterion of a specific p-value.<p>Second, the “run-in-parallel” approach has a well defined name in experimental design, called a factorial design. The diagram shown is an example of full factorial design in which each level of each factor is combined with each level of all other factors. The advantage of such design is that interactions between factors can be tested as well. If there are good reasons to believe that there are no interactions between the different factors then you could use a partial factorial design that, which has the advantage of having less total combinations of levels while still enabling estimation of effects of individual factors.</div><br/></div></div><div id="37492140" class="c"><input type="checkbox" id="c-37492140" checked=""/><div class="controls bullet"><span class="by">jvans</span><span>|</span><a href="#37492323">prev</a><span>|</span><a href="#37492278">next</a><span>|</span><label class="collapse" for="c-37492140">[-]</label><label class="expand" for="c-37492140">[4 more]</label></div><br/><div class="children"><div class="content">Building your own bayesian model with something like pymc3 is also a very reasonable approach to take with small data or data with too much variance to detect effects in a timely manner. This also forces you to think about the underlying distributions that generate your data which is an exercise in itself that can yield interesting insights.</div><br/><div id="37492253" class="c"><input type="checkbox" id="c-37492253" checked=""/><div class="controls bullet"><span class="by">AlexeyMK</span><span>|</span><a href="#37492140">parent</a><span>|</span><a href="#37492478">next</a><span>|</span><label class="collapse" for="c-37492253">[-]</label><label class="expand" for="c-37492253">[1 more]</label></div><br/><div class="children"><div class="content">[Author here] Heh - yes but don&#x27;t, though...<p>Yes: you could use bayesian priors and a custom model to give yourself more confidence from less data.  But...<p>Don&#x27;t: for most businesses that are so early they can&#x27;t get enough users to hit stat-sig, you&#x27;re likely to be better off leveraging your engineering efforts towards making the product better instead of building custom statistical models.  This is nerd-sniping-adjacent, (<a href="https:&#x2F;&#x2F;xkcd.com&#x2F;356&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;xkcd.com&#x2F;356&#x2F;</a>) a common trap engineers can fall into: it&#x27;s more fun to solve the novel technical problem than the actual business problem.<p>Though: there are a small set of companies with large scale but small data, for whom the custom stats approaches _do_ make sense.  When I was at Opendoor, even though we had billions of dollars of GMV, we only bought a few thousand homes a month, so the Data Science folks used fun statistical approaches like Pair Matching (<a href="https:&#x2F;&#x2F;www.rockstepsolutions.com&#x2F;blog&#x2F;pair-matching&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.rockstepsolutions.com&#x2F;blog&#x2F;pair-matching&#x2F;</a>) and CUPED (now available off the shelf - <a href="https:&#x2F;&#x2F;www.geteppo.com&#x2F;features&#x2F;cuped" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.geteppo.com&#x2F;features&#x2F;cuped</a>) to squeeze a bit more signal from less data.</div><br/></div></div><div id="37492478" class="c"><input type="checkbox" id="c-37492478" checked=""/><div class="controls bullet"><span class="by">j7ake</span><span>|</span><a href="#37492140">parent</a><span>|</span><a href="#37492253">prev</a><span>|</span><a href="#37492278">next</a><span>|</span><label class="collapse" for="c-37492478">[-]</label><label class="expand" for="c-37492478">[2 more]</label></div><br/><div class="children"><div class="content">I love fitting models.<p>I always say in my profession I will fit models for free, it’s having to clean data and “finish” a project that I demand payment.</div><br/><div id="37493358" class="c"><input type="checkbox" id="c-37493358" checked=""/><div class="controls bullet"><span class="by">kimi</span><span>|</span><a href="#37492140">root</a><span>|</span><a href="#37492478">parent</a><span>|</span><a href="#37492278">next</a><span>|</span><label class="collapse" for="c-37493358">[-]</label><label class="expand" for="c-37493358">[1 more]</label></div><br/><div class="children"><div class="content">...and pictures in a format the journal likes....</div><br/></div></div></div></div></div></div><div id="37492278" class="c"><input type="checkbox" id="c-37492278" checked=""/><div class="controls bullet"><span class="by">charlierguo</span><span>|</span><a href="#37492140">prev</a><span>|</span><a href="#37493027">next</a><span>|</span><label class="collapse" for="c-37492278">[-]</label><label class="expand" for="c-37492278">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Gut Check: Especially if you’re off by quite a bit, this is a chance to take a step back and ask whether the company has reached growth scale or not. It could be that there are plenty of obvious 0-1 tactics left. Not everything has to be an experiment.<p>This is a key point, imo. I have a sneaking suspicion that a lot of companies are running &quot;growth teams&quot; that don&#x27;t have the scale where it actually makes sense to do so.</div><br/></div></div><div id="37493027" class="c"><input type="checkbox" id="c-37493027" checked=""/><div class="controls bullet"><span class="by">malf</span><span>|</span><a href="#37492278">prev</a><span>|</span><label class="collapse" for="c-37493027">[-]</label><label class="expand" for="c-37493027">[3 more]</label></div><br/><div class="children"><div class="content">“Using modern experiment frameworks, all 3 of ideas can be safely tested at once, using parallel A&#x2F;B tests (see chart).”<p>Nooo! First, if one actually works, you’ve massively increased the “noise” for the other experiments, so your significance calculation is now off. Second, xkcd 882.</div><br/><div id="37493395" class="c"><input type="checkbox" id="c-37493395" checked=""/><div class="controls bullet"><span class="by">AlexeyMK</span><span>|</span><a href="#37493027">parent</a><span>|</span><label class="collapse" for="c-37493395">[-]</label><label class="expand" for="c-37493395">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Nooo! First, if one actually works, you’ve massively increased the “noise” for the other experiments<p>I get that a bunch at some of my clients.  It&#x27;s a common misconception. Let&#x27;s say experiment B is 10% better than control but we&#x27;re also running experiment C at the same time.  Since C&#x27;s participants are evenly distributed across B&#x27;s branches, by default they should have no impact on the other experiment.<p>If you do a pre&#x2F;post comparison, you&#x27;ll notice that for whatever reason, both branches of C are doing 5% better than prior time periods, and this is because half of them are in the winner branch of B.<p>NOW - imagine that the C variant is only an improvement _if_ you also include the B variant.  That&#x27;s where you need to be careful about monitoring experiment interactions, I called out in the guide.  But better so spend a half day writing an &quot;experiment interaction&quot; query than two weeks waiting for the experiments to run in sequence.<p>&gt; Second, xkcd 882 (<a href="https:&#x2F;&#x2F;xkcd.com&#x2F;882&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;xkcd.com&#x2F;882&#x2F;</a>)
I think you&#x27;re referencing P-hacking, right?<p>That is a valid concern to be vigilant for.  In this case, XKCD is calling out the &quot;find a subgroup that happens to be positive&quot; hack (also here, <a href="https:&#x2F;&#x2F;xkcd.com&#x2F;1478&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;xkcd.com&#x2F;1478&#x2F;</a>).  However, here we&#x27;re testing (a) 3 different ideas and (b) only testing each of them once on the entire population.  No p-hacking here (far as I can tell, happy to learn otherwise), but good that you&#x27;re keeping an eye out for it.</div><br/><div id="37506423" class="c"><input type="checkbox" id="c-37506423" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#37493027">root</a><span>|</span><a href="#37493395">parent</a><span>|</span><label class="collapse" for="c-37506423">[-]</label><label class="expand" for="c-37506423">[1 more]</label></div><br/><div class="children"><div class="content">The more experiments you run in parallel, the more likely it becomes that at least one experiment&#x27;s branches do not have an even distribution across all branches of all (combinations of) other experiments.<p>And the more experiments you run, whether in parallel or sequentially, the more likely you&#x27;re to get at least one false positive, i.e. p-hacking. XKCD is using &quot;find a subgroup that happens to be positive&quot; to make it funnier, but it&#x27;s simply &quot;find an experiment that happens to be positive&quot;. To correct for p-hacking, you would have to lower your threshold for each experiment, requiring a larger sample size, negating the benefits you thought you were getting by running more experiments with the same samples.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>