<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699779654698" as="style"/><link rel="stylesheet" href="styles.css?v=1699779654698"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/philippta/flyscrape">Show HN: Flyscrape – A standalone and scriptable web scraper in Go</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>philippta</span> | <span>47 comments</span></div><br/><div><div id="38237083" class="c"><input type="checkbox" id="c-38237083" checked=""/><div class="controls bullet"><span class="by">oefrha</span><span>|</span><a href="#38236897">next</a><span>|</span><label class="collapse" for="c-38237083">[-]</label><label class="expand" for="c-38237083">[5 more]</label></div><br/><div class="children"><div class="content">I had a look at <a href="https:&#x2F;&#x2F;github.com&#x2F;philippta&#x2F;flyscrape&#x2F;blob&#x2F;master&#x2F;scrape.go">https:&#x2F;&#x2F;github.com&#x2F;philippta&#x2F;flyscrape&#x2F;blob&#x2F;master&#x2F;scrape.go</a>. It’s just using the builtin HTTP client to fire off requests, with an identifying user agent. Which means it’s useless for scraping most real world sites you may want to scrape, unfortunately. You’ll get served a JavaScrpt challenge, or not even that (many sites will refuse to serve anything if they see a random user agent like flyscrape&#x2F;1.0).</div><br/><div id="38237410" class="c"><input type="checkbox" id="c-38237410" checked=""/><div class="controls bullet"><span class="by">1vuio0pswjnm7</span><span>|</span><a href="#38237083">parent</a><span>|</span><a href="#38236897">next</a><span>|</span><label class="collapse" for="c-38237410">[-]</label><label class="expand" for="c-38237410">[4 more]</label></div><br/><div class="children"><div class="content">What are some examples of &quot;most real world sites&quot;.<p>What are some examples of sites that are not &quot;most real world sites&quot;.<p>Is HN a &quot;real world site&quot;.<p>What percentage of sites submitted to HN are &quot;most real world sites&quot;.  (IME, it&#x27;s a minority fraction.)<p>Why not just delete or change the user-agent line in scrape.go before compiling.<p>(Personal experience: I have been successfully retrieving information from the www for decades without including a user-agent header. The number of sites I have found that require this header is relatively small.  It does not rise to the level of &quot;most&quot;.)<p>HN replies usually fail to include even a single example.<p>Similarly, do examples provided for scraper programs and libraries ever include &quot;most real world sites&quot;.</div><br/><div id="38237595" class="c"><input type="checkbox" id="c-38237595" checked=""/><div class="controls bullet"><span class="by">oefrha</span><span>|</span><a href="#38237083">root</a><span>|</span><a href="#38237410">parent</a><span>|</span><a href="#38237590">next</a><span>|</span><label class="collapse" for="c-38237595">[-]</label><label class="expand" for="c-38237595">[1 more]</label></div><br/><div class="children"><div class="content">“Most real world sites” and “most real world sites you may want to scrape” are different, especially when weighted by information volume, so your selective quoting doesn’t help. Alexa top 100 probably contain more information, especially new information, than the rest of Alexa top 10000 combined (random guess I pulled out of my ass, don’t quote me on that), so the overwhelmingly scraping-resistant Alexa top 100 is what most scraping effort is directed against.<p>Anyway, I’m not interested in a pedantic debate. Most (but not all) people who have attempted to scrape any popular site, or unpopular site behind Cloudflare at above-minimum protection level, in recent years should know exactly what I’m taking about.<p>Edit: In case you’re not aware, many of the sites I have in mind are&#x2F;were capable of defeating puppeteer-extra-plugin-stealth. User agent is only the most basic thing, like level 1 in a hundred-level dungeon.</div><br/></div></div><div id="38237590" class="c"><input type="checkbox" id="c-38237590" checked=""/><div class="controls bullet"><span class="by">Klonoar</span><span>|</span><a href="#38237083">root</a><span>|</span><a href="#38237410">parent</a><span>|</span><a href="#38237595">prev</a><span>|</span><a href="#38237662">next</a><span>|</span><label class="collapse" for="c-38237590">[-]</label><label class="expand" for="c-38237590">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, nobody cares that most sites will work with it. The number of sites that require it might be small, but that number tends to include <i>significant and notable ones</i> - the ones that most people actually wind up wanting to crawl. Cloudflare &amp; co make this more difficult with each passing year.<p>OP is making a very well known day-one point about this topic. It is somewhat surprising that the library doesn&#x27;t offer a way to dynamically set it.<p><i>Edit: and this isn&#x27;t even getting into stuff like Tls fingerprinting, header order, etc</i></div><br/></div></div></div></div></div></div><div id="38236897" class="c"><input type="checkbox" id="c-38236897" checked=""/><div class="controls bullet"><span class="by">1vuio0pswjnm7</span><span>|</span><a href="#38237083">prev</a><span>|</span><a href="#38236439">next</a><span>|</span><label class="collapse" for="c-38236897">[-]</label><label class="expand" for="c-38236897">[1 more]</label></div><br/><div class="children"><div class="content">&quot;default = 100 [requests per second]&quot;<p>How many new TCP connections per second.<p>Is this a &quot;scraper&quot; or a &quot;crawler&quot;.<p>It appears to accept a &quot;starting URL&quot; and to follow links.<p>Opening many TCP connections is arguably still a reason why website operators try to prevent crawling (except from Googlebot IPs).  As for scraping, it can be done with a single TCP connection.  Perhaps &quot;developers&quot; instead opt to use many TCP connections and then complain when they get blocked.</div><br/></div></div><div id="38236439" class="c"><input type="checkbox" id="c-38236439" checked=""/><div class="controls bullet"><span class="by">moehm</span><span>|</span><a href="#38236897">prev</a><span>|</span><a href="#38231427">next</a><span>|</span><label class="collapse" for="c-38236439">[-]</label><label class="expand" for="c-38236439">[2 more]</label></div><br/><div class="children"><div class="content">Interesting. Can you compare it to colly? [0]<p>Last time I looked it was the most popular choice for scraping in Go and I have some projects using it.<p>Is it similar? Does it have more&#x2F;less features or is it more suited for a different use case? (Which one?)<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;gocolly&#x2F;colly">https:&#x2F;&#x2F;github.com&#x2F;gocolly&#x2F;colly</a></div><br/><div id="38236548" class="c"><input type="checkbox" id="c-38236548" checked=""/><div class="controls bullet"><span class="by">philippta</span><span>|</span><a href="#38236439">parent</a><span>|</span><a href="#38231427">next</a><span>|</span><label class="collapse" for="c-38236548">[-]</label><label class="expand" for="c-38236548">[1 more]</label></div><br/><div class="children"><div class="content">Colly is a great scraping library if you are a Go developer.<p>Flyscrape on the other hand is a ready-made CLI tool that aims to be easy to use even for someone who is a little familiar with JavaScript. It just happens to be written in Go, but that should not matter to the end user.<p>It does not have full feature parity with Colly but most use cases should be covered.</div><br/></div></div></div></div><div id="38231427" class="c"><input type="checkbox" id="c-38231427" checked=""/><div class="controls bullet"><span class="by">bryanrasmussen</span><span>|</span><a href="#38236439">prev</a><span>|</span><a href="#38237229">next</a><span>|</span><label class="collapse" for="c-38231427">[-]</label><label class="expand" for="c-38231427">[3 more]</label></div><br/><div class="children"><div class="content">Looks like it doesn&#x27;t have the possibility of running it as a particular browser etc. Which I guess makes it fine for a lot of pages, but also a lot of scraping tasks would be affected. Am I right or did I miss something?</div><br/><div id="38231523" class="c"><input type="checkbox" id="c-38231523" checked=""/><div class="controls bullet"><span class="by">philippta</span><span>|</span><a href="#38231427">parent</a><span>|</span><a href="#38237229">next</a><span>|</span><label class="collapse" for="c-38231523">[-]</label><label class="expand" for="c-38231523">[2 more]</label></div><br/><div class="children"><div class="content">Yes, this is correct. As of right now there is no built-in support for running as a browser.<p>What is possible though, is to use a service like ScrapingBee (not affiliated) and set it as the proxy. This would render the page on their end, in a browser.</div><br/><div id="38233707" class="c"><input type="checkbox" id="c-38233707" checked=""/><div class="controls bullet"><span class="by">acheong08</span><span>|</span><a href="#38231427">root</a><span>|</span><a href="#38231523">parent</a><span>|</span><a href="#38237229">next</a><span>|</span><label class="collapse" for="c-38233707">[-]</label><label class="expand" for="c-38233707">[1 more]</label></div><br/><div class="children"><div class="content">Try tls-client. It gets around TLS fingerprinting by Cloudflare</div><br/></div></div></div></div></div></div><div id="38237229" class="c"><input type="checkbox" id="c-38237229" checked=""/><div class="controls bullet"><span class="by">krick</span><span>|</span><a href="#38231427">prev</a><span>|</span><a href="#38234840">next</a><span>|</span><label class="collapse" for="c-38237229">[-]</label><label class="expand" for="c-38237229">[1 more]</label></div><br/><div class="children"><div class="content">This looks like something I could use. Maybe not revolutional, but I do that from time to time, and even if only for organizational purposes it seems to make sense to store that stuff as a bucnh of configuration files for some external tool, rather than a bunch of python-scripts that I implement somewhat differently every time.<p>Right now I&#x27;m just wrapping my head around how this works, and didn&#x27;t try it hands-on yet, but I struggle to evaluate from the existing documentation, how useful this actually is. All examples in the repository right now are ultimately one-page scrappers, which, honestly, would be quite useless to me. Pretty much every scraper I write has at least 2-3 logical layers. Like, consider your HN-example, but you want to include top-10 comments for each post. Is it even possible? Well, I guess for HN you could just get by using allowedURLs and treating default function as a parser for the comment-page, but this isn&#x27;t generic enough. Consider some internet shop. That would be (1) product category tree, sometimes much easier to hard-code, rather than scrape it every time; hard-coding often is generative (e.g. example.com&#x2F;X&#x2F;A-B-C, where X is a string from the list, A, B and C are padded numbers, each with a different range) (2) you go into each category, retrieve either a sub-category list (possibly, js-rendered, multiple pages) or product list (same applies) (3) open each product url, do the actual parsing (name, price, specification, etc). Each of json-object from (3) often has to include some minimal parsed data from level (2) (like category name)<p>More advanced, but also way to popular to imagine a generic web-scraper without it: in addition to some json-metadata you download pictures, or pdf-files, etc. (Sometimes you don&#x27;t even need metadata.) Maybe just text files, but the result is several GBs, and isn&#x27;t suitable to be handled as a single json-object, but rather a file&#x2F;directory tree.<p>Is any of this possible with this tool?<p>Also, regardless of being it useful for my cases, some minor comments:<p>1. Links in docs&#x2F;readme.md#configuration don&#x27;t work (but the .md files for them actually exist).<p>2. I would suggest making &quot;url&quot; in the configuration either a list, or string|list. I suppose, that pretty much doesn&#x27;t change the logic, but would make a lot of basic use-cases much easier to implement.</div><br/></div></div><div id="38234840" class="c"><input type="checkbox" id="c-38234840" checked=""/><div class="controls bullet"><span class="by">fyzix</span><span>|</span><a href="#38237229">prev</a><span>|</span><a href="#38234554">next</a><span>|</span><label class="collapse" for="c-38234840">[-]</label><label class="expand" for="c-38234840">[1 more]</label></div><br/><div class="children"><div class="content">What happens if &#x27;find()&#x27; returns a list and you call &#x27;.text()&#x27;. Intuition tells me it should fail but maybe it implicitly gets the text from the first item if it exists.<p>Either way, I think you create a separate method &#x27;find_all()&#x27; that returns a list to make the API easier to reason about.</div><br/></div></div><div id="38234554" class="c"><input type="checkbox" id="c-38234554" checked=""/><div class="controls bullet"><span class="by">slig</span><span>|</span><a href="#38234840">prev</a><span>|</span><a href="#38231162">next</a><span>|</span><label class="collapse" for="c-38234554">[-]</label><label class="expand" for="c-38234554">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing! Just a small nit: the links at the bottom of this page are broken [1].<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;philippta&#x2F;flyscrape&#x2F;blob&#x2F;master&#x2F;docs&#x2F;readme.md#configuration">https:&#x2F;&#x2F;github.com&#x2F;philippta&#x2F;flyscrape&#x2F;blob&#x2F;master&#x2F;docs&#x2F;read...</a></div><br/><div id="38236565" class="c"><input type="checkbox" id="c-38236565" checked=""/><div class="controls bullet"><span class="by">philippta</span><span>|</span><a href="#38234554">parent</a><span>|</span><a href="#38231162">next</a><span>|</span><label class="collapse" for="c-38236565">[-]</label><label class="expand" for="c-38236565">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for spotting, will get this fixed quickly.</div><br/></div></div></div></div><div id="38231162" class="c"><input type="checkbox" id="c-38231162" checked=""/><div class="controls bullet"><span class="by">lucgagan</span><span>|</span><a href="#38234554">prev</a><span>|</span><a href="#38232032">next</a><span>|</span><label class="collapse" for="c-38231162">[-]</label><label class="expand" for="c-38231162">[2 more]</label></div><br/><div class="children"><div class="content">This looks great. I wish I had this a few months ago! Giving it a try.</div><br/><div id="38231712" class="c"><input type="checkbox" id="c-38231712" checked=""/><div class="controls bullet"><span class="by">philippta</span><span>|</span><a href="#38231162">parent</a><span>|</span><a href="#38232032">next</a><span>|</span><label class="collapse" for="c-38231712">[-]</label><label class="expand" for="c-38231712">[1 more]</label></div><br/><div class="children"><div class="content">Glad to hear! You’re welcome to leave any feedback on Github (as an Issue) or right in here.</div><br/></div></div></div></div><div id="38232032" class="c"><input type="checkbox" id="c-38232032" checked=""/><div class="controls bullet"><span class="by">sunshadow</span><span>|</span><a href="#38231162">prev</a><span>|</span><a href="#38231714">next</a><span>|</span><label class="collapse" for="c-38232032">[-]</label><label class="expand" for="c-38232032">[16 more]</label></div><br/><div class="children"><div class="content">These days, I&#x27;m not even using Go for scraping that much, as the webpage changes makes me crazy and JS code evaluation is a lifesaver, so I moved to Typescript+Playwright. (Crawlee framework is cool, while not strictly necessary).<p>Its been 8+ years since i started scraping. I even wrote a popular Go web scraping framework previously: (<a href="https:&#x2F;&#x2F;github.com&#x2F;geziyor&#x2F;geziyor">https:&#x2F;&#x2F;github.com&#x2F;geziyor&#x2F;geziyor</a>).<p>My favorite stack as of 2023: TypeScript+Playwright+Crawlee(Optional)
If you&#x27;re serious in scraping, you should learn javascript, thus, playwright should be good.<p>Note: There are niche cases where lower-level language would be required (C++, Go etc), but probably only &lt;%5</div><br/><div id="38232257" class="c"><input type="checkbox" id="c-38232257" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#38232032">parent</a><span>|</span><a href="#38236284">next</a><span>|</span><label class="collapse" for="c-38232257">[-]</label><label class="expand" for="c-38232257">[9 more]</label></div><br/><div class="children"><div class="content">How does that help you mitigate when a site changes? If you’re fetching some value in a given &lt;div&gt; under a long XPATH and they decide to change that path?</div><br/><div id="38232414" class="c"><input type="checkbox" id="c-38232414" checked=""/><div class="controls bullet"><span class="by">sunshadow</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232257">parent</a><span>|</span><a href="#38232617">next</a><span>|</span><label class="collapse" for="c-38232414">[-]</label><label class="expand" for="c-38232414">[6 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t use XPath&amp;CSS selectors at all (Except if you dont have choice). You rely on more generic stuff, e.g, &quot;the button that has &#x27;Sign in&#x27; on it&quot;:<p><pre><code>    await page.getByRole(&#x27;button&#x27;, { name: &#x27;Sign in&#x27; }).click();
</code></pre>
See playwright locators: <a href="https:&#x2F;&#x2F;playwright.dev&#x2F;docs&#x2F;locators" rel="nofollow noreferrer">https:&#x2F;&#x2F;playwright.dev&#x2F;docs&#x2F;locators</a></div><br/><div id="38232567" class="c"><input type="checkbox" id="c-38232567" checked=""/><div class="controls bullet"><span class="by">8n4vidtmkvmk</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232414">parent</a><span>|</span><a href="#38232850">next</a><span>|</span><label class="collapse" for="c-38232567">[-]</label><label class="expand" for="c-38232567">[3 more]</label></div><br/><div class="children"><div class="content">I started putting data-testid attributes in my web app for automated testing using playwright. Prevents me from breaking my own script but it sure would make me more scrapable if anyone cared. Well.. I guess I only do it on inputs, not the rendered page which is what scrapers care most about.</div><br/><div id="38232609" class="c"><input type="checkbox" id="c-38232609" checked=""/><div class="controls bullet"><span class="by">sunshadow</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232567">parent</a><span>|</span><a href="#38232850">next</a><span>|</span><label class="collapse" for="c-38232609">[-]</label><label class="expand" for="c-38232609">[2 more]</label></div><br/><div class="children"><div class="content">Unless you start a war against scrapers, you don&#x27;t need to worry about that as I&#x27;ll always find a way to scrape your site as long as its valuable to &#x27;me&#x27;. Even if it requires Real browser + OCR :)</div><br/><div id="38233442" class="c"><input type="checkbox" id="c-38233442" checked=""/><div class="controls bullet"><span class="by">erhaetherth</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232609">parent</a><span>|</span><a href="#38232850">next</a><span>|</span><label class="collapse" for="c-38233442">[-]</label><label class="expand" for="c-38233442">[1 more]</label></div><br/><div class="children"><div class="content">Oh I know I couldn&#x27;t prevent it. But if you wanted to scrape me, you&#x27;d have to pay the monthly subscription because everything is behind a pay wall&#x2F;login. And then you&#x27;d only have access to data you entered because it&#x27;s just that kind of app :-)</div><br/></div></div></div></div></div></div><div id="38233204" class="c"><input type="checkbox" id="c-38233204" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232414">parent</a><span>|</span><a href="#38232850">prev</a><span>|</span><a href="#38232617">next</a><span>|</span><label class="collapse" for="c-38233204">[-]</label><label class="expand" for="c-38233204">[1 more]</label></div><br/><div class="children"><div class="content">This is where you just train an LLM so you can write:<p>&#x27;get button named &quot;sign in&quot; and click&#x27;<p>Then on the back end, it generates your example code.</div><br/></div></div></div></div><div id="38232617" class="c"><input type="checkbox" id="c-38232617" checked=""/><div class="controls bullet"><span class="by">nurettin</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232257">parent</a><span>|</span><a href="#38232414">prev</a><span>|</span><a href="#38236284">next</a><span>|</span><label class="collapse" for="c-38232617">[-]</label><label class="expand" for="c-38232617">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t know about the poster, but I try to find divs and buttons in a fuzzy way. Usually via element text. Sometimes it mitigates changes, sometimes it doesn&#x27;t. It&#x27;s a guessing game. Especially when they start using shadow elements or iframes in the page. If I&#x27;m looking for something specific like a price or dimensions, I can sometimes get away with it by collecting dollar amounts or X x Y x Z from the raw text.</div><br/><div id="38235965" class="c"><input type="checkbox" id="c-38235965" checked=""/><div class="controls bullet"><span class="by">aynyc</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232617">parent</a><span>|</span><a href="#38236284">next</a><span>|</span><label class="collapse" for="c-38235965">[-]</label><label class="expand" for="c-38235965">[1 more]</label></div><br/><div class="children"><div class="content">iframes have been a pain the butt to scrape against. I see it more and more in websites now.</div><br/></div></div></div></div></div></div><div id="38236284" class="c"><input type="checkbox" id="c-38236284" checked=""/><div class="controls bullet"><span class="by">gymbeaux</span><span>|</span><a href="#38232032">parent</a><span>|</span><a href="#38232257">prev</a><span>|</span><a href="#38232152">next</a><span>|</span><label class="collapse" for="c-38236284">[-]</label><label class="expand" for="c-38236284">[1 more]</label></div><br/><div class="children"><div class="content">What are some examples of needing a lower-level language?</div><br/></div></div><div id="38232152" class="c"><input type="checkbox" id="c-38232152" checked=""/><div class="controls bullet"><span class="by">mikercampbell</span><span>|</span><a href="#38232032">parent</a><span>|</span><a href="#38236284">prev</a><span>|</span><a href="#38232160">next</a><span>|</span><label class="collapse" for="c-38232152">[-]</label><label class="expand" for="c-38232152">[4 more]</label></div><br/><div class="children"><div class="content">Have you seen Crul??<p>I love the JS flow, but I thought crul was an interesting newer tool!!<p>But I agree, you gotta get in there and it’s easier with JS</div><br/><div id="38232489" class="c"><input type="checkbox" id="c-38232489" checked=""/><div class="controls bullet"><span class="by">sunshadow</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232152">parent</a><span>|</span><a href="#38232425">next</a><span>|</span><label class="collapse" for="c-38232489">[-]</label><label class="expand" for="c-38232489">[1 more]</label></div><br/><div class="children"><div class="content">Crul looks nice, though, you cannot imagine how many startups that I&#x27;ve seen failed doing a very similar thing as Crul. Wouldn&#x27;t rely on it.
The problem is complex: Humans generating messy pages</div><br/></div></div><div id="38232425" class="c"><input type="checkbox" id="c-38232425" checked=""/><div class="controls bullet"><span class="by">reyostallenberg</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232152">parent</a><span>|</span><a href="#38232489">prev</a><span>|</span><a href="#38232160">next</a><span>|</span><label class="collapse" for="c-38232425">[-]</label><label class="expand" for="c-38232425">[2 more]</label></div><br/><div class="children"><div class="content">Can you add a link to it?</div><br/><div id="38233755" class="c"><input type="checkbox" id="c-38233755" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#38232032">root</a><span>|</span><a href="#38232425">parent</a><span>|</span><a href="#38232160">next</a><span>|</span><label class="collapse" for="c-38233755">[-]</label><label class="expand" for="c-38233755">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry to hear that your searches for that very specific name didn&#x27;t provide the information you were looking for<p>its show hn: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34970917">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34970917</a><p>tfl: <a href="https:&#x2F;&#x2F;www.crul.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.crul.com&#x2F;</a></div><br/></div></div></div></div></div></div></div></div><div id="38231714" class="c"><input type="checkbox" id="c-38231714" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#38232032">prev</a><span>|</span><a href="#38231550">next</a><span>|</span><label class="collapse" for="c-38231714">[-]</label><label class="expand" for="c-38231714">[8 more]</label></div><br/><div class="children"><div class="content">I like web scraping in Go. The support for parsing HTML in x&#x2F;text&#x2F;html is pretty good, and libraries like github.com&#x2F;PuerkitoBio&#x2F;goquery go a long way to matching ergonomics in other tools. This project uses both, but then also goes on to use github.com&#x2F;dop251&#x2F;goja, which is a JavaScript VM <i>and</i> it&#x27;s accompanying nodejs compatability layer <i>and</i> even esbuild, in order to <i>interpret scraping instruction scripts</i>.<p>I mean, at this point I am not sure Go is the right tool for the job (I am <i>actually</i> pretty confident that it is <i>not</i>).<p>A pretty neat stack of engineering, sure! This is cool, niely done. But I can&#x27;t help but feel disturbed.</div><br/><div id="38231760" class="c"><input type="checkbox" id="c-38231760" checked=""/><div class="controls bullet"><span class="by">cxr</span><span>|</span><a href="#38231714">parent</a><span>|</span><a href="#38232174">prev</a><span>|</span><a href="#38231550">next</a><span>|</span><label class="collapse" for="c-38231760">[-]</label><label class="expand" for="c-38231760">[5 more]</label></div><br/><div class="children"><div class="content">Your comment was posted 4 minutes ago. That means you still have enough time to edit your comment to change it so it contains real URLs that link to the project repos for the packages mentioned:<p>&lt;<a href="https:&#x2F;&#x2F;github.com&#x2F;PuerkitoBio&#x2F;goquery">https:&#x2F;&#x2F;github.com&#x2F;PuerkitoBio&#x2F;goquery</a>&gt;<p>&lt;<a href="https:&#x2F;&#x2F;github.com&#x2F;dop251&#x2F;goja">https:&#x2F;&#x2F;github.com&#x2F;dop251&#x2F;goja</a>&gt;<p>(Please do not reply to this comment of mine—if you do, I won&#x27;t be able to delete it once the previous post is fixed, because the existence of the replies will prevent that.)</div><br/><div id="38232390" class="c"><input type="checkbox" id="c-38232390" checked=""/><div class="controls bullet"><span class="by">xyzzy_plugh</span><span>|</span><a href="#38231714">root</a><span>|</span><a href="#38231760">parent</a><span>|</span><a href="#38232101">next</a><span>|</span><label class="collapse" for="c-38232390">[-]</label><label class="expand" for="c-38232390">[3 more]</label></div><br/><div class="children"><div class="content">Even if I saw this post in time, I wouldn&#x27;t have edited it. They are all proper Go package names.</div><br/></div></div><div id="38232101" class="c"><input type="checkbox" id="c-38232101" checked=""/><div class="controls bullet"><span class="by">cheapgeek</span><span>|</span><a href="#38231714">root</a><span>|</span><a href="#38231760">parent</a><span>|</span><a href="#38232390">prev</a><span>|</span><a href="#38231550">next</a><span>|</span><label class="collapse" for="c-38232101">[-]</label><label class="expand" for="c-38232101">[1 more]</label></div><br/><div class="children"><div class="content">Ok</div><br/></div></div></div></div></div></div><div id="38231550" class="c"><input type="checkbox" id="c-38231550" checked=""/><div class="controls bullet"><span class="by">snake117</span><span>|</span><a href="#38231714">prev</a><span>|</span><label class="collapse" for="c-38231550">[-]</label><label class="expand" for="c-38231550">[5 more]</label></div><br/><div class="children"><div class="content">Looks interesting, and thank you for sharing this! One common issue with scraping web pages is dealing with data that is dynamically loaded. Is there a solution for this? For example, when using Scrapy, you can have Splash running in Docker via scrapy-splash (<a href="https:&#x2F;&#x2F;github.com&#x2F;scrapy-plugins&#x2F;scrapy-splash">https:&#x2F;&#x2F;github.com&#x2F;scrapy-plugins&#x2F;scrapy-splash</a>).</div><br/><div id="38231687" class="c"><input type="checkbox" id="c-38231687" checked=""/><div class="controls bullet"><span class="by">philippta</span><span>|</span><a href="#38231550">parent</a><span>|</span><a href="#38231654">next</a><span>|</span><label class="collapse" for="c-38231687">[-]</label><label class="expand" for="c-38231687">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! As mentioned in another comment, currently there is no build in support for this yet.<p>As a workaround one could use a service like ScrapingBee (not affiliated) as a proxy, that renders the page in a browser for you.<p>Surely, relying on a service for this is not always ideal. I am also working on a small wrapper that turns Chrome into an HTTPS proxy, which you could plug right into flyscrape. Unfortunately it is very experimental still and not public yet. I have not yet decided if I release it as part of flyscrape or as a separate project.</div><br/></div></div><div id="38231654" class="c"><input type="checkbox" id="c-38231654" checked=""/><div class="controls bullet"><span class="by">figmert</span><span>|</span><a href="#38231550">parent</a><span>|</span><a href="#38231687">prev</a><span>|</span><a href="#38232179">next</a><span>|</span><label class="collapse" for="c-38231654">[-]</label><label class="expand" for="c-38231654">[2 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t you load the URL that is being dynamically loaded directly within your scraper?</div><br/><div id="38231922" class="c"><input type="checkbox" id="c-38231922" checked=""/><div class="controls bullet"><span class="by">mdaniel</span><span>|</span><a href="#38231550">root</a><span>|</span><a href="#38231654">parent</a><span>|</span><a href="#38232179">next</a><span>|</span><label class="collapse" for="c-38231922">[-]</label><label class="expand" for="c-38231922">[1 more]</label></div><br/><div class="children"><div class="content">Not only can you, in my experience it is substantially less drama and arguably less load on the target system since the full page may make many many other requests that a presentation layer would care about that I don&#x27;t<p>The trade-offs usually fall into:<p>- authing to the endpoint can sometimes be weird<p>- it for sure makes the traffic stand out since it isn&#x27;t otherwise surrounded by those extraneous requests<p>- it, as with all good things scraping, carries its own maintenance and monitoring burden<p>However, similar to those tradeoffs, it&#x27;s also been my experience that a full page load offers a ton more tracking opportunities that are not present in a direct endpoint fetch. I mean, look how many &quot;stealth&quot; plugins out there designed to mask the fact that a headless browser is headless<p>But, having said all of that: without question the biggest risk to modern day scraping is Cloudflare and Akamai gatekeeping. I do appreciate the arguments of &quot;but ddos!11&quot; and yet I would rather only actors that are actually exhibiting bad behavior[1] be blocked instead of everyone trying with a copy of python who have set reasonable rate limits<p>1 = this setting aside that &quot;bad behavior&quot; can be defined as &quot;downloading data that the site makes freely available to Chrome but not freely available to python&quot;</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>