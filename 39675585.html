<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710234066489" as="style"/><link rel="stylesheet" href="styles.css?v=1710234066489"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2403.05440">Is Cosine-Similarity of Embeddings Really About Similarity?</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>Jimmc414</span> | <span>59 comments</span></div><br/><div><div id="39677416" class="c"><input type="checkbox" id="c-39677416" checked=""/><div class="controls bullet"><span class="by">sweezyjeezy</span><span>|</span><a href="#39676157">next</a><span>|</span><label class="collapse" for="c-39677416">[-]</label><label class="expand" for="c-39677416">[1 more]</label></div><br/><div class="children"><div class="content">&gt; While cosine similarity is invariant under such rotations R, one of the
 key insights in this paper is that the first (but not the second) objective is
 also invariant to rescalings of the columns of A and B<p>Ha interesting I wrote a blog post where I pointed this out a few years ago [1], and how we got around it for item-item similarity at an old job.<p><a href="https:&#x2F;&#x2F;swarbrickjones.wordpress.com&#x2F;2016&#x2F;11&#x2F;24&#x2F;note-on-an-item-item-recommender-system-using-matrix-factorisation&#x2F;" rel="nofollow">https:&#x2F;&#x2F;swarbrickjones.wordpress.com&#x2F;2016&#x2F;11&#x2F;24&#x2F;note-on-an-i...</a></div><br/></div></div><div id="39676157" class="c"><input type="checkbox" id="c-39676157" checked=""/><div class="controls bullet"><span class="by">neoncontrails</span><span>|</span><a href="#39677416">prev</a><span>|</span><a href="#39676598">next</a><span>|</span><label class="collapse" for="c-39676157">[-]</label><label class="expand" for="c-39676157">[5 more]</label></div><br/><div class="children"><div class="content">&gt; In the following, we show that [taking cosine similarity between two features in a learned embedding] can lead to arbitrary results, and they may not even be unique.<p>Was uniqueness ever a guarantee? It&#x27;s a distance metric. It&#x27;s reasonable to assume that two features can be equidistant to the ideal solution to a linear system of equations. Maybe I&#x27;m missing something.</div><br/><div id="39677241" class="c"><input type="checkbox" id="c-39677241" checked=""/><div class="controls bullet"><span class="by">SimplyUnknown</span><span>|</span><a href="#39676157">parent</a><span>|</span><a href="#39677190">next</a><span>|</span><label class="collapse" for="c-39677241">[-]</label><label class="expand" for="c-39677241">[1 more]</label></div><br/><div class="children"><div class="content">I think maybe it&#x27;s poorly phrased. As far as I can tell, their linear regression example for eq. 2 has an unique solution, but I think they state I that when optimizing for cosine similarity you can find non-unique solutions. But I haven&#x27;t read in detail.<p>Then again, you could argue whether that is a problem when considering very high dimensional embeddings. Their conclusions seem to point in that direction but I would not agree on that.</div><br/></div></div><div id="39677190" class="c"><input type="checkbox" id="c-39677190" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39676157">parent</a><span>|</span><a href="#39677241">prev</a><span>|</span><a href="#39676499">next</a><span>|</span><label class="collapse" for="c-39677190">[-]</label><label class="expand" for="c-39677190">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not even a distance metric, it doesn&#x27;t obey the triangle inequality (hence the not-technically-meaningful name &quot;similarity&quot;, like &quot;collection&quot; as opposed to &quot;set&quot;).</div><br/></div></div><div id="39676499" class="c"><input type="checkbox" id="c-39676499" checked=""/><div class="controls bullet"><span class="by">pletnes</span><span>|</span><a href="#39676157">parent</a><span>|</span><a href="#39677190">prev</a><span>|</span><a href="#39676598">next</a><span>|</span><label class="collapse" for="c-39676499">[-]</label><label class="expand" for="c-39676499">[2 more]</label></div><br/><div class="children"><div class="content">I sure hope noone claimed that. You’re doing potentially huge dimensionality reduction, uniqueness would be like saying you cannot have md5 collisions.</div><br/><div id="39677396" class="c"><input type="checkbox" id="c-39677396" checked=""/><div class="controls bullet"><span class="by">JohnKemeny</span><span>|</span><a href="#39676157">root</a><span>|</span><a href="#39676499">parent</a><span>|</span><a href="#39676598">next</a><span>|</span><label class="collapse" for="c-39677396">[-]</label><label class="expand" for="c-39677396">[1 more]</label></div><br/><div class="children"><div class="content">Johnson and Lindenstrauss disagree.  <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstrauss_lemma" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstrauss_...</a></div><br/></div></div></div></div></div></div><div id="39676598" class="c"><input type="checkbox" id="c-39676598" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39676157">prev</a><span>|</span><a href="#39676572">next</a><span>|</span><label class="collapse" for="c-39676598">[-]</label><label class="expand" for="c-39676598">[11 more]</label></div><br/><div class="children"><div class="content">Embeddings result from computing what word can appear in a given context, so words that would appear in the same spot will have higher cosine score between themselves.<p>But it doesn&#x27;t differentiate further, so you can have &quot;beautiful&quot; and &quot;ugly&quot; embed very close to each other even though they are opposites - they tend to appear in similar places.<p>Another limitation of embeddings and cosine-similarity is that that they can&#x27;t tell you &quot;how similar&quot; - is it equivalence or just relatedness? They make a mess of equivalent, antonymous and related things.</div><br/><div id="39677439" class="c"><input type="checkbox" id="c-39677439" checked=""/><div class="controls bullet"><span class="by">bruturis</span><span>|</span><a href="#39676598">parent</a><span>|</span><a href="#39676691">next</a><span>|</span><label class="collapse" for="c-39677439">[-]</label><label class="expand" for="c-39677439">[1 more]</label></div><br/><div class="children"><div class="content">Yours comment  is very interesting,  I have reflected many times about how to  differentiate things that appears in the same context of things that are similar. Any big idea here could be the spark to create a great startup.</div><br/></div></div><div id="39676691" class="c"><input type="checkbox" id="c-39676691" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39676598">parent</a><span>|</span><a href="#39677439">prev</a><span>|</span><a href="#39676633">next</a><span>|</span><label class="collapse" for="c-39676691">[-]</label><label class="expand" for="c-39676691">[4 more]</label></div><br/><div class="children"><div class="content">For word2vec-esque word embeddings, yes.<p>For modern embedding models which effectively mean-pool the last hidden state of LLMs (and therefore make use of its optimizations such as attention tricks), embeddings can be much more robust to different contexts both local and global.</div><br/><div id="39676737" class="c"><input type="checkbox" id="c-39676737" checked=""/><div class="controls bullet"><span class="by">LunaSea</span><span>|</span><a href="#39676598">root</a><span>|</span><a href="#39676691">parent</a><span>|</span><a href="#39676633">next</a><span>|</span><label class="collapse" for="c-39676737">[-]</label><label class="expand" for="c-39676737">[3 more]</label></div><br/><div class="children"><div class="content">Would you have some in links in mind of models that were pulled from LLMs?<p>The last one I have in mind is BERT and it&#x27;s variants.</div><br/><div id="39676762" class="c"><input type="checkbox" id="c-39676762" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39676598">root</a><span>|</span><a href="#39676737">parent</a><span>|</span><a href="#39676633">next</a><span>|</span><label class="collapse" for="c-39676762">[-]</label><label class="expand" for="c-39676762">[2 more]</label></div><br/><div class="children"><div class="content">BERT embeddings were what proved that taking embeddings using the last hidden state works (in that case, using the [CLS] token representation which is IMO silly). Most of the top embedding models on the MTEB leaderboard are mean-pooled LLMs: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;mteb&#x2F;leaderboard</a><p>The not-quite-large embedding model I like to use now is nomic-embed-text-v1.5 (based on a BERT architecture), which supports a 8192 context window and MRL for reducing the dimensionality if needed: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;nomic-ai&#x2F;nomic-embed-text-v1.5" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;nomic-ai&#x2F;nomic-embed-text-v1.5</a></div><br/><div id="39676867" class="c"><input type="checkbox" id="c-39676867" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676598">root</a><span>|</span><a href="#39676762">parent</a><span>|</span><a href="#39676633">next</a><span>|</span><label class="collapse" for="c-39676867">[-]</label><label class="expand" for="c-39676867">[1 more]</label></div><br/><div class="children"><div class="content">Works for what? The leaderboards? BPE tiktokens, BPE GPT-2 tokens, SentencePiece, GloVe, word2vec, ..., take your pick, they all end up in a latent space of arbitrary dimensionality and arbitrary vocab size where they can be mapped onto images. This is never going to work for language. The only thing the leaderboards are good for is enabling you to charge more for your model than everyone else for a month or two. The only meaning hyperparameters like dimensionality and vocab size have is in their message that more is always better and scaling up is what matters.</div><br/></div></div></div></div></div></div></div></div><div id="39676633" class="c"><input type="checkbox" id="c-39676633" checked=""/><div class="controls bullet"><span class="by">mo_42</span><span>|</span><a href="#39676598">parent</a><span>|</span><a href="#39676691">prev</a><span>|</span><a href="#39677038">next</a><span>|</span><label class="collapse" for="c-39676633">[-]</label><label class="expand" for="c-39676633">[2 more]</label></div><br/><div class="children"><div class="content">Only if those two words appear in the same contexts with the same frequency. In natural language this is probably not the case. There are things typically considered beautiful and others as ugly.</div><br/><div id="39677285" class="c"><input type="checkbox" id="c-39677285" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#39676598">root</a><span>|</span><a href="#39676633">parent</a><span>|</span><a href="#39677038">next</a><span>|</span><label class="collapse" for="c-39677285">[-]</label><label class="expand" for="c-39677285">[1 more]</label></div><br/><div class="children"><div class="content">one key insights is that opposites do have <i>a lot</i> in common, they are often opposites in exactly one of n feature dimensions. For example black and white are both (arguably) colors, are related to (lack of) light, have representations in various formats (RGB, …), appear in the same grammatical position of ordered adjectives …</div><br/></div></div></div></div><div id="39677038" class="c"><input type="checkbox" id="c-39677038" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39676598">parent</a><span>|</span><a href="#39676633">prev</a><span>|</span><a href="#39676659">next</a><span>|</span><label class="collapse" for="c-39677038">[-]</label><label class="expand" for="c-39677038">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that kind of the point though? That beautiful and ugly are encoded closely as an &quot;idea&quot; when viewed from the appropriate angle?</div><br/><div id="39677087" class="c"><input type="checkbox" id="c-39677087" checked=""/><div class="controls bullet"><span class="by">Sai_</span><span>|</span><a href="#39676598">root</a><span>|</span><a href="#39677038">parent</a><span>|</span><a href="#39676659">next</a><span>|</span><label class="collapse" for="c-39677087">[-]</label><label class="expand" for="c-39677087">[1 more]</label></div><br/><div class="children"><div class="content">Apart from an ESL class explaining antonyms, I can’t think of any use case where an API which could equally return “ugly” or “beautiful” can be used.<p>We need embeddings to give relatedness across axes like synonymity etc.</div><br/></div></div></div></div><div id="39676659" class="c"><input type="checkbox" id="c-39676659" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676598">parent</a><span>|</span><a href="#39677038">prev</a><span>|</span><a href="#39676572">next</a><span>|</span><label class="collapse" for="c-39676659">[-]</label><label class="expand" for="c-39676659">[1 more]</label></div><br/><div class="children"><div class="content">They make a mess of language. They are not a suitable representation. They are suitable for their efficiency in information retrieval systems and for sometimes crudely capturing semantic attributes in a way that is unreliable and uninterpretable. It ends there. Here&#x27;s to ten more years of word2vec.</div><br/></div></div></div></div><div id="39676572" class="c"><input type="checkbox" id="c-39676572" checked=""/><div class="controls bullet"><span class="by">mo_42</span><span>|</span><a href="#39676598">prev</a><span>|</span><a href="#39676410">next</a><span>|</span><label class="collapse" for="c-39676572">[-]</label><label class="expand" for="c-39676572">[3 more]</label></div><br/><div class="children"><div class="content">I quickly read through the paper. One thing to note is that they use the Frobenius norm (at least I suppose this from the index F) for the matrix factorization. That is for their learning algorithm. Then, they use the cosine-similarity to evaluate. A metric that wasn&#x27;t used in the algorithm.<p>This is a long-standing question for me. Theoretically, I should use the CS in my optimization and then also in the evaluation. But I haven&#x27;t tested this empirically.<p>For example, there is sperical K-meams that clusters the data on the unit sphere.</div><br/><div id="39677232" class="c"><input type="checkbox" id="c-39677232" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#39676572">parent</a><span>|</span><a href="#39677196">prev</a><span>|</span><a href="#39676410">next</a><span>|</span><label class="collapse" for="c-39677232">[-]</label><label class="expand" for="c-39677232">[1 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s kind of the point of the paper. The model is based on un-normalized dot products, and wasn&#x27;t deliberately designed to produce meaningful cosine similarities. They are showing that, in that case, cosine similarities might be arbitrary and not as useful as people might assume or hope.</div><br/></div></div></div></div><div id="39676410" class="c"><input type="checkbox" id="c-39676410" checked=""/><div class="controls bullet"><span class="by">LifeIsBio</span><span>|</span><a href="#39676572">prev</a><span>|</span><a href="#39677426">next</a><span>|</span><label class="collapse" for="c-39676410">[-]</label><label class="expand" for="c-39676410">[1 more]</label></div><br/><div class="children"><div class="content">The paper kinda leaves you hanging on the &quot;alternatives&quot; front, even though they have a section dedicated to it.<p>In addition to the _quality_ of any proposed alternative(s), computational speed also has to be a consideration. I&#x27;ve run into multiple situations where you want to measure similarities on the order of millions&#x2F;billions of times. Especially for realtime applications (like RAG?) speed may even out weight quality.</div><br/></div></div><div id="39677426" class="c"><input type="checkbox" id="c-39677426" checked=""/><div class="controls bullet"><span class="by">jsn_5</span><span>|</span><a href="#39676410">prev</a><span>|</span><a href="#39677113">next</a><span>|</span><label class="collapse" for="c-39677426">[-]</label><label class="expand" for="c-39677426">[1 more]</label></div><br/><div class="children"><div class="content">All models are wrong, some are useful.</div><br/></div></div><div id="39677113" class="c"><input type="checkbox" id="c-39677113" checked=""/><div class="controls bullet"><span class="by">apstroll</span><span>|</span><a href="#39677426">prev</a><span>|</span><a href="#39676570">next</a><span>|</span><label class="collapse" for="c-39677113">[-]</label><label class="expand" for="c-39677113">[1 more]</label></div><br/><div class="children"><div class="content">Cosine Similarity is very much about similarity, but it&#x27;s quite fickle and indirect.<p>Given a function f(l, r) that measures, say, the logprobability of observing both l and r, and that the function takes the form f(l, r) = &lt;L(l), R(r)&gt;, i.e. the dot product between embeddings of l and r, then cosine similarity of x and y, i.e. normalized dot product of L(x) and L(y) is very closely related to the correlation of f(x, Z) and f(y, Z) when we let Z vary.</div><br/></div></div><div id="39676570" class="c"><input type="checkbox" id="c-39676570" checked=""/><div class="controls bullet"><span class="by">greesil</span><span>|</span><a href="#39677113">prev</a><span>|</span><a href="#39676326">next</a><span>|</span><label class="collapse" for="c-39676570">[-]</label><label class="expand" for="c-39676570">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s about keeping stuff on the unit hypersphere</div><br/></div></div><div id="39676326" class="c"><input type="checkbox" id="c-39676326" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#39676570">prev</a><span>|</span><a href="#39676485">next</a><span>|</span><label class="collapse" for="c-39676326">[-]</label><label class="expand" for="c-39676326">[6 more]</label></div><br/><div class="children"><div class="content">Distance metrics are an interesting topic. The field of ecology has a ton of them. For example see vegdist the Dissimilarity Indices for Community Ecologists function in the Vegan package in R:
<a href="https:&#x2F;&#x2F;rdrr.io&#x2F;cran&#x2F;vegan&#x2F;man&#x2F;vegdist.html" rel="nofollow">https:&#x2F;&#x2F;rdrr.io&#x2F;cran&#x2F;vegan&#x2F;man&#x2F;vegdist.html</a>
which includes, among others the &quot;canberra&quot;, &quot;clark&quot;, &quot;bray&quot;, &quot;kulczynski&quot;, &quot;gower&quot;, &quot;altGower&quot;, &quot;morisita&quot;, &quot;horn&quot;, &quot;mountford&quot;, &quot;raup&quot;, &quot;chao&quot;, &quot;cao&quot;, &quot;mahalanobis&quot;, &quot;chord&quot;, &quot;hellinger&quot;, &quot;aitchison&quot;, or &quot;robust.aitchison&quot;.<p>Generic distance metrics can often be replaced with context-specific ones for better utility; it makes me wonder whether that insight could be useful in deep learning.</div><br/><div id="39676331" class="c"><input type="checkbox" id="c-39676331" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39676326">parent</a><span>|</span><a href="#39676485">next</a><span>|</span><label class="collapse" for="c-39676331">[-]</label><label class="expand" for="c-39676331">[5 more]</label></div><br/><div class="children"><div class="content">What are good distance metrics applied to latent embeddings as part of a diversity loss function to prevent model collapse?</div><br/><div id="39676435" class="c"><input type="checkbox" id="c-39676435" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#39676326">root</a><span>|</span><a href="#39676331">parent</a><span>|</span><a href="#39676455">next</a><span>|</span><label class="collapse" for="c-39676435">[-]</label><label class="expand" for="c-39676435">[1 more]</label></div><br/><div class="children"><div class="content">hell if I know!! Sorry. I&#x27;ve used the vegan package for some analyses, but I&#x27;ve mostly used Manhattan and cosine metrics. I just wanted to bring up the idea that there are a lot of metrics out there that may not be generally appreciated.<p>Claude Opus says &quot;There are a few good distance metrics commonly used with latent embeddings to promote diversity and prevent model collapse:<p>1. Euclidean Distance (L2 Distance) 
2. Cosine distance
3. Kullback-Leibler (KL) Divergence:
KL divergence quantifies how much one probability distribution differs from another. It can be used to measure the difference between the distributions of latent embeddings. Minimizing KL divergence as a diversity loss would encourage the embedding distribution to be more uniform.
4. Maximum Mean Discrepancy (MMD):
MMD measures the difference between two distributions by comparing their moments (mean, variance, etc.) in a reproducing kernel Hilbert space. It&#x27;s useful for comparing high-dimensional distributions like those of embeddings. MMD loss promotes diversity by penalizing embeddings that are too clustered together.
5. Gaussian Annulus Loss:
This loss function encourages embeddings to lie within an annulus (ring) in the latent space defined by two Gaussian distributions. It promotes uniformity in the embedding norms while allowing angular diversity. This can be effective at preventing collapse to a single point.
But I haven&#x27;t checked for hallucinations.</div><br/></div></div><div id="39676455" class="c"><input type="checkbox" id="c-39676455" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#39676326">root</a><span>|</span><a href="#39676331">parent</a><span>|</span><a href="#39676435">prev</a><span>|</span><a href="#39676447">next</a><span>|</span><label class="collapse" for="c-39676455">[-]</label><label class="expand" for="c-39676455">[2 more]</label></div><br/><div class="children"><div class="content">I had no idea that diversity loss function was a topic in deep learning. I admit, I&#x27;m a bit fascinated, as a neuroimaging scientist.</div><br/><div id="39676538" class="c"><input type="checkbox" id="c-39676538" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#39676326">root</a><span>|</span><a href="#39676455">parent</a><span>|</span><a href="#39676447">next</a><span>|</span><label class="collapse" for="c-39676538">[-]</label><label class="expand" for="c-39676538">[1 more]</label></div><br/><div class="children"><div class="content">Have a look at section 3.2 of the Wav2vec2 paper:<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2006.11477.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2006.11477.pdf</a></div><br/></div></div></div></div><div id="39676447" class="c"><input type="checkbox" id="c-39676447" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#39676326">root</a><span>|</span><a href="#39676331">parent</a><span>|</span><a href="#39676455">prev</a><span>|</span><a href="#39676485">next</a><span>|</span><label class="collapse" for="c-39676447">[-]</label><label class="expand" for="c-39676447">[1 more]</label></div><br/><div class="children"><div class="content">To add further: 
<a href="https:&#x2F;&#x2F;cran.r-project.org&#x2F;web&#x2F;packages&#x2F;vegan&#x2F;vignettes&#x2F;diversity-vegan.pdf" rel="nofollow">https:&#x2F;&#x2F;cran.r-project.org&#x2F;web&#x2F;packages&#x2F;vegan&#x2F;vignettes&#x2F;dive...</a>
Vegan package is very much into methods to assess diversity in ecologies.<p>Beta diversity is one metric for examining diversity, define as the ratio between regional and local species diversity. 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beta_diversity" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Beta_diversity</a></div><br/></div></div></div></div></div></div><div id="39676485" class="c"><input type="checkbox" id="c-39676485" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676326">prev</a><span>|</span><a href="#39676654">next</a><span>|</span><label class="collapse" for="c-39676485">[-]</label><label class="expand" for="c-39676485">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s definitely not about semantics or language. As far as language is concerned similarity metrics are semantically vacuous and quantifying semantic similarity is a bogus enterprise.</div><br/><div id="39677025" class="c"><input type="checkbox" id="c-39677025" checked=""/><div class="controls bullet"><span class="by">blackbear_</span><span>|</span><a href="#39676485">parent</a><span>|</span><a href="#39676654">next</a><span>|</span><label class="collapse" for="c-39677025">[-]</label><label class="expand" for="c-39677025">[3 more]</label></div><br/><div class="children"><div class="content">Can you elaborate?</div><br/><div id="39677298" class="c"><input type="checkbox" id="c-39677298" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676485">root</a><span>|</span><a href="#39677025">parent</a><span>|</span><a href="#39676654">next</a><span>|</span><label class="collapse" for="c-39677298">[-]</label><label class="expand" for="c-39677298">[2 more]</label></div><br/><div class="children"><div class="content">Modeling language in a latent space is useful for certain kinds of analyses and certain aspects of language. It has its place as an empirical tool. That place is not the nuts and bolts of language itself. There are more suitable formalisms for this than directional magnitudes and BPE tiktokens.</div><br/><div id="39677354" class="c"><input type="checkbox" id="c-39677354" checked=""/><div class="controls bullet"><span class="by">blackbear_</span><span>|</span><a href="#39676485">root</a><span>|</span><a href="#39677298">parent</a><span>|</span><a href="#39676654">next</a><span>|</span><label class="collapse" for="c-39677354">[-]</label><label class="expand" for="c-39677354">[1 more]</label></div><br/><div class="children"><div class="content">What are those formalisms?</div><br/></div></div></div></div></div></div></div></div><div id="39676654" class="c"><input type="checkbox" id="c-39676654" checked=""/><div class="controls bullet"><span class="by">h_koko</span><span>|</span><a href="#39676485">prev</a><span>|</span><a href="#39676079">next</a><span>|</span><label class="collapse" for="c-39676654">[-]</label><label class="expand" for="c-39676654">[7 more]</label></div><br/><div class="children"><div class="content">What are better alternatives to cosine similarity?</div><br/><div id="39677323" class="c"><input type="checkbox" id="c-39677323" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#39676654">parent</a><span>|</span><a href="#39676708">next</a><span>|</span><label class="collapse" for="c-39677323">[-]</label><label class="expand" for="c-39677323">[1 more]</label></div><br/><div class="children"><div class="content">What happens when you mix cosine similarity with euclidean distance? at least give a small penalty if the euclidean distance is too far off?</div><br/></div></div><div id="39676708" class="c"><input type="checkbox" id="c-39676708" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39676654">parent</a><span>|</span><a href="#39677323">prev</a><span>|</span><a href="#39676953">next</a><span>|</span><label class="collapse" for="c-39676708">[-]</label><label class="expand" for="c-39676708">[2 more]</label></div><br/><div class="children"><div class="content">There aren&#x27;t any alternatives: cosine similarity is effectively an extension of Euclidian distance, which is the mathematical correct way for finding the distance between vectors.<p>You may not want to use cosine similarity as your <i>only</i> metric for rankings, however, and you may want to experiment with how you construct the embeddings.</div><br/><div id="39676769" class="c"><input type="checkbox" id="c-39676769" checked=""/><div class="controls bullet"><span class="by">rocqua</span><span>|</span><a href="#39676654">root</a><span>|</span><a href="#39676708">parent</a><span>|</span><a href="#39676953">next</a><span>|</span><label class="collapse" for="c-39676769">[-]</label><label class="expand" for="c-39676769">[1 more]</label></div><br/><div class="children"><div class="content">Euclidean distance is only &#x27;mathematically correct&#x27; if you care about rotations in your space being distance preserving. I don&#x27;t think that is really the case in these spaces.<p>There is a whole branch of mathematics called dedicated to other ways to measure distance.</div><br/></div></div></div></div><div id="39676953" class="c"><input type="checkbox" id="c-39676953" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676654">parent</a><span>|</span><a href="#39676708">prev</a><span>|</span><a href="#39676079">next</a><span>|</span><label class="collapse" for="c-39676953">[-]</label><label class="expand" for="c-39676953">[3 more]</label></div><br/><div class="children"><div class="content">For describing semantics in natural language? Pretty much anything else.</div><br/><div id="39677306" class="c"><input type="checkbox" id="c-39677306" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#39676654">root</a><span>|</span><a href="#39676953">parent</a><span>|</span><a href="#39676079">next</a><span>|</span><label class="collapse" for="c-39677306">[-]</label><label class="expand" for="c-39677306">[2 more]</label></div><br/><div class="children"><div class="content">like?</div><br/><div id="39677347" class="c"><input type="checkbox" id="c-39677347" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676654">root</a><span>|</span><a href="#39677306">parent</a><span>|</span><a href="#39676079">next</a><span>|</span><label class="collapse" for="c-39677347">[-]</label><label class="expand" for="c-39677347">[1 more]</label></div><br/><div class="children"><div class="content">Having a mid-century theory of natural language semantics isn&#x27;t necessarily a bad thing. You just have to pick the right one.</div><br/></div></div></div></div></div></div></div></div><div id="39676079" class="c"><input type="checkbox" id="c-39676079" checked=""/><div class="controls bullet"><span class="by">latency-guy2</span><span>|</span><a href="#39676654">prev</a><span>|</span><label class="collapse" for="c-39676079">[-]</label><label class="expand" for="c-39676079">[17 more]</label></div><br/><div class="children"><div class="content">Not reading the paper, cosine similarity has little to no semantic understanding of sentences.<p>E.g. the following triple<p>1: &quot;Yes, this is a demonstration&quot;<p>2: &quot;Yes, this isn&#x27;t a demonstration&quot;<p>3: &quot;Here is an example&quot;<p>&lt;1, 2&gt;, Has &quot;higher&quot; cosine similarity than &lt;1, 3&gt;, structurally equivalent except for one token&#x2F;word, &lt;1, 2&gt; semantically means the opposite of each other depending on what you&#x27;re targeting in that sentence. While &lt;1, 3&gt; means effectively the same thing.<p>If this paper is about persuading people about efficacy with regards to semantic understanding, OK, but that was always known. If its about something with relation to vectors and the underlying operations, then I&#x27;ll be interested.</div><br/><div id="39676285" class="c"><input type="checkbox" id="c-39676285" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#39676079">parent</a><span>|</span><a href="#39676117">next</a><span>|</span><label class="collapse" for="c-39676285">[-]</label><label class="expand" for="c-39676285">[4 more]</label></div><br/><div class="children"><div class="content">Whether your not the cosine similarity of either pair is higher depends on the mapping you create from the strings to the embedding vector. That mapping can be whichever function you choose, and your result will be entirely dependent on that.<p>If you choose a straight linear mapping of tokens to a number, then you&#x27;d be right.<p>Extending that, if you choose <i>any</i> mapping which does not do a more extensive remapping from raw syntactic structure to some sort of semantic representation, you&#x27;d be right.<p>But hence why we increasingly use models to create embeddings instead of simpler approaches before applying a similarity metric, whether cosine similarity or other.<p>Put another way, there is no inherent reason why you couldn&#x27;t have a model where the embeddings for 1 and 3 are <i>identical</i> even, and so it is meaningless to talk about the cosine similarity of your sentences without setting out your assumptions about how you will created embeddings from them.</div><br/><div id="39676772" class="c"><input type="checkbox" id="c-39676772" checked=""/><div class="controls bullet"><span class="by">latency-guy2</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676285">parent</a><span>|</span><a href="#39676517">next</a><span>|</span><label class="collapse" for="c-39676772">[-]</label><label class="expand" for="c-39676772">[1 more]</label></div><br/><div class="children"><div class="content">&gt; meaningless to talk about the cosine similarity of your sentences without setting out your assumptions about how you will created embeddings from them.<p>I agree, but from generics POV, you have to settle on a few things to compare between models. If you can&#x27;t, then benchmarks are useless too outside of extremely narrow measures.<p>I only address structure in the parent, and sure, it can be too generic of a statement by only touching on structure. But I would almost assert structure is still an important feature, and I would almost assert that it is required or otherwise a dominant feature when you want to deliver a product for general use.<p>I don&#x27;t think I get too much more incorrect going beyond a few dimensions given this.</div><br/></div></div><div id="39676517" class="c"><input type="checkbox" id="c-39676517" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676285">parent</a><span>|</span><a href="#39676772">prev</a><span>|</span><a href="#39676117">next</a><span>|</span><label class="collapse" for="c-39676517">[-]</label><label class="expand" for="c-39676517">[2 more]</label></div><br/><div class="children"><div class="content">It is meaningless to talk about cosine similarity of sentences, or words, at all. Choose whatever mapping you want. You&#x27;ll still be in Firth Mode.</div><br/><div id="39677015" class="c"><input type="checkbox" id="c-39677015" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676517">parent</a><span>|</span><a href="#39676117">next</a><span>|</span><label class="collapse" for="c-39677015">[-]</label><label class="expand" for="c-39677015">[1 more]</label></div><br/><div class="children"><div class="content">Uh oh. LOL. Got some angry Firthers out there.</div><br/></div></div></div></div></div></div><div id="39676117" class="c"><input type="checkbox" id="c-39676117" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#39676079">parent</a><span>|</span><a href="#39676285">prev</a><span>|</span><a href="#39676109">next</a><span>|</span><label class="collapse" for="c-39676117">[-]</label><label class="expand" for="c-39676117">[2 more]</label></div><br/><div class="children"><div class="content">That is entirely dependant on the model for the embeddings. You can fine tune for pretty much any outcome you want.</div><br/><div id="39676975" class="c"><input type="checkbox" id="c-39676975" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676117">parent</a><span>|</span><a href="#39676109">next</a><span>|</span><label class="collapse" for="c-39676975">[-]</label><label class="expand" for="c-39676975">[1 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t fine-tune for understanding or reasoning. You can&#x27;t &quot;get better performance&quot; on understanding. You&#x27;re either equipped for it or you&#x27;re not.</div><br/></div></div></div></div><div id="39676109" class="c"><input type="checkbox" id="c-39676109" checked=""/><div class="controls bullet"><span class="by">soarerz</span><span>|</span><a href="#39676079">parent</a><span>|</span><a href="#39676117">prev</a><span>|</span><a href="#39676212">next</a><span>|</span><label class="collapse" for="c-39676109">[-]</label><label class="expand" for="c-39676109">[5 more]</label></div><br/><div class="children"><div class="content">What is the cheapest way to capture similarity if not via dot product then?</div><br/><div id="39676723" class="c"><input type="checkbox" id="c-39676723" checked=""/><div class="controls bullet"><span class="by">latency-guy2</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676109">parent</a><span>|</span><a href="#39677422">next</a><span>|</span><label class="collapse" for="c-39676723">[-]</label><label class="expand" for="c-39676723">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have an answer for this really outside of silly ones like &quot;strict equality check&quot;, but I assert that no one else does either, at least today and right now, and its an inherent limitation due to the nature of embeddings and the space it desires to be (cheap, fast, good enough similarity for your use case).<p>You&#x27;re probably best off using the commercial suggestion, and if its dot product, go for it. I am no expert in this area and my interest wanes every day.</div><br/></div></div><div id="39677422" class="c"><input type="checkbox" id="c-39677422" checked=""/><div class="controls bullet"><span class="by">scotty79</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676109">parent</a><span>|</span><a href="#39676723">prev</a><span>|</span><a href="#39676362">next</a><span>|</span><label class="collapse" for="c-39677422">[-]</label><label class="expand" for="c-39677422">[1 more]</label></div><br/><div class="children"><div class="content">Instead of sums of multiplications you could for example use sum of squares of differences.<p>Means squared error instead of dot product, it&#x27;s not cheaper but it&#x27;s close<p>If you want to go cheaper you could use sum of abs of differences.</div><br/></div></div><div id="39676362" class="c"><input type="checkbox" id="c-39676362" checked=""/><div class="controls bullet"><span class="by">gajus</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676109">parent</a><span>|</span><a href="#39677422">prev</a><span>|</span><a href="#39676307">next</a><span>|</span><label class="collapse" for="c-39676362">[-]</label><label class="expand" for="c-39676362">[1 more]</label></div><br/><div class="children"><div class="content">Interested to know as well</div><br/></div></div></div></div><div id="39676212" class="c"><input type="checkbox" id="c-39676212" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#39676079">parent</a><span>|</span><a href="#39676109">prev</a><span>|</span><a href="#39676373">next</a><span>|</span><label class="collapse" for="c-39676212">[-]</label><label class="expand" for="c-39676212">[3 more]</label></div><br/><div class="children"><div class="content">That might be true for one-hot vectors but it&#x27;s not true for learned embedding through the lens of attention. That said, I only made to page 3&#x2F;9 of the paper before the mark-up for the math went over my head.</div><br/><div id="39676666" class="c"><input type="checkbox" id="c-39676666" checked=""/><div class="controls bullet"><span class="by">latency-guy2</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676212">parent</a><span>|</span><a href="#39676585">next</a><span>|</span><label class="collapse" for="c-39676666">[-]</label><label class="expand" for="c-39676666">[1 more]</label></div><br/><div class="children"><div class="content">If we&#x27;re talking about adding dimensionality and relying on the kernel, sure, but I only get so much incorrect by going from 1 to M dims.<p>I can&#x27;t of course be certain in all cases, but dimensions are typically (past experience, and using knowledge from word2vec experiments from years ago) derivative of higher dimensions. The kernel still operates on the same concept by applying a norm along with whatever weightings to each dim.<p>Semantic understanding is still not there in my opinion, we might feign it by increasing specificity, but only so much. Largest contributor will likely still the determining factor rather than the series of smaller, more specific dimensions.<p>I tested this using similar sentences as my original comment and failing in more scenarios than passing. I of course am biased since it may be given I did not select the right dimensions or measures.</div><br/></div></div><div id="39676585" class="c"><input type="checkbox" id="c-39676585" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676079">root</a><span>|</span><a href="#39676212">parent</a><span>|</span><a href="#39676666">prev</a><span>|</span><a href="#39676373">next</a><span>|</span><label class="collapse" for="c-39676585">[-]</label><label class="expand" for="c-39676585">[1 more]</label></div><br/><div class="children"><div class="content">It is true. And if you want to say anything about meaning this isn&#x27;t even the right math.</div><br/></div></div></div></div><div id="39676504" class="c"><input type="checkbox" id="c-39676504" checked=""/><div class="controls bullet"><span class="by">seanbethard</span><span>|</span><a href="#39676079">parent</a><span>|</span><a href="#39676373">prev</a><span>|</span><label class="collapse" for="c-39676504">[-]</label><label class="expand" for="c-39676504">[1 more]</label></div><br/><div class="children"><div class="content">No understanding. Embeddings are a semantically vacuous representation and similarity is a semantically vacuous interpretation.</div><br/></div></div></div></div></div></div></div></div></div></body></html>