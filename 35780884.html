<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683002452746" as="style"/><link rel="stylesheet" href="styles.css?v=1683002452746"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.quantamagazine.org/cynthia-rudin-builds-ai-that-humans-can-understand-20230427/">Cynthia Rudin and interpretable ML models</a> <span class="domain">(<a href="https://www.quantamagazine.org">www.quantamagazine.org</a>)</span></div><div class="subtext"><span>SirLJ</span> | <span>37 comments</span></div><br/><div><div id="35781927" class="c"><input type="checkbox" id="c-35781927" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#35782772">next</a><span>|</span><label class="collapse" for="c-35781927">[-]</label><label class="expand" for="c-35781927">[17 more]</label></div><br/><div class="children"><div class="content">I’ve always wondered what an sufficient explanation of a neural network would entail.<p>At a very low level, there’s no secret: all of the weights are there, all of the relationships between weights are known. The problem is that it doesn’t tell you anything about the emergent properties of the network, in the same way that quantum physics doesn’t give much insight into biology.<p>It may be possible that there is no English sentence you can utter about the network which is both explanatory and fully accurate. What is the network doing? It’s trying to approximate the function you’ve given it. That’s it.<p>You can try other things like ablation to find the effects of lobotomizing the network in certain ways, but this also can’t fully explain 2nd and higher order relationships.</div><br/><div id="35782356" class="c"><input type="checkbox" id="c-35782356" checked=""/><div class="controls bullet"><span class="by">kypro</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35783277">next</a><span>|</span><label class="collapse" for="c-35782356">[-]</label><label class="expand" for="c-35782356">[8 more]</label></div><br/><div class="children"><div class="content">This sentence from the article stood out to me:<p>&gt; The explanations have to be wrong, because if their explanations were always right, you could just replace the black box with the explanations.<p>I&#x27;ve never thought of it like that, but I think that really gets to the core of the issue...<p>To be able to reason about the behaviour of a neural network with 100% confidence you would need an explanation that incorporated every weight, otherwise you have to accept some degree of uncertainty.<p>But the idea you could ever explain something with billions of parameters in a way that a human could comprehend seems ridiculous. Imperfect generalisation therefore would inevitably be required – &quot;this bit of the network seems to perform x function&quot;. But in doing this you have to accept that this generalisation comes with uncertainty because such a high-level approximation is unlikely to capture all of the nuances. And if it did capture all nuances then the black box isn&#x27;t needed.</div><br/><div id="35782542" class="c"><input type="checkbox" id="c-35782542" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782356">parent</a><span>|</span><a href="#35782490">next</a><span>|</span><label class="collapse" for="c-35782542">[-]</label><label class="expand" for="c-35782542">[2 more]</label></div><br/><div class="children"><div class="content">Obviously debatable, but this seems wrong. It could easily be that we can&#x27;t figure out how to do something but when presented with a solution understand it. Many people understand public private key cryptography but could never in a million years come up with it. Same for much of physics, a lot of math etc. As for the sheer complexity of millions&#x2F;billions of parameters - people can explain and understand molecular dynamics simulations perfectly well. They can&#x27;t use a small number of words to explain a specific configuration, but it&#x27;s explainable.</div><br/><div id="35783175" class="c"><input type="checkbox" id="c-35783175" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782542">parent</a><span>|</span><a href="#35782490">next</a><span>|</span><label class="collapse" for="c-35783175">[-]</label><label class="expand" for="c-35783175">[1 more]</label></div><br/><div class="children"><div class="content">I think there’s a major difference though. You can understand public key crypto at varying levels of complexity because it is a topic created from first principles. Any sub-topic within the umbrella of public key crypto can be explored further. In this sense the knowledge is like a large program.<p>Neural networks aren’t like that. The whole thing works together simultaneously to derive a result. So I’m skeptical that there is a high level explanation other than “it’s doing it’s best to approximate the data it was trained on” which is not that useful.</div><br/></div></div></div></div><div id="35782490" class="c"><input type="checkbox" id="c-35782490" checked=""/><div class="controls bullet"><span class="by">foobarbecue</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782356">parent</a><span>|</span><a href="#35782542">prev</a><span>|</span><a href="#35782416">next</a><span>|</span><label class="collapse" for="c-35782490">[-]</label><label class="expand" for="c-35782490">[1 more]</label></div><br/><div class="children"><div class="content">All models are wrong, but some models are useful.</div><br/></div></div><div id="35782416" class="c"><input type="checkbox" id="c-35782416" checked=""/><div class="controls bullet"><span class="by">flir</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782356">parent</a><span>|</span><a href="#35782490">prev</a><span>|</span><a href="#35782596">next</a><span>|</span><label class="collapse" for="c-35782416">[-]</label><label class="expand" for="c-35782416">[2 more]</label></div><br/><div class="children"><div class="content">The map is not the territory.<p>Or, at more length and in more depth, The Analytical Language of John Wilkins by Borges.</div><br/><div id="35783221" class="c"><input type="checkbox" id="c-35783221" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782416">parent</a><span>|</span><a href="#35782596">next</a><span>|</span><label class="collapse" for="c-35783221">[-]</label><label class="expand" for="c-35783221">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for sharing, the wiki has a pretty funny note:<p>&gt; Lewis Carroll, in Sylvie and Bruno Concluded (1893), made the point humorously with his description of a fictional map that had &quot;the scale of a mile to the mile&quot;. A character notes some practical difficulties with such a map and states that &quot;we now use the country itself, as its own map, and I assure you it does nearly as well.&quot;</div><br/></div></div></div></div><div id="35782596" class="c"><input type="checkbox" id="c-35782596" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782356">parent</a><span>|</span><a href="#35782416">prev</a><span>|</span><a href="#35783277">next</a><span>|</span><label class="collapse" for="c-35782596">[-]</label><label class="expand" for="c-35782596">[2 more]</label></div><br/><div class="children"><div class="content">Sure — abstractions elide details.<p>But “this portion of the network appears to recognize faces” is still a useful abstraction. In the same way it is for neurology. Even if both NNs and brains sometimes see faces that aren’t there because the network isn’t “recognizing faces” but a system which signals on a complex set of heuristics that <i>mostly</i> detects faces.<p>How does the answer change if I only want 99% confidence and accept the existence of illusions? — can we describe the bulk of the behavior in a summary?</div><br/><div id="35782920" class="c"><input type="checkbox" id="c-35782920" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782596">parent</a><span>|</span><a href="#35783277">next</a><span>|</span><label class="collapse" for="c-35782920">[-]</label><label class="expand" for="c-35782920">[1 more]</label></div><br/><div class="children"><div class="content">It would be very satisfying if we could have an explanation like that. But that’s far from guaranteed, unless that was part of the explicit design of an MMOE for example. Very likely if you ask “what part of the network recognizes faces” the best answer is “the whole thing”.</div><br/></div></div></div></div></div></div><div id="35783277" class="c"><input type="checkbox" id="c-35783277" checked=""/><div class="controls bullet"><span class="by">bane</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35782356">prev</a><span>|</span><a href="#35782616">next</a><span>|</span><label class="collapse" for="c-35783277">[-]</label><label class="expand" for="c-35783277">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the same way that quantum physics doesn’t give much insight into biology<p>This is very insightful. I&#x27;ve long maintained that &quot;understanding&quot; requires some predictive ability in addition to an ability to explain what&#x27;s happened in the past.<p>In the case of neural networks, we should be able to &quot;simply&quot; generate the network that has the desired behavior.<p>But we can&#x27;t. We&#x27;re still shackled to compiling some collection of training data that may or may not produce the desired behavior, tested by some other collection of testing data that may or may not test the desired phenomenon.</div><br/></div></div><div id="35782616" class="c"><input type="checkbox" id="c-35782616" checked=""/><div class="controls bullet"><span class="by">keithwinstein</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35783277">prev</a><span>|</span><a href="#35782338">next</a><span>|</span><label class="collapse" for="c-35782616">[-]</label><label class="expand" for="c-35782616">[2 more]</label></div><br/><div class="children"><div class="content">One thing that would be fun to know is, e.g., &quot;If the LLM answers question X correctly, then what&#x27;s a minimal-sized set of things we could remove from the training set and cause it to get that question wrong?&quot; I think with current methods this would be pretty expensive to find out, but, in principle I&#x27;m guessing it would be pretty illuminating.</div><br/><div id="35782933" class="c"><input type="checkbox" id="c-35782933" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782616">parent</a><span>|</span><a href="#35782338">next</a><span>|</span><label class="collapse" for="c-35782933">[-]</label><label class="expand" for="c-35782933">[1 more]</label></div><br/><div class="children"><div class="content">Causal modeling lets you determine where particular facts are stored without too much computational cost and also how you can edit those facts in the model. That might allow you to decouple the problem into first finding the desired set of gradients (converging at the target modification, zero for the weights you don&#x27;t care about), and just do a linear solve (since probabilistic weightings on how likely an input is in the training set will linearly impact each of the gradients in basically every neural architecture of note, including most LLM modifications) to find an approximately minimal (IIRC, a true minimal subset is NP-hard or something) subset of the inputs which when combined would give you the inverse of the target gradient. Remove that much of the weighting for each of the inputs.</div><br/></div></div></div></div><div id="35782338" class="c"><input type="checkbox" id="c-35782338" checked=""/><div class="controls bullet"><span class="by">obviouslynotme</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35782616">prev</a><span>|</span><a href="#35782534">next</a><span>|</span><label class="collapse" for="c-35782338">[-]</label><label class="expand" for="c-35782338">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It may be possible that there is no English sentence you can utter about <i>anything</i> which is both explanatory and fully accurate.<p>FTFY. Philosophy aside, humans understand complex things by building simpler abstractions for them. I think the key to NN understandability centers on building simpler NNs focused on specific tasks that build into bigger ones.</div><br/></div></div><div id="35782534" class="c"><input type="checkbox" id="c-35782534" checked=""/><div class="controls bullet"><span class="by">hyoogle</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35782338">prev</a><span>|</span><a href="#35782238">next</a><span>|</span><label class="collapse" for="c-35782534">[-]</label><label class="expand" for="c-35782534">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It may be possible that there is no English sentence you can utter about the network which is both explanatory and fully accurate.<p>reminds me of: &quot;The Tao that can be told is not the eternal Tao.&quot;</div><br/></div></div><div id="35782238" class="c"><input type="checkbox" id="c-35782238" checked=""/><div class="controls bullet"><span class="by">sdwr</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35782534">prev</a><span>|</span><a href="#35782577">next</a><span>|</span><label class="collapse" for="c-35782238">[-]</label><label class="expand" for="c-35782238">[2 more]</label></div><br/><div class="children"><div class="content">Same thing as the brain pretty much, except we have root control over the NN in a way we don&#x27;t with the brain.<p>To my knowledge, some angles have been explored. Finding which inputs maximally stimulate indiv neurons, and tracing that up the chain.<p>There was a fantastic article that did this on ImageNet - the takeaway was that neurons adjacent to the input encode sharp, basic features (like edges), and &quot;higher-level&quot; ones encode more nuanced stuff (textures, fur, wheels).<p>Then ablations, as you mentioned. And finally, rerunning the same training data on differently sized + shaped architectures.<p>AI systems are missing at least one level of complexity. All neurons in a NN layer fire simultaneously, unlike brain neurons, which trigger each other async.</div><br/><div id="35783000" class="c"><input type="checkbox" id="c-35783000" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#35781927">root</a><span>|</span><a href="#35782238">parent</a><span>|</span><a href="#35782577">next</a><span>|</span><label class="collapse" for="c-35783000">[-]</label><label class="expand" for="c-35783000">[1 more]</label></div><br/><div class="children"><div class="content">The brain is heterogenous in ways that a neural network needn’t be. Certain brain regions definitely perform certain functions, certain regions are hardwired to other regions. You can chop out a chunk of a person’s brain and they’ll be completely unable to do certain things.<p>Afaik the properties you’re referring to on AlexNet (the network trained to perform on Image Net) have to do with the nature of repeated convolution operations which, while interesting, is not as deeply insightful as I believe the article is aiming for.</div><br/></div></div></div></div><div id="35782577" class="c"><input type="checkbox" id="c-35782577" checked=""/><div class="controls bullet"><span class="by">mxkopy</span><span>|</span><a href="#35781927">parent</a><span>|</span><a href="#35782238">prev</a><span>|</span><a href="#35782772">next</a><span>|</span><label class="collapse" for="c-35782577">[-]</label><label class="expand" for="c-35782577">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve thought about pattern matching NN weights against known graph structures. I&#x27;d imagine, for example, that decision trees emerge at some point in GPT.<p>The real problem IMO is our ability to represent natural concepts as such graphs. Is love, for example, a DFA? Can we search for its isomorphism?</div><br/></div></div></div></div><div id="35782772" class="c"><input type="checkbox" id="c-35782772" checked=""/><div class="controls bullet"><span class="by">armchairhacker</span><span>|</span><a href="#35781927">prev</a><span>|</span><a href="#35782215">next</a><span>|</span><label class="collapse" for="c-35782772">[-]</label><label class="expand" for="c-35782772">[1 more]</label></div><br/><div class="children"><div class="content">Getting AI to show its work isn&#x27;t just for accountability. &quot;Showing your work&quot; gives you a clearer picture of the problem &#x2F; solution and prevents &#x2F; fixes bugs in implicit reasoning, the key problem current AI has which prevents it from being truly autonomous.<p>Ask GPT4 to do a task, and then ask it to the same task showing its work; you&#x27;ll find that GPT4 is less likely to make mistakes on the latter. This is especially apparent for tasks like counting # of words and multi-step problems, which GPT normally has trouble with.<p>But GPT4 still tends to struggle even breaking the task down, to the point where it starts producing extremely obvious mistakes (e.g. &quot;the turtle moves 1 unit up, from (1, 0) to (2, 0)&quot;). One possibility is that it isn&#x27;t actually showing its work, it&#x27;s just generating backwards explanations from a latent conclusion. Maybe this research will clarify whether this is the case, and help us develop a more coherent LLM.</div><br/></div></div><div id="35782215" class="c"><input type="checkbox" id="c-35782215" checked=""/><div class="controls bullet"><span class="by">Blammar</span><span>|</span><a href="#35782772">prev</a><span>|</span><a href="#35781592">next</a><span>|</span><label class="collapse" for="c-35782215">[-]</label><label class="expand" for="c-35782215">[1 more]</label></div><br/><div class="children"><div class="content">I view explainability or interpretability of a network as the ability to take a network and replace it with a drastically smaller set of functions and tables that (a) you can explain and (b) work pretty much the same as the network does.<p>Because we understand these functions and tables, we understand exactly how well the network will work, and also what is missing (i.e., how we can expand its accuracy.)<p>I think this is a very hard problem, but it is one that needs to be solved.</div><br/></div></div><div id="35781592" class="c"><input type="checkbox" id="c-35781592" checked=""/><div class="controls bullet"><span class="by">derbOac</span><span>|</span><a href="#35782215">prev</a><span>|</span><a href="#35782251">next</a><span>|</span><label class="collapse" for="c-35781592">[-]</label><label class="expand" for="c-35781592">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always seen interpretability and explainability as different sides of the same coin.<p>If you take an information-theoretic approach to it, and think of a DL model like any other model, there is a certain equivalence in understanding the model features and how it behaves with reference to the universe of data it is applied to.<p>It was an interesting article but I felt like it created problems that need not be there (or maybe it&#x27;s just describing problems that others created?)</div><br/></div></div><div id="35782251" class="c"><input type="checkbox" id="c-35782251" checked=""/><div class="controls bullet"><span class="by">RosanaAnaDana</span><span>|</span><a href="#35781592">prev</a><span>|</span><a href="#35781967">next</a><span>|</span><label class="collapse" for="c-35782251">[-]</label><label class="expand" for="c-35782251">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes I think the interpretation of model parameters is all bunk. I think the legacy of over interpretation of parameters and results has resulted in the occification of some very shaky science.</div><br/></div></div><div id="35781967" class="c"><input type="checkbox" id="c-35781967" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#35782251">prev</a><span>|</span><a href="#35782167">next</a><span>|</span><label class="collapse" for="c-35781967">[-]</label><label class="expand" for="c-35781967">[2 more]</label></div><br/><div class="children"><div class="content">&gt; They extract deeply hidden patterns in large data sets that our limited human brains can’t parse.<p>I think that the bulk of ML has so far produced what our brains in fact easily see. We can easily perform classification, or generation.</div><br/><div id="35781995" class="c"><input type="checkbox" id="c-35781995" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#35781967">parent</a><span>|</span><a href="#35782167">next</a><span>|</span><label class="collapse" for="c-35781995">[-]</label><label class="expand" for="c-35781995">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re definitely worse than me at spotting spam.</div><br/></div></div></div></div><div id="35782167" class="c"><input type="checkbox" id="c-35782167" checked=""/><div class="controls bullet"><span class="by">HyperSane</span><span>|</span><a href="#35781967">prev</a><span>|</span><a href="#35781660">next</a><span>|</span><label class="collapse" for="c-35782167">[-]</label><label class="expand" for="c-35782167">[10 more]</label></div><br/><div class="children"><div class="content">We have no idea how the human brain works but no one seems to care.</div><br/><div id="35782299" class="c"><input type="checkbox" id="c-35782299" checked=""/><div class="controls bullet"><span class="by">edgefield</span><span>|</span><a href="#35782167">parent</a><span>|</span><a href="#35782277">next</a><span>|</span><label class="collapse" for="c-35782299">[-]</label><label class="expand" for="c-35782299">[7 more]</label></div><br/><div class="children"><div class="content">I’m pretty sure the field of neuroscience is focused on understanding how the brain works.</div><br/><div id="35782723" class="c"><input type="checkbox" id="c-35782723" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782299">parent</a><span>|</span><a href="#35782326">next</a><span>|</span><label class="collapse" for="c-35782723">[-]</label><label class="expand" for="c-35782723">[1 more]</label></div><br/><div class="children"><div class="content">Depends about on what you mean here.  The good scientist mostly give up when trying to &#x27;describe&#x27; higher level function (at best maybe generate some analytical expression which may correlate with IQ or something) and focus instead on biochemistry at the single neuron level.  That&#x27;s a very different science than trying to &#x27;explain AI&#x27;.</div><br/></div></div><div id="35782326" class="c"><input type="checkbox" id="c-35782326" checked=""/><div class="controls bullet"><span class="by">HyperSane</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782299">parent</a><span>|</span><a href="#35782723">prev</a><span>|</span><a href="#35782277">next</a><span>|</span><label class="collapse" for="c-35782326">[-]</label><label class="expand" for="c-35782326">[5 more]</label></div><br/><div class="children"><div class="content">Have we made any fundamental progress in understanding how the human brain works in the last 20 years? 50?</div><br/><div id="35782381" class="c"><input type="checkbox" id="c-35782381" checked=""/><div class="controls bullet"><span class="by">Blammar</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782326">parent</a><span>|</span><a href="#35782493">next</a><span>|</span><label class="collapse" for="c-35782381">[-]</label><label class="expand" for="c-35782381">[1 more]</label></div><br/><div class="children"><div class="content">Issue there is the ability to trace out the brain networks accurately. We&#x27;re getting closer. Once we&#x27;re in the ~0.5 micron scanning range, things will get VERY interesting.<p>I believe we&#x27;ve already mapped some invertebrate brain, maybe 250 neurons.</div><br/></div></div><div id="35782493" class="c"><input type="checkbox" id="c-35782493" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782326">parent</a><span>|</span><a href="#35782381">prev</a><span>|</span><a href="#35782574">next</a><span>|</span><label class="collapse" for="c-35782493">[-]</label><label class="expand" for="c-35782493">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve made pretty good progress. But the same problem that plagues explainabilty of ANN&#x27;s rears its ugly head in neuroscience.<p>Low level descriptions provide shockingly little insight into high level behavior.</div><br/></div></div><div id="35782574" class="c"><input type="checkbox" id="c-35782574" checked=""/><div class="controls bullet"><span class="by">andrewflnr</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782326">parent</a><span>|</span><a href="#35782493">prev</a><span>|</span><a href="#35782277">next</a><span>|</span><label class="collapse" for="c-35782574">[-]</label><label class="expand" for="c-35782574">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but also your original question was about how much people care, so questioning the amount of progress is at best a weird tangent.</div><br/><div id="35782934" class="c"><input type="checkbox" id="c-35782934" checked=""/><div class="controls bullet"><span class="by">HyperSane</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782574">parent</a><span>|</span><a href="#35782277">next</a><span>|</span><label class="collapse" for="c-35782934">[-]</label><label class="expand" for="c-35782934">[1 more]</label></div><br/><div class="children"><div class="content">I am genuinely curious. It doesn&#x27;t seem like we have made the slightest progress in truly explaining how the human brain works or why we are sentient and self-aware.</div><br/></div></div></div></div></div></div></div></div><div id="35782277" class="c"><input type="checkbox" id="c-35782277" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35782167">parent</a><span>|</span><a href="#35782299">prev</a><span>|</span><a href="#35781660">next</a><span>|</span><label class="collapse" for="c-35782277">[-]</label><label class="expand" for="c-35782277">[2 more]</label></div><br/><div class="children"><div class="content">The only ingesting things now are the machines, that’s where all the money goes now.<p>We’re just “meat bags”.</div><br/><div id="35782712" class="c"><input type="checkbox" id="c-35782712" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#35782167">root</a><span>|</span><a href="#35782277">parent</a><span>|</span><a href="#35781660">next</a><span>|</span><label class="collapse" for="c-35782712">[-]</label><label class="expand" for="c-35782712">[1 more]</label></div><br/><div class="children"><div class="content">Can you please stop posting unsubstantive comments? This is below the quality line for an interesting HN thread.</div><br/></div></div></div></div></div></div><div id="35781660" class="c"><input type="checkbox" id="c-35781660" checked=""/><div class="controls bullet"><span class="by">ImprobableTruth</span><span>|</span><a href="#35782167">prev</a><span>|</span><label class="collapse" for="c-35781660">[-]</label><label class="expand" for="c-35781660">[3 more]</label></div><br/><div class="children"><div class="content">Not sure if it was intentional,but the title of this article is pretty hilarious considering she explicitly badmouths explainability (trying to peer inside a black box) and advocates for interpretability (building models that are less black-boxy in nature).<p>Also, man, quanta feels really rough and popsci-y when it comes to CS.</div><br/><div id="35782548" class="c"><input type="checkbox" id="c-35782548" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#35781660">parent</a><span>|</span><a href="#35782226">next</a><span>|</span><label class="collapse" for="c-35782548">[-]</label><label class="expand" for="c-35782548">[1 more]</label></div><br/><div class="children"><div class="content">I was amused by her example of poetry needing interpretability, given that we already know why and did when they did that research: <a href="https:&#x2F;&#x2F;gwern.net&#x2F;gpt-3#bpes" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;gpt-3#bpes</a></div><br/></div></div><div id="35782226" class="c"><input type="checkbox" id="c-35782226" checked=""/><div class="controls bullet"><span class="by">OnlineGladiator</span><span>|</span><a href="#35781660">parent</a><span>|</span><a href="#35782548">prev</a><span>|</span><label class="collapse" for="c-35782226">[-]</label><label class="expand" for="c-35782226">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also, man, quanta feels really rough and popsci-y when it comes to CS.<p>I&#x27;m guessing this is just the Gell-Mann Amnesia effect.  Why do you think the quality is better for other fields?</div><br/></div></div></div></div></div></div></div></div></div></body></html>