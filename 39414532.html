<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1708246851043" as="style"/><link rel="stylesheet" href="styles.css?v=1708246851043"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://vgel.me/posts/representation-engineering/">Representation Engineering: Mistral-7B on Acid</a> <span class="domain">(<a href="https://vgel.me">vgel.me</a>)</span></div><div class="subtext"><span>alexmolas</span> | <span>31 comments</span></div><br/><div><div id="39416844" class="c"><input type="checkbox" id="c-39416844" checked=""/><div class="controls bullet"><span class="by">vood</span><span>|</span><a href="#39417240">next</a><span>|</span><label class="collapse" for="c-39416844">[-]</label><label class="expand" for="c-39416844">[2 more]</label></div><br/><div class="children"><div class="content">This is very well written and entertaining post. I enjoyed reading it.<p>Selfishly, would you mind sharing literature or blog posts that led you to this level of understanding of LLMs? I&#x27;m trying hard to understand the inner workings via experiments but definitely far behind your expertise.<p>Thanks</div><br/></div></div><div id="39417240" class="c"><input type="checkbox" id="c-39417240" checked=""/><div class="controls bullet"><span class="by">benob</span><span>|</span><a href="#39416844">prev</a><span>|</span><a href="#39416432">next</a><span>|</span><label class="collapse" for="c-39417240">[-]</label><label class="expand" for="c-39417240">[2 more]</label></div><br/><div class="children"><div class="content">This reminds me of bias tuning, a LoRA competitor. One can get decent adapters by only finetuning a vector added to each linear layer activations. I think I saw it first while reading [1] but there are other instances.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.15010.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2304.15010.pdf</a></div><br/><div id="39417344" class="c"><input type="checkbox" id="c-39417344" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#39417240">parent</a><span>|</span><a href="#39416432">next</a><span>|</span><label class="collapse" for="c-39417344">[-]</label><label class="expand" for="c-39417344">[1 more]</label></div><br/><div class="children"><div class="content">Please try to share abstract links instead of pdf links, for mobile or low connection readers.</div><br/></div></div></div></div><div id="39416432" class="c"><input type="checkbox" id="c-39416432" checked=""/><div class="controls bullet"><span class="by">cobbal</span><span>|</span><a href="#39417240">prev</a><span>|</span><a href="#39415991">next</a><span>|</span><label class="collapse" for="c-39416432">[-]</label><label class="expand" for="c-39416432">[1 more]</label></div><br/><div class="children"><div class="content">This article was very fun, and felt like a good counterpoint to the &quot;You Sound Like a Bot&quot; post recently that was talking about how AI is getting bland.<p>On a less serious note. This sentence should be something a fiction writer knows will only end in trouble for humanity:<p>&gt; I especially challenge someone to find a &quot;self-awareness&quot; vector that isn&#x27;t contaminated by ... human emotion!</div><br/></div></div><div id="39415991" class="c"><input type="checkbox" id="c-39415991" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39416432">prev</a><span>|</span><a href="#39417162">next</a><span>|</span><label class="collapse" for="c-39415991">[-]</label><label class="expand" for="c-39415991">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;d never seen an LLM summarized like this before, and I really like it:<p><pre><code>    hidden_state = self.embeddings(input_tokens)

    for layer in self.layers:
        hidden_state = layer(hidden_state)

    return transform_into_logits(hidden_state)</code></pre></div><br/><div id="39416055" class="c"><input type="checkbox" id="c-39416055" checked=""/><div class="controls bullet"><span class="by">rakejake</span><span>|</span><a href="#39415991">parent</a><span>|</span><a href="#39416057">next</a><span>|</span><label class="collapse" for="c-39416055">[-]</label><label class="expand" for="c-39416055">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow. Isn&#x27;t this the flow for practically every neutral network i.e you index the sampled inputs from the embedding Matrix, forward this through every hidden layer and then finally transform to the dimensions of your tokens so that it can be interpreted as log-counts?</div><br/><div id="39416077" class="c"><input type="checkbox" id="c-39416077" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39415991">root</a><span>|</span><a href="#39416055">parent</a><span>|</span><a href="#39416057">next</a><span>|</span><label class="collapse" for="c-39416077">[-]</label><label class="expand" for="c-39416077">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but I&#x27;ve never seen it expressed so clearly as pseudocode before.</div><br/><div id="39417354" class="c"><input type="checkbox" id="c-39417354" checked=""/><div class="controls bullet"><span class="by">elcomet</span><span>|</span><a href="#39415991">root</a><span>|</span><a href="#39416077">parent</a><span>|</span><a href="#39416057">next</a><span>|</span><label class="collapse" for="c-39417354">[-]</label><label class="expand" for="c-39417354">[1 more]</label></div><br/><div class="children"><div class="content">This is not specific to llms. So not really informative of how llms work. It also works for CNNs, LSTM, MLPs, or even any data processing program..</div><br/></div></div></div></div></div></div><div id="39416057" class="c"><input type="checkbox" id="c-39416057" checked=""/><div class="controls bullet"><span class="by">alexmolas</span><span>|</span><a href="#39415991">parent</a><span>|</span><a href="#39416055">prev</a><span>|</span><a href="#39417162">next</a><span>|</span><label class="collapse" for="c-39416057">[-]</label><label class="expand" for="c-39416057">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this the typical representation we used back then when working with LSTMs?</div><br/></div></div></div></div><div id="39417162" class="c"><input type="checkbox" id="c-39417162" checked=""/><div class="controls bullet"><span class="by">WiSaGaN</span><span>|</span><a href="#39415991">prev</a><span>|</span><a href="#39416916">next</a><span>|</span><label class="collapse" for="c-39417162">[-]</label><label class="expand" for="c-39417162">[1 more]</label></div><br/><div class="children"><div class="content">Great article. It was a joy to read. I have one question though: Why do we integrate the control vector across all layers of a neural network, rather than limiting its application to just the final layer or a subset of layers? Given that each vector influences every layer it passes through, resulting in a cumulative effect, isn&#x27;t there a risk of excessively skewing the data representation?</div><br/></div></div><div id="39416916" class="c"><input type="checkbox" id="c-39416916" checked=""/><div class="controls bullet"><span class="by">binsquare</span><span>|</span><a href="#39417162">prev</a><span>|</span><a href="#39416561">next</a><span>|</span><label class="collapse" for="c-39416916">[-]</label><label class="expand" for="c-39416916">[1 more]</label></div><br/><div class="children"><div class="content">Very hopeful to see the future of accessing models with the ability to inject vectors by layer instead of just a straight prompt + existing parameters</div><br/></div></div><div id="39416561" class="c"><input type="checkbox" id="c-39416561" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#39416916">prev</a><span>|</span><a href="#39415184">next</a><span>|</span><label class="collapse" for="c-39416561">[-]</label><label class="expand" for="c-39416561">[2 more]</label></div><br/><div class="children"><div class="content">What a fantastic article, well done!<p>&gt; When used with the prompt below, the honesty vector doesn&#x27;t change the model&#x27;s behavior—instead, it changes the model&#x27;s judgment of someone else&#x27;s behavior! This is the same honesty vector as before—generated by asking the model to act honest or untruthful! [...] How do you explain this?<p>Isn&#x27;t the control vector just pushing text generation towards the concept of honesty&#x2F;dishonesty? An LLM is &#x27;just&#x27; a text generator, so you get added honesty&#x2F;dishonesty irrespective of where in the bot&#x2F;human conversation text generation is occuring?</div><br/><div id="39416595" class="c"><input type="checkbox" id="c-39416595" checked=""/><div class="controls bullet"><span class="by">loa_in_</span><span>|</span><a href="#39416561">parent</a><span>|</span><a href="#39415184">next</a><span>|</span><label class="collapse" for="c-39416595">[-]</label><label class="expand" for="c-39416595">[1 more]</label></div><br/><div class="children"><div class="content">I agree. More sophisticated model might have two or more to follow narrating different characters... Which kind of brings a concept of character slots into the dimension space</div><br/></div></div></div></div><div id="39415184" class="c"><input type="checkbox" id="c-39415184" checked=""/><div class="controls bullet"><span class="by">batch12</span><span>|</span><a href="#39416561">prev</a><span>|</span><a href="#39415557">next</a><span>|</span><label class="collapse" for="c-39415184">[-]</label><label class="expand" for="c-39415184">[5 more]</label></div><br/><div class="children"><div class="content">Interesting, seems like control vectors could reduce the need to fine-tune a model.</div><br/><div id="39415554" class="c"><input type="checkbox" id="c-39415554" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#39415184">parent</a><span>|</span><a href="#39415557">next</a><span>|</span><label class="collapse" for="c-39415554">[-]</label><label class="expand" for="c-39415554">[4 more]</label></div><br/><div class="children"><div class="content">Not only that, you can change the behavior of the model as needed.
With 5 finetunes you need to host 5 copies or load and unload them.<p>With control vectors you can modify the model as needed</div><br/><div id="39416087" class="c"><input type="checkbox" id="c-39416087" checked=""/><div class="controls bullet"><span class="by">yberreby</span><span>|</span><a href="#39415184">root</a><span>|</span><a href="#39415554">parent</a><span>|</span><a href="#39415752">next</a><span>|</span><label class="collapse" for="c-39416087">[-]</label><label class="expand" for="c-39416087">[2 more]</label></div><br/><div class="children"><div class="content">&gt; With 5 finetunes you need to host 5 copies or load and unload them.<p>If you use LoRA, which many do when fine-tuning nowadays, you don&#x27;t need five full copies. You only need to store adapters, which can be in the tens of MBs range for a given finetune.</div><br/><div id="39416150" class="c"><input type="checkbox" id="c-39416150" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39415184">root</a><span>|</span><a href="#39416087">parent</a><span>|</span><a href="#39415752">next</a><span>|</span><label class="collapse" for="c-39416150">[-]</label><label class="expand" for="c-39416150">[1 more]</label></div><br/><div class="children"><div class="content">You can also batch requests using different LoRAs. See &quot;S-LoRA: Serving Thousands of Concurrent LoRA Adapters&quot;. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03285" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03285</a></div><br/></div></div></div></div><div id="39415752" class="c"><input type="checkbox" id="c-39415752" checked=""/><div class="controls bullet"><span class="by">batch12</span><span>|</span><a href="#39415184">root</a><span>|</span><a href="#39415554">parent</a><span>|</span><a href="#39416087">prev</a><span>|</span><a href="#39415557">next</a><span>|</span><label class="collapse" for="c-39415752">[-]</label><label class="expand" for="c-39415752">[1 more]</label></div><br/><div class="children"><div class="content">I think you could layer them too</div><br/></div></div></div></div></div></div><div id="39415557" class="c"><input type="checkbox" id="c-39415557" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#39415184">prev</a><span>|</span><a href="#39415874">next</a><span>|</span><label class="collapse" for="c-39415557">[-]</label><label class="expand" for="c-39415557">[1 more]</label></div><br/><div class="children"><div class="content">Nice! The anti-jailbreaking angle is extremely interesting for those of us working on commercial applications.</div><br/></div></div><div id="39415874" class="c"><input type="checkbox" id="c-39415874" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#39415557">prev</a><span>|</span><a href="#39415732">next</a><span>|</span><label class="collapse" for="c-39415874">[-]</label><label class="expand" for="c-39415874">[1 more]</label></div><br/><div class="children"><div class="content">Nice, so, can I get a visual way to browse for potentially powerful control vectors :)</div><br/></div></div><div id="39416782" class="c"><input type="checkbox" id="c-39416782" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#39415732">prev</a><span>|</span><a href="#39415346">next</a><span>|</span><label class="collapse" for="c-39416782">[-]</label><label class="expand" for="c-39416782">[1 more]</label></div><br/><div class="children"><div class="content">Not to be confused with the other story from a month ago about giving LLMs &quot;DRµGS&quot;.</div><br/></div></div><div id="39415346" class="c"><input type="checkbox" id="c-39415346" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#39416782">prev</a><span>|</span><a href="#39416786">next</a><span>|</span><label class="collapse" for="c-39415346">[-]</label><label class="expand" for="c-39415346">[4 more]</label></div><br/><div class="children"><div class="content">Very interesting! Can you see those helping for RAG scenarios? Specifically:<p>- decreasing models tendency to answer with ungrounded answers<p>- increase models ability to respond with the correct syntax for citations- the open models like llama2 dont seem to obey my prompt’s syntax instructions.</div><br/><div id="39415769" class="c"><input type="checkbox" id="c-39415769" checked=""/><div class="controls bullet"><span class="by">batch12</span><span>|</span><a href="#39415346">parent</a><span>|</span><a href="#39415958">next</a><span>|</span><label class="collapse" for="c-39415769">[-]</label><label class="expand" for="c-39415769">[1 more]</label></div><br/><div class="children"><div class="content">For the second item, I&#x27;ve had luck using grammar to overcome this issue. The easiest one to implement I&#x27;ve seen so far is Microsoft&#x27;s guidance-ai.</div><br/></div></div><div id="39415958" class="c"><input type="checkbox" id="c-39415958" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39415346">parent</a><span>|</span><a href="#39415769">prev</a><span>|</span><a href="#39416786">next</a><span>|</span><label class="collapse" for="c-39415958">[-]</label><label class="expand" for="c-39415958">[2 more]</label></div><br/><div class="children"><div class="content">You can use outlines <a href="https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines">https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines</a> to let models generate with correct syntax.</div><br/><div id="39416233" class="c"><input type="checkbox" id="c-39416233" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#39415346">root</a><span>|</span><a href="#39415958">parent</a><span>|</span><a href="#39416786">next</a><span>|</span><label class="collapse" for="c-39416233">[-]</label><label class="expand" for="c-39416233">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I havent had to use a syntax-enforcing framework with gpt-35, I’ll try outlines and guidance out to see if they help enforce syntax for the locally runnable models.</div><br/></div></div></div></div></div></div><div id="39416786" class="c"><input type="checkbox" id="c-39416786" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#39415346">prev</a><span>|</span><a href="#39416571">next</a><span>|</span><label class="collapse" for="c-39416786">[-]</label><label class="expand" for="c-39416786">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of the Westworld series in which they use these ipad like devices with sliders to change behavior of AIs. Little bit more humor, little bit more aggressive. Nice to these control options and its quick as well.</div><br/><div id="39416930" class="c"><input type="checkbox" id="c-39416930" checked=""/><div class="controls bullet"><span class="by">hskalin</span><span>|</span><a href="#39416786">parent</a><span>|</span><a href="#39416571">next</a><span>|</span><label class="collapse" for="c-39416930">[-]</label><label class="expand" for="c-39416930">[1 more]</label></div><br/><div class="children"><div class="content">Playing with LLMs like this always makes me feel like one of those Westworld engineers. Especially when I ask LLMs to roleplay. It also kind of freaks me out sometimes</div><br/></div></div></div></div><div id="39416571" class="c"><input type="checkbox" id="c-39416571" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#39416786">prev</a><span>|</span><label class="collapse" for="c-39416571">[-]</label><label class="expand" for="c-39416571">[1 more]</label></div><br/><div class="children"><div class="content">Awhile ago, I wrote a snarky complaint about the fact that work like this didn&#x27;t exist for far too long. <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a></div><br/></div></div></div></div></div></div></div></body></html>