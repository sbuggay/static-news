<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696496489533" as="style"/><link rel="stylesheet" href="styles.css?v=1696496489533"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ig.ft.com/generative-ai/">How Transformers Work</a> <span class="domain">(<a href="https://ig.ft.com">ig.ft.com</a>)</span></div><div class="subtext"><span>sblank</span> | <span>25 comments</span></div><br/><div><div id="37776247" class="c"><input type="checkbox" id="c-37776247" checked=""/><div class="controls bullet"><span class="by">nurettin</span><span>|</span><a href="#37775659">next</a><span>|</span><label class="collapse" for="c-37776247">[-]</label><label class="expand" for="c-37776247">[2 more]</label></div><br/><div class="children"><div class="content">This is more like &quot;what transformers do&quot; rather than &quot;how they work&quot;.</div><br/><div id="37776350" class="c"><input type="checkbox" id="c-37776350" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37776247">parent</a><span>|</span><a href="#37775659">next</a><span>|</span><label class="collapse" for="c-37776350">[-]</label><label class="expand" for="c-37776350">[1 more]</label></div><br/><div class="children"><div class="content">Yes well nobody really knows the &quot;how they work&quot; part.</div><br/></div></div></div></div><div id="37775659" class="c"><input type="checkbox" id="c-37775659" checked=""/><div class="controls bullet"><span class="by">KirillPanov</span><span>|</span><a href="#37776247">prev</a><span>|</span><a href="#37775126">next</a><span>|</span><label class="collapse" for="c-37775659">[-]</label><label class="expand" for="c-37775659">[6 more]</label></div><br/><div class="children"><div class="content">Honestly this single image does a better job of explaining transformers than <i>anything</i> else I&#x27;ve ever seen:<p><a href="https:&#x2F;&#x2F;people.idsia.ch&#x2F;~juergen&#x2F;fastweights754x288.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;people.idsia.ch&#x2F;~juergen&#x2F;fastweights754x288.png</a><p>It&#x27;s pretty damn simple: a linearized transformer is a &quot;slow&quot; neural net whose outputs determine the weights of another (&quot;fast&quot;) neural net.<p>It&#x27;s a NN that can use tools, subject to one major restriction: the tool must be another NN.  The restriction is is &quot;the trick&quot; that lets you backpropagate gradients through <i>both</i> NNs so you can train the slow NN based on an error function of the fast NN&#x27;s outputs.<p>The only difference between linearized transformers and the kind that OpenAI uses is adding a softmax operation in one place.</div><br/><div id="37775867" class="c"><input type="checkbox" id="c-37775867" checked=""/><div class="controls bullet"><span class="by">raylad</span><span>|</span><a href="#37775659">parent</a><span>|</span><a href="#37775126">next</a><span>|</span><label class="collapse" for="c-37775867">[-]</label><label class="expand" for="c-37775867">[5 more]</label></div><br/><div class="children"><div class="content">It seems to explain them well because you already know how they work.<p>To most people, that diagram would not explain very much.</div><br/><div id="37775909" class="c"><input type="checkbox" id="c-37775909" checked=""/><div class="controls bullet"><span class="by">KirillPanov</span><span>|</span><a href="#37775659">root</a><span>|</span><a href="#37775867">parent</a><span>|</span><a href="#37775126">next</a><span>|</span><label class="collapse" for="c-37775909">[-]</label><label class="expand" for="c-37775909">[4 more]</label></div><br/><div class="children"><div class="content">No, I didn&#x27;t!<p>Before I came across that image (and the paper <i>Linearized Transformers are Secretly Fast Weight Programmers</i>) I very much <i>did not</i> understand transformers.  I had spent at least 20 hours with <i>Attention Is All You Need</i>, and probably another dozen hours with <a href="https:&#x2F;&#x2F;e2eml.school&#x2F;transformers.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;e2eml.school&#x2F;transformers.html</a> and wasn&#x27;t getting much of anywhere.  I have no ML&#x2F;AI background; just a typical undergraduate-CS-level familiarity with neural networks -- the basic stuff that hasn&#x27;t changed since the 1990s.  I do have some experience with linear algebra, but that isn&#x27;t the hard part of any of this.<p>Frankly, most people who publish in this field go out of their way to obfuscate the key insights.  Mediocre physicists (but not the truly brilliant ones) do the same thing.  It&#x27;s very annoying.</div><br/><div id="37776085" class="c"><input type="checkbox" id="c-37776085" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#37775659">root</a><span>|</span><a href="#37775909">parent</a><span>|</span><a href="#37776090">next</a><span>|</span><label class="collapse" for="c-37776085">[-]</label><label class="expand" for="c-37776085">[1 more]</label></div><br/><div class="children"><div class="content">Different things click for different people; for me, the image you linked in the initial post of this thread… it makes as little sense <i>to me</i> as the Time Cube image or the widely mocked Pepsi rebrand document from 2008.<p>&quot;That it&#x27;s objectively true&quot; isn&#x27;t enough of an advantage, sadly.<p>&gt; Frankly, most people who publish in this field go out of their way to obfuscate the key insights. Mediocre physicists (but not the truly brilliant ones) do the same thing. It&#x27;s very annoying.<p>I certainly sympathise, but I think the overlap here is &quot;maths is hard to communicate&quot;, and the conflict between rigorously explaining vs. keeping everything simple enough to follow.<p>Grant Sanderson did a talk on this, though I appreciate an hour may be a bit too long: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;UOuxo6SA8Uc?si=e4dojDIXACdI44sb" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;UOuxo6SA8Uc?si=e4dojDIXACdI44sb</a></div><br/></div></div><div id="37776090" class="c"><input type="checkbox" id="c-37776090" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#37775659">root</a><span>|</span><a href="#37775909">parent</a><span>|</span><a href="#37776085">prev</a><span>|</span><a href="#37775126">next</a><span>|</span><label class="collapse" for="c-37776090">[-]</label><label class="expand" for="c-37776090">[2 more]</label></div><br/><div class="children"><div class="content">So the diagram makes sense after you spent at least 32 hours studying LLMs.</div><br/><div id="37776110" class="c"><input type="checkbox" id="c-37776110" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775659">root</a><span>|</span><a href="#37776090">parent</a><span>|</span><a href="#37775126">next</a><span>|</span><label class="collapse" for="c-37776110">[-]</label><label class="expand" for="c-37776110">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s this sentence about monoids...</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37775126" class="c"><input type="checkbox" id="c-37775126" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775659">prev</a><span>|</span><label class="collapse" for="c-37775126">[-]</label><label class="expand" for="c-37775126">[16 more]</label></div><br/><div class="children"><div class="content">Very well done animation. In no way does that change my mind about the eggregious continuing use of words like &quot;learn&quot; for what happens inside the software system.<p>To me, the english-language choices in how LLM and Transformers are described as working is part and parcel of the &quot;sell&quot; of steps-to-AGI because of normalising use of words which imply intentionality, introspection (as humans know it), thinking, cognition.<p>So I both love and loathe this animation. It re-inforces people&#x27;s naieve tendency to &quot;its alive&quot; when in fact, its doing a really good job of explaining how weighted sums and statistics inform the parse, and the consequences of modulation of the products of the parse.<p>The programmers were very smart people. They&#x27;ve done well and have codified a huge body of knowledge which helps systems with producing outputs from input strings, in line with our requirements. Sounds exciting doesn&#x27;t it!</div><br/><div id="37775703" class="c"><input type="checkbox" id="c-37775703" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#37775126">parent</a><span>|</span><a href="#37776106">next</a><span>|</span><label class="collapse" for="c-37775703">[-]</label><label class="expand" for="c-37775703">[11 more]</label></div><br/><div class="children"><div class="content">&gt; and have codified a huge body of knowledge which helps systems with producing outputs from input strings<p>That is specifically what the programmers have <i>not</i> done.</div><br/><div id="37775889" class="c"><input type="checkbox" id="c-37775889" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37775703">parent</a><span>|</span><a href="#37775775">next</a><span>|</span><label class="collapse" for="c-37775889">[-]</label><label class="expand" for="c-37775889">[7 more]</label></div><br/><div class="children"><div class="content">You&#x27;re saying that the billions of lines of text have not informed a statistical model of relationships of words in grammer? You think the &quot;body of knowledge&quot; has to be in the meaning of the words? No. It&#x27;s in the weighted maths of the likelihood of each element of the phrase. That statistical model is knowledge.  Some semantic intent is there too, to disambiguate who flies like an arrow or flies like a banana.</div><br/><div id="37775993" class="c"><input type="checkbox" id="c-37775993" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37775889">parent</a><span>|</span><a href="#37775775">next</a><span>|</span><label class="collapse" for="c-37775993">[-]</label><label class="expand" for="c-37775993">[6 more]</label></div><br/><div class="children"><div class="content">&gt; That statistical model is knowledge<p>Glad you agree.<p>None of the free parameters of these statistical models were selected by programmers.</div><br/><div id="37776060" class="c"><input type="checkbox" id="c-37776060" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37775993">parent</a><span>|</span><a href="#37775775">next</a><span>|</span><label class="collapse" for="c-37776060">[-]</label><label class="expand" for="c-37776060">[5 more]</label></div><br/><div class="children"><div class="content">Hmm. Is adjusting weights metaprogramming? It&#x27;s based on observations of the outputs of the system. It&#x27;s certainly not directed programming intent in the normal sense.<p>It shares one property of traditional programming: garbage in garbage out.<p>The fruits of their labour indirectly is a model. It&#x27;s proving valuable. You want to ascribe the value to .. the bit nobody claims to understand? I&#x27;m sticking to the labour theory of value here, either the robot makers imbued value, or we have to redefine the value.</div><br/><div id="37776153" class="c"><input type="checkbox" id="c-37776153" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37776060">parent</a><span>|</span><a href="#37776126">next</a><span>|</span><label class="collapse" for="c-37776153">[-]</label><label class="expand" for="c-37776153">[3 more]</label></div><br/><div class="children"><div class="content">Humans are not adjusting the weights. That is done automatically by the algorithm. The humans only provide training input.</div><br/><div id="37776217" class="c"><input type="checkbox" id="c-37776217" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37776153">parent</a><span>|</span><a href="#37776126">next</a><span>|</span><label class="collapse" for="c-37776217">[-]</label><label class="expand" for="c-37776217">[2 more]</label></div><br/><div class="children"><div class="content">So when people told the staff the models were racist you think they just &quot;asked it to be less racist&quot; I think they did a little more than just adjust the input data, they also modulated the logic behind the Weighting methods. No?</div><br/><div id="37776323" class="c"><input type="checkbox" id="c-37776323" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37776217">parent</a><span>|</span><a href="#37776126">next</a><span>|</span><label class="collapse" for="c-37776323">[-]</label><label class="expand" for="c-37776323">[1 more]</label></div><br/><div class="children"><div class="content">They just change the input data of the LLM and&#x2F;or the reward model.<p>Or better they shift the distribution by finetuning the model on curated data.<p>But no, no one change the parameters manually, it&#x27;s just too complex, an artificial brain.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37775775" class="c"><input type="checkbox" id="c-37775775" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37775703">parent</a><span>|</span><a href="#37775889">prev</a><span>|</span><a href="#37776106">next</a><span>|</span><label class="collapse" for="c-37775775">[-]</label><label class="expand" for="c-37775775">[3 more]</label></div><br/><div class="children"><div class="content">Yeah. Maybe it&#x27;s because we use the word &quot;train&quot; and that conjures up a mental model of some artisan distilling his process to an an apprentice ?</div><br/><div id="37775893" class="c"><input type="checkbox" id="c-37775893" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37775775">parent</a><span>|</span><a href="#37776106">next</a><span>|</span><label class="collapse" for="c-37775893">[-]</label><label class="expand" for="c-37775893">[2 more]</label></div><br/><div class="children"><div class="content">This artisan hasn&#x27;t been trained in how to make steel. It&#x27;s been trained in how to talk about steel making plausibly.</div><br/><div id="37775943" class="c"><input type="checkbox" id="c-37775943" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37775893">parent</a><span>|</span><a href="#37776106">next</a><span>|</span><label class="collapse" for="c-37775943">[-]</label><label class="expand" for="c-37775943">[1 more]</label></div><br/><div class="children"><div class="content">My man you&#x27;re completely off base here. No one is programming anything into neural networks and we have no idea how models like GPT-4 work at a high level.<p>The whole point of neural networks is that we don&#x27;t know shit and we don&#x27;t know shit about how to distill the mechanics of some very real, very important processes so we figured out a way to make the machine figure this out on its own.</div><br/></div></div></div></div></div></div></div></div><div id="37776106" class="c"><input type="checkbox" id="c-37776106" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#37775126">parent</a><span>|</span><a href="#37775703">prev</a><span>|</span><label class="collapse" for="c-37776106">[-]</label><label class="expand" for="c-37776106">[4 more]</label></div><br/><div class="children"><div class="content">So the term &quot;Machine Learning&quot; is egregious misuse of language? These models are not explicitly programmed. They actually learn a model from data automatically. The term is technically correct.</div><br/><div id="37776136" class="c"><input type="checkbox" id="c-37776136" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37776106">parent</a><span>|</span><label class="collapse" for="c-37776136">[-]</label><label class="expand" for="c-37776136">[3 more]</label></div><br/><div class="children"><div class="content">The &quot;learning&quot; part has always caused me grief. Always.<p>Unfortunately it&#x27;s beyond recovery. But &quot;hallucination&quot; and other new coined terms.. they definitely have problems.</div><br/><div id="37776159" class="c"><input type="checkbox" id="c-37776159" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37776136">parent</a><span>|</span><label class="collapse" for="c-37776159">[-]</label><label class="expand" for="c-37776159">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand how that would cause grief. Seems totally irrational to me.</div><br/><div id="37776199" class="c"><input type="checkbox" id="c-37776199" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37775126">root</a><span>|</span><a href="#37776159">parent</a><span>|</span><label class="collapse" for="c-37776199">[-]</label><label class="expand" for="c-37776199">[1 more]</label></div><br/><div class="children"><div class="content">These words are applied to living people. Applied to machines they create states of belief outside the cognoscenti what is going on.<p>It&#x27;s as if we called programming &quot;magic&quot; and programmers &quot;wizards&quot; and then kids ask &quot;is Harry potter real&quot; and somebody says &quot;yes&quot;<p>It&#x27;s not hallucinating. It&#x27;s an analogy. But analogies are not definitional. We&#x27;re going to wind up in conversations about AGI and use of imprecise analogistic language will alter how people  discuss this problem.<p>Sorry you think I&#x27;m irratinal. I think you&#x27;re being obtuse and not considering real world consequences of the ontologies used outside of the field.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>