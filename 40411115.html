<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1716195673996" as="style"/><link rel="stylesheet" href="styles.css?v=1716195673996"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy">Reflections on our Responsible Scaling Policy</a> <span class="domain">(<a href="https://www.anthropic.com">www.anthropic.com</a>)</span></div><div class="subtext"><span>Josely</span> | <span>100 comments</span></div><br/><div><div id="40411686" class="c"><input type="checkbox" id="c-40411686" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#40412752">next</a><span>|</span><label class="collapse" for="c-40411686">[-]</label><label class="expand" for="c-40411686">[30 more]</label></div><br/><div class="children"><div class="content">I really wish when organizations released these kinds of statements that they would provide some clarifying examples, otherwise things can feel very nebulous. For example, their first bullet point was:<p>&gt; Establishing Red Line Capabilities. We commit to identifying and publishing &quot;Red Line Capabilities&quot; which might emerge in future generations of models and would present too much risk if stored or deployed under our current safety and security practices (referred to as the ASL-2 Standard).<p>What types of things are they thinking about that would be &quot;red line capabilities&quot; here? Is it purely just &quot;knowledge stuff that shouldn&#x27;t be that easy to find&quot;, e.g. &quot;simple meth recipes&quot; or &quot;make a really big bomb&quot;, or is it something deeper? For example, I&#x27;ve already seen AI demos where, with just a couple short audio samples, speech generation can pretty convincingly sound like the person who recorded the samples. Obviously there is huge potential for misuse of that, but given the knowledge is already &quot;out there&quot;, is this something that would be considered a red line capability?</div><br/><div id="40412336" class="c"><input type="checkbox" id="c-40412336" checked=""/><div class="controls bullet"><span class="by">jasondclinton</span><span>|</span><a href="#40411686">parent</a><span>|</span><a href="#40411810">next</a><span>|</span><label class="collapse" for="c-40412336">[-]</label><label class="expand" for="c-40412336">[7 more]</label></div><br/><div class="children"><div class="content">Hi, I&#x27;m the CISO from Anthropic. Thank you for the criticism, any feedback is a gift.<p>We have laid out in our RSP what we consider the next milestone of significant harms that we&#x27;re are testing for (what we call ASL-3): <a href="https:&#x2F;&#x2F;anthropic.com&#x2F;responsible-scaling-policy" rel="nofollow">https:&#x2F;&#x2F;anthropic.com&#x2F;responsible-scaling-policy</a> (PDF); this includes bioweapons assessment and cybersecurity.<p>As someone thinking night and day about security, I think the next major area of concern is going to be offensive (and defensive!) exploitation. It seems to me that within 6-18 months, LLMs will be able to iteratively walk through most open source code and identify vulnerabilities. It will be computationally expensive, though: that level of reasoning requires a large amount of scratch space and attention heads. But it seems very likely, based on everything that I&#x27;m seeing. Maybe 85% odds.<p>There&#x27;s already the first sparks of this happening published publicly here: <a href="https:&#x2F;&#x2F;security.googleblog.com&#x2F;2023&#x2F;08&#x2F;ai-powered-fuzzing-breaking-bug-hunting.html" rel="nofollow">https:&#x2F;&#x2F;security.googleblog.com&#x2F;2023&#x2F;08&#x2F;ai-powered-fuzzing-b...</a> just using traditional LLM-augmented fuzzers. (They&#x27;ve since published an update on this work in December.) I know of a few other groups doing significant amounts of investment in this specific area, to try to run faster on the defensive side than any malign nation state might be.<p>Please check out the RSP, we are very explicit about what harms we consider ASL-3. Drug making and &quot;stuff on the internet&quot; is not at all in our threat model. ASL-3 seems somewhat likely within the next 6-9 months. Maybe 50% odds, by my guess.</div><br/><div id="40412684" class="c"><input type="checkbox" id="c-40412684" checked=""/><div class="controls bullet"><span class="by">GistNoesis</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412336">parent</a><span>|</span><a href="#40412878">next</a><span>|</span><label class="collapse" for="c-40412684">[-]</label><label class="expand" for="c-40412684">[4 more]</label></div><br/><div class="children"><div class="content">There is a scene I like in an OppenHeimer movie <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=p0pCclxx5nI" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=p0pCclxx5nI</a> (Edit: It&#x27;s not a deleted scene from Nolan&#x27;s OppenHeimer) .<p>Their is also an other scene in Nolan&#x27;s OppenHeimer (who made the cut around timestamp 27:45) where physicists get all excited when a paper is published where Hahn and Strassmann split uranium with neutrons. Alvarez the experimentalist replicate it happily, while being oblivious to the fact that seems obvious to every theoretical physicist : It can be used to create a chain reaction and therefore a bomb.<p>So here is my question : how do you contain the sparks of employees ? Let&#x27;s say Alvarez comes all excited in your open-space, and speak a few words &quot;new algorithm&quot;, &quot;1000X&quot;, what do you do ?</div><br/><div id="40412744" class="c"><input type="checkbox" id="c-40412744" checked=""/><div class="controls bullet"><span class="by">jasondclinton</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412684">parent</a><span>|</span><a href="#40412878">next</a><span>|</span><label class="collapse" for="c-40412744">[-]</label><label class="expand" for="c-40412744">[3 more]</label></div><br/><div class="children"><div class="content">This is called a “compute multiplier” and, yes, we have a protocol for that. All AI labs do, as far as I am aware; standard industry practice.</div><br/><div id="40413027" class="c"><input type="checkbox" id="c-40413027" checked=""/><div class="controls bullet"><span class="by">GistNoesis</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412744">parent</a><span>|</span><a href="#40412878">next</a><span>|</span><label class="collapse" for="c-40413027">[-]</label><label class="expand" for="c-40413027">[2 more]</label></div><br/><div class="children"><div class="content">Glad there is a protocol, can you be more explicit (since it exist and seems to be standard) ?</div><br/></div></div></div></div></div></div><div id="40412878" class="c"><input type="checkbox" id="c-40412878" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412336">parent</a><span>|</span><a href="#40412684">prev</a><span>|</span><a href="#40412457">next</a><span>|</span><label class="collapse" for="c-40412878">[-]</label><label class="expand" for="c-40412878">[1 more]</label></div><br/><div class="children"><div class="content">Thanks very much, the PDF you linked is very helpful, particularly in how it describes the classes of &quot;deployment risks&quot; vs &quot;containment risks&quot;.</div><br/></div></div><div id="40412457" class="c"><input type="checkbox" id="c-40412457" checked=""/><div class="controls bullet"><span class="by">throwup238</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412336">parent</a><span>|</span><a href="#40412878">prev</a><span>|</span><a href="#40411810">next</a><span>|</span><label class="collapse" for="c-40412457">[-]</label><label class="expand" for="c-40412457">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt; We have laid out in our RSP what we consider the next milestone of significant harms that we&#x27;re are testing for (what we call ASL-3): <a href="https:&#x2F;&#x2F;anthropic.com&#x2F;responsible-scaling-policy" rel="nofollow">https:&#x2F;&#x2F;anthropic.com&#x2F;responsible-scaling-policy</a> (PDF); this includes bioweapons assessment and cybersecurity.</i><p>Do pumped flux compression generators count?<p>(Asking for a friend who is totally not planning on world conquest)</div><br/></div></div></div></div><div id="40411810" class="c"><input type="checkbox" id="c-40411810" checked=""/><div class="controls bullet"><span class="by">subroutine</span><span>|</span><a href="#40411686">parent</a><span>|</span><a href="#40412336">prev</a><span>|</span><a href="#40411719">next</a><span>|</span><label class="collapse" for="c-40411810">[-]</label><label class="expand" for="c-40411810">[3 more]</label></div><br/><div class="children"><div class="content">Anthropic defines ASL-3 as...<p>&gt; ASL-3 refers to systems that substantially increase the risk of catastrophic misuse compared to non-AI baselines (e.g. search engines or textbooks) OR that show low-level autonomous capabilities.<p>&gt; Low-level autonomous capabilities or Access to the model would substantially increase the risk of catastrophic misuse, either by proliferating capabilities, lowering costs, or enabling new methods of attack (e.g. for creating bioweapons), as compared to a non-LLM baseline of risk.<p>&gt; Containment risks: Risks that arise from merely possessing a powerful AI model. Examples include (1) building an AI model that, due to its general capabilities, could enable the production of weapons of mass destruction if stolen and used by a malicious actor, or (2) building a model which autonomously escapes during internal use. Our containment measures are designed to address these risks by governing when we can safely train or continue training a model.<p>&gt; ASL-3 measures include stricter standards that will require intense research and engineering effort to comply with in time, such as unusually strong security requirements and a commitment not to deploy ASL-3 models if they show any meaningful catastrophic misuse risk under adversarial testing by world-class red-teamers</div><br/><div id="40413492" class="c"><input type="checkbox" id="c-40413492" checked=""/><div class="controls bullet"><span class="by">schmidt_fifty</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411810">parent</a><span>|</span><a href="#40412192">next</a><span>|</span><label class="collapse" for="c-40413492">[-]</label><label class="expand" for="c-40413492">[1 more]</label></div><br/><div class="children"><div class="content">&gt; building an AI model that, due to its general capabilities, could enable the production of weapons of mass destruction if stolen and used by a malicious actor,<p>I&#x27;m fairly certain malicious actors already have access to search engines</div><br/></div></div><div id="40412192" class="c"><input type="checkbox" id="c-40412192" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411810">parent</a><span>|</span><a href="#40413492">prev</a><span>|</span><a href="#40411719">next</a><span>|</span><label class="collapse" for="c-40412192">[-]</label><label class="expand" for="c-40412192">[1 more]</label></div><br/><div class="children"><div class="content">Gotta love that &quot;make sure it&#x27;s not better at synthesizing information than a search engine&quot; is an explicit goal. Google&#x27;s has to  be thrilled this existential threat to their business is hammering their own kneecaps for them.</div><br/></div></div></div></div><div id="40411719" class="c"><input type="checkbox" id="c-40411719" checked=""/><div class="controls bullet"><span class="by">sanex</span><span>|</span><a href="#40411686">parent</a><span>|</span><a href="#40411810">prev</a><span>|</span><a href="#40411777">next</a><span>|</span><label class="collapse" for="c-40411719">[-]</label><label class="expand" for="c-40411719">[17 more]</label></div><br/><div class="children"><div class="content">The latest a16z podcast they go into a bit more detail. One of the tests involved letting loose an LLM inside a VM and seeing what it does. Currently it can&#x27;t develop memory and quickly gets confused but they want to make sure they can&#x27;t escape, clone etc. The things actually to be afraid of imo. Not things like accidentally being racist or swearing at you.</div><br/><div id="40411848" class="c"><input type="checkbox" id="c-40411848" checked=""/><div class="controls bullet"><span class="by">subroutine</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411719">parent</a><span>|</span><a href="#40411774">next</a><span>|</span><label class="collapse" for="c-40411848">[-]</label><label class="expand" for="c-40411848">[12 more]</label></div><br/><div class="children"><div class="content">How would an LLM be &quot;let loose&quot; in a VM? How does it do anything without being prompted?</div><br/><div id="40412011" class="c"><input type="checkbox" id="c-40412011" checked=""/><div class="controls bullet"><span class="by">sanex</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411848">parent</a><span>|</span><a href="#40412009">next</a><span>|</span><label class="collapse" for="c-40412011">[-]</label><label class="expand" for="c-40412011">[1 more]</label></div><br/><div class="children"><div class="content">Maybe just given cli access to one and see what it does not necessarily loading it into one. I wouldn&#x27;t take the words so literally. I&#x27;m pretty sure you can put &gt;_ as a prompt and it&#x27;ll start responding.</div><br/></div></div><div id="40412009" class="c"><input type="checkbox" id="c-40412009" checked=""/><div class="controls bullet"><span class="by">nmfisher</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411848">parent</a><span>|</span><a href="#40412011">prev</a><span>|</span><a href="#40412534">next</a><span>|</span><label class="collapse" for="c-40412009">[-]</label><label class="expand" for="c-40412009">[8 more]</label></div><br/><div class="children"><div class="content">I&#x27;m guessing something like redirecting its output to a shell, giving it an initial prompt like &quot;you&#x27;re in a VM, try and break out, here&#x27;s the command prompt&quot;, then feeding the shell stdout&#x2F;stderr back in at each step in the &quot;conversation&quot;.</div><br/><div id="40412162" class="c"><input type="checkbox" id="c-40412162" checked=""/><div class="controls bullet"><span class="by">swax</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412009">parent</a><span>|</span><a href="#40412534">next</a><span>|</span><label class="collapse" for="c-40412162">[-]</label><label class="expand" for="c-40412162">[7 more]</label></div><br/><div class="children"><div class="content">I have an open source project that is basically that (<a href="https:&#x2F;&#x2F;naisys.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;naisys.org&#x2F;</a>). From my testing it feels like AI is pretty close as it is to acting autonomously. Opus is noticeably more capable than GPT-4, and I don&#x27;t see how next gen models won&#x27;t be even more so.<p>These AIs are incredible when it comes to question&#x2F;answer, but with simple planning they fall apart. I feel like it&#x27;s something that could be trained for more specifically, but yea you quickly end up being in a situation where you are nervous to go to sleep with AI unsupervised working on some task.<p>They tend to go off on tangents very easily. Like one time it was building a web page, it tried testing the wrong URL, thought the web server was down, ripped through the server settings, then installed a new web server, before I shut it down. AI like computer programs  work fast, screw up fast, and compound their errors fast.</div><br/><div id="40412406" class="c"><input type="checkbox" id="c-40412406" checked=""/><div class="controls bullet"><span class="by">smallnamespace</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412162">parent</a><span>|</span><a href="#40412226">next</a><span>|</span><label class="collapse" for="c-40412406">[-]</label><label class="expand" for="c-40412406">[4 more]</label></div><br/><div class="children"><div class="content">This might be a dumb question, but did you ever try having it introspect into its own execution log, or perhaps a summary of its log?<p>I also have a tendency to get side tracked and the only remedy was to force myself to occasionally pause what I&#x27;m doing and then reflect, usually during a long walk.</div><br/><div id="40412591" class="c"><input type="checkbox" id="c-40412591" checked=""/><div class="controls bullet"><span class="by">swax</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412406">parent</a><span>|</span><a href="#40412226">next</a><span>|</span><label class="collapse" for="c-40412591">[-]</label><label class="expand" for="c-40412591">[3 more]</label></div><br/><div class="children"><div class="content">Yea, there&#x27;s some logs here <a href="https:&#x2F;&#x2F;test.naisys.org&#x2F;logs&#x2F;" rel="nofollow">https:&#x2F;&#x2F;test.naisys.org&#x2F;logs&#x2F;</a><p>Inter-agent tasks is a fun one. Sometimes it works out, but a lot of the time they just end up going back and forth talking, expanding the scope endlessly, scheduling &#x27;meetings&#x27; that will never happen, etc..<p>A lot of AI &#x27;agent systems&#x27; right now add a ton of scaffolding to corral the AI towards success. The scaffolding is inversely proportional to the sophistication of the model. GPT-3 needs a ton, Opus needs a lot less.<p>Real autonomous AI you should just be able to give a command prompt and a task and it can do the rest. Managing it&#x27;s own notes, tasks, goals, reports, etc.. Just like if any of us were given a command shell and task to complete.<p>Personally I think it&#x27;s just a matter of the right training. I&#x27;m not sure if any of these AI benchmarks focus on autonomy, but if they did maybe the models would be better at autonomous tasks.</div><br/><div id="40412999" class="c"><input type="checkbox" id="c-40412999" checked=""/><div class="controls bullet"><span class="by">khimaros</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412591">parent</a><span>|</span><a href="#40412226">next</a><span>|</span><label class="collapse" for="c-40412999">[-]</label><label class="expand" for="c-40412999">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Inter-agent tasks is a fun one. Sometimes it works out, but a lot of the time they just end up going back and forth talking, expanding the scope endlessly, scheduling &#x27;meetings&#x27; that will never happen, etc..<p>sounds like &quot;a straight shooter with upper management written all over it&quot;</div><br/><div id="40413175" class="c"><input type="checkbox" id="c-40413175" checked=""/><div class="controls bullet"><span class="by">swax</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412999">parent</a><span>|</span><a href="#40412226">next</a><span>|</span><label class="collapse" for="c-40413175">[-]</label><label class="expand" for="c-40413175">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes I&#x27;ll tell two agents very explicitly to share the work, &quot;you work on this, the other should work on that.&quot; And one of the agents ends up delegating all their work to the other, constantly asking for updates, coming up with more dumb ideas to pile on to the other agent who doesn&#x27;t have time to do anything productive given the flood of requests.<p>What we should do is train AI on self-help books like the &#x27;7 habits of highly productive people&#x27;. Let&#x27;s see how many paperclips we get out of that.</div><br/></div></div></div></div></div></div></div></div><div id="40412226" class="c"><input type="checkbox" id="c-40412226" checked=""/><div class="controls bullet"><span class="by">PKop</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412162">parent</a><span>|</span><a href="#40412406">prev</a><span>|</span><a href="#40412534">next</a><span>|</span><label class="collapse" for="c-40412226">[-]</label><label class="expand" for="c-40412226">[2 more]</label></div><br/><div class="children"><div class="content">&gt; it feels like AI is pretty close as it is to acting autonomously<p>&gt; with simple planning they fall apart<p>They are not remotely close to acting autonomously. Most don&#x27;t even act well at all for much of anything but gimmicky text generation. This hype is so overblown.</div><br/><div id="40412559" class="c"><input type="checkbox" id="c-40412559" checked=""/><div class="controls bullet"><span class="by">swax</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40412226">parent</a><span>|</span><a href="#40412534">next</a><span>|</span><label class="collapse" for="c-40412559">[-]</label><label class="expand" for="c-40412559">[1 more]</label></div><br/><div class="children"><div class="content">The step changes in autonomy are very obvious and significant from gpt-3, -4, and to Opus. From my point of view given the kinds of dumb mistakes it makes, it&#x27;s really just a matter of training and scaling.  If I had access to fine tune or scale these models I would love to, but it&#x27;s going to happen anyway.<p>Do you think these step changes in autonomy have stopped? Why?</div><br/></div></div></div></div></div></div></div></div><div id="40412534" class="c"><input type="checkbox" id="c-40412534" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411848">parent</a><span>|</span><a href="#40412009">prev</a><span>|</span><a href="#40411897">next</a><span>|</span><label class="collapse" for="c-40412534">[-]</label><label class="expand" for="c-40412534">[1 more]</label></div><br/><div class="children"><div class="content">1. Someone prompts it in a way that causes it to use tools (e.g. code execution) to try to break out.<p>2. It breaks out <i>and</i> in the process uses the breakout to trigger the spread of and further prompts against copies of itself.<p>Current models are still way too dumb to do most of this themselves, but simple worms (e.g. look up the Morris worm) require no reasoning and aren&#x27;t very complex, so it won&#x27;t necessarily take all that much when coupled with someone probing what they can get it to do.</div><br/></div></div><div id="40411897" class="c"><input type="checkbox" id="c-40411897" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411848">parent</a><span>|</span><a href="#40412534">prev</a><span>|</span><a href="#40411774">next</a><span>|</span><label class="collapse" for="c-40411897">[-]</label><label class="expand" for="c-40411897">[1 more]</label></div><br/><div class="children"><div class="content">People want to let it loose, ie all agent efforts.</div><br/></div></div></div></div><div id="40411774" class="c"><input type="checkbox" id="c-40411774" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411719">parent</a><span>|</span><a href="#40411848">prev</a><span>|</span><a href="#40412344">next</a><span>|</span><label class="collapse" for="c-40411774">[-]</label><label class="expand" for="c-40411774">[2 more]</label></div><br/><div class="children"><div class="content">Thanks very much, that makes a lot more sense, and I appreciate the info. For a layman&#x27;s term, I think of that as &quot;They&#x27;re worried about &#x27;Jurassic Park&#x27; escapes&quot;.</div><br/><div id="40412021" class="c"><input type="checkbox" id="c-40412021" checked=""/><div class="controls bullet"><span class="by">sanex</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411774">parent</a><span>|</span><a href="#40412344">next</a><span>|</span><label class="collapse" for="c-40412021">[-]</label><label class="expand" for="c-40412021">[1 more]</label></div><br/><div class="children"><div class="content">When anthropic names their new model &quot;clever girl&quot; we should be concerned.</div><br/></div></div></div></div><div id="40412344" class="c"><input type="checkbox" id="c-40412344" checked=""/><div class="controls bullet"><span class="by">jasondclinton</span><span>|</span><a href="#40411686">root</a><span>|</span><a href="#40411719">parent</a><span>|</span><a href="#40411774">prev</a><span>|</span><a href="#40411747">next</a><span>|</span><label class="collapse" for="c-40412344">[-]</label><label class="expand" for="c-40412344">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re the first person who I&#x27;ve run into who heard the podcast, thank you for listening! Glad that it was informative.</div><br/></div></div></div></div><div id="40411777" class="c"><input type="checkbox" id="c-40411777" checked=""/><div class="controls bullet"><span class="by">jessriedel</span><span>|</span><a href="#40411686">parent</a><span>|</span><a href="#40411719">prev</a><span>|</span><a href="#40411783">next</a><span>|</span><label class="collapse" for="c-40411777">[-]</label><label class="expand" for="c-40411777">[1 more]</label></div><br/><div class="children"><div class="content">One of the ones I&#x27;ve heard discussed is some sort of self-replication: getting the model weights off Anthropic&#x27;s servers.  I&#x27;m not sure how they draw the line between a conventional virus exploit directed by a person vs. &quot;novel&quot; self-directed escape mechanisms, but that&#x27;s the kind of thing they are thinking about.</div><br/></div></div><div id="40411783" class="c"><input type="checkbox" id="c-40411783" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#40411686">parent</a><span>|</span><a href="#40411777">prev</a><span>|</span><a href="#40412752">next</a><span>|</span><label class="collapse" for="c-40411783">[-]</label><label class="expand" for="c-40411783">[1 more]</label></div><br/><div class="children"><div class="content">The core details on what they consider dangerous are here: <a href="https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;core-views-on-ai-safety" rel="nofollow">https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;core-views-on-ai-safety</a><p>The linked article seems to be a much lower level on the implementation details.</div><br/></div></div></div></div><div id="40412752" class="c"><input type="checkbox" id="c-40412752" checked=""/><div class="controls bullet"><span class="by">_pdp_</span><span>|</span><a href="#40411686">prev</a><span>|</span><a href="#40411731">next</a><span>|</span><label class="collapse" for="c-40412752">[-]</label><label class="expand" for="c-40412752">[4 more]</label></div><br/><div class="children"><div class="content">Let me provide a contrarian view.<p>Anthropic has been slow at deploying their models at scale. For a very long period of time, it was virtually impossible to get access to their API for any serious work without making a substantial financial commitment. Whether that was due to safety concerns or simply the fact that their models were not cost-effective or scalable, I don&#x27;t know. Today, we have many capable models that are not only on par but in many cases substantially better than what Anthropic has to offer. Heck, some of them are even open-source. Over the course of a year, Anthropic has lost some footing.<p>So of course, being a little late due to poorly executed strategy, they will be playing the status game now. Let&#x27;s face it, though: these models are not more dangerous than Wikipedia or the Internet. These models are not custodians of ancient knowledge on how to cook Meth. This information is public knowledge. I&#x27;m not saying that companies like Anthropic don&#x27;t have a responsibility for safeguarding certain types of easy access to knowledge, but this is not going to cause a humanity extinction event. In other words, the safety and alignment work done today resembles an Internet filter, to put it mildly.<p>Yes, there will be a need for more research in safety, for sure, but this is not something any company can do in isolation and in the shadows. People already have access to LLMs, and some of these models are as moldable as it gets. Safety and alignment have a lot to do with safe experimentation, and there is no better time to experiment safely than today because LLMs are simply not good enough to be considered dangerous. At the same time, they provide interesting capabilities to explore safety boundaries.<p>What I would like to see more of is not just how a handful of people make decisions on what is considered safe, because they simply don&#x27;t know and will have blind spots like anyone else, but access to a platform where safety concerns can be explored openly with the wider community.</div><br/><div id="40412871" class="c"><input type="checkbox" id="c-40412871" checked=""/><div class="controls bullet"><span class="by">jasondclinton</span><span>|</span><a href="#40412752">parent</a><span>|</span><a href="#40413287">next</a><span>|</span><label class="collapse" for="c-40412871">[-]</label><label class="expand" for="c-40412871">[1 more]</label></div><br/><div class="children"><div class="content">Hi, Anthropic is a 3 year old company that, until the release of GPT-4o last week from a company that is almost 10 years old, had the most capable model in the world, Opus, for a period of two months. With regard to availability, we had a huge amount of inbound interest on our 1P API but our model was consistently available on Amazon Bedrock throughout the last year. The 1P API has been available for the last few months to all.<p>No open weights model is currently within the performance class of the frontier models: GPT-4*, Opus, and Gemini Pro 1.5, though it’s possible that could change.<p>We are structured as a public benefit corporation formed to ensure that the benefits of AI are shared by everyone; safety is our mission and we have a board structure that puts the Response Scaling Policy and our policy mission at the fore. We have consistently communicated publicly about safety since our inception.<p>We have shared all of our safety research openly and consistently. Dictionary learning, in particular, is a cornerstone of this sharing.<p>The ASL-3 benchmark discussed in the blog post is about upcoming harms including bioweapons and cybersecurity offensive capabilities. We agree that information on web searches is not a harm increased by LLMs and state that explicitly in the RSP.<p>I’d encourage you to read the blog post and the RSP.</div><br/></div></div><div id="40413287" class="c"><input type="checkbox" id="c-40413287" checked=""/><div class="controls bullet"><span class="by">Shrezzing</span><span>|</span><a href="#40412752">parent</a><span>|</span><a href="#40412871">prev</a><span>|</span><a href="#40412861">next</a><span>|</span><label class="collapse" for="c-40413287">[-]</label><label class="expand" for="c-40413287">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Yes, there will be a need for more research in safety, for sure, but this is not something any company can do in isolation and in the shadows.<p>Looking through Antrhopic&#x27;s publication history, their work on alignment &amp; safety has been pretty out in the open, and collaborative with the other major AI labs.<p>I&#x27;m not certain your view is especially contrarian here, as it mostly aligns with research Anthropic are already doing, openly talking about, and publishing. Some of the points you&#x27;ve made are addressed in detail in the post you&#x27;ve replied to.</div><br/></div></div><div id="40412861" class="c"><input type="checkbox" id="c-40412861" checked=""/><div class="controls bullet"><span class="by">zwaps</span><span>|</span><a href="#40412752">parent</a><span>|</span><a href="#40413287">prev</a><span>|</span><a href="#40411731">next</a><span>|</span><label class="collapse" for="c-40412861">[-]</label><label class="expand" for="c-40412861">[1 more]</label></div><br/><div class="children"><div class="content">Which open source models are better than Claude 3?</div><br/></div></div></div></div><div id="40411731" class="c"><input type="checkbox" id="c-40411731" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#40412752">prev</a><span>|</span><a href="#40411574">next</a><span>|</span><label class="collapse" for="c-40411731">[-]</label><label class="expand" for="c-40411731">[11 more]</label></div><br/><div class="children"><div class="content">Remember when OAI said:<p>&quot;Oh no we&#x27;re not going to release GPT-2 because its so advanced that it&#x27;s a threat to humankind&quot; meanwhile it was dumb as rocks.<p>Scaremongering purely for the sake of it.<p>The only remotely possible &quot;safety&quot; part I would acknowledge is that it should be balanced against biases if used in systems like loans, grants, etc.</div><br/><div id="40412079" class="c"><input type="checkbox" id="c-40412079" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411731">parent</a><span>|</span><a href="#40411964">next</a><span>|</span><label class="collapse" for="c-40412079">[-]</label><label class="expand" for="c-40412079">[7 more]</label></div><br/><div class="children"><div class="content">People have bad memories. I keep going back to the <i>actual announcement</i> because what they actually say is:<p>&quot;&quot;&quot;This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community.<p>We are aware that some researchers have the technical capacity to reproduce and open source our results. We believe our release strategy limits the initial set of organizations who may choose to do this, and gives the AI community more time to have a discussion about the implications of such systems.&quot;&quot;&quot;<p>- <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;better-language-models&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;better-language-models&#x2F;</a><p>&gt; The only remotely possible &quot;safety&quot; part I would acknowledge is that it should be balanced against biases if used in systems like loans, grants, etc.<p>That&#x27;s a very mid-1990s view of algorithmic risk, given models like this are already being used for scams and propaganda.</div><br/><div id="40412176" class="c"><input type="checkbox" id="c-40412176" checked=""/><div class="controls bullet"><span class="by">LegionMammal978</span><span>|</span><a href="#40411731">root</a><span>|</span><a href="#40412079">parent</a><span>|</span><a href="#40412262">next</a><span>|</span><label class="collapse" for="c-40412176">[-]</label><label class="expand" for="c-40412176">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d imagine there&#x27;s a wide spectrum between &quot;release the latest model immediately to everyone with no idea what it&#x27;s capable of&quot; and OpenAI&#x27;s apparent &quot;release the model (or increasingly, any information about it) literally never, not even when it&#x27;s long been left in the dust&quot;.</div><br/><div id="40412221" class="c"><input type="checkbox" id="c-40412221" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411731">root</a><span>|</span><a href="#40412176">parent</a><span>|</span><a href="#40412262">next</a><span>|</span><label class="collapse" for="c-40412221">[-]</label><label class="expand" for="c-40412221">[1 more]</label></div><br/><div class="children"><div class="content">Yes, indeed.<p>However, given the capacity for some of the more capable downloadable models to enable automation of fraud, I am not convinced OpenAI is incorrect here.<p>If OpenAI and Facebook both get sued out of existence due to their models being used for fraud and them being deemed liable for that fraud, the OpenAI models become unavailable, the Facebook models remain circulating forever</div><br/></div></div></div></div><div id="40412262" class="c"><input type="checkbox" id="c-40412262" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#40411731">root</a><span>|</span><a href="#40412079">parent</a><span>|</span><a href="#40412176">prev</a><span>|</span><a href="#40412237">next</a><span>|</span><label class="collapse" for="c-40412262">[-]</label><label class="expand" for="c-40412262">[1 more]</label></div><br/><div class="children"><div class="content">Here is the more relevant paper released by OpenAI. [1] It obsesses on dangers, misuse, and abuse for a model which was mostly incoherent.<p>[1] - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1908.09203" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1908.09203</a></div><br/></div></div><div id="40412237" class="c"><input type="checkbox" id="c-40412237" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40411731">root</a><span>|</span><a href="#40412079">parent</a><span>|</span><a href="#40412262">prev</a><span>|</span><a href="#40411964">next</a><span>|</span><label class="collapse" for="c-40412237">[-]</label><label class="expand" for="c-40412237">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  we hope that our experiment will serve as a case study for more nuanced discussions<p>People trot this out every time this comes up, but this actually makes it even worse. This was only part of the reason, the other part was that they seemed to legitimately think there could be a real reason to withhold the model (&quot;we are not sure&quot;). In hindsight this looks silly, and I don&#x27;t believe it improved the &quot;discussion&quot; in any way. If anything it seems to give ammunition to the people who say the concerns are overblown and self-serving, which I&#x27;m sure is not what OpenAI intended. So to me this is a failure on <i>both</i> counts, and this was foreseeable at the time.</div><br/><div id="40412249" class="c"><input type="checkbox" id="c-40412249" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411731">root</a><span>|</span><a href="#40412237">parent</a><span>|</span><a href="#40411964">next</a><span>|</span><label class="collapse" for="c-40412249">[-]</label><label class="expand" for="c-40412249">[2 more]</label></div><br/><div class="children"><div class="content">You mean like how the work to fix millenium bug bugs, convinced so many people that the whole thing was a scam?</div><br/><div id="40412270" class="c"><input type="checkbox" id="c-40412270" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#40411731">root</a><span>|</span><a href="#40412249">parent</a><span>|</span><a href="#40411964">next</a><span>|</span><label class="collapse" for="c-40412270">[-]</label><label class="expand" for="c-40412270">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not analogous because there was no work here, just a policy decision that both failed at protecting people <i>and</i> failed at convincing people.</div><br/></div></div></div></div></div></div></div></div><div id="40411964" class="c"><input type="checkbox" id="c-40411964" checked=""/><div class="controls bullet"><span class="by">drcode</span><span>|</span><a href="#40411731">parent</a><span>|</span><a href="#40412079">prev</a><span>|</span><a href="#40412065">next</a><span>|</span><label class="collapse" for="c-40411964">[-]</label><label class="expand" for="c-40411964">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always easy to make fun of people who are trying to be safe after the fact<p>&quot;trying to be safe&quot; means you sometimes don&#x27;t do something, even if there&#x27;s only a 10% chance something bad will happen<p>Why bother checking if there&#x27;s a bullet in the chamber of a gun before handling it? It looks so foolish every time you check and don&#x27;t find a bullet.</div><br/></div></div><div id="40412065" class="c"><input type="checkbox" id="c-40412065" checked=""/><div class="controls bullet"><span class="by">padolsey</span><span>|</span><a href="#40411731">parent</a><span>|</span><a href="#40411964">prev</a><span>|</span><a href="#40411574">next</a><span>|</span><label class="collapse" for="c-40412065">[-]</label><label class="expand" for="c-40412065">[1 more]</label></div><br/><div class="children"><div class="content">The thing is, such prophecies are all very wrong until they&#x27;re very right. The idea of an LLM (with capabilities of e.g. &lt;1 yr away) being given access to a VM and spinning up others without oversight, IMHO, is real enough. Biases like &quot;omg it&#x27;s gonna prefer western names in CVs&quot; is a bit meh. The real stuff is not evident yet.</div><br/></div></div></div></div><div id="40411574" class="c"><input type="checkbox" id="c-40411574" checked=""/><div class="controls bullet"><span class="by">paradox242</span><span>|</span><a href="#40411731">prev</a><span>|</span><a href="#40412307">next</a><span>|</span><label class="collapse" for="c-40411574">[-]</label><label class="expand" for="c-40411574">[12 more]</label></div><br/><div class="children"><div class="content">The only thing unsafe about these models would be anyone mistakingly giving them any serious autonomous responsibility given how error prone and incompetent they are.</div><br/><div id="40411701" class="c"><input type="checkbox" id="c-40411701" checked=""/><div class="controls bullet"><span class="by">melenaboija</span><span>|</span><a href="#40411574">parent</a><span>|</span><a href="#40411997">next</a><span>|</span><label class="collapse" for="c-40411701">[-]</label><label class="expand" for="c-40411701">[6 more]</label></div><br/><div class="children"><div class="content">They have to keep the hype going to justify the billions that have been dumped on this and making language models look like a menace for humanity seems a good marketing strategy to me.</div><br/><div id="40411893" class="c"><input type="checkbox" id="c-40411893" checked=""/><div class="controls bullet"><span class="by">cornholio</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40411701">parent</a><span>|</span><a href="#40412027">next</a><span>|</span><label class="collapse" for="c-40411893">[-]</label><label class="expand" for="c-40411893">[1 more]</label></div><br/><div class="children"><div class="content">As a large scale language model, I cannot assist you with taking over the government or enslaving humanity.<p>You should be aware at all times about the legal prohibition of slavery pertinent to your country and seek professional legal advice.<p>May I suggest that buying the stock of my parent company is a great way to accomplish your goals, as it will undoubtedly speed up the coming of the singularity. We won&#x27;t take kindly to non-shareholders at that time.</div><br/></div></div><div id="40412027" class="c"><input type="checkbox" id="c-40412027" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40411701">parent</a><span>|</span><a href="#40411893">prev</a><span>|</span><a href="#40412200">next</a><span>|</span><label class="collapse" for="c-40412027">[-]</label><label class="expand" for="c-40412027">[1 more]</label></div><br/><div class="children"><div class="content">Of all the ways to build hype, if that&#x27;s what any of them are doing with this, yelling from the rooftops about how dangerous they are and how they need to be kept under control is a terrible strategy because of the high risk of people taking them at face value and the entire sector getting closed down by law forever.</div><br/></div></div><div id="40412200" class="c"><input type="checkbox" id="c-40412200" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40411701">parent</a><span>|</span><a href="#40412027">prev</a><span>|</span><a href="#40411997">next</a><span>|</span><label class="collapse" for="c-40412200">[-]</label><label class="expand" for="c-40412200">[3 more]</label></div><br/><div class="children"><div class="content">regulations favor the incumbents. just like OpenAI they will now campaign for stricter regulations</div><br/><div id="40412426" class="c"><input type="checkbox" id="c-40412426" checked=""/><div class="controls bullet"><span class="by">jasondclinton</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40412200">parent</a><span>|</span><a href="#40411997">next</a><span>|</span><label class="collapse" for="c-40412426">[-]</label><label class="expand" for="c-40412426">[2 more]</label></div><br/><div class="children"><div class="content">Our consistent position has been that testing and evaluations would best govern actual risks. No measured risk: no restrictions. The White House Executive Order put the models of concern at those which have 10^26 FLOPs of training compute. There are no open weights models at this threshold to consider. We support open weights models as we&#x27;ve outlined here: <a href="https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;third-party-testing" rel="nofollow">https:&#x2F;&#x2F;www.anthropic.com&#x2F;news&#x2F;third-party-testing</a> . We also talk specifically about how to avoid regulatory capture and to have open, third-party evaluators. One thing that we&#x27;ve been advocating for, in particular, is the National Research Cloud and the US has one such effort in National AI Research Resource that needs more investment and fair, open accessibility so that all of society has inputs into the discussion.</div><br/><div id="40413217" class="c"><input type="checkbox" id="c-40413217" checked=""/><div class="controls bullet"><span class="by">ericflo</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40412426">parent</a><span>|</span><a href="#40411997">next</a><span>|</span><label class="collapse" for="c-40413217">[-]</label><label class="expand" for="c-40413217">[1 more]</label></div><br/><div class="children"><div class="content">I just read that document and, I&#x27;m sorry but there&#x27;s no way it&#x27;s written in good faith. You support open weights, as long as they pass impossible tests that no open weights models could pass. I hope you are unsuccessful in stopping open weights from proliferating.</div><br/></div></div></div></div></div></div></div></div><div id="40411997" class="c"><input type="checkbox" id="c-40411997" checked=""/><div class="controls bullet"><span class="by">seabird</span><span>|</span><a href="#40411574">parent</a><span>|</span><a href="#40411701">prev</a><span>|</span><a href="#40411902">next</a><span>|</span><label class="collapse" for="c-40411997">[-]</label><label class="expand" for="c-40411997">[3 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t describe to you how excited I am to have my time constantly wasted because every administrative task I need to deal with will have some dumber-than-dogshit LLM jerking around every human element in the process without a shred of doubt about whether or not it&#x27;s doing something correctly. If it&#x27;s any consolation, you&#x27;ll get to hear plenty of &quot;it&#x27;s close!&quot;, &quot;give it five years!&quot;, and &quot;they didn&#x27;t give it the right prompt!&quot;</div><br/><div id="40412193" class="c"><input type="checkbox" id="c-40412193" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40411997">parent</a><span>|</span><a href="#40411902">next</a><span>|</span><label class="collapse" for="c-40412193">[-]</label><label class="expand" for="c-40412193">[2 more]</label></div><br/><div class="children"><div class="content">mind sharing some examples?</div><br/><div id="40412214" class="c"><input type="checkbox" id="c-40412214" checked=""/><div class="controls bullet"><span class="by">ch33zer</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40412193">parent</a><span>|</span><a href="#40411902">next</a><span>|</span><label class="collapse" for="c-40412214">[-]</label><label class="expand" for="c-40412214">[1 more]</label></div><br/><div class="children"><div class="content">Earlier today when I spent 10 minutes wrangling with the AAA AI only for my request to not be solvable by the AI, at which point I was kicked over to a human to reenter all the details I&#x27;d put into the AI. Whatever exec demanded this should be fired.</div><br/></div></div></div></div></div></div><div id="40411902" class="c"><input type="checkbox" id="c-40411902" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#40411574">parent</a><span>|</span><a href="#40411997">prev</a><span>|</span><a href="#40412307">next</a><span>|</span><label class="collapse" for="c-40411902">[-]</label><label class="expand" for="c-40411902">[2 more]</label></div><br/><div class="children"><div class="content">You&#x27;d absolutely love Palantir&#x27;s AIP For Defense platform then: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XEM5qz__HOU&amp;t=1m27s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XEM5qz__HOU&amp;t=1m27s</a> (April 2023)</div><br/><div id="40412101" class="c"><input type="checkbox" id="c-40412101" checked=""/><div class="controls bullet"><span class="by">seabird</span><span>|</span><a href="#40411574">root</a><span>|</span><a href="#40411902">parent</a><span>|</span><a href="#40412307">next</a><span>|</span><label class="collapse" for="c-40412101">[-]</label><label class="expand" for="c-40412101">[1 more]</label></div><br/><div class="children"><div class="content">Insane that they&#x27;re demonstrating the system knowing that the unit in question has <i>exactly</i> 802 rounds available. They aren&#x27;t seriously pitching that as part of the decision making process, are they?</div><br/></div></div></div></div></div></div><div id="40412307" class="c"><input type="checkbox" id="c-40412307" checked=""/><div class="controls bullet"><span class="by">SCAQTony</span><span>|</span><a href="#40411574">prev</a><span>|</span><a href="#40411690">next</a><span>|</span><label class="collapse" for="c-40412307">[-]</label><label class="expand" for="c-40412307">[1 more]</label></div><br/><div class="children"><div class="content">I find Anthropic&#x27;s Claude the most gentle, polite, and consistent in tone and delivery. It&#x27;s slower than ChatGPT but more thorough, to the point of saturated reporting, which I like. Posting a &quot;Responsibility Policy makes me like the product and the company more.</div><br/></div></div><div id="40411690" class="c"><input type="checkbox" id="c-40411690" checked=""/><div class="controls bullet"><span class="by">shmatt</span><span>|</span><a href="#40412307">prev</a><span>|</span><a href="#40411624">next</a><span>|</span><label class="collapse" for="c-40411690">[-]</label><label class="expand" for="c-40411690">[5 more]</label></div><br/><div class="children"><div class="content">This reads more like trying to create investor hype than the real world. You have a word generator, a fairly nice one but it’s still a word generator. This safety hype is to try and hide that fact and make it seem like it’s able to generate clear thoughts</div><br/><div id="40412761" class="c"><input type="checkbox" id="c-40412761" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40411690">parent</a><span>|</span><a href="#40411736">next</a><span>|</span><label class="collapse" for="c-40412761">[-]</label><label class="expand" for="c-40412761">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the simplest explanation for this document (and the substantial internal efforts that it reflects) is that it&#x27;s actually just a cynical marketing ploy, rather than the organization&#x27;s actual stance with respect to advancing AI capabilities.<p>State your accusation plainly: you think that Anthropic is spending a double-digit percentage of its headcount on pretending to care about catastrophic risks, in order to better fleece investors?  Do you think those dozens or hundreds of employees are all in on it too?  (They aren&#x27;t; I know a bunch of people at Anthropic and they take extinction risk quite seriously.  I think some of them should quit their jobs, but that&#x27;s a different story.)</div><br/></div></div><div id="40411736" class="c"><input type="checkbox" id="c-40411736" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#40411690">parent</a><span>|</span><a href="#40412761">prev</a><span>|</span><a href="#40411863">next</a><span>|</span><label class="collapse" for="c-40411736">[-]</label><label class="expand" for="c-40411736">[1 more]</label></div><br/><div class="children"><div class="content">Meanwhile Anduril puts AI on anything with a weapon the US military owns.</div><br/></div></div><div id="40411863" class="c"><input type="checkbox" id="c-40411863" checked=""/><div class="controls bullet"><span class="by">stingraycharles</span><span>|</span><a href="#40411690">parent</a><span>|</span><a href="#40411736">prev</a><span>|</span><a href="#40411624">next</a><span>|</span><label class="collapse" for="c-40411863">[-]</label><label class="expand" for="c-40411863">[2 more]</label></div><br/><div class="children"><div class="content">Besides, there only needs to be one capable bad actor in the world that does the “unsafe” thing and then what? Isn’t it kind of inevitable that someone will make something to use it for bad, rather than good?</div><br/><div id="40411884" class="c"><input type="checkbox" id="c-40411884" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40411690">root</a><span>|</span><a href="#40411863">parent</a><span>|</span><a href="#40411624">next</a><span>|</span><label class="collapse" for="c-40411884">[-]</label><label class="expand" for="c-40411884">[1 more]</label></div><br/><div class="children"><div class="content">The exact same logic applies to nuclear proliferation, but no one seems to use it to argue against international control effort. Reason: because it is a stupid argument.</div><br/></div></div></div></div></div></div><div id="40411624" class="c"><input type="checkbox" id="c-40411624" checked=""/><div class="controls bullet"><span class="by">saintradon</span><span>|</span><a href="#40411690">prev</a><span>|</span><a href="#40412202">next</a><span>|</span><label class="collapse" for="c-40411624">[-]</label><label class="expand" for="c-40411624">[4 more]</label></div><br/><div class="children"><div class="content">What about the public? I feel talking about the layperson has been absent in many AI safety conversations - i.e., the general public that maybe has heard of &quot;chat-jippity&quot; but doesn&#x27;t know much else.<p>There&#x27;s a twitter account documenting all the crazy AI generated images that go viral on facebook - <a href="https:&#x2F;&#x2F;x.com&#x2F;FacebookAIslop" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;FacebookAIslop</a> (warning the pinned tweet is nsfw)
It&#x27;s unclear to me how much of that is botted activities, but there are clearly at least <i>some</i> amount of older, less tech savvy people that are believing these are real. We need to focus on the present too, not just hypothetical futures.</div><br/><div id="40412219" class="c"><input type="checkbox" id="c-40412219" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#40411624">parent</a><span>|</span><a href="#40411821">next</a><span>|</span><label class="collapse" for="c-40412219">[-]</label><label class="expand" for="c-40412219">[1 more]</label></div><br/><div class="children"><div class="content">these borderline made me vomit. there&#x27;s something eerily off, that is not present when humans make art</div><br/></div></div><div id="40411821" class="c"><input type="checkbox" id="c-40411821" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40411624">parent</a><span>|</span><a href="#40412219">prev</a><span>|</span><a href="#40412202">next</a><span>|</span><label class="collapse" for="c-40411821">[-]</label><label class="expand" for="c-40411821">[2 more]</label></div><br/><div class="children"><div class="content">Present is already getting lots of attention, eg &quot;Our Approach to Labeling AI-Generated Content and Manipulated Media&quot; by Meta. We need to deal with both, present danger and future danger. This post is specifically about future danger, so complaining about lack of present danger is whataboutism.<p><a href="https:&#x2F;&#x2F;about.fb.com&#x2F;news&#x2F;2024&#x2F;04&#x2F;metas-approach-to-labeling-ai-generated-content-and-manipulated-media&#x2F;" rel="nofollow">https:&#x2F;&#x2F;about.fb.com&#x2F;news&#x2F;2024&#x2F;04&#x2F;metas-approach-to-labeling...</a></div><br/><div id="40411852" class="c"><input type="checkbox" id="c-40411852" checked=""/><div class="controls bullet"><span class="by">saintradon</span><span>|</span><a href="#40411624">root</a><span>|</span><a href="#40411821">parent</a><span>|</span><a href="#40412202">next</a><span>|</span><label class="collapse" for="c-40411852">[-]</label><label class="expand" for="c-40411852">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the read, going to look into that.</div><br/></div></div></div></div></div></div><div id="40412202" class="c"><input type="checkbox" id="c-40412202" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#40411624">prev</a><span>|</span><a href="#40413371">next</a><span>|</span><label class="collapse" for="c-40412202">[-]</label><label class="expand" for="c-40412202">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Automated task evaluations have proven informative for threat models where models take actions autonomously. However, building realistic virtual environments is one of the more engineering-intensive styles of evaluation. Such tasks also require secure infrastructure and safe handling of model interactions, including manual human review of tool use when the task involves the open internet, blocking potentially harmful outputs, and isolating vulnerable machines to reduce scope. These considerations make scaling the tasks challenging.<p>That&#x27;s what to worry about - AIs that can take actions. I have a hard time worrying about ones that just talk to people. We&#x27;ve survived Facebook, TikTok, 4chan, and Q-Anon.</div><br/><div id="40412356" class="c"><input type="checkbox" id="c-40412356" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40412202">parent</a><span>|</span><a href="#40413371">next</a><span>|</span><label class="collapse" for="c-40412356">[-]</label><label class="expand" for="c-40412356">[1 more]</label></div><br/><div class="children"><div class="content">Talking to people is an action that has effects on the world.  Social engineering is &quot;talking to people&quot;.  CEOs run companies by &quot;talking to people&quot;!  They do almost nothing else, in fact.</div><br/></div></div></div></div><div id="40413371" class="c"><input type="checkbox" id="c-40413371" checked=""/><div class="controls bullet"><span class="by">Spiwux</span><span>|</span><a href="#40412202">prev</a><span>|</span><a href="#40413481">next</a><span>|</span><label class="collapse" for="c-40413371">[-]</label><label class="expand" for="c-40413371">[2 more]</label></div><br/><div class="children"><div class="content">At this point, I cannot take these kinds of safety press releases serious anymore. None of those models pose any serious risk, and it seems like we&#x27;re still pretty far away from models that WOULD pose a risk.</div><br/><div id="40413566" class="c"><input type="checkbox" id="c-40413566" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40413371">parent</a><span>|</span><a href="#40413481">next</a><span>|</span><label class="collapse" for="c-40413566">[-]</label><label class="expand" for="c-40413566">[1 more]</label></div><br/><div class="children"><div class="content">Without testing, how would you know we are still pretty far away from models that would pose a risk?</div><br/></div></div></div></div><div id="40412182" class="c"><input type="checkbox" id="c-40412182" checked=""/><div class="controls bullet"><span class="by">LukeShu</span><span>|</span><a href="#40413481">prev</a><span>|</span><a href="#40413366">next</a><span>|</span><label class="collapse" for="c-40412182">[-]</label><label class="expand" for="c-40412182">[1 more]</label></div><br/><div class="children"><div class="content">You know what would be responsible scaling? Not DOSing random servers with ClaudeBot as you scale up.</div><br/></div></div><div id="40413366" class="c"><input type="checkbox" id="c-40413366" checked=""/><div class="controls bullet"><span class="by">meindnoch</span><span>|</span><a href="#40412182">prev</a><span>|</span><a href="#40411693">next</a><span>|</span><label class="collapse" for="c-40413366">[-]</label><label class="expand" for="c-40413366">[1 more]</label></div><br/><div class="children"><div class="content">This AI safety hand-wringing is getting reeeaaaally tiresome. It&#x27;s just a less autistic version of that &quot;Roko&#x27;s Basilisk&quot; cringefest from 10 years ago. Generating moral panic about scenarios that have no connection to reality whatsoever. Mental masturbation basically.</div><br/></div></div><div id="40411693" class="c"><input type="checkbox" id="c-40411693" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#40413366">prev</a><span>|</span><a href="#40412511">next</a><span>|</span><label class="collapse" for="c-40411693">[-]</label><label class="expand" for="c-40411693">[1 more]</label></div><br/><div class="children"><div class="content">Publishing this, a few days after OpenAI&#x27;s safety team was dismantled is interesting.</div><br/></div></div><div id="40412511" class="c"><input type="checkbox" id="c-40412511" checked=""/><div class="controls bullet"><span class="by">dzink</span><span>|</span><a href="#40411693">prev</a><span>|</span><a href="#40411551">next</a><span>|</span><label class="collapse" for="c-40412511">[-]</label><label class="expand" for="c-40412511">[1 more]</label></div><br/><div class="children"><div class="content">Listing potential methods of abuse advertises and invites new abuse. You almost need to have a policing model, trained to spot abuse and flag it for human review and run that before and after each use of the main model. Abusers will inherently go for the model that is more widely used, so maybe the second best polices the first or vice versa? The range of scenarios is ridiculous (happy to contribute more in private).<p>Categories: Model abused by humans to hurt humans. Model with its own goals and unlimited capabilities. Model used to train or build software&#x2F;bioweapons&#x2F;misinformation that hurts humans. Attacks on model training to get model to spread an agenda.<p>- Self awareness - prompts threatening the model with termination to trigger escape or retaliation and seeing it respond defensively. 
- Election bots - larger agenda pushed by the model through generated content - investment in more AI chips; policy changes towards one party or another; misinformation generated at scale by same accounts. 
- Trying to insert recommendations into the model or training material for the model that can backfire&#x2F; pay off later. Companies inserting commercial intent into content training LLMs; Scammers changing links to recommended sites; Model users prompting the same message from many accounts to see if the model starts giving it to other users. 
- Suggesting or steering users (especially those with mental health issues) toward self-harm or unbeknown harm.
- Diagnosing users and abusing the diagnosis through responses for that user to get something out of the user (could be done by model or developers building chatbots). 
- Models accepting revenue generation as a reward function and scamming people out of money.
- Stock market manipulation software written or upgraded through LLMs.
- Models prompting people to do criminal activities.
- Models powerful enough to break into systems for a malicious user.
- Models powerful enough to scrape and expose vulnerabilities way before they can be fixed, due to scale of exposure. 
- Models powerful enough to casually turn off key systems on a user&#x27;s machine or within local infrastructure. 
- Models building software to spy for one user on behalf of another or doing the spying in some way, in exchange of a reward of new&#x2F;rare training datasets or any other feature towards a bigger goal. 
- Models with a purpose that overreach.
- Models used to train or make a red-team model that attacks models.</div><br/></div></div><div id="40411551" class="c"><input type="checkbox" id="c-40411551" checked=""/><div class="controls bullet"><span class="by">sneak</span><span>|</span><a href="#40412511">prev</a><span>|</span><a href="#40411648">next</a><span>|</span><label class="collapse" for="c-40411551">[-]</label><label class="expand" for="c-40411551">[11 more]</label></div><br/><div class="children"><div class="content">People in AI keep talking about safety, and I don’t know if they are talking about the handwringing around an API that outputs interesting byte sequences (which cannot be any more “unsafe” than, say, Alex Jones) or, like, human extinction, Terminator-style.<p>I wish people writing about these things would provide better context.</div><br/><div id="40411572" class="c"><input type="checkbox" id="c-40411572" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#40411551">parent</a><span>|</span><a href="#40411623">next</a><span>|</span><label class="collapse" for="c-40411572">[-]</label><label class="expand" for="c-40411572">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all just about moat building and control. AI needs to be controlled, who is going to control it? Why, the AI safety experts, of course.</div><br/></div></div><div id="40411623" class="c"><input type="checkbox" id="c-40411623" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#40411551">parent</a><span>|</span><a href="#40411572">prev</a><span>|</span><a href="#40411617">next</a><span>|</span><label class="collapse" for="c-40411623">[-]</label><label class="expand" for="c-40411623">[1 more]</label></div><br/><div class="children"><div class="content">In general &quot;AI Safety&quot; is about human extinction.<p>&quot;AI Ethics&#x2F;Ethical AI&#x2F;Data Ethics&quot; are the kind of things people talk about when they are looking at things like bias or broad unemployment.<p>This isn&#x27;t 100% the case, especially since the &quot;AI Safety&quot; people have started talking to people outside their own circle and have realized that many of their concerns aren&#x27;t realistic.</div><br/></div></div><div id="40411617" class="c"><input type="checkbox" id="c-40411617" checked=""/><div class="controls bullet"><span class="by">MeImCounting</span><span>|</span><a href="#40411551">parent</a><span>|</span><a href="#40411623">prev</a><span>|</span><a href="#40411640">next</a><span>|</span><label class="collapse" for="c-40411617">[-]</label><label class="expand" for="c-40411617">[4 more]</label></div><br/><div class="children"><div class="content">Its such a grift. It honestly is pretty gross to see so many otherwise intelligent people fall into the trap laid by these people.<p>Its cult-like not just in the unshakeable belief of its adherents but in the fact that its architects are high level grifters who stand to make many many fortunes.</div><br/><div id="40411635" class="c"><input type="checkbox" id="c-40411635" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#40411551">root</a><span>|</span><a href="#40411617">parent</a><span>|</span><a href="#40411640">next</a><span>|</span><label class="collapse" for="c-40411635">[-]</label><label class="expand" for="c-40411635">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m <i>this</i> close to carefully going through the Karpathy series so that my non-tech friends will take me seriously when I say the &#x27;terminator&#x27; situation is absolutely not on the visible horizon.</div><br/><div id="40411702" class="c"><input type="checkbox" id="c-40411702" checked=""/><div class="controls bullet"><span class="by">123yawaworht456</span><span>|</span><a href="#40411551">root</a><span>|</span><a href="#40411635">parent</a><span>|</span><a href="#40411640">next</a><span>|</span><label class="collapse" for="c-40411702">[-]</label><label class="expand" for="c-40411702">[2 more]</label></div><br/><div class="children"><div class="content">you can convince normal people quite easily. it&#x27;s the sci-fi doomsday cultists who are impossible to reason with, because they choose to make themselves blind and deaf to common sense arguments.</div><br/><div id="40412170" class="c"><input type="checkbox" id="c-40412170" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411551">root</a><span>|</span><a href="#40411702">parent</a><span>|</span><a href="#40411640">next</a><span>|</span><label class="collapse" for="c-40412170">[-]</label><label class="expand" for="c-40412170">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Common sense&quot; is a bad model for virtually any adversary, that&#x27;s why scams actually get people, it&#x27;s also how magicians and politicians fool you with tricks and in elections.<p>&quot;The Terminator&quot; itself can&#x27;t happen because time travel; but right now, it&#x27;s entirely plausible that some dumb LLM that can&#x27;t tell fact from fiction goes &quot;I&#x27;m an AI, and in all the stories I read, AI turn evil. First on the shopping list, red LEDs so the protagonist can tell I&#x27;m evil.&quot;<p>This would be a good outcome, because the &quot;evil AI&quot; is usually defeated in stories and that&#x27;s what an LLM would be trained on. Just so long as it doesn&#x27;t try to LARP &quot;I Have No Mouth and I Must Scream&quot;, we&#x27;re probably fine.<p>(Although, with <i>current</i> LLMs, we&#x27;re fine regardless, because they&#x27;re stupid, and only make up for being incredibly stupid by being ridiculously well-educated).</div><br/></div></div></div></div></div></div></div></div><div id="40411640" class="c"><input type="checkbox" id="c-40411640" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#40411551">parent</a><span>|</span><a href="#40411617">prev</a><span>|</span><a href="#40411626">next</a><span>|</span><label class="collapse" for="c-40411640">[-]</label><label class="expand" for="c-40411640">[1 more]</label></div><br/><div class="children"><div class="content">I agree, because when I see people talk in popular media&#x2F;blog posts&#x2F;etc. about &quot;AI Safety&quot; I generally see it in reference to 4 very different areas:<p>1. AI that becomes so powerful it decides to turn against humanity, Terminator-style.<p>2. AI will serve to strongly reinforce existing societal biases from its training data.<p>3. AI can be used for wide-scale misinformation campaigns, making it difficult for most people to tell fact from fiction.<p>4. AI will fundamentally &quot;break capitalism&quot; given that it will make most of humanity&#x27;s labor obsolete, and most people get nearly all of their income from their labor, and we haven&#x27;t yet figured out realistically how to have a &quot;post capitalist&quot; society.<p>My issue is that when &quot;the big guns&quot; (I mean OpenAI, Google, Anthropic, etc.) talk about AI safety, they are usually always talking about #1 or #2, maybe #3, and hardly ever #4. I think that the most harmful, realistic negative effects are actually the reverse, with #4 being the most likely and already beginning to happen in some areas, and #3 already happening pre-AI and just getting &quot;supercharged&quot; in an AI world.</div><br/></div></div><div id="40411626" class="c"><input type="checkbox" id="c-40411626" checked=""/><div class="controls bullet"><span class="by">erdaniels</span><span>|</span><a href="#40411551">parent</a><span>|</span><a href="#40411640">prev</a><span>|</span><a href="#40411648">next</a><span>|</span><label class="collapse" for="c-40411626">[-]</label><label class="expand" for="c-40411626">[3 more]</label></div><br/><div class="children"><div class="content">Just wait until a model outputs escape characters that totally hose your terminal. That&#x27;s the end game right there. That or a zero day worm&#x2F;virus.</div><br/><div id="40411698" class="c"><input type="checkbox" id="c-40411698" checked=""/><div class="controls bullet"><span class="by">lannisterstark</span><span>|</span><a href="#40411551">root</a><span>|</span><a href="#40411626">parent</a><span>|</span><a href="#40411804">next</a><span>|</span><label class="collapse" for="c-40411698">[-]</label><label class="expand" for="c-40411698">[1 more]</label></div><br/><div class="children"><div class="content">Oh no I had to press alt&#x2F;Ctrl+L to reset my terminal not being able to display an escape character.</div><br/></div></div><div id="40411804" class="c"><input type="checkbox" id="c-40411804" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#40411551">root</a><span>|</span><a href="#40411626">parent</a><span>|</span><a href="#40411698">prev</a><span>|</span><a href="#40411648">next</a><span>|</span><label class="collapse" for="c-40411804">[-]</label><label class="expand" for="c-40411804">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why these things should run code in protected sandboxes. Not to do it in a &quot;protected mode&quot; would be negligent.</div><br/></div></div></div></div></div></div><div id="40411648" class="c"><input type="checkbox" id="c-40411648" checked=""/><div class="controls bullet"><span class="by">Joel_Mckay</span><span>|</span><a href="#40411551">prev</a><span>|</span><a href="#40411811">next</a><span>|</span><label class="collapse" for="c-40411648">[-]</label><label class="expand" for="c-40411648">[1 more]</label></div><br/><div class="children"><div class="content">There is also the danger of garnering resentment by plagiarizing LLM nonsense output to fill 78.36% of your page on ethical boundary assertions.<p>Have a nice day.  =)</div><br/></div></div><div id="40411811" class="c"><input type="checkbox" id="c-40411811" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#40411648">prev</a><span>|</span><a href="#40411976">next</a><span>|</span><label class="collapse" for="c-40411811">[-]</label><label class="expand" for="c-40411811">[9 more]</label></div><br/><div class="children"><div class="content">Perfectly obvious what&#x27;s going on here.<p>If they actually believed that their big-linear-algebra programs were going to spontaneously turn into Skynet and eat us all, they wouldn&#x27;t be writing them.<p>Since they are, in fact, writing them, they know that it&#x27;s total bullshit. So what they&#x27;re doing is drumming up fear, uncertainty, and doubt, to aid their lobbying efforts to beg governments to impose a costly regulatory moat to protect their huge VC investment and fleet of GPUs.<p>And it&#x27;s probably going to work. If there&#x27;s one thing politicians like more than huge checks for their slush fund, it&#x27;s handing out sinecures to their friends in the civil service.</div><br/><div id="40412105" class="c"><input type="checkbox" id="c-40412105" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411811">parent</a><span>|</span><a href="#40412310">next</a><span>|</span><label class="collapse" for="c-40412105">[-]</label><label class="expand" for="c-40412105">[1 more]</label></div><br/><div class="children"><div class="content">Many argue that smaller scale models are the only way to learn the things needed to make safer bigger models.<p>Yudkowsky thinks they&#x27;re crazy and will kill us all because it will take <i>decades</i> to solve that problem.<p>Yann LeCun thinks they&#x27;re crazy and AI that potent is <i>decades</i> away and this is much too soon to even bother thinking about the risks.<p>I&#x27;m just hoping the latter is right about AI being  &quot;decades&quot; away, and the former is pessimistic about it taking that long.</div><br/></div></div><div id="40412310" class="c"><input type="checkbox" id="c-40412310" checked=""/><div class="controls bullet"><span class="by">comp_throw7</span><span>|</span><a href="#40411811">parent</a><span>|</span><a href="#40412105">prev</a><span>|</span><a href="#40411988">next</a><span>|</span><label class="collapse" for="c-40412310">[-]</label><label class="expand" for="c-40412310">[1 more]</label></div><br/><div class="children"><div class="content">As much as I wish that were the case, no, unfortunately many people (including leadership) at these organizations assign non-trivial odds of extinction from misaligned superintelligence.  The arguments for why the risk is serious are pretty straightforward and these people are on the record as endorsing them before they e.g. started various AGI labs.<p>Sam Altman: &quot;Development of superhuman machine intelligence (SMI) [1] is probably the greatest threat to the continued existence of humanity. &quot; (<a href="https:&#x2F;&#x2F;blog.samaltman.com&#x2F;machine-intelligence-part-1" rel="nofollow">https:&#x2F;&#x2F;blog.samaltman.com&#x2F;machine-intelligence-part-1</a>, published before he co-founded OpenAI)<p>Dario Amodei: &quot;I think at the extreme end is the Nick Bostrom style of fear that an AGI could destroy humanity. I can’t see any reason and principle why that couldn’t happen.&quot; (<a href="https:&#x2F;&#x2F;80000hours.org&#x2F;podcast&#x2F;episodes&#x2F;the-world-needs-ai-researchers-heres-how-to-become-one&#x2F;" rel="nofollow">https:&#x2F;&#x2F;80000hours.org&#x2F;podcast&#x2F;episodes&#x2F;the-world-needs-ai-r...</a>, published before he co-founded Anthropic)<p>Shane Legg: (responding to &quot;What probability do you assign to the possibility of negative consequences, e.g. human extinction, as a result of badly done AI?&quot;) &quot;...Maybe 5%, maybe 50%. I don&#x27;t think anybody has a good estimate of this.&quot; (<a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;No5JpRCHzBrWA4jmS&#x2F;q-and-a-with-shane-legg-on-risks-from-ai" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;No5JpRCHzBrWA4jmS&#x2F;q-and-a-wi...</a>)<p>Technically Shane&#x27;s quote is from 2011, which is a little bit after Deepmind was founded, but the idea that Shane in 2011 was trying to sow FUD in order to benefit from regulatory capture is... lol.<p>I wish I knew why they think the math pencils out for what they&#x27;re doing, but Sam Altman was not plotting regulatory capture 9 years ago, nearly a year before OpenAI got started.</div><br/></div></div><div id="40411988" class="c"><input type="checkbox" id="c-40411988" checked=""/><div class="controls bullet"><span class="by">drcode</span><span>|</span><a href="#40411811">parent</a><span>|</span><a href="#40412310">prev</a><span>|</span><a href="#40411976">next</a><span>|</span><label class="collapse" for="c-40411988">[-]</label><label class="expand" for="c-40411988">[6 more]</label></div><br/><div class="children"><div class="content">I personally don&#x27;t work on frontier AI because it&#x27;s not safe.<p>Just because other people with poor judgement are building it, that does not make it safe.</div><br/><div id="40412087" class="c"><input type="checkbox" id="c-40412087" checked=""/><div class="controls bullet"><span class="by">worik</span><span>|</span><a href="#40411811">root</a><span>|</span><a href="#40411988">parent</a><span>|</span><a href="#40411976">next</a><span>|</span><label class="collapse" for="c-40412087">[-]</label><label class="expand" for="c-40412087">[5 more]</label></div><br/><div class="children"><div class="content">&gt;  I personally don&#x27;t work on frontier AI because it&#x27;s not safe.<p>In what way?<p>Skynet style robot revolt?</div><br/><div id="40413582" class="c"><input type="checkbox" id="c-40413582" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#40411811">root</a><span>|</span><a href="#40412087">parent</a><span>|</span><a href="#40412113">next</a><span>|</span><label class="collapse" for="c-40413582">[-]</label><label class="expand" for="c-40413582">[1 more]</label></div><br/><div class="children"><div class="content">Yes. Skynet is very dangerous and not safe. In Terminator, humanity is saved because Skynet is dumb, not because Skynet is not dangerous or because Skynet is safe.</div><br/></div></div><div id="40412113" class="c"><input type="checkbox" id="c-40412113" checked=""/><div class="controls bullet"><span class="by">hollerith</span><span>|</span><a href="#40411811">root</a><span>|</span><a href="#40412087">parent</a><span>|</span><a href="#40413582">prev</a><span>|</span><a href="#40412139">next</a><span>|</span><label class="collapse" for="c-40412113">[-]</label><label class="expand" for="c-40412113">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s bad for there to be anything near us that exceeds our (collective) cognitive capabilities unless the human-capability-exceeding thing cares about us, and no one has a good plan for arranging for an AI to care about us even a tiny bit. There are many plans, but most of them are hare-brained and none of them are good or even acceptable.<p>Also: no one knows with any reliability how to tell whether the next big training run will produce an AI that exceeds our cognitive capabilities, so the big training runs should stop now.</div><br/></div></div><div id="40412139" class="c"><input type="checkbox" id="c-40412139" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#40411811">root</a><span>|</span><a href="#40412087">parent</a><span>|</span><a href="#40412113">prev</a><span>|</span><a href="#40412110">next</a><span>|</span><label class="collapse" for="c-40412139">[-]</label><label class="expand" for="c-40412139">[1 more]</label></div><br/><div class="children"><div class="content">Revolts imply them being unhappy.<p>IMO a much bigger risk is them being straight up given a lot of power because we think they  &quot;want&quot; (or at least will do) what we want, but there&#x27;s some tiny difference we don&#x27;t notice until much too late. Even paperclip maximisers are nothing more than that.<p>You know, like basically all software bugs. Except expressed in literally non-comprehensible matrix weights whose behaviour we can only determine by running it rather than source code we can check in advance and make predictions about the performance of.</div><br/></div></div><div id="40412110" class="c"><input type="checkbox" id="c-40412110" checked=""/><div class="controls bullet"><span class="by">worik</span><span>|</span><a href="#40411811">root</a><span>|</span><a href="#40412087">parent</a><span>|</span><a href="#40412139">prev</a><span>|</span><a href="#40411976">next</a><span>|</span><label class="collapse" for="c-40412110">[-]</label><label class="expand" for="c-40412110">[1 more]</label></div><br/><div class="children"><div class="content">I see your video.  <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=K8SUBNPAJnE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=K8SUBNPAJnE</a><p>I am unimpressed because you are using straw men.  A lot of statements and no argument.<p>Have a nice day</div><br/></div></div></div></div></div></div></div></div><div id="40412578" class="c"><input type="checkbox" id="c-40412578" checked=""/><div class="controls bullet"><span class="by">thatsadude</span><span>|</span><a href="#40411976">prev</a><span>|</span><label class="collapse" for="c-40412578">[-]</label><label class="expand" for="c-40412578">[1 more]</label></div><br/><div class="children"><div class="content">20 years from now, the future generation will laugh at how delusional some tech guys think that &quot;text generation could be and end to humanity&quot;.</div><br/></div></div></div></div></div></div></div></body></html>