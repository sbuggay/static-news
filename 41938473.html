<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1729846877298" as="style"/><link rel="stylesheet" href="styles.css?v=1729846877298"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/?_fb_noscript=1">Quantized Llama models with increased speed and a reduced memory footprint</a> <span class="domain">(<a href="https://ai.meta.com">ai.meta.com</a>)</span></div><div class="subtext"><span>egnehots</span> | <span>93 comments</span></div><br/><div><div id="41940358" class="c"><input type="checkbox" id="c-41940358" checked=""/><div class="controls bullet"><span class="by">tveita</span><span>|</span><a href="#41939330">next</a><span>|</span><label class="collapse" for="c-41940358">[-]</label><label class="expand" for="c-41940358">[15 more]</label></div><br/><div class="children"><div class="content">So SpinQuant learns a rotation for activations and weights that, to my understanding, &quot;smear&quot; the outlier weights out so you don&#x27;t get extreme values in any one weight.<p>Random anecdote warning - In the old days, before vector search became AI and everyone and their dog offered a vector database, I had a task that required nearest neighbour search in a decent amount of high-dimensional vectors.<p>I tried quantizing them to bit vectors in an index and scanning through it to get an initial set of candidates.
Performance was actually quite decent - reading through RAM linearly is fast! But the selectivity wasn&#x27;t great.<p>Somewhere along the way I found this paper[1] that iteratively finds a rotation to apply before quantization to reduce the quantization error. Very similar goal to SpinQuant, but focused on bit quantization only.<p>As it turns out the &#x27;random rotation&#x27; baseline they benchmark against worked great for my use case, so I never tried implementing the fancier algorithm. But it&#x27;s a pretty rare day at work that &quot;apply a random rotation matrix to a 128-dimensional vector&quot; is the solution to my problem.<p>[1] <a href="https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;6296665" rel="nofollow">https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;abstract&#x2F;document&#x2F;6296665</a> &#x2F; <a href="https:&#x2F;&#x2F;slazebni.cs.illinois.edu&#x2F;publications&#x2F;ITQ.pdf" rel="nofollow">https:&#x2F;&#x2F;slazebni.cs.illinois.edu&#x2F;publications&#x2F;ITQ.pdf</a></div><br/><div id="41941325" class="c"><input type="checkbox" id="c-41941325" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#41940358">parent</a><span>|</span><a href="#41941741">next</a><span>|</span><label class="collapse" for="c-41941325">[-]</label><label class="expand" for="c-41941325">[11 more]</label></div><br/><div class="children"><div class="content">&gt; But it&#x27;s a pretty rare day at work that &quot;apply a random rotation matrix to a 128-dimensional vector&quot; is the solution to my problem.<p>Funny enough, if you visualize a vector-embedding&#x27;s latent-space features using that &quot;points on the surface of a hypersphere&quot; analogy that ML programmers like to use — and you assume a really low quantization, say, 1-bit — then you can almost picture the hypersphere surface as a black-and-white vector image, the points as arbitrary-precision vector positions where you want to place dots... and your goal as quantizing those positions to reduce the storage costs down to storing a raster bitmap.<p>And <i>that</i> problem has a name: dithering!<p>Oddly enough, for what may or may not be coincidental reasons, what we want in ML terms (keeping the learned associational weights between features constant) is very similar to what we want from the output of image dithering: to not allow the dots to come together to create false features or false voids.<p>And how do we do that? In dithering, we usually apply a set of random perturbations to the vectorized points. Which, for image dithering, just look like translations in 2D space... but, in a higher-dimensional space, might very well best be analytically modelled as rotations about the origin!</div><br/><div id="41943217" class="c"><input type="checkbox" id="c-41943217" checked=""/><div class="controls bullet"><span class="by">arijo</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941325">parent</a><span>|</span><a href="#41941713">next</a><span>|</span><label class="collapse" for="c-41943217">[-]</label><label class="expand" for="c-41943217">[1 more]</label></div><br/><div class="children"><div class="content">Another way to understand dithering is by smearing the frequency spectrum of the original image preventing extreme frequency values to distort the image after quantization - this can be done by applying kernel filters on the original image.<p>Which I think is what is happening with SpinQuant as well - a smoothing of the frequency spectrum of the model weights, confirmed by the smearing of the singular values of the weight matrices.</div><br/></div></div><div id="41941713" class="c"><input type="checkbox" id="c-41941713" checked=""/><div class="controls bullet"><span class="by">eirikbakke</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941325">parent</a><span>|</span><a href="#41943217">prev</a><span>|</span><a href="#41942768">next</a><span>|</span><label class="collapse" for="c-41941713">[-]</label><label class="expand" for="c-41941713">[4 more]</label></div><br/><div class="children"><div class="content">Fascinating! Does that mean you could improve performance further with Floyd–Steinberg dithering? (I.e. instead of rotating randomly, you track accumulated quantization error and add that amount instead.)</div><br/><div id="41941770" class="c"><input type="checkbox" id="c-41941770" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941713">parent</a><span>|</span><a href="#41942768">next</a><span>|</span><label class="collapse" for="c-41941770">[-]</label><label class="expand" for="c-41941770">[3 more]</label></div><br/><div class="children"><div class="content">Floyd-Steinberg etc mostly look better to the human eye, but I&#x27;m not sure in what more &#x27;objective&#x27; sense they would be better than random dithering?</div><br/><div id="41943045" class="c"><input type="checkbox" id="c-41943045" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941770">parent</a><span>|</span><a href="#41942768">next</a><span>|</span><label class="collapse" for="c-41943045">[-]</label><label class="expand" for="c-41943045">[2 more]</label></div><br/><div class="children"><div class="content">Floyd-Steinberg is one sort of quasi-random algorithm, but there are others. People often use quasi-random rather than true randomness when they want to avoid sample points bunching together. They tend to be more evenly distributed. That can get more important in higher-dimension space where it&#x27;s easy to completely miss sampling large volumes because a truly random point set has too many degrees of freedom.</div><br/><div id="41943107" class="c"><input type="checkbox" id="c-41943107" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41943045">parent</a><span>|</span><a href="#41942768">next</a><span>|</span><label class="collapse" for="c-41943107">[-]</label><label class="expand" for="c-41943107">[1 more]</label></div><br/><div class="children"><div class="content">Interesting.<p>What you are describing reminds me of Low discrepancy sequences: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Low-discrepancy_sequence" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Low-discrepancy_sequence</a><p>Though these methods have their problems and blind-spots, too, and are often outdone by random sampling with even slightly higher sample count, while preserving all the simplicity and (statistical) guarantees you get from randomness.</div><br/></div></div></div></div></div></div></div></div><div id="41942768" class="c"><input type="checkbox" id="c-41942768" checked=""/><div class="controls bullet"><span class="by">127</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941325">parent</a><span>|</span><a href="#41941713">prev</a><span>|</span><a href="#41941813">next</a><span>|</span><label class="collapse" for="c-41942768">[-]</label><label class="expand" for="c-41942768">[1 more]</label></div><br/><div class="children"><div class="content">The best type of dithering is done with error diffusion. There&#x27;s a convolutional kernel the diffuses the error over multiple adjacent data points.</div><br/></div></div><div id="41941813" class="c"><input type="checkbox" id="c-41941813" checked=""/><div class="controls bullet"><span class="by">arijo</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941325">parent</a><span>|</span><a href="#41942768">prev</a><span>|</span><a href="#41941363">next</a><span>|</span><label class="collapse" for="c-41941813">[-]</label><label class="expand" for="c-41941813">[1 more]</label></div><br/><div class="children"><div class="content">Seems really intriguing could you help me grok how this random perturbations of the points of the hypersphere surface are related to smearing the model weights?</div><br/></div></div><div id="41941363" class="c"><input type="checkbox" id="c-41941363" checked=""/><div class="controls bullet"><span class="by">digdugdirk</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941325">parent</a><span>|</span><a href="#41941813">prev</a><span>|</span><a href="#41941741">next</a><span>|</span><label class="collapse" for="c-41941363">[-]</label><label class="expand" for="c-41941363">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry, I don&#x27;t understand the language you&#x27;re speaking. English please?<p>(Just kidding - but if you have any recommendations for learning resources to get started being able to understand what you&#x27;re talking about, I&#x27;d greatly appreciate it.)</div><br/><div id="41941689" class="c"><input type="checkbox" id="c-41941689" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941363">parent</a><span>|</span><a href="#41941741">next</a><span>|</span><label class="collapse" for="c-41941689">[-]</label><label class="expand" for="c-41941689">[2 more]</label></div><br/><div class="children"><div class="content">Rabbit hole ahoy:<p><a href="https:&#x2F;&#x2F;surma.dev&#x2F;things&#x2F;ditherpunk&#x2F;" rel="nofollow">https:&#x2F;&#x2F;surma.dev&#x2F;things&#x2F;ditherpunk&#x2F;</a></div><br/><div id="41941810" class="c"><input type="checkbox" id="c-41941810" checked=""/><div class="controls bullet"><span class="by">sbierwagen</span><span>|</span><a href="#41940358">root</a><span>|</span><a href="#41941689">parent</a><span>|</span><a href="#41941741">next</a><span>|</span><label class="collapse" for="c-41941810">[-]</label><label class="expand" for="c-41941810">[1 more]</label></div><br/><div class="children"><div class="content">One of the fun things about signals theory is how the same basic concept will show up in apparently unrelated places.<p>Example from electrical engineering: microprocessors will have a &quot;clock&quot; frequency, say, 16Mhz. But when you haul a wire up to VCC and pull it back down to ground, some amount of the power will be radiated away as radio waves. If your clock is at a constant rate, then you&#x27;ll have a big spike of radiated noise at 16MHz, and the FCC will be unhappy.<p>So modern devices cheat it by dithering around the central frequency. If you bounce from 15.9998MHz to 16.001 to 15.998 then the same <i>amount</i> of power will be radiated, but smeared across a bigger frequency, enough to get you lower than the regulatory threshold. Spread spectrum clock generation. <a href="https:&#x2F;&#x2F;www.analog.com&#x2F;en&#x2F;resources&#x2F;technical-articles&#x2F;clock-generation-with-spread-spectrum.html" rel="nofollow">https:&#x2F;&#x2F;www.analog.com&#x2F;en&#x2F;resources&#x2F;technical-articles&#x2F;clock...</a><p>If you look in your PC&#x27;s BIOS settings, spread spectrum is usually an option, and you can disable it if you want your computer to be slightly noisier.</div><br/></div></div></div></div></div></div></div></div><div id="41941741" class="c"><input type="checkbox" id="c-41941741" checked=""/><div class="controls bullet"><span class="by">ninja3925</span><span>|</span><a href="#41940358">parent</a><span>|</span><a href="#41941325">prev</a><span>|</span><a href="#41940862">next</a><span>|</span><label class="collapse" for="c-41941741">[-]</label><label class="expand" for="c-41941741">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly, FAISS does exactly that before doing Product Quantization and it works very well (errors are much lower compared to no rotation). They call it “optimal PQ”. During training time, they iterate to find a good candidate and save the best one.<p>Perhaps not entirely coincidentally, FAISS is also maintained by FB.<p><a href="https:&#x2F;&#x2F;faiss.ai&#x2F;cpp_api&#x2F;struct&#x2F;structfaiss_1_1OPQMatrix.html" rel="nofollow">https:&#x2F;&#x2F;faiss.ai&#x2F;cpp_api&#x2F;struct&#x2F;structfaiss_1_1OPQMatrix.htm...</a></div><br/></div></div><div id="41940862" class="c"><input type="checkbox" id="c-41940862" checked=""/><div class="controls bullet"><span class="by">arijo</span><span>|</span><a href="#41940358">parent</a><span>|</span><a href="#41941741">prev</a><span>|</span><a href="#41940931">next</a><span>|</span><label class="collapse" for="c-41940862">[-]</label><label class="expand" for="c-41940862">[1 more]</label></div><br/><div class="children"><div class="content">I find the geometrical intuition of rotating a vector in high dimensional space to minimize its largest values (vector basis projections) beautiful.<p>I&#x27;m no expert and I&#x27;m sure this has been tried by many people already - but would it be possible to reduce the computational effort instead by using SVD decomposition, spreading the singular values and then reapplying the original singular values and recomposing the matrix using the quantized versions of the SVD matrices?</div><br/></div></div><div id="41940931" class="c"><input type="checkbox" id="c-41940931" checked=""/><div class="controls bullet"><span class="by">govg</span><span>|</span><a href="#41940358">parent</a><span>|</span><a href="#41940862">prev</a><span>|</span><a href="#41939330">next</a><span>|</span><label class="collapse" for="c-41940931">[-]</label><label class="expand" for="c-41940931">[1 more]</label></div><br/><div class="children"><div class="content">Tangentially related to the idea of &quot;apply a random rotation matrix&quot; is one where you apply a random matrix to a set of points to preserve distances between them but transform them into a lower dimensional space. This is captured by the JL Lemma [1].<p>[1] - <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstrauss_lemma" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Johnson%E2%80%93Lindenstrauss_...</a></div><br/></div></div></div></div><div id="41939330" class="c"><input type="checkbox" id="c-41939330" checked=""/><div class="controls bullet"><span class="by">nisten</span><span>|</span><a href="#41940358">prev</a><span>|</span><a href="#41939589">next</a><span>|</span><label class="collapse" for="c-41939330">[-]</label><label class="expand" for="c-41939330">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty interesting that the new SpinQuant method did not manage to be better than good old nf4bit QLORA training (Tim Dettmers really cooked with that one).<p>Really appreciate that Meta published both results+model quants and didn&#x27;t just make some bs claim about a new sota quant like most other bigger companies would&#x27;ve done.</div><br/><div id="41942160" class="c"><input type="checkbox" id="c-41942160" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41939330">parent</a><span>|</span><a href="#41940657">next</a><span>|</span><label class="collapse" for="c-41942160">[-]</label><label class="expand" for="c-41942160">[1 more]</label></div><br/><div class="children"><div class="content">The naming is unfortunate but in this blog QLoRA is referring to Quantization-Aware Training with LoRA adaptor</div><br/></div></div><div id="41940657" class="c"><input type="checkbox" id="c-41940657" checked=""/><div class="controls bullet"><span class="by">Aeolun</span><span>|</span><a href="#41939330">parent</a><span>|</span><a href="#41942160">prev</a><span>|</span><a href="#41942314">next</a><span>|</span><label class="collapse" for="c-41940657">[-]</label><label class="expand" for="c-41940657">[3 more]</label></div><br/><div class="children"><div class="content">It’s a little bizarre that I feel like I’m actually starting to respect this little bit of Meta…</div><br/><div id="41940980" class="c"><input type="checkbox" id="c-41940980" checked=""/><div class="controls bullet"><span class="by">FuckButtons</span><span>|</span><a href="#41939330">root</a><span>|</span><a href="#41940657">parent</a><span>|</span><a href="#41942314">next</a><span>|</span><label class="collapse" for="c-41940980">[-]</label><label class="expand" for="c-41940980">[2 more]</label></div><br/><div class="children"><div class="content">I think meta and facebook before it have always valued a very high standard of engineering, and have also been generally pretty good about open sourcing a lot of that work in a way that allows a lot of people to work with their tools. This doesn’t seem all that out of character.</div><br/><div id="41941155" class="c"><input type="checkbox" id="c-41941155" checked=""/><div class="controls bullet"><span class="by">ipaddr</span><span>|</span><a href="#41939330">root</a><span>|</span><a href="#41940980">parent</a><span>|</span><a href="#41942314">next</a><span>|</span><label class="collapse" for="c-41941155">[-]</label><label class="expand" for="c-41941155">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a huge company with a lot of different voices.  One may create react and open source it while another would add a clause that if you sue facebook over anything your react license disappears.  When they are good they are really good.</div><br/></div></div></div></div></div></div><div id="41942314" class="c"><input type="checkbox" id="c-41942314" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#41939330">parent</a><span>|</span><a href="#41940657">prev</a><span>|</span><a href="#41941192">next</a><span>|</span><label class="collapse" for="c-41942314">[-]</label><label class="expand" for="c-41942314">[1 more]</label></div><br/><div class="children"><div class="content">Those are different approaches afaict.</div><br/></div></div><div id="41941192" class="c"><input type="checkbox" id="c-41941192" checked=""/><div class="controls bullet"><span class="by">miven</span><span>|</span><a href="#41939330">parent</a><span>|</span><a href="#41942314">prev</a><span>|</span><a href="#41939589">next</a><span>|</span><label class="collapse" for="c-41941192">[-]</label><label class="expand" for="c-41941192">[1 more]</label></div><br/><div class="children"><div class="content">I mean, it&#x27;s no free lunch, you still need to expend significantly more compute for the QLoRA training compared to any usual PTQ method, be it SpinQuant or any other more conventional quantization approaches.</div><br/></div></div></div></div><div id="41939589" class="c"><input type="checkbox" id="c-41939589" checked=""/><div class="controls bullet"><span class="by">theanonymousone</span><span>|</span><a href="#41939330">prev</a><span>|</span><a href="#41940556">next</a><span>|</span><label class="collapse" for="c-41939589">[-]</label><label class="expand" for="c-41939589">[19 more]</label></div><br/><div class="children"><div class="content">May I ask if anyone has successfully used 1B and 3B models in production and if yes, in what use cases? I seem to be failing even in seemingly simpler tasks such as word translation or zero-shot classification. For example, they seem to not care about instructions to only write a response and no explanation, thus making it impossible to use them in a pipeline :&#x2F;</div><br/><div id="41940038" class="c"><input type="checkbox" id="c-41940038" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41939668">next</a><span>|</span><label class="collapse" for="c-41940038">[-]</label><label class="expand" for="c-41940038">[5 more]</label></div><br/><div class="children"><div class="content">3B models are perfectly capable, I&#x27;ve had great luck with Phi 3.5.<p>&gt;  For example, they seem to not care about instructions to only write a response and no explanation<p>You need to use tools to force the model to adhere to a schema. Or you can learn to parse out the part of the response you want, both work.<p>You&#x27;ll also need to make good use of robust examples in your initial prompt, and give lots of examples of how you want the output to look. (Yes this quickly burns up the limited context length!)<p>Finally, embrace the fact that these models are tuned for chat, so the more conversational you make the back and forth the less you are stretching the models abilities.<p>I wrote a very small blog post at <a href="https:&#x2F;&#x2F;meanderingthoughts.hashnode.dev&#x2F;unlock-the-full-potential-of-llms-ditch-json-for-dsls" rel="nofollow">https:&#x2F;&#x2F;meanderingthoughts.hashnode.dev&#x2F;unlock-the-full-pote...</a> explaining some of this.</div><br/><div id="41940371" class="c"><input type="checkbox" id="c-41940371" checked=""/><div class="controls bullet"><span class="by">teleforce</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41940038">parent</a><span>|</span><a href="#41939668">next</a><span>|</span><label class="collapse" for="c-41940371">[-]</label><label class="expand" for="c-41940371">[4 more]</label></div><br/><div class="children"><div class="content">I wonder if CUE can help the situation in similar fashion to the DSL methods that you&#x27;ve described in your blog post [1]. After all CUE fundamentals are based on feature structure from the deterministic approach of NLP unlike LLM that&#x27;s stochastic NLP [2],[3]. Perhaps deterministic and non-deterministic approaches is the potent combination that can effectively help reduce much of the footprint to get to the same results and being energy efficient in the process.<p>[1] Cue – A language for defining, generating, and validating data:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20847943">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20847943</a><p>[2] Feature structure:<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Feature_structure" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Feature_structure</a><p>[3] The Logic of CUE:<p><a href="https:&#x2F;&#x2F;cuelang.org&#x2F;docs&#x2F;concept&#x2F;the-logic-of-cue&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cuelang.org&#x2F;docs&#x2F;concept&#x2F;the-logic-of-cue&#x2F;</a></div><br/><div id="41940431" class="c"><input type="checkbox" id="c-41940431" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41940371">parent</a><span>|</span><a href="#41939668">next</a><span>|</span><label class="collapse" for="c-41940431">[-]</label><label class="expand" for="c-41940431">[3 more]</label></div><br/><div class="children"><div class="content">On my LinkedIn post about this topic someone actually replied with a superior method of steering LLM output compared to anything else I&#x27;ve ever heard of, so I&#x27;ve decided that until I find time to implement their method, I&#x27;m not going to worry about things.<p>tl;dr you put into the prompt all the JSON up until what you want the LLM to say, and you set the stop token to the end token of the current JSON item (so &#x27;,&#x27; or &#x27;}&#x27; &#x27;]&#x27;, whatever) and you then your code fills out the rest of the JSON syntax up until another LLM generated value is needed.<p>I hope that makes sense.<p>It is super cool, and I am pretty sure there is a way to make a generator that takes in an arbitrary JSON schema and builds a state machine to do the above.<p>The performance should be super fast on locally hosted models that are using context caching.<p>Eh I should write this up as a blog post, hope someone else implements it, and if not, just do it myself.</div><br/><div id="41940752" class="c"><input type="checkbox" id="c-41940752" checked=""/><div class="controls bullet"><span class="by">shawnz</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41940431">parent</a><span>|</span><a href="#41939668">next</a><span>|</span><label class="collapse" for="c-41940752">[-]</label><label class="expand" for="c-41940752">[2 more]</label></div><br/><div class="children"><div class="content">There are many solutions for constrained&#x2F;structured generation with LLMs these days, here is a blog post my employer published about this a while back: <a href="https:&#x2F;&#x2F;monadical.com&#x2F;posts&#x2F;how-to-make-llms-speak-your-language.html" rel="nofollow">https:&#x2F;&#x2F;monadical.com&#x2F;posts&#x2F;how-to-make-llms-speak-your-lang...</a><p>I&#x27;m partial to Outlines lately, but they all have various upsides and downsides.<p>OpenAI even natively added support for this on their platform recently: <a href="https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;introducing-structured-outputs-in-the-api&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;introducing-structured-outputs-in-t...</a></div><br/><div id="41941672" class="c"><input type="checkbox" id="c-41941672" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41940752">parent</a><span>|</span><a href="#41939668">next</a><span>|</span><label class="collapse" for="c-41941672">[-]</label><label class="expand" for="c-41941672">[1 more]</label></div><br/><div class="children"><div class="content">This is a really good post. I did find one error, Instructor works well with at least one other back end (Ollama).<p>Outlines looks quite interesting but I wasn&#x27;t able to get it to work reliably.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41939668" class="c"><input type="checkbox" id="c-41939668" checked=""/><div class="controls bullet"><span class="by">wswope</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41940038">prev</a><span>|</span><a href="#41940202">next</a><span>|</span><label class="collapse" for="c-41939668">[-]</label><label class="expand" for="c-41939668">[1 more]</label></div><br/><div class="children"><div class="content">I’ve only toyed with them a bit, and had a similar experience - but did find I got better output by forcing them to adhere to a fixed grammar: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;grammars">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;grammars</a><p>For context, I was playing with a script to bulk download podcasts, transcribe with whisper, pass the transcription to llama.cpp to ID ads, then slice the ads out with ffmpeg. I started with the generic json_array example grammar, then iteratively tweaked it.</div><br/></div></div><div id="41940202" class="c"><input type="checkbox" id="c-41940202" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41939668">prev</a><span>|</span><a href="#41940328">next</a><span>|</span><label class="collapse" for="c-41940202">[-]</label><label class="expand" for="c-41940202">[2 more]</label></div><br/><div class="children"><div class="content">For me, it was almost random if I would get a little spiel at the beginning of my response - even on the unquantized 8b instruct. Since ollama doesn’t support grammars, I was trying to get it to work where I had a prompt that summarized an article and extracted and classified certain information that I requested. Then I had another prompt that would digest the summary and spit out a structured JSON output. It was much better than trying to do it in one prompt, but still far too random even with temperature at 0. Sometimes the first prompt misclassified things. Sometimes the second prompt would include a “here’s your structured output”.<p>And Claude did everything perfectly ;)</div><br/><div id="41943441" class="c"><input type="checkbox" id="c-41943441" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41940202">parent</a><span>|</span><a href="#41940328">next</a><span>|</span><label class="collapse" for="c-41943441">[-]</label><label class="expand" for="c-41943441">[1 more]</label></div><br/><div class="children"><div class="content">Why not preprompt with ```json {</div><br/></div></div></div></div><div id="41940328" class="c"><input type="checkbox" id="c-41940328" checked=""/><div class="controls bullet"><span class="by">scriptsmith</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41940202">prev</a><span>|</span><a href="#41939835">next</a><span>|</span><label class="collapse" for="c-41940328">[-]</label><label class="expand" for="c-41940328">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I&#x27;ve used the v3.2 3B-Instruct model in a Slack app. Specifically using vLLM, with a template: <a href="https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm&#x2F;blob&#x2F;main&#x2F;examples&#x2F;tool_chat_template_llama3.2_json.jinja">https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm&#x2F;blob&#x2F;main&#x2F;examples&#x2F;tool...</a><p>Works as expected if you provide a few system prompts with context.</div><br/></div></div><div id="41939835" class="c"><input type="checkbox" id="c-41939835" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41940328">prev</a><span>|</span><a href="#41942328">next</a><span>|</span><label class="collapse" for="c-41939835">[-]</label><label class="expand" for="c-41939835">[3 more]</label></div><br/><div class="children"><div class="content">Not in production, but I&#x27;ve used a 3B model to test a local LLM application I&#x27;m working on. I needed a full end-to-end request&#x2F;response and it&#x27;s a lot faster asking a 3B model than an 8B model. I could setup a test harness and replay the responses... but this was a lot simpler.</div><br/><div id="41940128" class="c"><input type="checkbox" id="c-41940128" checked=""/><div class="controls bullet"><span class="by">jdthedisciple</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41939835">parent</a><span>|</span><a href="#41942328">next</a><span>|</span><label class="collapse" for="c-41940128">[-]</label><label class="expand" for="c-41940128">[2 more]</label></div><br/><div class="children"><div class="content">If for testing then why not just mock the whole thing for ultimate performance ... ?</div><br/><div id="41940167" class="c"><input type="checkbox" id="c-41940167" checked=""/><div class="controls bullet"><span class="by">nkozyra</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41940128">parent</a><span>|</span><a href="#41942328">next</a><span>|</span><label class="collapse" for="c-41940167">[-]</label><label class="expand" for="c-41940167">[1 more]</label></div><br/><div class="children"><div class="content">Probably faster to use off the shelf model with llama.cpp than to mock it</div><br/></div></div></div></div></div></div><div id="41942328" class="c"><input type="checkbox" id="c-41942328" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41939835">prev</a><span>|</span><a href="#41940125">next</a><span>|</span><label class="collapse" for="c-41942328">[-]</label><label class="expand" for="c-41942328">[3 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t expect a 1B model to perform as well as 7B or chatGPT, probably the best use case is speculative decoding or to use to fine tune for a specific use case.</div><br/><div id="41942663" class="c"><input type="checkbox" id="c-41942663" checked=""/><div class="controls bullet"><span class="by">theanonymousone</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41942328">parent</a><span>|</span><a href="#41940125">next</a><span>|</span><label class="collapse" for="c-41942663">[-]</label><label class="expand" for="c-41942663">[2 more]</label></div><br/><div class="children"><div class="content">What is &quot;speculative decoding&quot;?</div><br/><div id="41943068" class="c"><input type="checkbox" id="c-41943068" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#41939589">root</a><span>|</span><a href="#41942663">parent</a><span>|</span><a href="#41940125">next</a><span>|</span><label class="collapse" for="c-41943068">[-]</label><label class="expand" for="c-41943068">[1 more]</label></div><br/><div class="children"><div class="content">Speculative decoding is using a small model to quickly generate a sequence that every so often you pass through a larger model to check and correct. It can be much faster than just using the larger model, with tolerably close accuracy.</div><br/></div></div></div></div></div></div><div id="41940125" class="c"><input type="checkbox" id="c-41940125" checked=""/><div class="controls bullet"><span class="by">JohnHammersley</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41942328">prev</a><span>|</span><a href="#41941603">next</a><span>|</span><label class="collapse" for="c-41940125">[-]</label><label class="expand" for="c-41940125">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For example, they seem to not care about instructions to only write a response and no explanation, thus making it impossible to use them in a pipeline<p>I was doing some local tidying up of recording transcripts, using a fairly long system prompt, and I saw the same behaviour you mention if the transcript I was passing in was too long -- batching it up to make sure to be under the max length prevented this.<p>Might not be what&#x27;s happening in your case, but I mention it because it wasn&#x27;t immediately obvious to me when I first saw the behaviour.</div><br/></div></div><div id="41941603" class="c"><input type="checkbox" id="c-41941603" checked=""/><div class="controls bullet"><span class="by">nikolayasdf123</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41940125">prev</a><span>|</span><a href="#41940410">next</a><span>|</span><label class="collapse" for="c-41941603">[-]</label><label class="expand" for="c-41941603">[1 more]</label></div><br/><div class="children"><div class="content">+1 1B and 3B models perform so poorly, it is bellow any acceptance for us. and we have fairly simple natural language understanding.</div><br/></div></div><div id="41940410" class="c"><input type="checkbox" id="c-41940410" checked=""/><div class="controls bullet"><span class="by">bloomingkales</span><span>|</span><a href="#41939589">parent</a><span>|</span><a href="#41941603">prev</a><span>|</span><a href="#41940556">next</a><span>|</span><label class="collapse" for="c-41940410">[-]</label><label class="expand" for="c-41940410">[1 more]</label></div><br/><div class="children"><div class="content">Qwen2.5 3b is very very good.</div><br/></div></div></div></div><div id="41940556" class="c"><input type="checkbox" id="c-41940556" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41939589">prev</a><span>|</span><a href="#41939204">next</a><span>|</span><label class="collapse" for="c-41940556">[-]</label><label class="expand" for="c-41940556">[9 more]</label></div><br/><div class="children"><div class="content">Hi I&#x27;m Mark I work on torchao which was used for the quantization aware training and ARM kernels in this blog. If you have any questions about quantization or performance more generally feel free to let me know!</div><br/><div id="41940616" class="c"><input type="checkbox" id="c-41940616" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#41940556">parent</a><span>|</span><a href="#41942068">next</a><span>|</span><label class="collapse" for="c-41940616">[-]</label><label class="expand" for="c-41940616">[4 more]</label></div><br/><div class="children"><div class="content">What was the &quot;vanilla post-training quantization&quot; used for comparison? There are 22 GGUF quantization variants smaller than 16 bits per weight and I can&#x27;t tell which one is being compared with:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;hub&#x2F;en&#x2F;gguf#quantization-types" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;hub&#x2F;en&#x2F;gguf#quantization-types</a><p>It might even mean a non-GGUF quantization scheme; I&#x27;m just an intermediate user of local models, not an expert user or developer.</div><br/><div id="41940712" class="c"><input type="checkbox" id="c-41940712" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41940556">root</a><span>|</span><a href="#41940616">parent</a><span>|</span><a href="#41942068">next</a><span>|</span><label class="collapse" for="c-41940712">[-]</label><label class="expand" for="c-41940712">[3 more]</label></div><br/><div class="children"><div class="content">So this should be referring to w8a8 (weights and activations in 8 bit)<p>So this is gonna be 8 bit weights, 8 bit activations, group size of 256, symmetric quantization. Not sure how to map this to the GGUF variants because they don&#x27;t mention how they don&#x27;t do activation quantization</div><br/><div id="41942422" class="c"><input type="checkbox" id="c-41942422" checked=""/><div class="controls bullet"><span class="by">imjonse</span><span>|</span><a href="#41940556">root</a><span>|</span><a href="#41940712">parent</a><span>|</span><a href="#41942068">next</a><span>|</span><label class="collapse" for="c-41942422">[-]</label><label class="expand" for="c-41942422">[2 more]</label></div><br/><div class="children"><div class="content">Were there comparisons made to AWS, Smoothquant, GPTQ or other non-vanilla PTQ methods? Thanks.</div><br/><div id="41942578" class="c"><input type="checkbox" id="c-41942578" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41940556">root</a><span>|</span><a href="#41942422">parent</a><span>|</span><a href="#41942068">next</a><span>|</span><label class="collapse" for="c-41942578">[-]</label><label class="expand" for="c-41942578">[1 more]</label></div><br/><div class="children"><div class="content">Not that I know of for this study, at least for the specific scope torchao we want to make it easier for researchers to create new quantization algorithms in python and have those algorithms run fast and you can see a lot of those algorithms here <a href="https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;ao&#x2F;tree&#x2F;main&#x2F;torchao&#x2F;prototype">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;ao&#x2F;tree&#x2F;main&#x2F;torchao&#x2F;prototype</a><p>So for example for AWQ and GPTQ we can accelerate them by using a fast int4 kernel called tinygemm</div><br/></div></div></div></div></div></div></div></div><div id="41942068" class="c"><input type="checkbox" id="c-41942068" checked=""/><div class="controls bullet"><span class="by">Evidlo</span><span>|</span><a href="#41940556">parent</a><span>|</span><a href="#41940616">prev</a><span>|</span><a href="#41941963">next</a><span>|</span><label class="collapse" for="c-41942068">[-]</label><label class="expand" for="c-41942068">[2 more]</label></div><br/><div class="children"><div class="content">I have a non-ML question.<p>In vanilla Pytorch I have the following expression:<p><pre><code>    t.sum(values[inds] * weights)
</code></pre>
If &#x27;inds&#x27; is int8, I get &quot;IndexError: tensors used as indices must be long, int, byte or bool tensors&quot;.<p>Is this still true if I use torchao?</div><br/><div id="41942599" class="c"><input type="checkbox" id="c-41942599" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41940556">root</a><span>|</span><a href="#41942068">parent</a><span>|</span><a href="#41941963">next</a><span>|</span><label class="collapse" for="c-41942599">[-]</label><label class="expand" for="c-41942599">[1 more]</label></div><br/><div class="children"><div class="content">The issue here is memory in PyTorch is byte addressable and that&#x27;s a limitation we can&#x27;t solve without making a lot more changes to PyTorch. But in your specific case, if you&#x27;d like to pack more data into `values` you can use a combination of clever bit shifting, torch.cat and other bit twiddling pytorch like ops to pack more data. It&#x27;s a trick we use quite heavily in torchao</div><br/></div></div></div></div><div id="41941963" class="c"><input type="checkbox" id="c-41941963" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41940556">parent</a><span>|</span><a href="#41942068">prev</a><span>|</span><a href="#41939204">next</a><span>|</span><label class="collapse" for="c-41941963">[-]</label><label class="expand" for="c-41941963">[2 more]</label></div><br/><div class="children"><div class="content">Do you ever pronounce torchao in a way that rhymes with &quot;wow&quot;</div><br/><div id="41942018" class="c"><input type="checkbox" id="c-41942018" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41940556">root</a><span>|</span><a href="#41941963">parent</a><span>|</span><a href="#41939204">next</a><span>|</span><label class="collapse" for="c-41942018">[-]</label><label class="expand" for="c-41942018">[1 more]</label></div><br/><div class="children"><div class="content">My wife calls it torch AAAW</div><br/></div></div></div></div></div></div><div id="41939204" class="c"><input type="checkbox" id="c-41939204" checked=""/><div class="controls bullet"><span class="by">philipkglass</span><span>|</span><a href="#41940556">prev</a><span>|</span><a href="#41942095">next</a><span>|</span><label class="collapse" for="c-41939204">[-]</label><label class="expand" for="c-41939204">[1 more]</label></div><br/><div class="children"><div class="content">These quantized models show much less degradation compared to a &quot;vanilla post-training-quantization&quot; but there are a bunch of PTQ schemes that people have already applied to Llama models [1]. I didn&#x27;t see any details about the vanilla PTQ they used as a baseline. Has it been written about elsewhere?<p>[1] <a href="https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;llama3.2&#x2F;tags">https:&#x2F;&#x2F;ollama.com&#x2F;library&#x2F;llama3.2&#x2F;tags</a></div><br/></div></div><div id="41942095" class="c"><input type="checkbox" id="c-41942095" checked=""/><div class="controls bullet"><span class="by">Evidlo</span><span>|</span><a href="#41939204">prev</a><span>|</span><a href="#41940495">next</a><span>|</span><label class="collapse" for="c-41942095">[-]</label><label class="expand" for="c-41942095">[2 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t they actually say what the size of the model is in GB?<p>That and average inference times on common hardware is what I&#x27;m curious about.</div><br/><div id="41942261" class="c"><input type="checkbox" id="c-41942261" checked=""/><div class="controls bullet"><span class="by">Ardren</span><span>|</span><a href="#41942095">parent</a><span>|</span><a href="#41940495">next</a><span>|</span><label class="collapse" for="c-41942261">[-]</label><label class="expand" for="c-41942261">[1 more]</label></div><br/><div class="children"><div class="content">The last table shows memory usage and performance on an Android phone.<p>&gt; Decode latency improved by 2.5x and prefill latency improved by 4.2x on average, while model size decreased by 56% and memory usage reduced by 41% on average. The benchmarks can be reproducible today via ExecuTorch Llama instructions. The table above shows results using an Android OnePlus 12 device—however, we’ve also verified similar relative performance on Samsung S24+ for 1B and 3B and Samsung S22 for 1B.</div><br/></div></div></div></div><div id="41940495" class="c"><input type="checkbox" id="c-41940495" checked=""/><div class="controls bullet"><span class="by">ed</span><span>|</span><a href="#41942095">prev</a><span>|</span><a href="#41941597">next</a><span>|</span><label class="collapse" for="c-41940495">[-]</label><label class="expand" for="c-41940495">[1 more]</label></div><br/><div class="children"><div class="content">Oh cool! I’ve been playing with quantized llama 3B for the last week. (4-bit spinquant). The code for spinquant has been public for a bit.<p>It’s pretty adept at most natural language tasks (“summarize this”) and performance on iPhone is usable. It’s even decent at tool once you get the chat template right.<p>But it struggles with json and html syntax (correctly escaping characters), and isn’t great at planning, which makes it a bad fit for most agenetic uses.<p>My plan was to let llama communicate with more advanced AI’s, using natural language to offload tool use to them, but very quickly llama goes rogue and starts doing things you didn’t ask it to, like trying to delete data.<p>Still - the progress Meta has made here is incredible and it seems we’ll have capable on-device agents in the next generation or two.</div><br/></div></div><div id="41941597" class="c"><input type="checkbox" id="c-41941597" checked=""/><div class="controls bullet"><span class="by">nikolayasdf123</span><span>|</span><a href="#41940495">prev</a><span>|</span><a href="#41942652">next</a><span>|</span><label class="collapse" for="c-41941597">[-]</label><label class="expand" for="c-41941597">[1 more]</label></div><br/><div class="children"><div class="content">what&#x27;s your opinion on LlamaStack?<p>for me it is nothing short of bad experience. it is way over-engineered with poor quality and just plain does not work, and maintainers are questionable. I would rather call HuggingFace python code for inference or anything else.<p>is ExecuTorch any better?</div><br/></div></div><div id="41942652" class="c"><input type="checkbox" id="c-41942652" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#41941597">prev</a><span>|</span><a href="#41940079">next</a><span>|</span><label class="collapse" for="c-41942652">[-]</label><label class="expand" for="c-41942652">[1 more]</label></div><br/><div class="children"><div class="content">From TFA:<p>&gt; <i>At Connect 2024 last month, we open sourced Llama 3.2 1B and 3B</i><p>No you did not. There is no source (in this case: training data) included. Stop changing the meaning of &quot;open source&quot;, Meta!</div><br/></div></div><div id="41940079" class="c"><input type="checkbox" id="c-41940079" checked=""/><div class="controls bullet"><span class="by">justanotheratom</span><span>|</span><a href="#41942652">prev</a><span>|</span><a href="#41939571">next</a><span>|</span><label class="collapse" for="c-41940079">[-]</label><label class="expand" for="c-41940079">[1 more]</label></div><br/><div class="children"><div class="content">Any pointers no how to finetune this on my dataset and package and run it in my swift ios app?</div><br/></div></div><div id="41939571" class="c"><input type="checkbox" id="c-41939571" checked=""/><div class="controls bullet"><span class="by">EliBullockPapa</span><span>|</span><a href="#41940079">prev</a><span>|</span><a href="#41940072">next</a><span>|</span><label class="collapse" for="c-41939571">[-]</label><label class="expand" for="c-41939571">[7 more]</label></div><br/><div class="children"><div class="content">Anyone know a nice iOS app to run these locally?</div><br/><div id="41939828" class="c"><input type="checkbox" id="c-41939828" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41939571">parent</a><span>|</span><a href="#41940483">next</a><span>|</span><label class="collapse" for="c-41939828">[-]</label><label class="expand" for="c-41939828">[2 more]</label></div><br/><div class="children"><div class="content">MLC Chat is a great iPhone app for running models (it&#x27;s on Android too) and currently ships with Llama 3.2 3B Instruct - not the version Meta released today, its a quantized version of their previous release.<p>I wouldn&#x27;t be surprised to see it add the new ones shortly, it&#x27;s quite actively maintained.<p><a href="https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;mlc-chat&#x2F;id6448482937" rel="nofollow">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;mlc-chat&#x2F;id6448482937</a></div><br/><div id="41941255" class="c"><input type="checkbox" id="c-41941255" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41939571">root</a><span>|</span><a href="#41939828">parent</a><span>|</span><a href="#41940483">next</a><span>|</span><label class="collapse" for="c-41941255">[-]</label><label class="expand" for="c-41941255">[1 more]</label></div><br/><div class="children"><div class="content">Seems much more stable than the last time I tried it too</div><br/></div></div></div></div><div id="41940483" class="c"><input type="checkbox" id="c-41940483" checked=""/><div class="controls bullet"><span class="by">drilbo</span><span>|</span><a href="#41939571">parent</a><span>|</span><a href="#41939828">prev</a><span>|</span><a href="#41939702">next</a><span>|</span><label class="collapse" for="c-41940483">[-]</label><label class="expand" for="c-41940483">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;a-ghorbani&#x2F;pocketpal-ai">https:&#x2F;&#x2F;github.com&#x2F;a-ghorbani&#x2F;pocketpal-ai</a><p>This was just recently open sourced and is pretty nice. Only issue I&#x27;ve had is very minor UI stuff (on Android, sounds like it runs better on iOS from skimming comments)</div><br/></div></div><div id="41939702" class="c"><input type="checkbox" id="c-41939702" checked=""/><div class="controls bullet"><span class="by">Arcuru</span><span>|</span><a href="#41939571">parent</a><span>|</span><a href="#41940483">prev</a><span>|</span><a href="#41940878">next</a><span>|</span><label class="collapse" for="c-41939702">[-]</label><label class="expand" for="c-41939702">[1 more]</label></div><br/><div class="children"><div class="content">I access them by running the models in Ollama (on my own hardware), and then using my app Chaz[1] to access it through my normal Matrix client.<p>[1] - <a href="https:&#x2F;&#x2F;github.com&#x2F;arcuru&#x2F;chaz">https:&#x2F;&#x2F;github.com&#x2F;arcuru&#x2F;chaz</a></div><br/></div></div><div id="41940878" class="c"><input type="checkbox" id="c-41940878" checked=""/><div class="controls bullet"><span class="by">evbogue</span><span>|</span><a href="#41939571">parent</a><span>|</span><a href="#41939702">prev</a><span>|</span><a href="#41939884">next</a><span>|</span><label class="collapse" for="c-41940878">[-]</label><label class="expand" for="c-41940878">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m on Android, however my somewhat elaborate solution was to install Ollama on my home laptop computer and then ssh in when I want to query a model. I figured that&#x27;d be better for my phone battery. Since my home computer is behind NAT I run yggdrasil on everything so I can access my AI on the go.</div><br/></div></div><div id="41939884" class="c"><input type="checkbox" id="c-41939884" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#41939571">parent</a><span>|</span><a href="#41940878">prev</a><span>|</span><a href="#41940072">next</a><span>|</span><label class="collapse" for="c-41939884">[-]</label><label class="expand" for="c-41939884">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using PocketGPT.</div><br/></div></div></div></div><div id="41939891" class="c"><input type="checkbox" id="c-41939891" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#41940072">prev</a><span>|</span><a href="#41939198">next</a><span>|</span><label class="collapse" for="c-41939891">[-]</label><label class="expand" for="c-41939891">[9 more]</label></div><br/><div class="children"><div class="content">Does anyone know why the most common method to speed up inference time is quantization? I keep hearing about all sorts of new methods but nearly none of them is implemented in practice (except for flash attention).</div><br/><div id="41943096" class="c"><input type="checkbox" id="c-41943096" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#41939891">parent</a><span>|</span><a href="#41940570">next</a><span>|</span><label class="collapse" for="c-41943096">[-]</label><label class="expand" for="c-41943096">[1 more]</label></div><br/><div class="children"><div class="content">In addition to the other answers in this thread, there&#x27;s a practical one: sometimes (ok, often) you want to run a model on a card that doesn&#x27;t have enough VRAM for it. Quantisation is a way to squeeze it down so it fits. For instance I&#x27;ve got a 4090 that won&#x27;t fit the original Llama3 70b at 16 bits per param, but it <i>will</i> give me usable token rates at 2 bits.</div><br/></div></div><div id="41940570" class="c"><input type="checkbox" id="c-41940570" checked=""/><div class="controls bullet"><span class="by">formalsystem</span><span>|</span><a href="#41939891">parent</a><span>|</span><a href="#41943096">prev</a><span>|</span><a href="#41940263">next</a><span>|</span><label class="collapse" for="c-41940570">[-]</label><label class="expand" for="c-41940570">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s particularly useful in memory bound workflows like batch size = 1 LLM inference where you&#x27;re bottlenecked by how quickly you can send weights to your GPU. This is why at least in torchao we strongly recommend people try out int4 quantization.<p>At larger batch sizes you become compute bound so quantization matters less and you have to rely on hardware support to accelerate smaller dtypes like fp8</div><br/></div></div><div id="41940263" class="c"><input type="checkbox" id="c-41940263" checked=""/><div class="controls bullet"><span class="by">o11c</span><span>|</span><a href="#41939891">parent</a><span>|</span><a href="#41940570">prev</a><span>|</span><a href="#41941818">next</a><span>|</span><label class="collapse" for="c-41940263">[-]</label><label class="expand" for="c-41940263">[5 more]</label></div><br/><div class="children"><div class="content">Because the way LLMs work is more-or-less &quot;for every token, read the entire matrix from memory and do math on it&quot;. Math is fast, so if you manage to use only half the bits to store each item in the matrix, you only have to do half as much work. Of course, sometimes those least-significant-bits were relied-upon in the original training.</div><br/><div id="41941214" class="c"><input type="checkbox" id="c-41941214" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#41939891">root</a><span>|</span><a href="#41940263">parent</a><span>|</span><a href="#41941818">next</a><span>|</span><label class="collapse" for="c-41941214">[-]</label><label class="expand" for="c-41941214">[4 more]</label></div><br/><div class="children"><div class="content">Has anyone worked on making tokens &#x27;clusters of words with specific semantic meaning&#x27;?<p>e.g. instead of tokens [&#x27;i&#x27;, &#x27;am&#x27;, &#x27;beautiful&#x27;] having tokens [&#x27;I am&#x27;, &#x27;beautiful&#x27;] on the premise that &#x27;I am&#x27; is a common set of bytes for a semantic token that identifies a &#x27;property of self&#x27;?<p>Or taking that further and having much larger tokens based on statistical analysis of common phrases of ~5 words or such?</div><br/><div id="41942946" class="c"><input type="checkbox" id="c-41942946" checked=""/><div class="controls bullet"><span class="by">pizza</span><span>|</span><a href="#41939891">root</a><span>|</span><a href="#41941214">parent</a><span>|</span><a href="#41941997">next</a><span>|</span><label class="collapse" for="c-41942946">[-]</label><label class="expand" for="c-41942946">[1 more]</label></div><br/><div class="children"><div class="content">I think you might be thinking of applying a kind of low-rank decomposition to the vocabulary embeddings. A quick search on Google Scholar suggests that this might be useful in the context of multilingual tokenization.</div><br/></div></div><div id="41941997" class="c"><input type="checkbox" id="c-41941997" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41939891">root</a><span>|</span><a href="#41941214">parent</a><span>|</span><a href="#41942946">prev</a><span>|</span><a href="#41941401">next</a><span>|</span><label class="collapse" for="c-41941997">[-]</label><label class="expand" for="c-41941997">[1 more]</label></div><br/><div class="children"><div class="content">yes, look up Byte Pair Encoding<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;chapter6&#x2F;5" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;chapter6&#x2F;5</a></div><br/></div></div><div id="41941401" class="c"><input type="checkbox" id="c-41941401" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#41939891">root</a><span>|</span><a href="#41941214">parent</a><span>|</span><a href="#41941997">prev</a><span>|</span><a href="#41941818">next</a><span>|</span><label class="collapse" for="c-41941401">[-]</label><label class="expand" for="c-41941401">[1 more]</label></div><br/><div class="children"><div class="content">Much larger tokens require a much larger token vocabulary.</div><br/></div></div></div></div></div></div><div id="41941818" class="c"><input type="checkbox" id="c-41941818" checked=""/><div class="controls bullet"><span class="by">xcodevn</span><span>|</span><a href="#41939891">parent</a><span>|</span><a href="#41940263">prev</a><span>|</span><a href="#41939198">next</a><span>|</span><label class="collapse" for="c-41941818">[-]</label><label class="expand" for="c-41941818">[1 more]</label></div><br/><div class="children"><div class="content">During inference, it is not a matrix x matrix operation, but rather a weight matrix x input vector operation, as we are generating one token at a time. The bottleneck now is how fast we can load the weight matrix from memory to tensor cores, hence the need for weight quantization.</div><br/></div></div></div></div><div id="41939198" class="c"><input type="checkbox" id="c-41939198" checked=""/><div class="controls bullet"><span class="by">arnaudsm</span><span>|</span><a href="#41939891">prev</a><span>|</span><a href="#41939622">next</a><span>|</span><label class="collapse" for="c-41939198">[-]</label><label class="expand" for="c-41939198">[2 more]</label></div><br/><div class="children"><div class="content">How do they compare to their original quants on ollama like q4_K_S?</div><br/><div id="41939226" class="c"><input type="checkbox" id="c-41939226" checked=""/><div class="controls bullet"><span class="by">tcdent</span><span>|</span><a href="#41939198">parent</a><span>|</span><a href="#41939622">next</a><span>|</span><label class="collapse" for="c-41939226">[-]</label><label class="expand" for="c-41939226">[1 more]</label></div><br/><div class="children"><div class="content">These undergo additional fine tuning (QLoRA) using some or all of the original dataset, so they&#x27;re able to get the weights to align to the nf4 dtype better, which increases the accuracy.</div><br/></div></div></div></div><div id="41939004" class="c"><input type="checkbox" id="c-41939004" checked=""/><div class="controls bullet"><span class="by">newfocogi</span><span>|</span><a href="#41939622">prev</a><span>|</span><a href="#41939739">next</a><span>|</span><label class="collapse" for="c-41939004">[-]</label><label class="expand" for="c-41939004">[2 more]</label></div><br/><div class="children"><div class="content">TLDR: Quantized versions of Llama 3.2 1B and 3B models with &quot;competitive accuracy&quot; to the original versions (meaning some degraded performance; plots included in the release notes).</div><br/><div id="41939012" class="c"><input type="checkbox" id="c-41939012" checked=""/><div class="controls bullet"><span class="by">newfocogi</span><span>|</span><a href="#41939004">parent</a><span>|</span><a href="#41939739">next</a><span>|</span><label class="collapse" for="c-41939012">[-]</label><label class="expand" for="c-41939012">[1 more]</label></div><br/><div class="children"><div class="content">Quantization schemes include post-training quantization (PTQ), SpinQuant, and QLoRA.</div><br/></div></div></div></div><div id="41939739" class="c"><input type="checkbox" id="c-41939739" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#41939004">prev</a><span>|</span><label class="collapse" for="c-41939739">[-]</label><label class="expand" for="c-41939739">[13 more]</label></div><br/><div class="children"><div class="content">[flagged]</div><br/><div id="41939813" class="c"><input type="checkbox" id="c-41939813" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41939784">next</a><span>|</span><label class="collapse" for="c-41939813">[-]</label><label class="expand" for="c-41939813">[1 more]</label></div><br/><div class="children"><div class="content">Two days ago there was a pretty big discussion on this topic:<p><pre><code>    Computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku
    https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41914989
    1421 points, 717 comments</code></pre></div><br/></div></div><div id="41939784" class="c"><input type="checkbox" id="c-41939784" checked=""/><div class="controls bullet"><span class="by">pryelluw</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41939813">prev</a><span>|</span><a href="#41939969">next</a><span>|</span><label class="collapse" for="c-41939784">[-]</label><label class="expand" for="c-41939784">[4 more]</label></div><br/><div class="children"><div class="content">I don’t get the comment. For one I’m excited for developments in the field. Not afraid it will “replace me” as technology has replaced me multiple times over. I’m looking towards working with these models more and more.</div><br/><div id="41939841" class="c"><input type="checkbox" id="c-41939841" checked=""/><div class="controls bullet"><span class="by">mmaunder</span><span>|</span><a href="#41939739">root</a><span>|</span><a href="#41939784">parent</a><span>|</span><a href="#41939969">next</a><span>|</span><label class="collapse" for="c-41939841">[-]</label><label class="expand" for="c-41939841">[3 more]</label></div><br/><div class="children"><div class="content">No, I meant that a lot of us are working very fast on a pre-launch product, implementing some cutting edge ideas using e.g. the incredible speedup in a small fast inference model like quantized 3B in combination with other tools, and I think there&#x27;s quite a bit of paranoia out there that someone else will beat you to market. And so not a lot of sharing going on in the comments. At least not as much as previously, and not as much technical discussion vs other non-AI threads on HN.</div><br/><div id="41940193" class="c"><input type="checkbox" id="c-41940193" checked=""/><div class="controls bullet"><span class="by">pryelluw</span><span>|</span><a href="#41939739">root</a><span>|</span><a href="#41939841">parent</a><span>|</span><a href="#41939981">next</a><span>|</span><label class="collapse" for="c-41940193">[-]</label><label class="expand" for="c-41940193">[1 more]</label></div><br/><div class="children"><div class="content">Ok, thank you for pointing that out.<p>I’m focused on making models play nice with each other rather than building a feature that relies on it. That’s where I see the more relevant work being. Why such news are exciting!</div><br/></div></div><div id="41939981" class="c"><input type="checkbox" id="c-41939981" checked=""/><div class="controls bullet"><span class="by">mattgreenrocks</span><span>|</span><a href="#41939739">root</a><span>|</span><a href="#41939841">parent</a><span>|</span><a href="#41940193">prev</a><span>|</span><a href="#41939969">next</a><span>|</span><label class="collapse" for="c-41939981">[-]</label><label class="expand" for="c-41939981">[1 more]</label></div><br/><div class="children"><div class="content">This thread attracts a smaller audience than, say, a new version of ChatGPT.</div><br/></div></div></div></div></div></div><div id="41939969" class="c"><input type="checkbox" id="c-41939969" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41939784">prev</a><span>|</span><a href="#41939844">next</a><span>|</span><label class="collapse" for="c-41939969">[-]</label><label class="expand" for="c-41939969">[1 more]</label></div><br/><div class="children"><div class="content">What kind of fundamental discussion are you hoping to see under an article about an iterative improvement to a known model?<p>&quot;AI will destroy the world&quot;? &quot;AI is great and will save humanity&quot;? If you&#x27;re seriously missing that, there&#x27;s really enough platforms (and articles for more fundamental announcements&#x2F;propositions on this one) where you can have these.</div><br/></div></div><div id="41939844" class="c"><input type="checkbox" id="c-41939844" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41939969">prev</a><span>|</span><a href="#41941956">next</a><span>|</span><label class="collapse" for="c-41939844">[-]</label><label class="expand" for="c-41939844">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t we all just tired of arguing the same points?</div><br/></div></div><div id="41939824" class="c"><input type="checkbox" id="c-41939824" checked=""/><div class="controls bullet"><span class="by">flawn</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41939827">prev</a><span>|</span><a href="#41940765">next</a><span>|</span><label class="collapse" for="c-41939824">[-]</label><label class="expand" for="c-41939824">[1 more]</label></div><br/><div class="children"><div class="content">A sign of the ongoing commoditization?</div><br/></div></div><div id="41940765" class="c"><input type="checkbox" id="c-41940765" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41939824">prev</a><span>|</span><a href="#41940325">next</a><span>|</span><label class="collapse" for="c-41940765">[-]</label><label class="expand" for="c-41940765">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t be so haughty and presumptive of your understanding of things is as they are: this doesn&#x27;t have practical applications.<p>No one serious is going to build on some horror of Python interpreter running inside your app to run an LLM when llama.cpp is right there, with more quants available. In practice, on mobile, you run out of RAM headroom way more quickly than CPU headroom. You&#x27;ve been able to run llama.cpp 3B models for almost a year now on iOS, whereas here, they&#x27;re just starting to be able to. (allocating 6 GB is a quick way to get autokill&#x27;d on iOS...2.5GB? Doable)<p>It looks like spinquant is effectively Q8, in widespread blind testing over months, empirically, we found Q5 is assuredly indistinguishable from the base model.<p>(edit: just saw your comment. oy. best of luck! generally, I don&#x27;t bother with these sorts of &#x27;lived experience&#x27; details, because no one wants to hear they don&#x27;t get it, and most LLM comments on HN are from ppl who don&#x27;t have the same luck as  to work on it fulltime. so you&#x27;re either stuck aggressively asserting you&#x27;re right in practice and they don&#x27;t know what you&#x27;re talking about, or, you&#x27;re stuck being talked down to about things you&#x27;ve seen, even if they don&#x27;t match a first-pass based on theory) <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41939841">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41939841</a></div><br/></div></div><div id="41940325" class="c"><input type="checkbox" id="c-41940325" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#41939739">parent</a><span>|</span><a href="#41940765">prev</a><span>|</span><label class="collapse" for="c-41940325">[-]</label><label class="expand" for="c-41940325">[1 more]</label></div><br/><div class="children"><div class="content">I mean, this outcome of LLMs is expected and the frequency of LLM drops are too fast, and definitely too fast to wait for Meta to do an annual conference with a ton of hype, and furthermore these things are just prerequisites for a massive lemming rush of altering these models for the real fun, which occurs in other communities</div><br/></div></div></div></div></div></div></div></div></div></body></html>