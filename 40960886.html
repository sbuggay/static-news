<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1721034065349" as="style"/><link rel="stylesheet" href="styles.css?v=1721034065349"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://zenodo.org/records/12740116">CURLoRA: Stable LLM Fine-Tuning and Catastrophic Forgetting Mitigation</a> <span class="domain">(<a href="https://zenodo.org">zenodo.org</a>)</span></div><div class="subtext"><span>mnoorfawi</span> | <span>9 comments</span></div><br/><div><div id="40964047" class="c"><input type="checkbox" id="c-40964047" checked=""/><div class="controls bullet"><span class="by">Majromax</span><span>|</span><a href="#40962988">next</a><span>|</span><label class="collapse" for="c-40964047">[-]</label><label class="expand" for="c-40964047">[1 more]</label></div><br/><div class="children"><div class="content">I think there&#x27;s one significant flaw in the analysis.  Section 4.3.1 states:<p>&gt;&gt; In CURLoRA, we decompose the original weight matrix W as: W ≈ CUR<p>… but C and R are chosen with respect to the <i>inverse</i> of their contribution towards W, with lower-norm rows and columns more likely to be chosen.  In some sense, the CUR of this fine-tuning is as far as possible from the full weight matrix W while still being drawn from it.<p>To the extent the method works, it might work by defining a very low-dimensional space for the fine-tuning that is vaguely compatible with the original weight matrix.<p>I also think that there&#x27;s something a bit off in the results (section 7).  Neither the test nor control (LoRA) results show improving accuracy on the first (MRPC) fine-tuning task with increasing rank, suggesting that this task was never constrained by the dimensionality available to the fine-tuning.  The orders-of-magnitude difference in the number of trainable parameters also suggests that LoRA-1&#x2F;2&#x2F;4 could have been fairly considered; it&#x27;s not obvious that CURLoRA-n is even approximately equivalent to LoRA-n.</div><br/></div></div><div id="40962988" class="c"><input type="checkbox" id="c-40962988" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#40964047">prev</a><span>|</span><a href="#40965698">next</a><span>|</span><label class="collapse" for="c-40962988">[-]</label><label class="expand" for="c-40962988">[1 more]</label></div><br/><div class="children"><div class="content">I didn’t want to read this, so I asked Claude to ELI5 it to an EE-graduate level. I hope this summary is useful:<p>CURLoRA is a new way to fine-tune large language models (LLMs) that aims to solve two main problems:<p>1. Catastrophic forgetting: When you fine-tune an LLM on a new task, it often &quot;forgets&quot; what it learned before.<p>2. Computational efficiency: Fine-tuning LLMs usually requires a lot of computing power.<p>Here&#x27;s how CURLoRA works:<p>1. It uses a matrix decomposition method called CUR decomposition. This breaks down a big weight matrix (W) into three smaller matrices: C, U, and R.<p>2. The clever part is how they choose C and R:
   - They pick columns and rows that are less &quot;important&quot; in the original matrix.
   - This acts like a built-in regularization, preventing the model from changing too much.<p>3. They initialize the U matrix with all zeros and only train this matrix.<p>The benefits:<p>1. It helps prevent catastrophic forgetting because the changes are constrained to less important parts of the original weights.<p>2. It&#x27;s more memory-efficient than full fine-tuning or even LoRA (another popular fine-tuning method) because you&#x27;re only training the U matrix.<p>3. It maintains the model&#x27;s general language understanding better than LoRA, as shown by perplexity scores on a general language task.<p>The experiments showed that CURLoRA:
- Performed better on specific tasks (like sentiment analysis) compared to LoRA.
- Maintained performance across multiple tasks better than LoRA (showing less forgetting).
- Kept its general language understanding intact, while LoRA&#x27;s understanding degraded significantly.<p>In essence, CURLoRA is like giving the model a very selective memory upgrade. It allows the model to learn new tasks efficiently while keeping most of its original knowledge intact.</div><br/></div></div><div id="40965698" class="c"><input type="checkbox" id="c-40965698" checked=""/><div class="controls bullet"><span class="by">sulandor</span><span>|</span><a href="#40962988">prev</a><span>|</span><label class="collapse" for="c-40965698">[-]</label><label class="expand" for="c-40965698">[6 more]</label></div><br/><div class="children"><div class="content">concerning reuse of significant acronyms.<p>&quot;just&quot; marketing or heightened malicious intent?</div><br/><div id="40965714" class="c"><input type="checkbox" id="c-40965714" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#40965698">parent</a><span>|</span><label class="collapse" for="c-40965714">[-]</label><label class="expand" for="c-40965714">[5 more]</label></div><br/><div class="children"><div class="content">No, it&#x27;s a LoRA variant, so there&#x27;s nothing strange about LoRA being in the name.</div><br/><div id="40965735" class="c"><input type="checkbox" id="c-40965735" checked=""/><div class="controls bullet"><span class="by">sulandor</span><span>|</span><a href="#40965698">root</a><span>|</span><a href="#40965714">parent</a><span>|</span><label class="collapse" for="c-40965735">[-]</label><label class="expand" for="c-40965735">[4 more]</label></div><br/><div class="children"><div class="content">for the uninitiated<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CURL" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;CURL</a><p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;LoRa" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;LoRa</a></div><br/><div id="40965799" class="c"><input type="checkbox" id="c-40965799" checked=""/><div class="controls bullet"><span class="by">sulandor</span><span>|</span><a href="#40965698">root</a><span>|</span><a href="#40965735">parent</a><span>|</span><label class="collapse" for="c-40965799">[-]</label><label class="expand" for="c-40965799">[3 more]</label></div><br/><div class="children"><div class="content">i take it from the downvotes that you really didn&#x27;t know</div><br/><div id="40965818" class="c"><input type="checkbox" id="c-40965818" checked=""/><div class="controls bullet"><span class="by">impossiblefork</span><span>|</span><a href="#40965698">root</a><span>|</span><a href="#40965799">parent</a><span>|</span><label class="collapse" for="c-40965818">[-]</label><label class="expand" for="c-40965818">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t downvoted anybody; and I do know about cURL and LoRaWAN, but LoRA is already &#x27;long&#x27; established as a DL term.<p>I didn&#x27;t connect this acronym to cURL though.</div><br/><div id="40965860" class="c"><input type="checkbox" id="c-40965860" checked=""/><div class="controls bullet"><span class="by">sulandor</span><span>|</span><a href="#40965698">root</a><span>|</span><a href="#40965818">parent</a><span>|</span><label class="collapse" for="c-40965860">[-]</label><label class="expand" for="c-40965860">[1 more]</label></div><br/><div class="children"><div class="content">my apologies then<p>i guess we established, once again, why naming stuff is a hard problem (not just in cs)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>