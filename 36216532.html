<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686128463708" as="style"/><link rel="stylesheet" href="styles.css?v=1686128463708"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/princeton-nlp/MeZO">MeZO: Fine-Tuning Language Models with Just Forward Passes</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>behnamoh</span> | <span>21 comments</span></div><br/><div><div id="36218220" class="c"><input type="checkbox" id="c-36218220" checked=""/><div class="controls bullet"><span class="by">ssivark</span><span>|</span><a href="#36220307">next</a><span>|</span><label class="collapse" for="c-36218220">[-]</label><label class="expand" for="c-36218220">[8 more]</label></div><br/><div class="children"><div class="content">Some context, for those who might be getting carried away:<p>The essential idea here is estimating gradients using numerical (forward pass) function evaluation. The reason it’s called “zeroth order” is that it’s a much much worse approximation to the gradient than first order methods (Eg: computing the gradient via back propagation). Here’s the catch: Bad gradient estimates mean that the optimization takes much longer — in this paper, MeZO has been allowed 100x as many optimization steps as the baseline fine tuning method. That is why this method isn’t commonly used despite being one of the oldest &#x2F; simplest &#x2F; most obvious.<p>That it seems to still be (empirically) good enough for fine tuning is certainly interesting &amp; encouraging. Further, 100x optimization steps might even be a worthwhile trade off in practice (comparing hardware availability, costs, etc)! But at the moment it seems like a pretty niche improvement, unless this phenomenon turns out to generalize for some principled reason.</div><br/><div id="36219455" class="c"><input type="checkbox" id="c-36219455" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#36218220">parent</a><span>|</span><a href="#36222050">next</a><span>|</span><label class="collapse" for="c-36219455">[-]</label><label class="expand" for="c-36219455">[2 more]</label></div><br/><div class="children"><div class="content">Other points to note: this is a lot like evolution strategies or CMA-ES if you&#x27;re familiar with those, so inherits their strengths and weaknesses; it&#x27;s sample&#x2F;compute-inefficient because it doesn&#x27;t use real gradients, but that also means you can use non-differentiable tools, so this might be useful in a RL context or a tool&#x2F;API context, which are increasingly big deals for LLM usecases.<p>And it&#x27;s a lot less sample&#x2F;compute-inefficient than you might expect from such low-quality optimization applied to such highly-parameterized models; so this is another example of the blessings of scale, and how very large neural net models are, in some sense, simpler, more linear&#x2F;interpretible, more generalizable, and more efficient than they &#x27;ought&#x27; to be.</div><br/><div id="36222420" class="c"><input type="checkbox" id="c-36222420" checked=""/><div class="controls bullet"><span class="by">startupsfail</span><span>|</span><a href="#36218220">root</a><span>|</span><a href="#36219455">parent</a><span>|</span><a href="#36222050">next</a><span>|</span><label class="collapse" for="c-36222420">[-]</label><label class="expand" for="c-36222420">[1 more]</label></div><br/><div class="children"><div class="content">To me it seems that a plug-and-play integration with HuggingFace transformers is the killer feature here. If this simply allows to fine tune a model on your local GPU, that you otherwise can’t train, this is pretty interesting.<p>It seems that it also might work for 4bit-quantized  models. And removing the step of model quantization and acceleration in terms of TOPS for quantized inference can maybe compensate for the need to do more steps.<p>Also, a good question of how well it works in a distributed&#x2F;batched setting.</div><br/></div></div></div></div><div id="36222050" class="c"><input type="checkbox" id="c-36222050" checked=""/><div class="controls bullet"><span class="by">arugulum</span><span>|</span><a href="#36218220">parent</a><span>|</span><a href="#36219455">prev</a><span>|</span><a href="#36218831">next</a><span>|</span><label class="collapse" for="c-36222050">[-]</label><label class="expand" for="c-36222050">[1 more]</label></div><br/><div class="children"><div class="content">Right, I hate to be that guy, but it essentially boils down to &quot;take a random step and see if it&#x27;s good, if so update&quot; (with some fanciness). It should go without saying that this is horribly inefficient in a very high dimensional space (e.g. the number of parameters).<p>The results are decent, but it&#x27;s just another in a long line of work (and more recent work <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.08587" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.08587</a> <a href="https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=5i7lJLuhTm" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=5i7lJLuhTm</a>) trying to use backprop algorithm with some promising results but still falling far short of the standard backprop workhorse.</div><br/></div></div><div id="36218831" class="c"><input type="checkbox" id="c-36218831" checked=""/><div class="controls bullet"><span class="by">oersted</span><span>|</span><a href="#36218220">parent</a><span>|</span><a href="#36222050">prev</a><span>|</span><a href="#36219884">next</a><span>|</span><label class="collapse" for="c-36218831">[-]</label><label class="expand" for="c-36218831">[3 more]</label></div><br/><div class="children"><div class="content">Since you seem knowledgeable about the topic and have explained it quite well, I&#x27;d like to ask a follow-up question.<p>Could you summarize how this &quot;zeroth order&quot; gradient approximation works? In simple terms, how do we go from a measurement of the forward-pass output error (loss), to knowing in which direction to nudge each parameter to approach a minimum?<p>Is it some kind of swarm optimization (not sure if it&#x27;s the right term), where the local neighborhood in parameter space is sampled to approximate the gradient?<p>If that is the case, it&#x27;s not just that more passes are required due to bad gradient approximations, but they also need multiple passes to get each approximate gradient (presumably many of them due to the high dimensionality).</div><br/><div id="36218973" class="c"><input type="checkbox" id="c-36218973" checked=""/><div class="controls bullet"><span class="by">ssivark</span><span>|</span><a href="#36218220">root</a><span>|</span><a href="#36218831">parent</a><span>|</span><a href="#36219884">next</a><span>|</span><label class="collapse" for="c-36218973">[-]</label><label class="expand" for="c-36218973">[2 more]</label></div><br/><div class="children"><div class="content">You evaluate the function at two neighboring points, and the difference of function values (divided by the distance between the two points) gives you an estimate of the gradient projected along the direction between the two points. It’s literally the definition of the derivative — but you cannot practically take an infinitesimal step, so the points are a finite distance away (substantially bigger than machine precision).<p>Another way to think about it — you choose a random line through your current location, and test neighboring points in two opposite directions, and step in the direction of whichever seems better. That’s why it costs as much as two forward passes. If you squint a little, the random choice of direction (line) makes it look kinda like evolutionary search.<p>This is a textbook method, so I’m sure there must be some description on the web with pretty pictures and clearly worked out math — but I’m AFK right now and can’t find a good one through my phone :-(</div><br/><div id="36220026" class="c"><input type="checkbox" id="c-36220026" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#36218220">root</a><span>|</span><a href="#36218973">parent</a><span>|</span><a href="#36219884">next</a><span>|</span><label class="collapse" for="c-36220026">[-]</label><label class="expand" for="c-36220026">[1 more]</label></div><br/><div class="children"><div class="content">Maybe check out this paper, the first page has a decent diagram and caption!<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.06224" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.06224</a></div><br/></div></div></div></div></div></div><div id="36219884" class="c"><input type="checkbox" id="c-36219884" checked=""/><div class="controls bullet"><span class="by">bloaf</span><span>|</span><a href="#36218220">parent</a><span>|</span><a href="#36218831">prev</a><span>|</span><a href="#36220307">next</a><span>|</span><label class="collapse" for="c-36219884">[-]</label><label class="expand" for="c-36219884">[1 more]</label></div><br/><div class="children"><div class="content">Those method would work on recurrent neutral nets, not just feed forward.</div><br/></div></div></div></div><div id="36220307" class="c"><input type="checkbox" id="c-36220307" checked=""/><div class="controls bullet"><span class="by">born-jre</span><span>|</span><a href="#36218220">prev</a><span>|</span><a href="#36216751">next</a><span>|</span><label class="collapse" for="c-36220307">[-]</label><label class="expand" for="c-36220307">[1 more]</label></div><br/><div class="children"><div class="content">sounds cool,<p>i wonder if its possible to do similar with &quot;The Forward-Forward Algorithm&quot; by  Geoff Hilton maybe on a few add on layers like with lora<p>[0]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.13345.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2212.13345.pdf</a></div><br/></div></div><div id="36216751" class="c"><input type="checkbox" id="c-36216751" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36220307">prev</a><span>|</span><label class="collapse" for="c-36216751">[-]</label><label class="expand" for="c-36216751">[11 more]</label></div><br/><div class="children"><div class="content">A repost, but it should be reposted. This is amazing, maybe even scary.</div><br/><div id="36217016" class="c"><input type="checkbox" id="c-36217016" checked=""/><div class="controls bullet"><span class="by">pm</span><span>|</span><a href="#36216751">parent</a><span>|</span><a href="#36219463">next</a><span>|</span><label class="collapse" for="c-36217016">[-]</label><label class="expand" for="c-36217016">[7 more]</label></div><br/><div class="children"><div class="content">As someone who&#x27;s not familiar enough with LLMs to deduce why this is amazing or scary, would you kindly explain as to why this is so?</div><br/><div id="36217187" class="c"><input type="checkbox" id="c-36217187" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#36216751">root</a><span>|</span><a href="#36217016">parent</a><span>|</span><a href="#36217131">next</a><span>|</span><label class="collapse" for="c-36217187">[-]</label><label class="expand" for="c-36217187">[2 more]</label></div><br/><div class="children"><div class="content">heyitsguay is correct, but in addition:<p>- The researchers didn&#x27;t even explore quantization. In theory 4 bit quant would allow for training on even more modest hardware.<p>- Memory use aside, forward pass only is potentially a big training speed increase.<p>- This method is very amenable to decentralized training.<p>It scares me because it feels like a gateway to networked, self training LLMs on commodity hardware. I thought this was a long way away... now it doesn&#x27;t feel so far away.</div><br/><div id="36218457" class="c"><input type="checkbox" id="c-36218457" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#36216751">root</a><span>|</span><a href="#36217187">parent</a><span>|</span><a href="#36217131">next</a><span>|</span><label class="collapse" for="c-36218457">[-]</label><label class="expand" for="c-36218457">[1 more]</label></div><br/><div class="children"><div class="content">&gt; - Memory use aside, forward pass only is potentially a big training speed increase.<p>Forward-pass only doesn&#x27;t mean it&#x27;s faster. It converges much slower than even SGD. It is a memory-time tradeoff, but that&#x27;s it.</div><br/></div></div></div></div><div id="36217131" class="c"><input type="checkbox" id="c-36217131" checked=""/><div class="controls bullet"><span class="by">heyitsguay</span><span>|</span><a href="#36216751">root</a><span>|</span><a href="#36217016">parent</a><span>|</span><a href="#36217187">prev</a><span>|</span><a href="#36217204">next</a><span>|</span><label class="collapse" for="c-36217131">[-]</label><label class="expand" for="c-36217131">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not. That&#x27;s part of the uninformed AI hype train that will consistently be posting first on AI stuff for the next 6 months.<p>Right now the main bottleneck for LLM size is GPU memory (VRAM). Training requires much more VRAM than inference, which limits the ability for entities that aren&#x27;t Google or OpenAI-scale to finetune models (aka do a little more training on your custom dataset).<p>The paper here suggests that one can actually finetune LLMs with inference-sized VRAM usage instead of training-sized VRAM usage. If true, it will be possible to fine tune larger models on smaller (though still expensive) GPUs -- like a single 3090 instead of 1x or 8xA100s. So, more people can create more customized models.</div><br/><div id="36217361" class="c"><input type="checkbox" id="c-36217361" checked=""/><div class="controls bullet"><span class="by">vvladymyrov</span><span>|</span><a href="#36216751">root</a><span>|</span><a href="#36217131">parent</a><span>|</span><a href="#36217204">next</a><span>|</span><label class="collapse" for="c-36217361">[-]</label><label class="expand" for="c-36217361">[2 more]</label></div><br/><div class="children"><div class="content">Inference can be done on CPU+RAM, but it is much slower (like tens of seconds per token). So reducing the amount of memory used by model during training would reduce the number of compute operations potentially could make CPU+RAM more suitable for fine tuning within reasonable amount of time too.
Basically 12x less GPU memory requirement also translates to 12x less compute operations (doing compute on CPU allows less parallelism them on GPU).<p>The paper doesn’t focus on CPU or GPU training time improvements - I’d assume there is no significant improvement in GPU training case. For CPU it is logical to expect 12x training speed improvement, but it is still too slow to be consistent practically useful.</div><br/><div id="36217868" class="c"><input type="checkbox" id="c-36217868" checked=""/><div class="controls bullet"><span class="by">gliptic</span><span>|</span><a href="#36216751">root</a><span>|</span><a href="#36217361">parent</a><span>|</span><a href="#36217204">next</a><span>|</span><label class="collapse" for="c-36217868">[-]</label><label class="expand" for="c-36217868">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For CPU it is logical to expect 12x training speed improvement, but it is still too slow to be consistent practically useful.<p>I don&#x27;t see what you base this on. MeZO trades one back-propagation pass for another forward pass. Why would that be 12x faster?
It&#x27;s also clear the convergence rate is slower than plain SGD (never mind AdamW) by a factor proportional to the effective rank of the Hessian.</div><br/></div></div></div></div></div></div></div></div><div id="36219463" class="c"><input type="checkbox" id="c-36219463" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#36216751">parent</a><span>|</span><a href="#36217016">prev</a><span>|</span><a href="#36217419">next</a><span>|</span><label class="collapse" for="c-36219463">[-]</label><label class="expand" for="c-36219463">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.17333" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.17333</a></div><br/></div></div><div id="36217419" class="c"><input type="checkbox" id="c-36217419" checked=""/><div class="controls bullet"><span class="by">empalms</span><span>|</span><a href="#36216751">parent</a><span>|</span><a href="#36219463">prev</a><span>|</span><label class="collapse" for="c-36217419">[-]</label><label class="expand" for="c-36217419">[2 more]</label></div><br/><div class="children"><div class="content">Could you point to the OP(s) please? No luck here with HN search</div><br/><div id="36218195" class="c"><input type="checkbox" id="c-36218195" checked=""/><div class="controls bullet"><span class="by">icpmacdo</span><span>|</span><a href="#36216751">root</a><span>|</span><a href="#36217419">parent</a><span>|</span><label class="collapse" for="c-36218195">[-]</label><label class="expand" for="c-36218195">[1 more]</label></div><br/><div class="children"><div class="content">Maybe a reference to GGML thread</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>