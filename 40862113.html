<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1719997253630" as="style"/><link rel="stylesheet" href="styles.css?v=1719997253630"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/palico-ai/palico-ai">Show HN: Improve LLM Performance by Maximizing Iterative Development</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>asif_</span> | <span>10 comments</span></div><br/><div><div id="40864041" class="c"><input type="checkbox" id="c-40864041" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#40862894">next</a><span>|</span><label class="collapse" for="c-40864041">[-]</label><label class="expand" for="c-40864041">[1 more]</label></div><br/><div class="children"><div class="content">It seems to me that the fastest way for iterative improvement is to use LLMs with pure Python with as few frameworks in between as possible:<p>You know exactly what goes into the prompt, how it’s parsed, what params are used or when they are changed. You can abstract away as much or as little of it as you like. Your API is going to change only when you make it so. And everything you learn about patterns in the process will be applicable to Python in general - not just one framework that may be replaced two months from now.</div><br/></div></div><div id="40862894" class="c"><input type="checkbox" id="c-40862894" checked=""/><div class="controls bullet"><span class="by">orliesaurus</span><span>|</span><a href="#40864041">prev</a><span>|</span><a href="#40863608">next</a><span>|</span><label class="collapse" for="c-40862894">[-]</label><label class="expand" for="c-40862894">[2 more]</label></div><br/><div class="children"><div class="content">This is a good idea, I wonder if you have a write-up&#x2F;blog about the performance gains in real world applications?</div><br/><div id="40863925" class="c"><input type="checkbox" id="c-40863925" checked=""/><div class="controls bullet"><span class="by">asif_</span><span>|</span><a href="#40862894">parent</a><span>|</span><a href="#40863608">next</a><span>|</span><label class="collapse" for="c-40863925">[-]</label><label class="expand" for="c-40863925">[1 more]</label></div><br/><div class="children"><div class="content">Hey, thanks for checking out the framework! We just released this week so there aren&#x27;t any data-points to share yet. But as we onboard more dev teams, we&#x27;ll are planning on writing about their process and outcomes over the next few months.<p>If you are curious about the theory and best practices behind iterating on LLM applications to improve it&#x27;s performance, this is a good blog-post from Data Science at Microsoft: <a href="https:&#x2F;&#x2F;medium.com&#x2F;data-science-at-microsoft&#x2F;evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;data-science-at-microsoft&#x2F;evaluating-llm-...</a><p>I am also working on takes the theory behind the blog-post above, and converting that to a more practical guide using our framework. It should be out within the next two weeks. You can get notified when we release a blog by signing up for our newsletter: <a href="https:&#x2F;&#x2F;palico.us22.list-manage.com&#x2F;subscribe?u=84ba2d0a4c03cc320b5fba0c1&amp;id=bd89db131c" rel="nofollow">https:&#x2F;&#x2F;palico.us22.list-manage.com&#x2F;subscribe?u=84ba2d0a4c03...</a></div><br/></div></div></div></div><div id="40863608" class="c"><input type="checkbox" id="c-40863608" checked=""/><div class="controls bullet"><span class="by">f6v</span><span>|</span><a href="#40862894">prev</a><span>|</span><a href="#40863330">next</a><span>|</span><label class="collapse" for="c-40863608">[-]</label><label class="expand" for="c-40863608">[1 more]</label></div><br/><div class="children"><div class="content">Could you comment on this? <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40862436">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40862436</a></div><br/></div></div><div id="40863330" class="c"><input type="checkbox" id="c-40863330" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#40863608">prev</a><span>|</span><a href="#40863234">next</a><span>|</span><label class="collapse" for="c-40863330">[-]</label><label class="expand" for="c-40863330">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>quickly iterate towards your accuracy goals</i><p>Do not you have a phenomenon akin to overfitting? How do you ensure that enhancing accuracy on foreseen input does not weaken results under unforseen future ones?</div><br/></div></div><div id="40863234" class="c"><input type="checkbox" id="c-40863234" checked=""/><div class="controls bullet"><span class="by">jonnycoder</span><span>|</span><a href="#40863330">prev</a><span>|</span><a href="#40863277">next</a><span>|</span><label class="collapse" for="c-40863234">[-]</label><label class="expand" for="c-40863234">[2 more]</label></div><br/><div class="children"><div class="content">How does this intersect with evaluation in LLM integration &amp; testing?</div><br/><div id="40864071" class="c"><input type="checkbox" id="c-40864071" checked=""/><div class="controls bullet"><span class="by">asif_</span><span>|</span><a href="#40863234">parent</a><span>|</span><a href="#40863277">next</a><span>|</span><label class="collapse" for="c-40864071">[-]</label><label class="expand" for="c-40864071">[1 more]</label></div><br/><div class="children"><div class="content">Hey, thanks for the question. Are you talking about standard evaluation tools like promptfoo? These evaluation frameworks are often just tools that helps you grade the response of your LLM application. They however do not help you to build an LLM application that makes it easy to test different configurations of your application and evaluate them. That is where we different -- we help you build an application that is made for easily testing different configurations of your application so you can evaluate them much faster.<p>So the process we see when companies are trying to adopt a evaluation framework is that when they want to try a new configuration, they completely change their code-base, create the code to run an evaluation, and review that result independently and try to compare with other changes they have made sometimes in the past. This usually leads to a very slow process for making new changes and becomes very unorganized.<p>With us, we help you build your LLM application where it&#x27;s easy to swap components. From there, when you want to see how your application works with a certain configuration, we have a UI where you can pass in the configuration settings for your application, and run an evaluation. We also save all your previous evaluations so you can easily compare them with each other. As a result, it&#x27;s very easy and fast to test different configurations of your application and evaluate them with us.</div><br/></div></div></div></div><div id="40863277" class="c"><input type="checkbox" id="c-40863277" checked=""/><div class="controls bullet"><span class="by">E_Bfx</span><span>|</span><a href="#40863234">prev</a><span>|</span><label class="collapse" for="c-40863277">[-]</label><label class="expand" for="c-40863277">[2 more]</label></div><br/><div class="children"><div class="content">How easy is it to switch from OpenAI to testing a LLM on premise ?</div><br/><div id="40864081" class="c"><input type="checkbox" id="c-40864081" checked=""/><div class="controls bullet"><span class="by">asif_</span><span>|</span><a href="#40863277">parent</a><span>|</span><label class="collapse" for="c-40864081">[-]</label><label class="expand" for="c-40864081">[1 more]</label></div><br/><div class="children"><div class="content">We provide complete flexibility on how you call your LLM model. So if you have your on-prem LLM behind an API, you would just write the standard code to call your API from within our framework.</div><br/></div></div></div></div></div></div></div></div></div></body></html>