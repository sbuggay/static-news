<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1733562053692" as="style"/><link rel="stylesheet" href="styles.css?v=1733562053692"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arcprize.org/2024-results">Arc Prize 2024 Winners and Technical Report</a> <span class="domain">(<a href="https://arcprize.org">arcprize.org</a>)</span></div><div class="subtext"><span>alphabetting</span> | <span>43 comments</span></div><br/><div><div id="42343621" class="c"><input type="checkbox" id="c-42343621" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#42343975">next</a><span>|</span><label class="collapse" for="c-42343621">[-]</label><label class="expand" for="c-42343621">[15 more]</label></div><br/><div class="children"><div class="content">Author here -- six months ago we launched ARC Prize, a huge $1M experiment, to test if we need new ideas for AGI. The ARC-AGI benchmark remains unbeaten and I think we can now definitely say &quot;yes&quot;.<p>One big update since June is that progress is no longer stalled. Coming into 2024, the public consensus vibe was that pure deep learning &#x2F; LLMs would continue scaling to AGI. The fundamental architecture of these systems hasn&#x27;t changed since ~2019.<p>But this flipped late summer. AlphaProof and o1 are evidence of this new reality. All frontier AI systems are now incorporating components beyond pure deep learning like program synthesis and program search.<p>I believe ARC Prize played a role here too. All the winners this year are leveraging new AGI reasoning approaches like deep-learning guided program synthesis, and test-time training&#x2F;fine-tuning. We&#x27;ll be seeing a lot more of these in frontier AI systems in coming years.<p>And I&#x27;m proud to say that all the code and papers from this year&#x27;s winners are now open source!<p>We&#x27;re going to keep running this thing annually until its defeated. And we&#x27;ve got ARC-AGI-2 in the works to improve on several of the v1 flaws (more here: <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;arc-prize-2024-winners-technical-report" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;blog&#x2F;arc-prize-2024-winners-technical-r...</a>)<p>The ARC-AGI community keeps surprising me. From initial launch, through o1 testing, to the final 48 hours when the winning team jumped 10% and both winning papers dropped out of nowhere. I&#x27;m incredibly grateful to everyone and we will do our best to steward this attention towards AGI.<p>We&#x27;ll be back in 2025!</div><br/><div id="42344486" class="c"><input type="checkbox" id="c-42344486" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#42343621">parent</a><span>|</span><a href="#42344483">next</a><span>|</span><label class="collapse" for="c-42344486">[-]</label><label class="expand" for="c-42344486">[9 more]</label></div><br/><div class="children"><div class="content">As a rather experienced ML researcher, ARC is a great benchmark on its own, but is punching below its weight in terms of claiming that it is a gate (or in terms of this post -- a &quot;steward&quot;) towards AGI, and in my perspective and the perspective of several researchers near me this has watered down the value of the ARC benchmark as a test.<p>It is a great unit test for reasoning -- that&#x27;s fantastic! And maybe it is indeed the best way to test for this -- who knows  exactly. But the claim is a little grandiose for what it is, this is somewhat similar to saying that testing on string parity is the One True Test for testing an optimizer&#x27;s efficiency.<p>I&#x27;d heartily recommend maybe taking down the marketing vibrance down a notch and keep things a bit more measured, it&#x27;s not entirely a meme, though some of the more-serious researchers don&#x27;t take it as seriously as a result. And that&#x27;s the kind of people that you want to attract to this sort of thing!<p>I think there is a potentially good future for ARC! But it might struggle to attract some of the kind of talent that you want to work on this problem as a result.</div><br/><div id="42345021" class="c"><input type="checkbox" id="c-42345021" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42344486">parent</a><span>|</span><a href="#42344483">next</a><span>|</span><label class="collapse" for="c-42345021">[-]</label><label class="expand" for="c-42345021">[8 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d heartily recommend maybe taking down the marketing vibrance down a notch and keep things a bit more measured, it&#x27;s not entirely a meme, though some of the more-serious researchers don&#x27;t take it as seriously as a result.<p>This is fair critique. ARC Prize&#x27;s 2024 messaging was sharp to break through the noise floor -- ARC has been around since 2019 but most only learned about it this summer. Now that it has garnered awareness, it is no longer useful, and in same cases hurting progress like you point out. The messaging needs to evolve and mature next year to be more neutral&#x2F;academic.</div><br/><div id="42345198" class="c"><input type="checkbox" id="c-42345198" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345021">parent</a><span>|</span><a href="#42344483">next</a><span>|</span><label class="collapse" for="c-42345198">[-]</label><label class="expand" for="c-42345198">[7 more]</label></div><br/><div class="children"><div class="content">I feel rather consternated that this response effectively boils down to &quot;yes, we know we overhyped this to get people&#x27;s attention, and now that we have it we can be more honest about it&quot;. Fighting for place in the attention economy is understandable, being deceptive about it is not.<p>This is part of the ethical morass of why some more serious researchers aren&#x27;t touching the benchmark. People are not going to take it seriously if it continues like this!</div><br/><div id="42345467" class="c"><input type="checkbox" id="c-42345467" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345198">parent</a><span>|</span><a href="#42345477">next</a><span>|</span><label class="collapse" for="c-42345467">[-]</label><label class="expand" for="c-42345467">[5 more]</label></div><br/><div class="children"><div class="content">I think we agree; to clarify, sharp messaging isn&#x27;t inaccurate messaging. And I believe the story is not overhyped given the evidence: the benchmark resisted a $1M prize pool for ~6 months. But I concede we did obsess about the story to give it the best chance of survival in the marketplace of ideas against the incumbent AI research meme (LLM scaling). Now that the AI research field is coming around to the idea that something beyond deep learning is needed, the story matters less, and the benchmark, and future versions, can stand on their utility as a compass towards AGI.</div><br/><div id="42346013" class="c"><input type="checkbox" id="c-42346013" checked=""/><div class="controls bullet"><span class="by">mrandish</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345467">parent</a><span>|</span><a href="#42345611">next</a><span>|</span><label class="collapse" for="c-42346013">[-]</label><label class="expand" for="c-42346013">[1 more]</label></div><br/><div class="children"><div class="content">Mike - please know that not everyone who appreciates ARC feels the same way as the GP. I&#x27;m not an academic researcher but I am quite sensitive to hype and excessive marketing. I&#x27;ve never felt the ARC site was anything other than appropriately professional.<p>Even revisiting it now, I don&#x27;t see anything wrong with being concisely clear and even a little provocative in stating your case on your own site. Especially since a key value of ARC is getting more objectively grounded regarding progress toward AGI. On top of that ARC is &quot;A non-profit for the public advancement of open artificial general intelligence&quot; that you guys are personally donating serious money and time to that&#x27;s helping a field where a lot of entrepreneurs are going to make money and academics are going to advance their careers.<p>My perception is ARC tried it the other way for years but a lot of academics and AI pundits ignored or dismissed it without ever meaningfully engaging with it. &quot;Sharpening&quot; the message this year has clearly paid off in bringing attention that&#x27;s shifted the conversation and is helping advance progress toward AGI in ways nothing else has. I also greatly appreciate the time and care you and Francois have put into making the ARC proposition clear enough for non-technical people to understand. That&#x27;s hard to do and doesn&#x27;t happen by accident.<p>Personally, I&#x27;ve found ARC valuable in the real world outside of academia and domain experts because it provides a conceptually simple starting place to discuss with non-technical people what the term AGI might even mean. My high school-aged daughter asked me about vague AGI impending doom scenarios she heard on TikTok. I had her solve a couple ARC samples and then pointed out that today&#x27;s best AIs aren&#x27;t yet close to doing the same. This counter-intuitive revelation got her pondering the &quot;Why?&quot; which led to a deep discussion about the multi-dimensional breadth of human creativity and an appreciation of the many ways artificial intelligences might differ from human intelligence.</div><br/></div></div><div id="42345611" class="c"><input type="checkbox" id="c-42345611" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345467">parent</a><span>|</span><a href="#42346013">prev</a><span>|</span><a href="#42345477">next</a><span>|</span><label class="collapse" for="c-42345611">[-]</label><label class="expand" for="c-42345611">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Now that the AI research field is coming around to the idea that something beyond deep learning is needed,<p>I have not heard this from anyone that I work with! It would be a curious violation of info theory were this to be the case.<p>Certainly, some things cannot efficiently be learned from data. This is a case where some other kind of inductive bias or prior is needed (again, from info theory) -- but replacing deep learning entirely would be rather silly.<p>Part of the reason that a number of researchers don&#x27;t take the benchmark more seriously is because it&#x27;s meant to cripple the results. For example, in the name of reducing brute force search, the compute was severely limited! This turned many off to begin with. The general contention as I understand was to let compute be a reasonable amount, but this would not play well with the numbers game. Because if you restrict compute beyond a reasonable point, it makes the numbers artificially low for people who don&#x27;t know what&#x27;s going on behind the scenes. And this ends up biasing the results unreasonably to favor the original messaging, (i.e., &quot;We need something other than deep learning.&quot;)<p>If it was structured with a reasonable amount of compute, and instead, time-accuracy gates were used for prizes, it would be much more open. But people do not use it because the game is rigged to begin with!<p>Unfortunately due to that, plus the consistent goal-post moving of the benchmark is why it&#x27;s generally not really held with staying power in the research community -- the messaging changes based upon what is convenient for publicity, and there&#x27;s unfortunately been a history of similar things in the past in the pedigree leading up to the ARC prize itself.<p>It is not entirely unsalvageable, but there really needs to be a turnaround of how the competition and prize is managed in order to win back people&#x27;s trust. Placing a thumb on the scales to confirm a prior bias&#x2F;previous messaging may work for a little while, but over time it robs the metric of its usability over time as the greater research community loses trust.</div><br/><div id="42346093" class="c"><input type="checkbox" id="c-42346093" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345611">parent</a><span>|</span><a href="#42345477">next</a><span>|</span><label class="collapse" for="c-42346093">[-]</label><label class="expand" for="c-42346093">[2 more]</label></div><br/><div class="children"><div class="content">I think you’re overly fixated on some minor points relative to the overall utility on offer here.  And also skewing the facts a bit.  For example at one point you quote the OP on words that were never said as far as I can see. At another point, you characterize their position as “replacing deep learning entirely” which, as far as I can tell, has never been advocated for in this comment thread or on behalf of ARC.</div><br/><div id="42346243" class="c"><input type="checkbox" id="c-42346243" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42346093">parent</a><span>|</span><a href="#42345477">next</a><span>|</span><label class="collapse" for="c-42346243">[-]</label><label class="expand" for="c-42346243">[1 more]</label></div><br/><div class="children"><div class="content">That is an understandable statement, and probably fair as well I feel.<p>Much of this comes in reference to statements from fchollet w.r.t. replacing deep learning -- around the time of the initial prize, with a lot of the much more hype marketing, this was essentially the thru-line that was used, and it left a bitter taste in a number of peoples&#x27; mouths. W.r.t. misquoting, they did say that we needed something &quot;beyond&quot; deep learning, not &quot;other than&quot; here, and that is on me.<p>The utility is certainly still present, if I feel diminished, and it probably is a case of my own frustrations due to previous similar issues leading up to the ARC prize.<p>That being said, I do agree in retrospect that my response skewed from being objective -- it is a benchmark with a mixed history, but that doesn&#x27;t mean that I should get personally caught up in it.</div><br/></div></div></div></div></div></div></div></div><div id="42345477" class="c"><input type="checkbox" id="c-42345477" checked=""/><div class="controls bullet"><span class="by">iwsk</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345198">parent</a><span>|</span><a href="#42345467">prev</a><span>|</span><a href="#42344483">next</a><span>|</span><label class="collapse" for="c-42345477">[-]</label><label class="expand" for="c-42345477">[1 more]</label></div><br/><div class="children"><div class="content">we live in a society</div><br/></div></div></div></div></div></div></div></div><div id="42344483" class="c"><input type="checkbox" id="c-42344483" checked=""/><div class="controls bullet"><span class="by">mrandish</span><span>|</span><a href="#42343621">parent</a><span>|</span><a href="#42344486">prev</a><span>|</span><a href="#42344957">next</a><span>|</span><label class="collapse" for="c-42344483">[-]</label><label class="expand" for="c-42344483">[1 more]</label></div><br/><div class="children"><div class="content">Congrats to you and Francois on the success of ARC-AGI 24 and thanks so much for doing it. I just finished the technical report and am encouraged! It&#x27;s great to finally see some tangible progress in research that is both novel and plausibly in fruitful directions.</div><br/></div></div><div id="42344957" class="c"><input type="checkbox" id="c-42344957" checked=""/><div class="controls bullet"><span class="by">padswo1</span><span>|</span><a href="#42343621">parent</a><span>|</span><a href="#42344483">prev</a><span>|</span><a href="#42344563">next</a><span>|</span><label class="collapse" for="c-42344957">[-]</label><label class="expand" for="c-42344957">[3 more]</label></div><br/><div class="children"><div class="content">I don’t think ARC has particularly advanced the research. The approaches that are successful were developed elsewhere and then applied to ARC. Happy to be shown somewhere this is not the case.<p>In the case of TTT, I wouldn’t really describe that as a ‘new AGI reasoning approach’. People have been fine tuning deep learning models on specific tasks for a long time.<p>The fundamental instinct driving the creation of ARC - that ‘deep learning cannot do system 2 thinking’, is under threat of being proven wrong very soon. Attempts to define the approaches that are working as somehow not ‘traditional deep learning’ really seem like shifting the goal posts.</div><br/><div id="42345368" class="c"><input type="checkbox" id="c-42345368" checked=""/><div class="controls bullet"><span class="by">mikeknoop</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42344957">parent</a><span>|</span><a href="#42344563">next</a><span>|</span><label class="collapse" for="c-42345368">[-]</label><label class="expand" for="c-42345368">[2 more]</label></div><br/><div class="children"><div class="content">Correct, fine-tuning is not new. It&#x27;s long been used to augment foundational LLMs with private data. Eg. private enterprise data. We do this at Zapier, for instance.<p>The new and surprising thing about test-time training (TTT) is how effective it is an approach to deal with novel abstract reasoning problems like ARC-AGI.<p>TTT was pioneered by Jack Cole last year and popularized this year by several teams, including this winning paper: <a href="https:&#x2F;&#x2F;ekinakyurek.github.io&#x2F;papers&#x2F;ttt.pdf" rel="nofollow">https:&#x2F;&#x2F;ekinakyurek.github.io&#x2F;papers&#x2F;ttt.pdf</a></div><br/><div id="42347008" class="c"><input type="checkbox" id="c-42347008" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#42343621">root</a><span>|</span><a href="#42345368">parent</a><span>|</span><a href="#42344563">next</a><span>|</span><label class="collapse" for="c-42347008">[-]</label><label class="expand" for="c-42347008">[1 more]</label></div><br/><div class="children"><div class="content">How is TTT anything other than a deep learning algorithm? We have a deep learning model, we generate training data based on an example and use a stochastic gradient descent to update the model weights to improve its predictions according to the training data. This is a classic DL paradigm. I just don’t see why would you consider this an advancement if you your goal is to move “beyond” deep learning.</div><br/></div></div></div></div></div></div><div id="42344563" class="c"><input type="checkbox" id="c-42344563" checked=""/><div class="controls bullet"><span class="by">trott</span><span>|</span><a href="#42343621">parent</a><span>|</span><a href="#42344957">prev</a><span>|</span><a href="#42343975">next</a><span>|</span><label class="collapse" for="c-42344563">[-]</label><label class="expand" for="c-42344563">[1 more]</label></div><br/><div class="children"><div class="content">Mike and François,<p>Compute is limited during inference, and this naturally limits brute-force program search.<p>But this doesn&#x27;t prevent one from creating a huge ARC-like dataset ahead of time, like BARC did (but bigger), and training a correspondingly huge NN on it.<p>Placing a limit on the submission size could foil this kind of brute-force approach though. I wonder if you are considering this for 2025?</div><br/></div></div></div></div><div id="42343975" class="c"><input type="checkbox" id="c-42343975" checked=""/><div class="controls bullet"><span class="by">celeritascelery</span><span>|</span><a href="#42343621">prev</a><span>|</span><a href="#42344336">next</a><span>|</span><label class="collapse" for="c-42343975">[-]</label><label class="expand" for="c-42343975">[10 more]</label></div><br/><div class="children"><div class="content">What surprises me about this is how poorly general-purpose LLMs do. The best one is OpenAI o1-preview at 18%. This is significantly worse than the purpose-built models like ARChitects (which scored 53.5). This model used TTT to train on the ARC-AGI task specification (amoung other things). It seems that even if someone creates a model that can &quot;solve&quot; ARC, it still is not indicative of AGI since it is not &quot;general&quot; anymore, it is just specialized to this particular task. Similar to how chess engines are not AGI, despite being superhuman at chess. It will be much more convincing when general models not trained specifically for ARC can still score well on it.<p>They do mention that some of the tasks here are susceptible to brute force and they plan to address that in ARC-AGI-2.<p>&gt; nearly half (49%) of the private evaluation set was solved by at least one team during the original 2020 Kaggle competition all of which were using some variant of brute-force program search. This suggests a large fraction of ARC-AGI-1 tasks are susceptible to this kind of method and does not carry much useful signal towards general intelligence.</div><br/><div id="42344172" class="c"><input type="checkbox" id="c-42344172" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42343975">parent</a><span>|</span><a href="#42344692">next</a><span>|</span><label class="collapse" for="c-42344172">[-]</label><label class="expand" for="c-42344172">[4 more]</label></div><br/><div class="children"><div class="content">It is correct that the first model that will beat ARC-AGI will only be able to handle ARC-AGI tasks. However, the idea is that the <i>architecture</i> of that model should be able to be repurposed to arbitrary problems. That is what makes ARC-AGI a good compass towards AGI (unlike chess).<p>For instance, current top models use TTT, which is a completely general-purpose technique that provides the most significant boost to DL model&#x27;s generalization power in recent memory.<p>The other category of approach that is working well is program synthesis -- if pushed to the extent that it could solve ARC-AGI, the same system could be redeployed to solve arbitrary programming tasks, as well as tasks isomorphic to programming (such as theorem proving).</div><br/><div id="42344382" class="c"><input type="checkbox" id="c-42344382" checked=""/><div class="controls bullet"><span class="by">scoobertdoobert</span><span>|</span><a href="#42343975">root</a><span>|</span><a href="#42344172">parent</a><span>|</span><a href="#42344692">next</a><span>|</span><label class="collapse" for="c-42344382">[-]</label><label class="expand" for="c-42344382">[3 more]</label></div><br/><div class="children"><div class="content">François, have you coded and tested a solution yourself that you think will work best?</div><br/><div id="42346334" class="c"><input type="checkbox" id="c-42346334" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#42343975">root</a><span>|</span><a href="#42344382">parent</a><span>|</span><a href="#42344692">next</a><span>|</span><label class="collapse" for="c-42346334">[-]</label><label class="expand" for="c-42346334">[2 more]</label></div><br/><div class="children"><div class="content">Hey, he&#x27;s the visionary. You come up with the nuts and bolts.</div><br/><div id="42347795" class="c"><input type="checkbox" id="c-42347795" checked=""/><div class="controls bullet"><span class="by">homarp</span><span>|</span><a href="#42343975">root</a><span>|</span><a href="#42346334">parent</a><span>|</span><a href="#42344692">next</a><span>|</span><label class="collapse" for="c-42347795">[-]</label><label class="expand" for="c-42347795">[1 more]</label></div><br/><div class="children"><div class="content">is keras nuts and bolts enough?</div><br/></div></div></div></div></div></div></div></div><div id="42344692" class="c"><input type="checkbox" id="c-42344692" checked=""/><div class="controls bullet"><span class="by">mrandish</span><span>|</span><a href="#42343975">parent</a><span>|</span><a href="#42344172">prev</a><span>|</span><a href="#42346267">next</a><span>|</span><label class="collapse" for="c-42344692">[-]</label><label class="expand" for="c-42344692">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It seems that even if someone creates a model that can &quot;solve&quot; ARC, it still is not indicative of AGI since it is not &quot;general&quot; anymore<p>I recently explained why I like ARC to a non-technical friend this way: &quot;When an AI solves ARC it won&#x27;t be proof of AGI. It&#x27;s the opposite. As long as ARC remains unsolved I&#x27;m confident we&#x27;re not even close to AGI.&quot;<p>For the sake of being provocative, I&#x27;d even argue that ARC remaining unsolved is a sign we&#x27;re not yet making meaningful progress in the right direction. AGI is the top of Everest. ARC is base camp.</div><br/><div id="42345543" class="c"><input type="checkbox" id="c-42345543" checked=""/><div class="controls bullet"><span class="by">iwsk</span><span>|</span><a href="#42343975">root</a><span>|</span><a href="#42344692">parent</a><span>|</span><a href="#42346267">next</a><span>|</span><label class="collapse" for="c-42345543">[-]</label><label class="expand" for="c-42345543">[2 more]</label></div><br/><div class="children"><div class="content">in other words, solving ARC is necessary but not sufficient for AGI</div><br/><div id="42346258" class="c"><input type="checkbox" id="c-42346258" checked=""/><div class="controls bullet"><span class="by">mrandish</span><span>|</span><a href="#42343975">root</a><span>|</span><a href="#42345543">parent</a><span>|</span><a href="#42346267">next</a><span>|</span><label class="collapse" for="c-42346258">[-]</label><label class="expand" for="c-42346258">[1 more]</label></div><br/><div class="children"><div class="content">Yes! That&#x27;s the exact phrase I would have used with someone on HN. But that doesn&#x27;t describe my non-technical friend. :-)</div><br/></div></div></div></div></div></div><div id="42346267" class="c"><input type="checkbox" id="c-42346267" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#42343975">parent</a><span>|</span><a href="#42344692">prev</a><span>|</span><a href="#42344336">next</a><span>|</span><label class="collapse" for="c-42346267">[-]</label><label class="expand" for="c-42346267">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What surprises me about this is how poorly general-purpose LLMs do. The best one is OpenAI o1-preview at 18%.<p>o1-preview doesn&#x27;t even have image input, so I wonder how they used it.<p>Also, Ryan Greenblatts solution basically does &quot;best of 4000&quot; iirc. Presumably o1-preview was single shot.</div><br/><div id="42347333" class="c"><input type="checkbox" id="c-42347333" checked=""/><div class="controls bullet"><span class="by">celeritascelery</span><span>|</span><a href="#42343975">root</a><span>|</span><a href="#42346267">parent</a><span>|</span><a href="#42344336">next</a><span>|</span><label class="collapse" for="c-42347333">[-]</label><label class="expand" for="c-42347333">[1 more]</label></div><br/><div class="children"><div class="content">None of the models use images, they all operate and a json format the describes the input squares.</div><br/></div></div></div></div></div></div><div id="42344336" class="c"><input type="checkbox" id="c-42344336" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#42343975">prev</a><span>|</span><a href="#42346897">next</a><span>|</span><label class="collapse" for="c-42344336">[-]</label><label class="expand" for="c-42344336">[9 more]</label></div><br/><div class="children"><div class="content">The first question I still have is what happened to core knowledge priors. The white paper that introduced ARC made a big todo about how core knowledge priors are necessary to solve ARC tasks but from what I can tell none of the best-performing (or at-all performing) systems have anything to do with core knowlege priors.<p>So what happened to that assumption? Is it dead?<p>The second question I still have is about the defenses of ARC against memorisation-based, big-data approaches. I note that the second best system is based on an LLM with &quot;test time training&quot; where the first two steps are:<p><pre><code>  initial finetuning on similar tasks 
  auxiliary task format and augmentations
</code></pre>
Which is to say, a data augmentation approach. With big data comes great responsibility and the authors of the second-best system don&#x27;t disappoint: they claim that by training on more examples they achieve reasoning.<p>So what happened to the claim that ARC is secure against big-data approaches? Is it dead?</div><br/><div id="42344425" class="c"><input type="checkbox" id="c-42344425" checked=""/><div class="controls bullet"><span class="by">fchollet</span><span>|</span><a href="#42344336">parent</a><span>|</span><a href="#42344753">next</a><span>|</span><label class="collapse" for="c-42344425">[-]</label><label class="expand" for="c-42344425">[4 more]</label></div><br/><div class="children"><div class="content">What all top models do is recombine at test time the knowledge they already have. So they all possess Core Knowledge priors. Techniques to acquire them vary:<p>* Use a pretrained LLM and hope that relevant programs will be memorized via exposure to text data (this doesn&#x27;t work that well)<p>* Pretrain a LLM on ARC-AGI-like data<p>* Hardcode the priors into a DSL<p>&gt; Which is to say, a data augmentation approach<p>The key bit isn&#x27;t the data augmentation but the TTT. TTT is a way to lift the #1 issue with DL models: that they cannot recombine their knowledge at test time to adapt to something they haven&#x27;t seen before (strong generalization). You can argue whether TTT is the right way to achieve this, but there is no doubt that TTT is a major advance in this direction.<p>The top ARC-AGI models perform well not because they&#x27;re trained on tons of data, but because they can adapt to novelty at test time (usually via TTT). For instance, if you drop the TTT component you will see that these large models trained on millions of synthetic ARC-AGI tasks drop to &lt;10% accuracy. This demonstrates empirically that ARC-AGI cannot be solved purely via memorization and interpolation.</div><br/><div id="42345067" class="c"><input type="checkbox" id="c-42345067" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#42344336">root</a><span>|</span><a href="#42344425">parent</a><span>|</span><a href="#42344626">next</a><span>|</span><label class="collapse" for="c-42345067">[-]</label><label class="expand" for="c-42345067">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; So they all possess Core Knowledge priors.<p>Do you mean the ones from your white paper? The same ones that humans possess? How do you know this?<p>&gt;&gt; The key bit isn&#x27;t the data augmentation but the TTT.<p>I haven&#x27;t had the chance to read the papers carefully. Have they done ablation studies? For instance, is the following a guess or is it an empirical result?<p>&gt;&gt; For instance, if you drop the TTT component you will see that these large models trained on millions of synthetic ARC-AGI tasks drop to &lt;10% accuracy.</div><br/></div></div><div id="42344626" class="c"><input type="checkbox" id="c-42344626" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#42344336">root</a><span>|</span><a href="#42344425">parent</a><span>|</span><a href="#42345067">prev</a><span>|</span><a href="#42344753">next</a><span>|</span><label class="collapse" for="c-42344626">[-]</label><label class="expand" for="c-42344626">[2 more]</label></div><br/><div class="children"><div class="content">&gt;This demonstrates empirically that ARC-AGI cannot be solved purely via memorization and interpolation<p>Now that the current challenge is over, and a successor dataset is in the works, can we see how well the leading LLMs perform against the private test set?</div><br/><div id="42345406" class="c"><input type="checkbox" id="c-42345406" checked=""/><div class="controls bullet"><span class="by">tuukkah</span><span>|</span><a href="#42344336">root</a><span>|</span><a href="#42344626">parent</a><span>|</span><a href="#42344753">next</a><span>|</span><label class="collapse" for="c-42345406">[-]</label><label class="expand" for="c-42345406">[1 more]</label></div><br/><div class="children"><div class="content">I think the &quot;semi-private&quot; numbers here already measure that: <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;2024-results" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;2024-results</a><p>For example, Claude 3.5 gets 14% in semi-private eval vs 21% in public eval. I remember reading an explanation of &quot;semi-private&quot; earlier but cannot find it now.</div><br/></div></div></div></div></div></div><div id="42344753" class="c"><input type="checkbox" id="c-42344753" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42344336">parent</a><span>|</span><a href="#42344425">prev</a><span>|</span><a href="#42346897">next</a><span>|</span><label class="collapse" for="c-42344753">[-]</label><label class="expand" for="c-42344753">[4 more]</label></div><br/><div class="children"><div class="content">Even the strongest possible interpretation of the results wouldn&#x27;t conclude &quot;ARC-AGI is dead&quot; because none of the submissions came especially close to human-level performance; the criteria was 85% success but the best in 2024 was 55%.<p>That said, I think there should be consideration via information thermodynamics: even with TTT these program-generating systems are using an enormous amount of bits compared to a human mind, a tiny portion of which solves ARC quickly and easily using causality-first principles of reasoning.<p>Another point: suppose a system solves ARC-AGI with 99% accuracy. Then it should be tested on &quot;HARC-HAGI,&quot; a variant that uses hexagons instead of squares. This likely wouldn&#x27;t trip up a human very much - perhaps a small decrease due to increased surface area for brain farts. But if the AI  needs to be retrained on a ton of hexagonal examples, then that AI can&#x27;t be an AGI candidate.</div><br/><div id="42345261" class="c"><input type="checkbox" id="c-42345261" checked=""/><div class="controls bullet"><span class="by">szvsw</span><span>|</span><a href="#42344336">root</a><span>|</span><a href="#42344753">parent</a><span>|</span><a href="#42346897">next</a><span>|</span><label class="collapse" for="c-42345261">[-]</label><label class="expand" for="c-42345261">[3 more]</label></div><br/><div class="children"><div class="content">&gt; That said, I think there should be consideration via information thermodynamics: even with TTT these program-generating systems are using an enormous amount of bits compared to a human mind, a tiny portion of which solves ARC quickly and easily using causality-first principles of reasoning.<p>This isn’t my area of expertise, but it seems plausible to me that what you said is completely erroneous or at the very least completely unverifiable at this point in time. How do you quantify how many bits it takes a human mind to solve one of the ARC problems?<p>That seems likely beyond the level of insight we have into the structure of cognition and information storage etc etc in wetware. I could of course be wrong and would love to be corrected if so! You mentioned a “tiny portion” of the human mind, but (as far as I’m aware), any given “small” part of human cognition still involves huge amounts of complexity and compute.<p>Maybe you are saying that the high level decision making a human goes through when solving can be represented with a relatively small number of pieces of information&#x2F;logical operations (as opposed to a much lower level notion closer to the wetware of the quantity of information) but then it seems unfair to compare to the low level equivalent (weights &amp; biases, FLOPs etc) in the ML system when there may be higher order equivalents.<p>I do appreciate the general notion of wanting to normalize against <i>something</i> though, and some notion of information seems like a reasonable choice, but practically out of our reach. Maybe something like peak power or total energy consumption would be a more reasonable choice, which we can at least get a lower and upper bounds on in the human case (metabolic rates are pretty well studied, and even if we don’t have a good idea of how much energy is involved in completing cognitive tasks we can at least get bounds for running the entire system in that period of time) and close to a precise value in the ML case.</div><br/><div id="42345690" class="c"><input type="checkbox" id="c-42345690" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#42344336">root</a><span>|</span><a href="#42345261">parent</a><span>|</span><a href="#42346897">next</a><span>|</span><label class="collapse" for="c-42345690">[-]</label><label class="expand" for="c-42345690">[2 more]</label></div><br/><div class="children"><div class="content">I was speaking loosely but the operative term is &quot;information thermodynamics&quot;: comparing bits of AI output versus bits of <i>intentional</i> human thought, ignoring statistical&#x2F;physical bits related to ANN inference or biological neuron activity. The &quot;tiny chunk of the human mind&quot; thing was a distraction I shouldn&#x27;t have included.<p>These AI output as tokens hundreds of potential solutions, whereas a human solving a very tricky ARC problem might need at most a few dozen cases to run through. There&#x27;s a big mess of ANN linear algebra &#x2F; human subconscious thought and I agree these messes can&#x27;t be compared (or even identified in the human case). But we can compare the efficiency of the solution. It is possible that subconsciously humans &quot;generate&quot; hundreds of solutions that are mostly discarded, but I don&#x27;t think the brain is fast enough to do that at the speed of conscious thought: it&#x27;s a 50bn core processor but each core is only 200Hz and they aren&#x27;t general-purpose CPUs. It also seems inconsistent with how humans solve these problems.<p>I believe energy usage would be even more misleading: in terms of operations&#x2F;second a human brain is comparable to a 2020s supercomputer running at 30MW, but it only consumes 300 watts. (I was thinking about this with the &quot;tiny portion&quot; comment but it is irrelevant.)</div><br/><div id="42347069" class="c"><input type="checkbox" id="c-42347069" checked=""/><div class="controls bullet"><span class="by">szvsw</span><span>|</span><a href="#42344336">root</a><span>|</span><a href="#42345690">parent</a><span>|</span><a href="#42346897">next</a><span>|</span><label class="collapse" for="c-42347069">[-]</label><label class="expand" for="c-42347069">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the response! I was trying to allude to what you are describing with the bit (ha) I mentioned about higher order thinking but you obviously articulated it much more effectively.<p>I guess I’m not sure it’s obvious where the right line to draw the boundary for “intentional human thought” is? Surely there is a lot of cognition and representation going on at extraordinary speeds that exist in some hazy border region between instinct&#x2F;reflex&#x2F;subconscious and conscious thought.  Still, having said that, I do see what you are saying about trying to compare the complexity of the formal path to the solution, or at least what the human thinks their formal path was.<p>I’m generally of the mind (also, ha) that we won’t really ever be able to quantify any of this in a meaningful way in the short term and if anything which qualifies as AGI does emerge, it might only be something which is an “I know it when I see it” kind of evaluation…<p>Where are you getting 300W from? The body only dumps 100W of heat at rest and uses like 300-400W during moderate physical activity, so I’m a little confused about what you are describing there. The typical estimates I’ve seen are like 20W or so for the brain.<p>Edit: I should also say that what you describe does seem like a great way to compare solutions between computational systems currently being developed and a good one to use to try to push development forward; it just seems quixotic to try to be able to use it comparatively with human cognition or to be able to meaningfully use it to define where AGI is, which might not be what you were advocating for at all, in which case, sorry for misinterpreting!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="42346897" class="c"><input type="checkbox" id="c-42346897" checked=""/><div class="controls bullet"><span class="by">nnx</span><span>|</span><a href="#42344336">prev</a><span>|</span><a href="#42347358">next</a><span>|</span><label class="collapse" for="c-42346897">[-]</label><label class="expand" for="c-42346897">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m unable to figure out how to solve current Daily Puzzle (Puzzle ID: 79369cc6) at <a href="https:&#x2F;&#x2F;arcprize.org&#x2F;play" rel="nofollow">https:&#x2F;&#x2F;arcprize.org&#x2F;play</a><p>Either I&#x27;m really dumb or the test is getting into captcha-like territory where humans aren&#x27;t really good at solving&#x2F;deciphering the test anymore.</div><br/></div></div><div id="42347358" class="c"><input type="checkbox" id="c-42347358" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#42346897">prev</a><span>|</span><a href="#42345844">next</a><span>|</span><label class="collapse" for="c-42347358">[-]</label><label class="expand" for="c-42347358">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a little surprised by the seeming enthusiasm in the report for TTT as an approach. The results speak for themselves and TTT seems like a powerful approach. But the dependence on large amounts of synthetic pre-training data seems to contradict the philosophical ideas behind the competition.</div><br/></div></div><div id="42345844" class="c"><input type="checkbox" id="c-42345844" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#42347358">prev</a><span>|</span><a href="#42344624">next</a><span>|</span><label class="collapse" for="c-42345844">[-]</label><label class="expand" for="c-42345844">[2 more]</label></div><br/><div class="children"><div class="content">Reasons that I can&#x27;t take this benchmark seriously:<p>1. Existing brute force algorithms solve 40% of this &quot;reasoning&quot; and &quot;generalization&quot; test.<p>2. AGI must evidently fit on a single 16GB, decade-old GPU?<p>3. If ARC fails blind people, it&#x27;s not a reasoning test. Reasoning is independent of visual acuity. So ARC is at best a vision processing <i>then</i> reasoning test. SotA model &quot;failure&quot; is meaningless. (&quot;But what about the other format, JSON?&quot; Yeah, I would <i>love</i> to see the human solve rate on that...)</div><br/><div id="42346386" class="c"><input type="checkbox" id="c-42346386" checked=""/><div class="controls bullet"><span class="by">pshirshov</span><span>|</span><a href="#42345844">parent</a><span>|</span><a href="#42344624">next</a><span>|</span><label class="collapse" for="c-42346386">[-]</label><label class="expand" for="c-42346386">[1 more]</label></div><br/><div class="children"><div class="content">Ergh. This test checks how good you can infer cellular automaton rules. Considering that CAs are Turing-complete, that might be a very good entry-level intelligence detector.<p>If it&#x27;s so easy to brute force, why wouldn&#x27;t you claim the $1M?</div><br/></div></div></div></div><div id="42344624" class="c"><input type="checkbox" id="c-42344624" checked=""/><div class="controls bullet"><span class="by">hulium</span><span>|</span><a href="#42345844">prev</a><span>|</span><a href="#42344287">next</a><span>|</span><label class="collapse" for="c-42344624">[-]</label><label class="expand" for="c-42344624">[2 more]</label></div><br/><div class="children"><div class="content">Were there any interesting non-neural approaches? I was wondering whether there is any underlying structure in the ARC tasks that could tell us something about algorithms for &quot;reasoning&quot; problems in general.</div><br/><div id="42344661" class="c"><input type="checkbox" id="c-42344661" checked=""/><div class="controls bullet"><span class="by">neoneye2</span><span>|</span><a href="#42344624">parent</a><span>|</span><a href="#42344287">next</a><span>|</span><label class="collapse" for="c-42344661">[-]</label><label class="expand" for="c-42344661">[1 more]</label></div><br/><div class="children"><div class="content">The 3rd place solution by Agnis Liukis, solves 40 tasks.
<a href="https:&#x2F;&#x2F;www.kaggle.com&#x2F;code&#x2F;gregkamradt&#x2F;arc-prize-2024-solution-4th-place-score-40-811b72" rel="nofollow">https:&#x2F;&#x2F;www.kaggle.com&#x2F;code&#x2F;gregkamradt&#x2F;arc-prize-2024-solut...</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>