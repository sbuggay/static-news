<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723539678927" as="style"/><link rel="stylesheet" href="styles.css?v=1723539678927"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/judofyr/spice">Spice: Fine-grained parallelism with sub-nanosecond overhead in Zig</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>dsp_person</span> | <span>36 comments</span></div><br/><div><div id="41233487" class="c"><input type="checkbox" id="c-41233487" checked=""/><div class="controls bullet"><span class="by">lcof</span><span>|</span><a href="#41231023">next</a><span>|</span><label class="collapse" for="c-41233487">[-]</label><label class="expand" for="c-41233487">[1 more]</label></div><br/><div class="children"><div class="content">Interesting research work! Besides the code itself, there is some good reasoning and the documentation is well written<p>The 2018 paper on heartbeat scheduling is also an interesting read <a href="https:&#x2F;&#x2F;www.andrew.cmu.edu&#x2F;user&#x2F;mrainey&#x2F;papers&#x2F;heartbeat.pdf" rel="nofollow">https:&#x2F;&#x2F;www.andrew.cmu.edu&#x2F;user&#x2F;mrainey&#x2F;papers&#x2F;heartbeat.pdf</a></div><br/></div></div><div id="41231023" class="c"><input type="checkbox" id="c-41231023" checked=""/><div class="controls bullet"><span class="by">nirushiv</span><span>|</span><a href="#41233487">prev</a><span>|</span><a href="#41231779">next</a><span>|</span><label class="collapse" for="c-41231023">[-]</label><label class="expand" for="c-41231023">[20 more]</label></div><br/><div class="children"><div class="content">I haven’t read through the code in detail but I can tell you “sub-nanosecond overhead” is misleading and marketing fluff. On first look, the measure seems to be some convoluted “time per thing” where the number of threads is far far smaller than the number of “thing”s</div><br/><div id="41232892" class="c"><input type="checkbox" id="c-41232892" checked=""/><div class="controls bullet"><span class="by">judofyr</span><span>|</span><a href="#41231023">parent</a><span>|</span><a href="#41231256">next</a><span>|</span><label class="collapse" for="c-41232892">[-]</label><label class="expand" for="c-41232892">[3 more]</label></div><br/><div class="children"><div class="content">Author here.<p>I knew that some people would react negatively to the term, but I can assure the intention is for you to have a <i>better</i> understanding of exactly how and when you should use Spice and Rayon. I would recommend reading the benchmark document: <a href="https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;blob&#x2F;main&#x2F;bench&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;blob&#x2F;main&#x2F;bench&#x2F;README.md</a>.<p>What people typically do when comparing parallel code is to <i>only</i> compare the sequential&#x2F;baseline with a parallel version running at all threads (16). Let&#x27;s use the numbers for Rayon that I got for the 100M case:<p>- Sequential version: 7.48 ns.<p>- Rayon: 1.64 ns.<p>Then they go &quot;For this problem Rayon showed a 4.5x speed-up, but uses 16 threads. Oh no, this is a bad fit.&quot; That&#x27;s very true, but you don&#x27;t learn anything from that. How can I use apply this knowledge to other types of problems?<p>However, if you run the same benchmark on varying number of threads you learn something more interesting: The scheduler in Rayon is actually pretty good at giving work to separate threads, but the overall work execution mechanism has a ~15 ns overhead. Despite this being an utterly useless program we&#x27;ve learnt something that we can apply later on: Our smallest unit of work should probably be a bit bigger than ~7 ns before we reach for Rayon. (Unless it&#x27;s more important for use to reduce overall latency at the cost of the throughput of the whole system.)<p>In comparison, if you read the Rayon documentation they will not attempt to give you <i>any</i> number. They just say &quot;Conceptually, calling join() is similar to spawning two threads, one executing each of the two closures. However, the implementation is quite different and <i>incurs very low overhead</i>&quot;: <a href="https:&#x2F;&#x2F;docs.rs&#x2F;rayon&#x2F;latest&#x2F;rayon&#x2F;fn.join.html" rel="nofollow">https:&#x2F;&#x2F;docs.rs&#x2F;rayon&#x2F;latest&#x2F;rayon&#x2F;fn.join.html</a>.<p>(Also: If I wanted to be misleading I would say &quot;Spice is twice as fast as Rayon since it gets 10x speed-up compared to 4.5x speed-up&quot;)</div><br/><div id="41233595" class="c"><input type="checkbox" id="c-41233595" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232892">parent</a><span>|</span><a href="#41233224">next</a><span>|</span><label class="collapse" for="c-41233595">[-]</label><label class="expand" for="c-41233595">[1 more]</label></div><br/><div class="children"><div class="content">You can just divide the speed-up by the number of cores, and that gives you the parallelization efficiency.<p>I&#x27;ve seen systems that can achieve 99% efficiency on thousands on nodes for real useful applications that involve non-trivial synchronization. Now that is an impressive feat.<p>Sure, there is probably some extra latency to get everything running, but for a sufficiently long program run, that is all irrelevant.</div><br/></div></div><div id="41233224" class="c"><input type="checkbox" id="c-41233224" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232892">parent</a><span>|</span><a href="#41233595">prev</a><span>|</span><a href="#41231256">next</a><span>|</span><label class="collapse" for="c-41233224">[-]</label><label class="expand" for="c-41233224">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the answer, this part is particularly interesting indeed:<p>&gt; Despite this being an utterly useless program we&#x27;ve learnt something that we can apply later on: Our smallest unit of work should probably be a bit bigger than ~7 ns before we reach for Rayon.<p>That&#x27;s a very interesting project.<p>The big limitation I see with the current approach is that the usability of the library is much worth than what Rayon offers.<p>The true magic of Rayon is that you just replace `iter()` with `par_iter()` in you code and <i>voilà!</i> now you have a parallel execution.
But yes it has some overhead, so maybe Rayon could try and implement this kind of scheduling as an alternative so that people pick what works best for their use-case.</div><br/></div></div></div></div><div id="41231256" class="c"><input type="checkbox" id="c-41231256" checked=""/><div class="controls bullet"><span class="by">x-complexity</span><span>|</span><a href="#41231023">parent</a><span>|</span><a href="#41232892">prev</a><span>|</span><a href="#41231167">next</a><span>|</span><label class="collapse" for="c-41231256">[-]</label><label class="expand" for="c-41231256">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  I can tell you “sub-nanosecond overhead” is misleading and marketing fluff<p>If and only if (1-thread Spice - non-parallelized baseline) &gt; 1ns, which their tests back up their claims.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;tree&#x2F;main&#x2F;bench">https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;tree&#x2F;main&#x2F;bench</a></div><br/><div id="41232995" class="c"><input type="checkbox" id="c-41232995" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41231256">parent</a><span>|</span><a href="#41231167">next</a><span>|</span><label class="collapse" for="c-41232995">[-]</label><label class="expand" for="c-41232995">[2 more]</label></div><br/><div class="children"><div class="content">At your link it also says:<p>&quot;Spice shows subpar scalability: The speed-up of using 16 threads was merely ~11x&quot;<p>If that is true, then &quot;Spice&quot; is suitable only for small tasks, which can be completed at most in milliseconds, which can benefit from its low overhead, while for any bigger tasks something better must be used.</div><br/><div id="41233095" class="c"><input type="checkbox" id="c-41233095" checked=""/><div class="controls bullet"><span class="by">judofyr</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232995">parent</a><span>|</span><a href="#41231167">next</a><span>|</span><label class="collapse" for="c-41233095">[-]</label><label class="expand" for="c-41233095">[1 more]</label></div><br/><div class="children"><div class="content">Author here.<p>I’d maybe phrase it as “Spice is not optimal” instead of “Spice is not suited for”, but yes, that’s the conclusion for this benchmark.<p>I’m hoping&#x2F;assuming that for a more typical case (more CPU work being done) Spice will scale better, but I haven’t done the benchmark yet.</div><br/></div></div></div></div></div></div><div id="41231167" class="c"><input type="checkbox" id="c-41231167" checked=""/><div class="controls bullet"><span class="by">mananaysiempre</span><span>|</span><a href="#41231023">parent</a><span>|</span><a href="#41231256">prev</a><span>|</span><a href="#41232139">next</a><span>|</span><label class="collapse" for="c-41231167">[-]</label><label class="expand" for="c-41231167">[7 more]</label></div><br/><div class="children"><div class="content">That is the ecological niche of Rayon (cited) as well, isn’t it? You need to process a lot of things (thousands to millions), you want to parallelize that processing as much as possible (couple dozen of cores tops), you want to not get killed by scheduling overhead. So you account for the per-thing overhead.</div><br/><div id="41231415" class="c"><input type="checkbox" id="c-41231415" checked=""/><div class="controls bullet"><span class="by">CyberDildonics</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41231167">parent</a><span>|</span><a href="#41232139">next</a><span>|</span><label class="collapse" for="c-41231415">[-]</label><label class="expand" for="c-41231415">[6 more]</label></div><br/><div class="children"><div class="content">Anything can have its overhead amortized, but claiming that amortization as your actual overhead is just a lie.</div><br/><div id="41232055" class="c"><input type="checkbox" id="c-41232055" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41231415">parent</a><span>|</span><a href="#41232139">next</a><span>|</span><label class="collapse" for="c-41232055">[-]</label><label class="expand" for="c-41232055">[5 more]</label></div><br/><div class="children"><div class="content">Most engineers aren&#x27;t precise with throughput vs latency. Ideally you should report both figures (and anything else salient in performance-sensitive spaces), but it&#x27;s less a lie and more an extremely commonplace mode of thinking and speaking.<p>Moreover, I think that mode of thought comes from the fact that most programming problems don&#x27;t have hard latency bounds, so throughput dominates the conversation. If I&#x27;m spending 10us on average while handling a 10ms soft deadline, every single component can easily be occasionally 100x more expensive (latency) without me caring, and if it buys me another 1us on average (throughput) then I&#x27;ll save gobs of money in compute.</div><br/><div id="41232307" class="c"><input type="checkbox" id="c-41232307" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232055">parent</a><span>|</span><a href="#41232204">next</a><span>|</span><label class="collapse" for="c-41232307">[-]</label><label class="expand" for="c-41232307">[2 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re trying to split a sub-second task across multiple threads, then latency is probably your main concern.</div><br/><div id="41232742" class="c"><input type="checkbox" id="c-41232742" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232307">parent</a><span>|</span><a href="#41232204">next</a><span>|</span><label class="collapse" for="c-41232742">[-]</label><label class="expand" for="c-41232742">[1 more]</label></div><br/><div class="children"><div class="content">Kind of, but not in a way that matters for this scheduler (at least, I posit, not usually). If you have a single task with a wall-clock time in the 10-1000ms range and want to make it &quot;faster,&quot; yes, you probably want to improve the latency. However, the latency introduced by this experimental library is negligible on the timescales being considered. With that in mind, a throughput improvement is a better description of the sorts of engineering efforts you need to accomplish your goals.<p>From a slightly different perspective, one thing the library does on top of handling small workloads well is handling _finely grained_ workloads. That&#x27;s super important from a usability point of view, since you can express the parallelism in the most natural way and let the library handle the fact that it&#x27;s hard to schedule that task (e.g., most schedulers suck as soon as you&#x27;re talking about work items on the order of a single cache line). Just being able to have a convenient abstraction when writing a long-running job is also a fantastic feature. In that case, we would still care about throughput, not latency.<p>Separately, though definitely more rarely, there are jobs which are sub-second but where it&#x27;s hard to batch many of them at once. If you&#x27;re forced to execute many and care about the time till completion (latency, again at a much longer timescale), being able to make each one much faster is a big deal.<p>I really think that first paragraph is a commonplace occurrence though. Something important is slow (measured in &quot;human&quot; blinking timescales), so you slap a better scheduler and some parallelism at it and start work on the next ticket. Ripgrep, at some level (it has lots of other technical accomplishments; I don&#x27;t want to let this comment give the mistaken impression that I&#x27;m demeaning the project) is useful precisely because it throws parallelism at sub-second problems.</div><br/></div></div></div></div><div id="41232204" class="c"><input type="checkbox" id="c-41232204" checked=""/><div class="controls bullet"><span class="by">CyberDildonics</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232055">parent</a><span>|</span><a href="#41232307">prev</a><span>|</span><a href="#41232139">next</a><span>|</span><label class="collapse" for="c-41232204">[-]</label><label class="expand" for="c-41232204">[2 more]</label></div><br/><div class="children"><div class="content"><i>Most engineers aren&#x27;t precise with throughput vs latency.</i><p>Anyone making claims about performance should know the difference.<p>This isn&#x27;t even about either, it&#x27;s lying about overhead by not counting it correctly.<p>If someone asks what your cable bill is and you say it&#x27;s only $2.50 a month because you have 32 TVs, no one is going to say that makes sense.</div><br/><div id="41232849" class="c"><input type="checkbox" id="c-41232849" checked=""/><div class="controls bullet"><span class="by">hansvm</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232204">parent</a><span>|</span><a href="#41232139">next</a><span>|</span><label class="collapse" for="c-41232849">[-]</label><label class="expand" for="c-41232849">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not &quot;lying&quot; to use the words most likely to correctly get your point across to your target audience. Maybe they could have communicated better (for a seemingly dead project, IMO they put enough time in regardless), but it&#x27;s not lying.<p>&gt; Anyone making claims about performance should know the difference.<p>They probably do know the difference. Knowing the difference isn&#x27;t the thing you&#x27;re quibbling with.<p>&gt; If someone asks what your cable bill is and you say it&#x27;s only $2.50 a month because you have 32 TVs, no one is going to say that makes sense.<p>Sure...because when asking about your cable bill it&#x27;s unambiguous that you want the total number of dollars. The whole reason there&#x27;s any issue at all here is that some of their language is ambiguous if you don&#x27;t consider the target audience and don&#x27;t analyze their charts or read the descriptions of those charts. Picking an analogy which resonates precisely because of the lack of ambiguity doesn&#x27;t say a whole lot about the actual problem at hand.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41232139" class="c"><input type="checkbox" id="c-41232139" checked=""/><div class="controls bullet"><span class="by">jnordwick</span><span>|</span><a href="#41231023">parent</a><span>|</span><a href="#41231167">prev</a><span>|</span><a href="#41231779">next</a><span>|</span><label class="collapse" for="c-41232139">[-]</label><label class="expand" for="c-41232139">[6 more]</label></div><br/><div class="children"><div class="content">Yesterday, he posted on Reddit and I expressed some concern with the benchmarks. The benchmarks are claiming 0.36 ns of overhead per call, but only the computing function. There is a second thread running doing the schedule that the overhead numbers don&#x27;t include. It seems pretty clear he&#x27;s running on a hyperthreaded 8 core machine (so 16 threads), I was guessing 3 Ghz, so that literally a single cycle of overhead.<p>Each extra thread adds more overhead from lock contention. At 16 threads overhead is up to 3.6 ns (so 10 times more). I&#x27;m guessing, but that would mean the 0.36 ns of overhead included an uncontested lock? That&#x27;s not possible. There&#x27;s some other weirdness going on in the benchmark data too. So either I&#x27;m not understanding what he&#x27;s actually timing or maybe there is a bug in the benchmark code.<p>Also, if you multiply all the values out, I think he&#x27;s timing in milliseconds (when runtime is calculated and converted to millis, they come out as whole numbers). Don&#x27;t most benchmarkers have better precision than that? Maybe he&#x27;s just using `time prog` and the data is just really dirty. Or maybe he&#x27;s just choosing really, really bad metrics that totally useless for this (this is probably correct, just not sure if there are more issues).</div><br/><div id="41232574" class="c"><input type="checkbox" id="c-41232574" checked=""/><div class="controls bullet"><span class="by">Veedrac</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232139">parent</a><span>|</span><a href="#41232902">next</a><span>|</span><label class="collapse" for="c-41232574">[-]</label><label class="expand" for="c-41232574">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why it&#x27;s unbelievable. They walk through the required logic to perform scheduling and dispatch; it looks incredibly simple and extremely suited to hiding on an OoO. The benchmark is summing down a binary tree which is going to give you a lot of space to hide what&#x27;s at most a handful of instructions. There&#x27;s obviously no lock contention because, like, look at the algorithm, what locks contending on what?<p>AFAICT this is just a really cool and really practical algorithm for microthreads with near zero cost per semantic thread.</div><br/><div id="41232920" class="c"><input type="checkbox" id="c-41232920" checked=""/><div class="controls bullet"><span class="by">chc4</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232574">parent</a><span>|</span><a href="#41232902">next</a><span>|</span><label class="collapse" for="c-41232920">[-]</label><label class="expand" for="c-41232920">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s obviously no lock<p>What? The threadpool has a shared mutex accessed by each worker thread, which is used for pushing work on heartbeat and for dequeuing work. <a href="https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;blob&#x2F;167deba6e4d319f96d9d67868843fbe9a5b2643f&#x2F;src&#x2F;root.zig#L100">https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;blob&#x2F;167deba6e4d319f96d9d67...</a><p>&quot;Adding more threads to the system will not make your program any slower&quot; is an insane claim to be making for any lightweight task scheduling system, and trivially not true looking at this: if you have 1000 threads as workers and degenerate tree program than each worker will take turns serializing themselves through the mutex. Things like these are massive red flags to anyone reading the README.</div><br/><div id="41233125" class="c"><input type="checkbox" id="c-41233125" checked=""/><div class="controls bullet"><span class="by">Veedrac</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232920">parent</a><span>|</span><a href="#41232902">next</a><span>|</span><label class="collapse" for="c-41233125">[-]</label><label class="expand" for="c-41233125">[1 more]</label></div><br/><div class="children"><div class="content">The README covers this. Its argument is persuasive. If your point is that the constant is badly tuned for theoretical 1000 core machines that don&#x27;t exist, I&#x27;m not sure I care. A 100ns stall at most every 100us becoming more likely when you approach multiple hundreds of cores is hardly a disaster. In the context of the comment I replied to, the difference between 8 and 16 workers is literally zero, as the wakeups are spaced so the locks will never conflict.<p>Actually, if you did have a 32k core machine somehow with magical sufficiently-uniform memory for microthreading to be sensible for it, I think it&#x27;s not even hard to extend the algorithm to work with that. Just put the workers on a 3D torus and only share orthogonally. It means you don&#x27;t have perfect work sharing, but I&#x27;m also pretty sure it doesn&#x27;t matter.</div><br/></div></div></div></div></div></div><div id="41232902" class="c"><input type="checkbox" id="c-41232902" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232139">parent</a><span>|</span><a href="#41232574">prev</a><span>|</span><a href="#41231779">next</a><span>|</span><label class="collapse" for="c-41232902">[-]</label><label class="expand" for="c-41232902">[2 more]</label></div><br/><div class="children"><div class="content">The lock is acquired every 100 microseconds and it is expected that only one thread accesses it at a time.</div><br/><div id="41233631" class="c"><input type="checkbox" id="c-41233631" checked=""/><div class="controls bullet"><span class="by">jnordwick</span><span>|</span><a href="#41231023">root</a><span>|</span><a href="#41232902">parent</a><span>|</span><a href="#41231779">next</a><span>|</span><label class="collapse" for="c-41233631">[-]</label><label class="expand" for="c-41233631">[1 more]</label></div><br/><div class="children"><div class="content">I know. But I have no other explanation as to the latency increase that is seen as the number of threads increases. Do you have a better theory?</div><br/></div></div></div></div></div></div></div></div><div id="41231779" class="c"><input type="checkbox" id="c-41231779" checked=""/><div class="controls bullet"><span class="by">akovaski</span><span>|</span><a href="#41231023">prev</a><span>|</span><a href="#41230940">next</a><span>|</span><label class="collapse" for="c-41231779">[-]</label><label class="expand" for="c-41231779">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not terribly familiar with this space, but I do like the concurrency model presented here.<p>I think the README here is very well written, and I have a good idea of what&#x27;s going on just from reading it, but there are a few areas where I&#x27;m left scratching my head. Thankfully the code is fairly easy to read.</div><br/></div></div><div id="41230940" class="c"><input type="checkbox" id="c-41230940" checked=""/><div class="controls bullet"><span class="by">shoggouth</span><span>|</span><a href="#41231779">prev</a><span>|</span><a href="#41232287">next</a><span>|</span><label class="collapse" for="c-41230940">[-]</label><label class="expand" for="c-41230940">[5 more]</label></div><br/><div class="children"><div class="content">List of limitations of the project: <a href="https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice?tab=readme-ov-file#limitations">https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice?tab=readme-ov-file#limitati...</a></div><br/><div id="41231124" class="c"><input type="checkbox" id="c-41231124" checked=""/><div class="controls bullet"><span class="by">jd3</span><span>|</span><a href="#41230940">parent</a><span>|</span><a href="#41232287">next</a><span>|</span><label class="collapse" for="c-41231124">[-]</label><label class="expand" for="c-41231124">[4 more]</label></div><br/><div class="children"><div class="content">Want to state off the bat this this project is awesome and huge kudos to the author for spending their time, attention, and energy 1) working diligently to get this working at all and 2) sharing it with the broader HN community, who are generally known to by hyper-critical to a pedantic degree and&#x2F;or overly pessimistic (<i>cough</i> the initial Docker project Show HN thread <i>cough</i>)<p>I also really appreciate that the author recognizes the limits of their own project, which preemptively addresses most of the usual snark.<p>&gt; Lack of tests: Spice contains a lot of gnarly concurrent code, but has zero testing coverage. This would have be improved before Spice can be responsibly used for critical tasks.<p>Testing correctness of execution for critical tasks is one thing, but I would expect a library which implements &quot;gnarly concurrent code&quot; to at least have regression tests — what guarantee is there to an end-user that functionality which exists in a working state today might not break tomorrow due to a subtle yet nefarious regression?<p>sqlite has 590 times as much test code and test scripts as it does raw c source code [0]; this fact, along with its stability and portability, is one of the numerous reasons why it has proliferated to become the defacto embedded database used across the planet. While we&#x27;re comparing apples to oranges in this contrived example, the general point still stands — regression tests beget stability and confidence in a project.<p>In epics where I work, if we _must_ defer baseline regression tests, we usually create a follow-up ticket inside of the same epic to at least write them before feature&#x2F;epic launch, usually.<p>[0]: <a href="https:&#x2F;&#x2F;www.sqlite.org&#x2F;testing.html" rel="nofollow">https:&#x2F;&#x2F;www.sqlite.org&#x2F;testing.html</a></div><br/><div id="41231137" class="c"><input type="checkbox" id="c-41231137" checked=""/><div class="controls bullet"><span class="by">pverghese</span><span>|</span><a href="#41230940">root</a><span>|</span><a href="#41231124">parent</a><span>|</span><a href="#41231210">next</a><span>|</span><label class="collapse" for="c-41231137">[-]</label><label class="expand" for="c-41231137">[2 more]</label></div><br/><div class="children"><div class="content">You are welcome to add it. This is a proof of concept</div><br/><div id="41231170" class="c"><input type="checkbox" id="c-41231170" checked=""/><div class="controls bullet"><span class="by">jd3</span><span>|</span><a href="#41230940">root</a><span>|</span><a href="#41231137">parent</a><span>|</span><a href="#41231210">next</a><span>|</span><label class="collapse" for="c-41231170">[-]</label><label class="expand" for="c-41231170">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Spice is primarily a research project. Read along to learn more about it, but if you&#x27;re considering using it in production you should be aware of its many limitations.<p>Ah, I missed that upon first read. In that case, that caveat&#x2F;limitation is definitely justified.</div><br/></div></div></div></div><div id="41231210" class="c"><input type="checkbox" id="c-41231210" checked=""/><div class="controls bullet"><span class="by">dsp_person</span><span>|</span><a href="#41230940">root</a><span>|</span><a href="#41231124">parent</a><span>|</span><a href="#41231137">prev</a><span>|</span><a href="#41232287">next</a><span>|</span><label class="collapse" for="c-41231210">[-]</label><label class="expand" for="c-41231210">[1 more]</label></div><br/><div class="children"><div class="content">&gt; sharing it with the broader HN community<p>Note that I posted this, but I am not the author</div><br/></div></div></div></div></div></div><div id="41232287" class="c"><input type="checkbox" id="c-41232287" checked=""/><div class="controls bullet"><span class="by">gyrovagueGeist</span><span>|</span><a href="#41230940">prev</a><span>|</span><a href="#41232913">next</a><span>|</span><label class="collapse" for="c-41232287">[-]</label><label class="expand" for="c-41232287">[1 more]</label></div><br/><div class="children"><div class="content">This is neat and links to some great papers. I wish the comparison was with OpenMP tasks though; I’ve heard Rayon has a reputation for being a bit slow</div><br/></div></div><div id="41232913" class="c"><input type="checkbox" id="c-41232913" checked=""/><div class="controls bullet"><span class="by">assafe</span><span>|</span><a href="#41232287">prev</a><span>|</span><a href="#41231189">next</a><span>|</span><label class="collapse" for="c-41232913">[-]</label><label class="expand" for="c-41232913">[1 more]</label></div><br/><div class="children"><div class="content">This is great!</div><br/></div></div><div id="41231189" class="c"><input type="checkbox" id="c-41231189" checked=""/><div class="controls bullet"><span class="by">raggi</span><span>|</span><a href="#41232913">prev</a><span>|</span><a href="#41230345">next</a><span>|</span><label class="collapse" for="c-41231189">[-]</label><label class="expand" for="c-41231189">[3 more]</label></div><br/><div class="children"><div class="content">cooperative scheduling is the basis for so many patterns with great metrics :)</div><br/><div id="41231269" class="c"><input type="checkbox" id="c-41231269" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#41231189">parent</a><span>|</span><a href="#41230345">next</a><span>|</span><label class="collapse" for="c-41231269">[-]</label><label class="expand" for="c-41231269">[2 more]</label></div><br/><div class="children"><div class="content">But it&#x27;s not very cooperative, as in tasks yielding to each other. They mostly cooperate by letting some tasks to be given to other threads, and not all the time, but once in a heartbeat.  The scheduling happens rarely, so its <i>amortized</i> cost is low.</div><br/><div id="41231830" class="c"><input type="checkbox" id="c-41231830" checked=""/><div class="controls bullet"><span class="by">raggi</span><span>|</span><a href="#41231189">root</a><span>|</span><a href="#41231269">parent</a><span>|</span><a href="#41230345">next</a><span>|</span><label class="collapse" for="c-41231830">[-]</label><label class="expand" for="c-41231830">[1 more]</label></div><br/><div class="children"><div class="content">sure, but that&#x27;s all details, and there are other cooperative scheduling models which aren&#x27;t excessively eager. this isn&#x27;t to be a downer on this implementation, or other implementations, but just generally pointing at cooperative advantages, of which there are many, with one particular implicit and major downside, which is starvation when the cooperation contract is not adhered to.</div><br/></div></div></div></div></div></div><div id="41230345" class="c"><input type="checkbox" id="c-41230345" checked=""/><div class="controls bullet"><span class="by">dsp_person</span><span>|</span><a href="#41231189">prev</a><span>|</span><a href="#41232685">next</a><span>|</span><label class="collapse" for="c-41230345">[-]</label><label class="expand" for="c-41230345">[1 more]</label></div><br/><div class="children"><div class="content">see also readme under bench <a href="https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;blob&#x2F;main&#x2F;bench&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;judofyr&#x2F;spice&#x2F;blob&#x2F;main&#x2F;bench&#x2F;README.md</a></div><br/></div></div></div></div></div></div></div></body></html>