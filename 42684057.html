<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1736845251939" as="style"/><link rel="stylesheet" href="styles.css?v=1736845251939"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.economist.com/science-and-technology/2025/01/08/training-ai-models-might-not-need-enormous-data-centres">Training AI models might not need enormous data centres</a> <span class="domain">(<a href="https://www.economist.com">www.economist.com</a>)</span></div><div class="subtext"><span>jkuria</span> | <span>12 comments</span></div><br/><div><div id="42693674" class="c"><input type="checkbox" id="c-42693674" checked=""/><div class="controls bullet"><span class="by">jkuria</span><span>|</span><a href="#42694685">next</a><span>|</span><label class="collapse" for="c-42693674">[-]</label><label class="expand" for="c-42693674">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;kRfd2" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;kRfd2</a></div><br/></div></div><div id="42694685" class="c"><input type="checkbox" id="c-42694685" checked=""/><div class="controls bullet"><span class="by">openrisk</span><span>|</span><a href="#42693674">prev</a><span>|</span><a href="#42693803">next</a><span>|</span><label class="collapse" for="c-42694685">[-]</label><label class="expand" for="c-42694685">[6 more]</label></div><br/><div class="children"><div class="content">Open source public models trained on kosher data are substantially derisking the AI hype. It makes a lot of sense to push this approach as far as it can get. Its similar to SETI at home etc. but potentially with far more impact.</div><br/><div id="42695117" class="c"><input type="checkbox" id="c-42695117" checked=""/><div class="controls bullet"><span class="by">musha68k</span><span>|</span><a href="#42694685">parent</a><span>|</span><a href="#42695072">next</a><span>|</span><label class="collapse" for="c-42695117">[-]</label><label class="expand" for="c-42695117">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been in the air; and bittensor, hyperbolic &amp; co have been on some of the angles for a while. Models, by the people and for the people, Wikipedia + &quot;SETI at home&quot; style. Eventually and with pre&#x2F;training ofc, this will include inference too.</div><br/></div></div><div id="42694767" class="c"><input type="checkbox" id="c-42694767" checked=""/><div class="controls bullet"><span class="by">grumbelbart2</span><span>|</span><a href="#42694685">parent</a><span>|</span><a href="#42695072">prev</a><span>|</span><a href="#42693803">next</a><span>|</span><label class="collapse" for="c-42694767">[-]</label><label class="expand" for="c-42694767">[3 more]</label></div><br/><div class="children"><div class="content">They could, would and should. But: Training a state of the art LLM costs <i>millions</i> in GPU, electricity alone. There is no &quot;open&quot; organization at this point that can cover this. Current &quot;open source public models&quot; are shared by big players like Meta to undermine the competition. And they only publish their weights, not the training data, training protocols, training code; meaning it&#x27;s not reproducible, and questionable if the training data is kosher.</div><br/><div id="42695137" class="c"><input type="checkbox" id="c-42695137" checked=""/><div class="controls bullet"><span class="by">lz400</span><span>|</span><a href="#42694685">root</a><span>|</span><a href="#42694767">parent</a><span>|</span><a href="#42694946">next</a><span>|</span><label class="collapse" for="c-42695137">[-]</label><label class="expand" for="c-42695137">[1 more]</label></div><br/><div class="children"><div class="content">I understood SETI style meaning crowdsourced. Instead of mining bitcoin you mine LLMs. It&#x27;s a nice idea I think. Not sure about technical details, bandwidth limitations, performance, etc.</div><br/></div></div><div id="42694946" class="c"><input type="checkbox" id="c-42694946" checked=""/><div class="controls bullet"><span class="by">sebmellen</span><span>|</span><a href="#42694685">root</a><span>|</span><a href="#42694767">parent</a><span>|</span><a href="#42695137">prev</a><span>|</span><a href="#42693803">next</a><span>|</span><label class="collapse" for="c-42694946">[-]</label><label class="expand" for="c-42694946">[1 more]</label></div><br/><div class="children"><div class="content">Doesn’t Deepseek somewhat counter this narrative?</div><br/></div></div></div></div></div></div><div id="42693803" class="c"><input type="checkbox" id="c-42693803" checked=""/><div class="controls bullet"><span class="by">gnabgib</span><span>|</span><a href="#42694685">prev</a><span>|</span><a href="#42693837">next</a><span>|</span><label class="collapse" for="c-42693803">[-]</label><label class="expand" for="c-42693803">[2 more]</label></div><br/><div class="children"><div class="content">Related:<p><i>New Training Technique for Highly Efficient AI Methods</i> (2 points, 5 hours ago) <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42690664">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42690664</a><p><i>DiLoCo: Distributed Low-Communication Training of Language Models</i> (46 points, 1 year ago, 14 comments) <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38549337">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38549337</a></div><br/><div id="42694340" class="c"><input type="checkbox" id="c-42694340" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#42693803">parent</a><span>|</span><a href="#42693837">next</a><span>|</span><label class="collapse" for="c-42694340">[-]</label><label class="expand" for="c-42694340">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the links, some interesting discussion there.<p>The second article you linked indicates there will still be intense bandwidth requirements during training, shipping around gradient differentials.<p>What has changed in the past year?  Is this technique looking better, worse, or the same?</div><br/></div></div></div></div><div id="42693837" class="c"><input type="checkbox" id="c-42693837" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#42693803">prev</a><span>|</span><label class="collapse" for="c-42693837">[-]</label><label class="expand" for="c-42693837">[2 more]</label></div><br/><div class="children"><div class="content">It’s talking about training 10b parameters “capable models” with less compute using  ew techniques, but top models will always need more</div><br/><div id="42694964" class="c"><input type="checkbox" id="c-42694964" checked=""/><div class="controls bullet"><span class="by">kurthr</span><span>|</span><a href="#42693837">parent</a><span>|</span><label class="collapse" for="c-42694964">[-]</label><label class="expand" for="c-42694964">[1 more]</label></div><br/><div class="children"><div class="content">Wow, yeah a 10B parameter model is pretty tiny and 300 3-GPU clusters for $18M is not really cheap.<p>I guess enormous is in the eye of the beholder.</div><br/></div></div></div></div></div></div></div></div></div></body></html>