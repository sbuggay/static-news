<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726304449836" as="style"/><link rel="stylesheet" href="styles.css?v=1726304449836"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/tzakharko/m4-sme-exploration">Exploring the scalable matrix extension of the Apple M4 processor</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>gok</span> | <span>53 comments</span></div><br/><div><div id="41538487" class="c"><input type="checkbox" id="c-41538487" checked=""/><div class="controls bullet"><span class="by">rerdavies</span><span>|</span><a href="#41530894">next</a><span>|</span><label class="collapse" for="c-41538487">[-]</label><label class="expand" for="c-41538487">[1 more]</label></div><br/><div class="children"><div class="content">[delayed]</div><br/></div></div><div id="41530894" class="c"><input type="checkbox" id="c-41530894" checked=""/><div class="controls bullet"><span class="by">dividuum</span><span>|</span><a href="#41538487">prev</a><span>|</span><a href="#41506631">next</a><span>|</span><label class="collapse" for="c-41530894">[-]</label><label class="expand" for="c-41530894">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Although Apple has included a matrix accelerator in its devices since 2019, it used a proprietary instruction set inaccessible to developers, who officially could only use Apple-provided numerical libraries.<p>How does that work? Does the hardware throw some kind of fault when using those instructions? Or are they merely undocumented and you could use them if you figure out how they work? I guess the second, as hinted by the &quot;officially&quot;?</div><br/><div id="41531385" class="c"><input type="checkbox" id="c-41531385" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41530894">parent</a><span>|</span><a href="#41531222">next</a><span>|</span><label class="collapse" for="c-41531385">[-]</label><label class="expand" for="c-41531385">[1 more]</label></div><br/><div class="children"><div class="content">As others have said, just undocumented.<p>IIRC there was a BLIS fork that used AMX instructions. I think it was unofficial though(?). It is hard to do science without properly documented tools.</div><br/></div></div><div id="41531222" class="c"><input type="checkbox" id="c-41531222" checked=""/><div class="controls bullet"><span class="by">jonstewart</span><span>|</span><a href="#41530894">parent</a><span>|</span><a href="#41531385">prev</a><span>|</span><a href="#41530903">next</a><span>|</span><label class="collapse" for="c-41531222">[-]</label><label class="expand" for="c-41531222">[1 more]</label></div><br/><div class="children"><div class="content">Peter Cawley has a good write-up on the undocumented M1&#x2F;M2&#x2F;M3 AMX instructions: <a href="https:&#x2F;&#x2F;github.com&#x2F;corsix&#x2F;amx">https:&#x2F;&#x2F;github.com&#x2F;corsix&#x2F;amx</a></div><br/></div></div><div id="41530903" class="c"><input type="checkbox" id="c-41530903" checked=""/><div class="controls bullet"><span class="by">my123</span><span>|</span><a href="#41530894">parent</a><span>|</span><a href="#41531222">prev</a><span>|</span><a href="#41506631">next</a><span>|</span><label class="collapse" for="c-41530903">[-]</label><label class="expand" for="c-41530903">[1 more]</label></div><br/><div class="children"><div class="content">Merely undocumented</div><br/></div></div></div></div><div id="41506631" class="c"><input type="checkbox" id="c-41506631" checked=""/><div class="controls bullet"><span class="by">freeqaz</span><span>|</span><a href="#41530894">prev</a><span>|</span><a href="#41531133">next</a><span>|</span><label class="collapse" for="c-41506631">[-]</label><label class="expand" for="c-41506631">[10 more]</label></div><br/><div class="children"><div class="content">Any comparison with how much faster this is compared with the previous way of doing things on the CPU?</div><br/><div id="41506894" class="c"><input type="checkbox" id="c-41506894" checked=""/><div class="controls bullet"><span class="by">svnt</span><span>|</span><a href="#41506631">parent</a><span>|</span><a href="#41531133">next</a><span>|</span><label class="collapse" for="c-41506894">[-]</label><label class="expand" for="c-41506894">[9 more]</label></div><br/><div class="children"><div class="content">Based on my understanding from the description, it is ~8x faster (250 GFLOPS) for vector ops (vs. SVE mode at 31 GFLOPS which is CPU-ish) and 60-100 times faster (e.g. 2005 GFLOPS) for matrix multiplication for single-precision values.</div><br/><div id="41532061" class="c"><input type="checkbox" id="c-41532061" checked=""/><div class="controls bullet"><span class="by">jandrese</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41506894">parent</a><span>|</span><a href="#41535997">next</a><span>|</span><label class="collapse" for="c-41532061">[-]</label><label class="expand" for="c-41532061">[6 more]</label></div><br/><div class="children"><div class="content">That&#x27;s alright, but not mindblowing.  How does it compare to doing the same work on a GPU?  Is there a particular set of tasks that GPUs struggle with that would be well suited for this?  Or is this more a fig leaf over lousy GPU compute support in Apple land?</div><br/><div id="41534875" class="c"><input type="checkbox" id="c-41534875" checked=""/><div class="controls bullet"><span class="by">huijzer</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41532061">parent</a><span>|</span><a href="#41532230">next</a><span>|</span><label class="collapse" for="c-41534875">[-]</label><label class="expand" for="c-41534875">[3 more]</label></div><br/><div class="children"><div class="content">60 times faster could mean 2 minutes instead of 2 hours, or 2 seconds instead of 2 minutes. How is that not mind blowing, or at least very useful (for specific uses)?</div><br/><div id="41535411" class="c"><input type="checkbox" id="c-41535411" checked=""/><div class="controls bullet"><span class="by">jandrese</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41534875">parent</a><span>|</span><a href="#41532230">next</a><span>|</span><label class="collapse" for="c-41535411">[-]</label><label class="expand" for="c-41535411">[2 more]</label></div><br/><div class="children"><div class="content">Compared to 600 or 6000 times faster on a GPU though?</div><br/><div id="41536587" class="c"><input type="checkbox" id="c-41536587" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41535411">parent</a><span>|</span><a href="#41532230">next</a><span>|</span><label class="collapse" for="c-41536587">[-]</label><label class="expand" for="c-41536587">[1 more]</label></div><br/><div class="children"><div class="content">M* CPUs aren&#x27;t made for maximum performance, but for maximum power&#x2F;performance tradeoffs, since they&#x27;re mostly used in portables.</div><br/></div></div></div></div></div></div><div id="41532230" class="c"><input type="checkbox" id="c-41532230" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41532061">parent</a><span>|</span><a href="#41534875">prev</a><span>|</span><a href="#41535997">next</a><span>|</span><label class="collapse" for="c-41532230">[-]</label><label class="expand" for="c-41532230">[2 more]</label></div><br/><div class="children"><div class="content">Apple should mostly care about power-efficient inference I think, right? Not training. Spinning up a GPU seems like something to avoid.<p>I mean, I wonder how this thing compares to a gemm using all the cores in a cpu cluster. They might be ok with not even meeting that performance, if the accelerator can not hog all the cores and power.<p>At least that’s what my uninformed gut says. The workload for these things is like: little AI enhancements inside conventional apps, I think.</div><br/><div id="41535660" class="c"><input type="checkbox" id="c-41535660" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41532230">parent</a><span>|</span><a href="#41535997">next</a><span>|</span><label class="collapse" for="c-41535660">[-]</label><label class="expand" for="c-41535660">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Spinning up a GPU seems like something to avoid.<p>You can do inference on GPUs as well, and for anything other than very small&#x2F;lightweight models, such as noise cancellation or maybe speech recognition, it&#x27;s probably worth the initial overhead.<p>I believe CoreML already splits workloads between CPU, NPU, and GPU as appropriate.</div><br/></div></div></div></div></div></div><div id="41535997" class="c"><input type="checkbox" id="c-41535997" checked=""/><div class="controls bullet"><span class="by">TinkersW</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41506894">parent</a><span>|</span><a href="#41532061">prev</a><span>|</span><a href="#41531133">next</a><span>|</span><label class="collapse" for="c-41535997">[-]</label><label class="expand" for="c-41535997">[2 more]</label></div><br/><div class="children"><div class="content">What? The article says this thing does 2005 GFOPs, aka 2 TFLOPS, which is decent, but we have had CPUs that could do more than this for a long time now. My Zen2 12 core does about 3 TFLOPs, and a modern 16 core Zen5 can do 8-10 TFLOPS(I&#x27;m unsure what clock speed it can maintain with all cores engaged). And that is generally purpose SIMD not specialized matrix stuff(less generally useful).<p>Apple CPU&#x27;s kinda suck at vector ops, but they aren&#x27;t <i>that</i> bad, this thing is only mildly better. I would guess power savings is a big part of why they use this SVE streaming matrix mode.</div><br/><div id="41536855" class="c"><input type="checkbox" id="c-41536855" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#41506631">root</a><span>|</span><a href="#41535997">parent</a><span>|</span><a href="#41531133">next</a><span>|</span><label class="collapse" for="c-41536855">[-]</label><label class="expand" for="c-41536855">[1 more]</label></div><br/><div class="children"><div class="content">IIUC this the cpu in an iPad. The pro&#x2F;max versions would be more appropriate to compare against the Zen when they are released.</div><br/></div></div></div></div></div></div></div></div><div id="41531133" class="c"><input type="checkbox" id="c-41531133" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#41506631">prev</a><span>|</span><a href="#41538039">next</a><span>|</span><label class="collapse" for="c-41531133">[-]</label><label class="expand" for="c-41531133">[6 more]</label></div><br/><div class="children"><div class="content">If Apple’s going for one SME accelerator per base M4 chiplet, it’ll be interesting to see how to program scalably for Pro&#x2F;Max&#x2F;Ultra variants.</div><br/><div id="41531687" class="c"><input type="checkbox" id="c-41531687" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41531133">parent</a><span>|</span><a href="#41538039">next</a><span>|</span><label class="collapse" for="c-41531687">[-]</label><label class="expand" for="c-41531687">[5 more]</label></div><br/><div class="children"><div class="content">You should be thinking in terms of CPU clusters, not chiplets. The Ultra is the only one with multiple chiplets, but all of their processors have multiple CPU clusters, and so far it&#x27;s one AMX&#x2F;SME per cluster.</div><br/><div id="41535559" class="c"><input type="checkbox" id="c-41535559" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#41531133">root</a><span>|</span><a href="#41531687">parent</a><span>|</span><a href="#41532181">next</a><span>|</span><label class="collapse" for="c-41535559">[-]</label><label class="expand" for="c-41535559">[1 more]</label></div><br/><div class="children"><div class="content">Ah, thank you! That’s the right word. They’re on the same die, no, so “chiplet” isn’t the appropriate word?</div><br/></div></div><div id="41532181" class="c"><input type="checkbox" id="c-41532181" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#41531133">root</a><span>|</span><a href="#41531687">parent</a><span>|</span><a href="#41535559">prev</a><span>|</span><a href="#41538039">next</a><span>|</span><label class="collapse" for="c-41532181">[-]</label><label class="expand" for="c-41532181">[3 more]</label></div><br/><div class="children"><div class="content">I guess the the CPU&#x2F;cluster and cluster&#x2F;chiplet ratios change from generation to generation?</div><br/><div id="41532310" class="c"><input type="checkbox" id="c-41532310" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41531133">root</a><span>|</span><a href="#41532181">parent</a><span>|</span><a href="#41538039">next</a><span>|</span><label class="collapse" for="c-41532310">[-]</label><label class="expand" for="c-41532310">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not constant even within a generation. The M3, M3 Pro, and M3 Max are each monolithic SoCs of different sizes (no chiplets) with different CPU cluster configurations, and the phone chip of the same generation is yet another configuration.</div><br/><div id="41536582" class="c"><input type="checkbox" id="c-41536582" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41531133">root</a><span>|</span><a href="#41532310">parent</a><span>|</span><a href="#41538039">next</a><span>|</span><label class="collapse" for="c-41536582">[-]</label><label class="expand" for="c-41536582">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t hard to deal with because it&#x27;s just an evolution of having to check the # of CPU cores to know how many worker threads to start.<p>But there are a few more problems because of cache hierarchies; touching the same memory from different CPU clusters at once can be extra slow, possibly even slower than fetching it from DRAM.<p>This is called NUMA (which is ironic for a unified memory SoC.)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41538039" class="c"><input type="checkbox" id="c-41538039" checked=""/><div class="controls bullet"><span class="by">DanielLee5</span><span>|</span><a href="#41531133">prev</a><span>|</span><a href="#41507526">next</a><span>|</span><label class="collapse" for="c-41538039">[-]</label><label class="expand" for="c-41538039">[1 more]</label></div><br/><div class="children"><div class="content">Great review.</div><br/></div></div><div id="41507526" class="c"><input type="checkbox" id="c-41507526" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#41538039">prev</a><span>|</span><a href="#41533599">next</a><span>|</span><label class="collapse" for="c-41507526">[-]</label><label class="expand" for="c-41507526">[11 more]</label></div><br/><div class="children"><div class="content">I’m not sure why they added this feature. All Apple SoCs have far more energy efficient compute than the CPU. This would only make sense for really tiny models which need extremely quick forward pass. For such models the overhead of a GPU  or Neural Engine kernel launch would be quite noticeable. But for those the old NEON was already OK, and if not, there also is a dedicated matrix unit there called AMX. Seems kinda random to me.</div><br/><div id="41509205" class="c"><input type="checkbox" id="c-41509205" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41507526">parent</a><span>|</span><a href="#41531730">next</a><span>|</span><label class="collapse" for="c-41509205">[-]</label><label class="expand" for="c-41509205">[3 more]</label></div><br/><div class="children"><div class="content">This replaces AMX, it is its successor.<p>The older Apple CPUs implemented a custom form of AMX that was not standardized by Arm.<p>Presumably as a result of cooperation with Apple, the Arm ISA now includes a set of instructions with the same purpose like the original Apple AMX.<p>The newer Apple CPUs have been updated to use the standard Arm ISA, instead of their older proprietary ISA.<p>In the Apple CPUs, the former AMX and the current SME provide a much higher throughput than the CPU cores, even if lower than the GPU, and a much lower latency than the GPU, even if higher than the CPU cores.<p>AMX&#x2F;SME is implemented as a separate accelerator, distinct from the CPU cores, because this saves power and area in comparison with implementing such instructions in each CPU core. The Apple CPUs do not attempt to compete in high-performance computing applications, so the extra throughput provided by a separate shared matrix operation accelerator is good enough for them.</div><br/><div id="41532672" class="c"><input type="checkbox" id="c-41532672" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#41507526">root</a><span>|</span><a href="#41509205">parent</a><span>|</span><a href="#41531730">next</a><span>|</span><label class="collapse" for="c-41532672">[-]</label><label class="expand" for="c-41532672">[2 more]</label></div><br/><div class="children"><div class="content">This has both actually.</div><br/><div id="41538458" class="c"><input type="checkbox" id="c-41538458" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#41507526">root</a><span>|</span><a href="#41532672">parent</a><span>|</span><a href="#41531730">next</a><span>|</span><label class="collapse" for="c-41538458">[-]</label><label class="expand" for="c-41538458">[1 more]</label></div><br/><div class="children"><div class="content">That must be for preserving the compatibility with the older software versions.<p>The same happens in x86, where there are hundreds of obsolete instructions, which have been replaced by better instructions, but which are still supported to allow the execution of old programs.<p>Bot the new Arm SME instructions and the old Apple AMX instructions are executed by the same hardware matrix operation accelerator.<p>Previously Arm has extended the Aarch64 ISA with the SVE instructions, in order to support the Fujitsu supercomputer.<p>Then they were not satisfied with the original SVE and they have extended it into SVE2.<p>I suppose that something similar has happened with SME. Apple must have negotiated with Arm the inclusion of matrix and vector operations implemented by a separate shared accelerator. The result was SME, which differs from the original AMX either because Apple has thought some improvements based on the experience with the first instruction set or because Arm had desired some changes from the Apple proposal.</div><br/></div></div></div></div></div></div><div id="41531730" class="c"><input type="checkbox" id="c-41531730" checked=""/><div class="controls bullet"><span class="by">GeekyBear</span><span>|</span><a href="#41507526">parent</a><span>|</span><a href="#41509205">prev</a><span>|</span><a href="#41534186">next</a><span>|</span><label class="collapse" for="c-41531730">[-]</label><label class="expand" for="c-41531730">[1 more]</label></div><br/><div class="children"><div class="content">Matrix multiplication is very commonly used in science and engineering, not just machine learning.<p>The neural engine is optimized for machine learning use cases.<p>This standardized successor to AMX is more general purpose than the neural engine and has much improved matrix multiplication performance vs NEON.<p>As a bonus, since this is no longer just an experimental implementation of a matrix unit, you get documented access to the new ARM standardized low level instruction set.</div><br/></div></div><div id="41534186" class="c"><input type="checkbox" id="c-41534186" checked=""/><div class="controls bullet"><span class="by">brigade</span><span>|</span><a href="#41507526">parent</a><span>|</span><a href="#41531730">prev</a><span>|</span><a href="#41531435">next</a><span>|</span><label class="collapse" for="c-41534186">[-]</label><label class="expand" for="c-41534186">[1 more]</label></div><br/><div class="children"><div class="content">The neural engine by design cannot handle all possible kernels, and the GPU is significantly slower for integer math, and cannot do fp64. Then for the iPhone SoCs with 4 or 5 core GPUs, the GPU is a bit slower for fp16 and fp32 too.</div><br/></div></div><div id="41531435" class="c"><input type="checkbox" id="c-41531435" checked=""/><div class="controls bullet"><span class="by">Archit3ch</span><span>|</span><a href="#41507526">parent</a><span>|</span><a href="#41534186">prev</a><span>|</span><a href="#41535665">next</a><span>|</span><label class="collapse" for="c-41531435">[-]</label><label class="expand" for="c-41531435">[4 more]</label></div><br/><div class="children"><div class="content">Dedicated FP64 is great for real-time audio processing. Like an included DSP chip.</div><br/><div id="41533741" class="c"><input type="checkbox" id="c-41533741" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#41507526">root</a><span>|</span><a href="#41531435">parent</a><span>|</span><a href="#41535665">next</a><span>|</span><label class="collapse" for="c-41533741">[-]</label><label class="expand" for="c-41533741">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t FP32 sufficient for audio processing? Even though we have 24bit DACs and ADCs these days I feel like 16bit was really good enough. FP32 with 24bit mantissa should avoid rounding errors at the 16bit level right?</div><br/><div id="41534044" class="c"><input type="checkbox" id="c-41534044" checked=""/><div class="controls bullet"><span class="by">Archit3ch</span><span>|</span><a href="#41507526">root</a><span>|</span><a href="#41533741">parent</a><span>|</span><a href="#41538339">next</a><span>|</span><label class="collapse" for="c-41534044">[-]</label><label class="expand" for="c-41534044">[1 more]</label></div><br/><div class="children"><div class="content">It depends on the application.<p>16bit is enough for representation.<p>24bit is enough for recording (some leeway because recording levels won&#x27;t be ideal).<p>FP32 for processing with simple effects (e.g. mixer, some EQs). If that&#x27;s enough for your needs, you can SIMD&#x2F;GPU to your heart&#x27;s content.<p>FP64 for high Q filters, phasors, LU decompositions.</div><br/></div></div><div id="41538339" class="c"><input type="checkbox" id="c-41538339" checked=""/><div class="controls bullet"><span class="by">rerdavies</span><span>|</span><a href="#41507526">root</a><span>|</span><a href="#41533741">parent</a><span>|</span><a href="#41534044">prev</a><span>|</span><a href="#41535665">next</a><span>|</span><label class="collapse" for="c-41538339">[-]</label><label class="expand" for="c-41538339">[1 more]</label></div><br/><div class="children"><div class="content">##### Is 16 bits good enough?<p>Amp models need all the precision they can get. The effective range of output signals is greatly compressed because the signal is soft-clipped by the amplifier&#x27;s non-linear response. Real guitar amplifiers have significant levels of noise in their output signals; digital guitar simulations of guitar amplifiers are typically even more sensitive to noise in their input signals. Currently, probably the easiest way to tell the difference between recordings of real guitar amps and neural model simulations of guitar amps: how they respond to signal noise in their inputs.<p>A really good ADC may have a 24-bit representation, but it will only have an 18 to 20 bit signal to noise ratio. Cheap audio adapters (pretty much all the audio adapters costing less than $100) will happily deliver you an input signal in 24-bit (or even 32-bit) representation, but will have less than 16 bits of signal above the noise floor.<p>For example, I have an M-AUDIO Fast Track usb audio adapter ($50) that provides 24-bit input but only has 12 bits of signal above the noise floor, even with levels meticulously set. Guitar amp models sound horrible when using this device. But when I use my MOTU-M2 (~$200) which probably provides a full 20 bits of signal above the noise floor, the same models sound faaabulous!<p>Those extra bits of SN&#x2F;R are precious. An amp simulation of an input signal on a cheap ADC sounds noticeably &quot;fizzier&quot; than an amp simulation of the same input signal on an ADC with 19 actual significant bits of actual signal above the noise floor.<p>So 16 bits is not good enough. And 24 bits does make a difference (even if it&#x27;s only 19 bits of actual difference)<p>##### Would FP64 be better?<p>Currently, Machine Learning models of guitar amps use FP32, because they are extremely compute-intensive when running in realtime (and extremely compute intensive when training the model in realtime).<p>Would FP64 calculations improve the quality of amp simulations? That would depend on how much precision gets lost while performing ML simulation. Probably a fair bit of precision does gets lost, between the massive matrix multiplies that are involved, and the calculation of non-linear activation functions (typically atan functions in current ML guitar models).<p>Roughly, I think the answer goes like this. We have an input signal with 19 bits of precision. And the 19th bit seems to make a difference. FP32 provides 24 bits of precision -- 5 extra bits of precision -- to avoid rounding errors while calculation massive matrix multiplies, and at least two rounds of atan activation functions (some of which are in a feedback loop). Are those five extra bits of guard precision being consumed during processing? Heck yes!<p>I&#x27;m almost certain that the quality of amp models would improve if the models were trained in FP64, and am reasonably certain that quality would improve if realtime calculations were performed in FP64 as well.<p>But on a Raspberry Pi (and probably on a x64 device as well), neural models cannot be run with FP64 precision in realtime. An ML-based amp model consumes bout 45% of available CPU bandwidth running with FP32 precision. Running with FP64 precious would add least quadruple that.<p>As a point of interest, matrix multiplies running on a Raspberry Pi 4 Arm Cortex A72 are almost completely limited by memory bandwidth to L2 cache and main memory. And that performance is (mostly) constrained by the tile size used in the matrix multiples, which (when using A72 neon registers) is constrained by the number of neon registers available. I believe that performance would roughly increase linearly as a function of available tile size. Whether it&#x27;s linear or not depends a bit on how well matrix units deal with Nx1 matrices (vectors). Although the to perform NxM matrix multiples dominates, a significant amount of execution time also gets spent doing Nx1 and&#x2F;or vector processing. Whether the corresponding performance boost is good enough to allow realtime audio processing at FP64.... the only way to find out would be to do it.<p>* Results based on extensive optimization and profiling of Toob ML and TooB Neural Amp Modeler guitar effects hosted by [PiPedal](<a href="https:&#x2F;&#x2F;rerdavies.github.io&#x2F;pipedal&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rerdavies.github.io&#x2F;pipedal&#x2F;</a>)</div><br/></div></div></div></div></div></div><div id="41535665" class="c"><input type="checkbox" id="c-41535665" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41507526">parent</a><span>|</span><a href="#41531435">prev</a><span>|</span><a href="#41533599">next</a><span>|</span><label class="collapse" for="c-41535665">[-]</label><label class="expand" for="c-41535665">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if not, there also is a dedicated matrix unit there called AMX<p>This seems to be the successor to AMX.</div><br/></div></div></div></div><div id="41533599" class="c"><input type="checkbox" id="c-41533599" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#41507526">prev</a><span>|</span><a href="#41531183">next</a><span>|</span><label class="collapse" for="c-41533599">[-]</label><label class="expand" for="c-41533599">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m dim, whats the difference between SVE and SME?</div><br/><div id="41533619" class="c"><input type="checkbox" id="c-41533619" checked=""/><div class="controls bullet"><span class="by">mrmuagi</span><span>|</span><a href="#41533599">parent</a><span>|</span><a href="#41531183">next</a><span>|</span><label class="collapse" for="c-41533619">[-]</label><label class="expand" for="c-41533619">[1 more]</label></div><br/><div class="children"><div class="content">Vector vs Matrices. Higher dimensional.</div><br/></div></div></div></div><div id="41531183" class="c"><input type="checkbox" id="c-41531183" checked=""/><div class="controls bullet"><span class="by">softwaredoug</span><span>|</span><a href="#41533599">prev</a><span>|</span><a href="#41532461">next</a><span>|</span><label class="collapse" for="c-41531183">[-]</label><label class="expand" for="c-41531183">[2 more]</label></div><br/><div class="children"><div class="content">I just wish they’d make native tensorflow installation actually work without a million apple silicon specific exceptions :)</div><br/><div id="41532613" class="c"><input type="checkbox" id="c-41532613" checked=""/><div class="controls bullet"><span class="by">TheFuzzball</span><span>|</span><a href="#41531183">parent</a><span>|</span><a href="#41532461">next</a><span>|</span><label class="collapse" for="c-41532613">[-]</label><label class="expand" for="c-41532613">[1 more]</label></div><br/><div class="children"><div class="content">They will, just in time for everyone to have switched to pytorch!</div><br/></div></div></div></div><div id="41532461" class="c"><input type="checkbox" id="c-41532461" checked=""/><div class="controls bullet"><span class="by">kjkjadksj</span><span>|</span><a href="#41531183">prev</a><span>|</span><label class="collapse" for="c-41532461">[-]</label><label class="expand" for="c-41532461">[15 more]</label></div><br/><div class="children"><div class="content">I wish they made computers that ran software like games again. Seems like the last few iterations they’ve been working hard on making computers that are able to run ai models a little faster. Are people really asking for that? I would think far more people would like to play a video game over rolling their own matrix multiplication, but I guess that’s why they pay the people at apple the big bucks because they must know best.</div><br/><div id="41532535" class="c"><input type="checkbox" id="c-41532535" checked=""/><div class="controls bullet"><span class="by">jwells89</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41534448">next</a><span>|</span><label class="collapse" for="c-41532535">[-]</label><label class="expand" for="c-41532535">[1 more]</label></div><br/><div class="children"><div class="content">Overall, GPU strength is the best it&#x27;s ever been in portable Apple devices by a significant margin. The problem isn&#x27;t the hardware, it&#x27;s that game developers are reticent to support anything that&#x27;s not x86 Windows+DirectX or one of the consoles.<p>It&#x27;s often said that macOS&#x2F;iOS supporting Vulkan would help and while I think that&#x27;s true to an extent, native Vulkan support is still rare enough that it&#x27;s not going to change all that much in terms of ease of porting. It might improve things on the front of running games through WINE (DirectX → Vulkan translation), but unless developers produce ARM builds of their games there&#x27;s always going to be the overhead of being run through an x86 translator, which varies depending on how CPU heavy the game is.</div><br/></div></div><div id="41534448" class="c"><input type="checkbox" id="c-41534448" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41532535">prev</a><span>|</span><a href="#41535626">next</a><span>|</span><label class="collapse" for="c-41534448">[-]</label><label class="expand" for="c-41534448">[2 more]</label></div><br/><div class="children"><div class="content">You can spend a small amount of die space on something that will yield 10x performance benefits for some things, and you can spend a lot of die space on something that will only yield a general 5% improvement. Which you choose depends on a lot of factors. In other words, the relationship between the &quot;things on the chip&quot; and general performance, or specific application performance, is not a strictly linear relationship.<p>The 20 series Nvidia GPUs with RTX were a good example. RT cores were added and took up significant die space, people said &quot;why not more CUDA cores&quot;, but given the design of consumer GPUs it&#x27;s extremely unlikely that just replacing those with more CUDA cores would have had a proportional uplift. In Nvidia&#x27;s case, they realized RT cores were a better bet and served their customer bases (industrial graphics, gaming) better than just more raw numbers.<p>As it stands, specialization like this is a key element of new designs on leading edge processes. You&#x27;re going to see more of it, not less.<p>&gt; I guess that’s why they pay the people at apple the big bucks because they must know best.<p>Well I don&#x27;t know about &quot;best&quot;, they almost certainly know ~infinitely more about their customers and workloads than random people like us do, I can at least say that much.</div><br/><div id="41534721" class="c"><input type="checkbox" id="c-41534721" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#41532461">root</a><span>|</span><a href="#41534448">parent</a><span>|</span><a href="#41535626">next</a><span>|</span><label class="collapse" for="c-41534721">[-]</label><label class="expand" for="c-41534721">[1 more]</label></div><br/><div class="children"><div class="content"><i>The 20 series Nvidia GPUs with RTX were a good example. RT cores were added and took up significant die space, people said &quot;why not more CUDA cores&quot;</i><p>Or they could have had the same number of CUDA cores without RT at a lower price (the fabled &quot;1180&quot;)...</div><br/></div></div></div></div><div id="41535626" class="c"><input type="checkbox" id="c-41535626" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41534448">prev</a><span>|</span><a href="#41536604">next</a><span>|</span><label class="collapse" for="c-41535626">[-]</label><label class="expand" for="c-41535626">[1 more]</label></div><br/><div class="children"><div class="content">Are you implying that recent Apple SoCs <i>can&#x27;t</i> run games?<p>While there&#x27;s the ML-centric &quot;Neural Engine&quot;, the GPU really isn&#x27;t stagnating by any means: Just in the iPhone 16 presentation this week, ray tracing and a 20% faster GPU were among the headline features. Gaming got its own section in the video presentation!<p>The fastest GPU I own is in my Mac; the second fastest is in my iPhone. My dedicated (last-gen) game consoles are a distant third and forth, respectively.</div><br/></div></div><div id="41536604" class="c"><input type="checkbox" id="c-41536604" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41535626">prev</a><span>|</span><a href="#41533143">next</a><span>|</span><label class="collapse" for="c-41536604">[-]</label><label class="expand" for="c-41536604">[1 more]</label></div><br/><div class="children"><div class="content">Higher AI performance actually means higher game performance, because you can render the game at lower resolution and use ML upscaling. Very popular technique now, especially because people prefer higher frame rates over higher resolutions.</div><br/></div></div><div id="41533143" class="c"><input type="checkbox" id="c-41533143" checked=""/><div class="controls bullet"><span class="by">naming_the_user</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41536604">prev</a><span>|</span><a href="#41535228">next</a><span>|</span><label class="collapse" for="c-41533143">[-]</label><label class="expand" for="c-41533143">[6 more]</label></div><br/><div class="children"><div class="content">Modern Apple Silicon based laptops have fantastic graphics performance, manufacturers just aren&#x27;t that interested in supporting them.<p>It&#x27;s probably a bit of a chicken and egg thing at this point, plus the fact that most &quot;serious&quot; gamers are going to have desktop PC&#x27;s anyway.</div><br/><div id="41535640" class="c"><input type="checkbox" id="c-41535640" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41532461">root</a><span>|</span><a href="#41533143">parent</a><span>|</span><a href="#41534126">next</a><span>|</span><label class="collapse" for="c-41535640">[-]</label><label class="expand" for="c-41535640">[4 more]</label></div><br/><div class="children"><div class="content">&gt; manufacturers just aren&#x27;t that interested in supporting them<p>AAA games are starting to show up on Steam for macOS these days. Baldur&#x27;s Gate 3 runs pretty well, for example!<p>The real shame is that some older indie games are disappearing just as easily, given Apple&#x27;s deprecation strategy – while Microsoft basically never breaks backwards compatibility, Apple recently cut off 32 bit games (killing about half my Steam library), and presumably Intel-only binaries are next.</div><br/><div id="41535912" class="c"><input type="checkbox" id="c-41535912" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#41532461">root</a><span>|</span><a href="#41535640">parent</a><span>|</span><a href="#41534126">next</a><span>|</span><label class="collapse" for="c-41535912">[-]</label><label class="expand" for="c-41535912">[3 more]</label></div><br/><div class="children"><div class="content">Apple dropped support for 32-bit Mac applications five years ago; recent only by comparison to Microsoft&#x27;s theoretical backwards compatibility. Apple dropped support for 32-bit Mac hardware, firmware, and drivers in 2012, so there was a period of seven years where game developers had every reason to make their Mac releases 64-bit, but to a disappointingly large degree they didn&#x27;t.<p>This was probably due in large part to a lack of pressure on the Windows side. It was absolutely absurd that even a big budget (and memory-hungry) game like Skyrim was released in 2011 as a 32-bit only game, and didn&#x27;t get a 64-bit release until 2016.<p>I didn&#x27;t enjoy macOS killing compatibility with so much of my Steam library either, but I do at least respect that Apple had some solid reasons, and save some of my ire for the game devs that shipped outdated binaries.</div><br/><div id="41536551" class="c"><input type="checkbox" id="c-41536551" checked=""/><div class="controls bullet"><span class="by">diebeforei485</span><span>|</span><a href="#41532461">root</a><span>|</span><a href="#41535912">parent</a><span>|</span><a href="#41534126">next</a><span>|</span><label class="collapse" for="c-41536551">[-]</label><label class="expand" for="c-41536551">[2 more]</label></div><br/><div class="children"><div class="content">Dropping 32bit support was the right decision. Most of those games work fine in emulation on Apple Silicon if they are single player, or alternatively in a cloud gaming service like Nvidia&#x27;s that you can use to access your Steam library directly.</div><br/><div id="41536705" class="c"><input type="checkbox" id="c-41536705" checked=""/><div class="controls bullet"><span class="by">lxgr</span><span>|</span><a href="#41532461">root</a><span>|</span><a href="#41536551">parent</a><span>|</span><a href="#41534126">next</a><span>|</span><label class="collapse" for="c-41536705">[-]</label><label class="expand" for="c-41536705">[1 more]</label></div><br/><div class="children"><div class="content">Well, as I said, about half my library is gone due to the lack of 32 bit support. The entire Orange Box by Valve, a few indie games...<p>Not sure if many of them are even available in emulators, and a cloud gaming service for a 2D indie game seems like overkill.<p>And yes, I generally agree with Apple deprecating technologies after a while (sometimes it&#x27;s better to make a clear cut by forcing a minimum API version, CPU architecture etc. rather than to have compatibility be hit and miss for really old things), but in the case of gaming specifically, I sometimes prefer Microsoft&#x27;s approach.</div><br/></div></div></div></div></div></div></div></div><div id="41534126" class="c"><input type="checkbox" id="c-41534126" checked=""/><div class="controls bullet"><span class="by">Detrytus</span><span>|</span><a href="#41532461">root</a><span>|</span><a href="#41533143">parent</a><span>|</span><a href="#41535640">prev</a><span>|</span><a href="#41535228">next</a><span>|</span><label class="collapse" for="c-41534126">[-]</label><label class="expand" for="c-41534126">[1 more]</label></div><br/><div class="children"><div class="content">I thought one of the reasons to bring Apple Silicon to Mac was that all the iPhone games can now be easily ported?</div><br/></div></div></div></div><div id="41535228" class="c"><input type="checkbox" id="c-41535228" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41533143">prev</a><span>|</span><a href="#41535043">next</a><span>|</span><label class="collapse" for="c-41535228">[-]</label><label class="expand" for="c-41535228">[1 more]</label></div><br/><div class="children"><div class="content">Apple literally marketed the new iPhone running Death Stranding</div><br/></div></div><div id="41535043" class="c"><input type="checkbox" id="c-41535043" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41535228">prev</a><span>|</span><a href="#41533006">next</a><span>|</span><label class="collapse" for="c-41535043">[-]</label><label class="expand" for="c-41535043">[1 more]</label></div><br/><div class="children"><div class="content">They are! The graphics for video games are just a series of matrix multiplications. Before it can get shown to the screen, the graphics are a bunch of triangles, represented by matrices, and in order to do anything in game, those matrices need to be multiplied in order to move them around in 3d space, before getting rendered out to the screen. Making computers better at matrix math means better rendering for video games.</div><br/></div></div><div id="41533006" class="c"><input type="checkbox" id="c-41533006" checked=""/><div class="controls bullet"><span class="by">samatman</span><span>|</span><a href="#41532461">parent</a><span>|</span><a href="#41535043">prev</a><span>|</span><label class="collapse" for="c-41533006">[-]</label><label class="expand" for="c-41533006">[1 more]</label></div><br/><div class="children"><div class="content">Are you under the impression that fast matrix operations in the CPU are useless for,, games?<p>Where did you get that idea?</div><br/></div></div></div></div></div></div></div></div></div></body></html>