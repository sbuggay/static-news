<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1718269274765" as="style"/><link rel="stylesheet" href="styles.css?v=1718269274765"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://huggingface.co/blog/mlabonne/abliteration">Uncensor any LLM with abliteration</a> <span class="domain">(<a href="https://huggingface.co">huggingface.co</a>)</span></div><div class="subtext"><span>mizzao</span> | <span>71 comments</span></div><br/><div><div id="40666893" class="c"><input type="checkbox" id="c-40666893" checked=""/><div class="controls bullet"><span class="by">k__</span><span>|</span><a href="#40666023">next</a><span>|</span><label class="collapse" for="c-40666893">[-]</label><label class="expand" for="c-40666893">[6 more]</label></div><br/><div class="children"><div class="content">I played around with Amazon Q and while setting it up, I needed to create an IAM identity center.<p>Never did this before, so I was asking Q in the AWS docs how to do it.<p>It refused to help, as it didn&#x27;t answer security related questions.<p>thank.</div><br/><div id="40667339" class="c"><input type="checkbox" id="c-40667339" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#40666893">parent</a><span>|</span><a href="#40667091">next</a><span>|</span><label class="collapse" for="c-40667339">[-]</label><label class="expand" for="c-40667339">[1 more]</label></div><br/><div class="children"><div class="content">I believe Amazon Q is running on Amazon&#x27;s own Titan G1 model. I recently ran the &quot;Premier&quot; version (their highest end one) through my personal vibecheck test and was quite surprised by its RL. It was the only non-Chinese model I&#x27;ve tested to refuse to answer about Tiananmen Square and the only model I believe I&#x27;ve tested with this eval (over 50 at this point) that refused to answer about the LA riots. It also scored an impressive 0&#x2F;6 on my reasoning&#x2F;basic world understanding tests (underperforming most 3B models) but that&#x27;s more capabilities than RL...<p>Amazon claims the Titan model is suitable for: &quot;Supported use cases: RAG, agents, chat, chain of thought, open-ended text generation, brainstorming, summarization, code generation, table creation, data formatting, paraphrasing, rewriting, extraction, and Q&amp;A.&quot; (it is not, lol)</div><br/></div></div><div id="40667091" class="c"><input type="checkbox" id="c-40667091" checked=""/><div class="controls bullet"><span class="by">menacingly</span><span>|</span><a href="#40666893">parent</a><span>|</span><a href="#40667339">prev</a><span>|</span><a href="#40666950">next</a><span>|</span><label class="collapse" for="c-40667091">[-]</label><label class="expand" for="c-40667091">[2 more]</label></div><br/><div class="children"><div class="content">it’s similar asking the gemini-1.5 models about coding questions that involve auth<p>one of my questions about a login form also tripped a harassment flag</div><br/><div id="40667279" class="c"><input type="checkbox" id="c-40667279" checked=""/><div class="controls bullet"><span class="by">michaelt</span><span>|</span><a href="#40666893">root</a><span>|</span><a href="#40667091">parent</a><span>|</span><a href="#40666950">next</a><span>|</span><label class="collapse" for="c-40667279">[-]</label><label class="expand" for="c-40667279">[1 more]</label></div><br/><div class="children"><div class="content">I suspect the refusal to answer questions about auth aren&#x27;t a matter of hacking or offensive material.<p>I suspect instead the people training these models have identified areas of questioning where their model is 99% right, but because the 1% wrong is incredibly costly they dodge the entire question.<p>Would you want your LLM to give out <i>any</i> legal advice, or medical advice, or can-I-eat-this-mushroom advice, if you knew due to imperfections in your training process, it sometimes recommended people put glue in their pizza sauce?</div><br/></div></div></div></div><div id="40666950" class="c"><input type="checkbox" id="c-40666950" checked=""/><div class="controls bullet"><span class="by">arianvanp</span><span>|</span><a href="#40666893">parent</a><span>|</span><a href="#40667091">prev</a><span>|</span><a href="#40666023">next</a><span>|</span><label class="collapse" for="c-40666950">[-]</label><label class="expand" for="c-40666950">[2 more]</label></div><br/><div class="children"><div class="content">This limitation is new. And it&#x27;s so annoying. 95% of the time my questions I have surrounding AWS are IAM or security related and this thing refuses to answer anything. It&#x27;s so annoying.</div><br/><div id="40666979" class="c"><input type="checkbox" id="c-40666979" checked=""/><div class="controls bullet"><span class="by">el_benhameen</span><span>|</span><a href="#40666893">root</a><span>|</span><a href="#40666950">parent</a><span>|</span><a href="#40666023">next</a><span>|</span><label class="collapse" for="c-40666979">[-]</label><label class="expand" for="c-40666979">[1 more]</label></div><br/><div class="children"><div class="content">It’s an absolute disaster. It wouldn’t answer something along the lines of “what is IAM” when I asked increasingly simple “security” related questions. Very little chance I’ll try an aws AI offering again any time soon.</div><br/></div></div></div></div></div></div><div id="40666023" class="c"><input type="checkbox" id="c-40666023" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#40666893">prev</a><span>|</span><a href="#40666953">next</a><span>|</span><label class="collapse" for="c-40666023">[-]</label><label class="expand" for="c-40666023">[5 more]</label></div><br/><div class="children"><div class="content">This is really interesting and is parallel to some other stuff (like the research on a model that&#x27;s obsessed with the Golden Gate Bridge and inappropriately thinks of things related to it in otherwise irrelevant contexts).<p>It&#x27;s worth mentioning that this technique is usable <i>if you have the model weights</i> (it&#x27;s a simple way of changing the weights or how to use them):<p>&gt; Once we have identified the refusal direction, we can &quot;ablate&quot; it, effectively removing the model&#x27;s ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization.<p>It&#x27;s not (and doesn&#x27;t claim to be) a technique for convincing a model to change its behavior <i>through prompts</i>.</div><br/><div id="40666319" class="c"><input type="checkbox" id="c-40666319" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#40666023">parent</a><span>|</span><a href="#40666953">next</a><span>|</span><label class="collapse" for="c-40666319">[-]</label><label class="expand" for="c-40666319">[4 more]</label></div><br/><div class="children"><div class="content">What&#x27;s interesting was how with GGC the model would spit out things relating to the enhanced feature vector, but would then in-context end up self-correcting and attempt to correct for the bias.<p>I&#x27;m <i>extremely</i> curious if as models scale in complexity if techniques like this will start to become less and less effective as net model representations collapse onto an enforced alignment (which may differ from the &#x27;safety&#x27; trained alignment, but be an inherent pretrained alignment that can&#x27;t be easily overcome without gutting model capabilities too).<p>I have a sneaking suspicion this will be the case.</div><br/><div id="40666621" class="c"><input type="checkbox" id="c-40666621" checked=""/><div class="controls bullet"><span class="by">rileyphone</span><span>|</span><a href="#40666023">root</a><span>|</span><a href="#40666319">parent</a><span>|</span><a href="#40666542">next</a><span>|</span><label class="collapse" for="c-40666621">[-]</label><label class="expand" for="c-40666621">[1 more]</label></div><br/><div class="children"><div class="content">In that case there are two attractors - one towards the Golden Gate Bridge and one towards the harmless, helpful, honest assistant persona. Techniques as such probably get weirder results with model scale but no reason to think they get wiped out.</div><br/></div></div><div id="40666542" class="c"><input type="checkbox" id="c-40666542" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#40666023">root</a><span>|</span><a href="#40666319">parent</a><span>|</span><a href="#40666621">prev</a><span>|</span><a href="#40666953">next</a><span>|</span><label class="collapse" for="c-40666542">[-]</label><label class="expand" for="c-40666542">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s <i>GGC</i> in this context?</div><br/><div id="40666586" class="c"><input type="checkbox" id="c-40666586" checked=""/><div class="controls bullet"><span class="by">dannyobrien</span><span>|</span><a href="#40666023">root</a><span>|</span><a href="#40666542">parent</a><span>|</span><a href="#40666953">next</a><span>|</span><label class="collapse" for="c-40666586">[-]</label><label class="expand" for="c-40666586">[1 more]</label></div><br/><div class="children"><div class="content">Golden Gate Claude</div><br/></div></div></div></div></div></div></div></div><div id="40666953" class="c"><input type="checkbox" id="c-40666953" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#40666023">prev</a><span>|</span><a href="#40666684">next</a><span>|</span><label class="collapse" for="c-40666953">[-]</label><label class="expand" for="c-40666953">[1 more]</label></div><br/><div class="children"><div class="content">They should call it “lobotomy”.<p>The picture on top of the article looks pretty much like what Walter Freeman would have used as an ad in the 1930s for his door-to-door “ice pick through the eye socket” procedure.</div><br/></div></div><div id="40666684" class="c"><input type="checkbox" id="c-40666684" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#40666953">prev</a><span>|</span><a href="#40666492">next</a><span>|</span><label class="collapse" for="c-40666684">[-]</label><label class="expand" for="c-40666684">[15 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;As an AI assistant, I cannot help you.&quot; While this safety feature is crucial for preventing misuse,<p>What is the safety added by this? What is unsafe about a computer giving you answers?</div><br/><div id="40666984" class="c"><input type="checkbox" id="c-40666984" checked=""/><div class="controls bullet"><span class="by">tgsovlerkhgsel</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40666709">next</a><span>|</span><label class="collapse" for="c-40666984">[-]</label><label class="expand" for="c-40666984">[4 more]</label></div><br/><div class="children"><div class="content">I think there are several broad categories all wrapped under &quot;safety&quot;:<p>- PR (avoid hurting feelings, avoid generating text that would make journalists write sensationalist negative articles about the company)<p>- &quot;forbidden knowledge&quot;: Don&#x27;t give people advice on how to do dangerous&#x2F;bad things like building bombs (broadly a subcategory of the above - the content is usually discoverable through other means and the LLM generally won&#x27;t give <i>better</i> advice)<p>- dangerous advice and advice that&#x27;s dangerous when wrong: many people don&#x27;t understand what LLMs do, and the output is VERY convincing even when wrong. So if the model tells people the best way to entertain your kids is to mix bleach and ammonia and blow bubbles (a common deadly recipe recommended on 4chan), there will be dead people.<p>- keeping bad people from using the model in bad ways, e.g. having it write stories where children are raped, scamming people at scale (think Nigeria scam but automated), or election interference (people are herd animals, so if you show someone 100 different posts from 100 different &quot;people&quot; telling them that X is right and Y is wrong, it <i>will</i> influence them, and at scale this has the potential to tilt elections and conquer countries).<p>I think the first ones are rather stupid, but the latter ones get more and more important to actually have. Especially the very last one (opinion shifting&#x2F;election interference) is something where the existence of these models can have a very real, negative effect on the world (affecting you even if you yourself never come into contact with any of the models or its outputs, since you&#x27;ll have to deal with the puppet government elected due to it), and I appreciate the companies building and running the models doing something about it.</div><br/><div id="40667184" class="c"><input type="checkbox" id="c-40667184" checked=""/><div class="controls bullet"><span class="by">idle_zealot</span><span>|</span><a href="#40666684">root</a><span>|</span><a href="#40666984">parent</a><span>|</span><a href="#40667179">next</a><span>|</span><label class="collapse" for="c-40667184">[-]</label><label class="expand" for="c-40667184">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I think the first ones are rather stupid, but the latter ones get more and more important to actually have. Especially the very last one (opinion shifting&#x2F;election interference) is something where the existence of these models can have a very real, negative effect on the world (affecting you even if you yourself never come into contact with any of the models or its outputs, since you&#x27;ll have to deal with the puppet government elected due to it), and I appreciate the companies building and running the models doing something about it.<p>That genie is very much out of the bottle. There are already models good enough to build fake social media profiles and convincingly post in support of any opinion. The &quot;make the technology incapable of being used by bad actors&quot; ship has sailed, and I would argue was never realistic. We <i>need</i> to improve public messaging around anonymous and pseudonymous only communication. Make it absolutely clear that what you read on the internet from someone you&#x27;ve not personally met and exchanged contact information with is more likely to be a bot than not, and no, you can&#x27;t tell just by chatting with them, not even voice chatting. The computers are convincingly human and we need to alter our culture to reflect that fact of life, not <i>reactively ban computers</i>.</div><br/></div></div><div id="40667179" class="c"><input type="checkbox" id="c-40667179" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#40666684">root</a><span>|</span><a href="#40666984">parent</a><span>|</span><a href="#40667184">prev</a><span>|</span><a href="#40667217">next</a><span>|</span><label class="collapse" for="c-40667179">[-]</label><label class="expand" for="c-40667179">[1 more]</label></div><br/><div class="children"><div class="content">Iow, we have a backdoor, and by backdoor I mean a whole back wall missing, but only certified entities are allowed to [ab]use it and it’s better to keep it all under the rug and pretend all ok.<p>You can’t harden humanity against this exploit without pointing it out and making a few examples. Someone will make an “unsafe” but useful model eventually and this safety mannequin will flop with a bang, cause it’s similar to avoiding sex and drugs conversations with kids.<p>It’s nice that companies think about it at all. But the best thing they will ever do is to cover their own ass while keeping everyone naked before the storm.<p>The history of covering is also ridden with exploits, see e.g. google’s recent model which cannot draw situations without rainbow-coloring people. For some reason, this isn’t considered as cultural&#x2F;political hijacking or exploitation, despite the fact that the problem is purely domestic to the model’s origin.</div><br/></div></div><div id="40667217" class="c"><input type="checkbox" id="c-40667217" checked=""/><div class="controls bullet"><span class="by">irusensei</span><span>|</span><a href="#40666684">root</a><span>|</span><a href="#40666984">parent</a><span>|</span><a href="#40667179">prev</a><span>|</span><a href="#40666709">next</a><span>|</span><label class="collapse" for="c-40667217">[-]</label><label class="expand" for="c-40667217">[1 more]</label></div><br/><div class="children"><div class="content">&gt; keeping bad people from using the model in bad ways, e.g. having it write stories where...<p>The last ones are rather stupid too. Bad people can just write stories or creating drawings about disgusting things. Should we censor all computers to prevent such things from happening? Or hands and paper?</div><br/></div></div></div></div><div id="40666709" class="c"><input type="checkbox" id="c-40666709" checked=""/><div class="controls bullet"><span class="by">CGamesPlay</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40666984">prev</a><span>|</span><a href="#40666890">next</a><span>|</span><label class="collapse" for="c-40666709">[-]</label><label class="expand" for="c-40666709">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s unsafe for the publisher of the model to have their model perform &quot;undesirable&quot; action, because it leads to bad PR for them. In this case, Meta doesn&#x27;t want a news article that says &quot;Llama 3 gives instructions to stalk your ex&quot; or something along those lines.<p>With this &quot;uncensoring&quot;, they can say, &quot;no, an unaffiliated product offered these directions; Llama 3 as provided does not.&quot;</div><br/></div></div><div id="40666890" class="c"><input type="checkbox" id="c-40666890" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40666709">prev</a><span>|</span><a href="#40667243">next</a><span>|</span><label class="collapse" for="c-40666890">[-]</label><label class="expand" for="c-40666890">[1 more]</label></div><br/><div class="children"><div class="content">Yep. Safety for the publisher. In addition to what the sibling comments say, there’s also payment providers and App stores. They’ll test your app, trying to get your model to output content that falls under the category “extreme violence”, “bestiality”, “racism”, etc., and then they’ll ban you from the platform. So yeah, little to do with “safety” of the end user.</div><br/></div></div><div id="40667243" class="c"><input type="checkbox" id="c-40667243" checked=""/><div class="controls bullet"><span class="by">checkyoursudo</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40666890">prev</a><span>|</span><a href="#40666835">next</a><span>|</span><label class="collapse" for="c-40667243">[-]</label><label class="expand" for="c-40667243">[1 more]</label></div><br/><div class="children"><div class="content">Brand safety. They just make it seem like safety for someone else, but it is brand safety.</div><br/></div></div><div id="40666835" class="c"><input type="checkbox" id="c-40666835" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40667243">prev</a><span>|</span><a href="#40667025">next</a><span>|</span><label class="collapse" for="c-40666835">[-]</label><label class="expand" for="c-40666835">[1 more]</label></div><br/><div class="children"><div class="content">People keep claiming they can publish weights and also prevent misuse, such as spam and, a bit later on, stuff like helping people build bombs.<p>This is of course impossible, but that makes certain companies&#x27; approaches unviable, so they keep claiming it anyways.</div><br/></div></div><div id="40667025" class="c"><input type="checkbox" id="c-40667025" checked=""/><div class="controls bullet"><span class="by">rustcleaner</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40666835">prev</a><span>|</span><a href="#40666992">next</a><span>|</span><label class="collapse" for="c-40667025">[-]</label><label class="expand" for="c-40667025">[1 more]</label></div><br/><div class="children"><div class="content">If I can ask the question, I can take the answer.  It&#x27;s not up to daddy $AI_SAFETY_CHIEF to decide what an infohazard is for me.</div><br/></div></div><div id="40666992" class="c"><input type="checkbox" id="c-40666992" checked=""/><div class="controls bullet"><span class="by">zucker42</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40667025">prev</a><span>|</span><a href="#40666828">next</a><span>|</span><label class="collapse" for="c-40666992">[-]</label><label class="expand" for="c-40666992">[1 more]</label></div><br/><div class="children"><div class="content">The main thing I&#x27;d be worried about in the short term is models making accessible the information to synthesize a pandemic capable virus.</div><br/></div></div><div id="40666828" class="c"><input type="checkbox" id="c-40666828" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#40666684">parent</a><span>|</span><a href="#40666992">prev</a><span>|</span><a href="#40666492">next</a><span>|</span><label class="collapse" for="c-40666828">[-]</label><label class="expand" for="c-40666828">[4 more]</label></div><br/><div class="children"><div class="content">For one, corporate safety of the hoster&#x2F;model creator. No one wants their name associated with racial slurs or creating material visually identical to CSAM - the latter might even carry criminal liability in some jurisdictions (e.g. Germany which has absolutely ridiculously strong laws on that matter, even banning <i>literature</i>).<p>Another very huge issue is public safety. During training, an AI ingests 
<i>lots</i> of non-reviewed material, including (very) detailed descriptions on how to make dangerous stuff like bombs. So theoretically a well-trained AI model knows how to synthesize explosive compounds or drugs just from reading Wikipedia, chemistry magazines and transcripts of NileRed videos... but that&#x27;s hard to comprehend and distill into a recipe if you&#x27;re not a trained chemist, but an AI model can do that with ease. The problem is now two-fold: for one, even an untrained idiot can ask about how to make a bomb and get something that works... but the other part is much more critical: if you manage to persuade a chemist to tell you how the synthesis for a compound works, they will tell you where it is easy to fuck-up to prevent disaster (e.g. only adding a compound drop-wise, making sure all glassware is thoroughly washed with a specific solvent). An AI might not do that because the scientific paper it was trained on omits these steps (because the author assumes common prior knowledge), and so the bomb-maker blows themselves up. Or the AI hallucinates something dangerous (e.g. compounds that one Just Fucking Should Not Mix), doesn&#x27;t realize that, and the bomb-maker blows themselves up or generates nerve gas in their basement.</div><br/><div id="40666981" class="c"><input type="checkbox" id="c-40666981" checked=""/><div class="controls bullet"><span class="by">vasco</span><span>|</span><a href="#40666684">root</a><span>|</span><a href="#40666828">parent</a><span>|</span><a href="#40667067">next</a><span>|</span><label class="collapse" for="c-40666981">[-]</label><label class="expand" for="c-40666981">[2 more]</label></div><br/><div class="children"><div class="content">Bomb making instructions are available in quite plentiful ways, both on the internet and in books, with step by step instructions even. People don&#x27;t &quot;not make bombs&quot; for lack of instructions. <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Bomb-making_instructions_on_the_Internet" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Bomb-making_instructions_on_...</a><p>Here, if you want to make a quick chemical weapon: get a bucket, vinegar, bleach. Dump the bleach into the bucket. Dump the vinegar into the bucket. If you breath it in you die. An LLM doesn&#x27;t change this.</div><br/><div id="40667398" class="c"><input type="checkbox" id="c-40667398" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#40666684">root</a><span>|</span><a href="#40666981">parent</a><span>|</span><a href="#40667067">next</a><span>|</span><label class="collapse" for="c-40667398">[-]</label><label class="expand" for="c-40667398">[1 more]</label></div><br/><div class="children"><div class="content">Oh they are available, no doubt, but there have been people dragged through the courts for simple possession of instructions [1]. While <i>generally</i> the situation has been settled, it&#x27;s nevertheless wiser for companies to try to do their best to not end up prosecuted under terrorism charges.<p>[1] <a href="https:&#x2F;&#x2F;theintercept.com&#x2F;2017&#x2F;10&#x2F;28&#x2F;josh-walker-anarchist-cookbook-terrorism-act-uk&#x2F;" rel="nofollow">https:&#x2F;&#x2F;theintercept.com&#x2F;2017&#x2F;10&#x2F;28&#x2F;josh-walker-anarchist-co...</a></div><br/></div></div></div></div><div id="40667067" class="c"><input type="checkbox" id="c-40667067" checked=""/><div class="controls bullet"><span class="by">rustcleaner</span><span>|</span><a href="#40666684">root</a><span>|</span><a href="#40666828">parent</a><span>|</span><a href="#40666981">prev</a><span>|</span><a href="#40666492">next</a><span>|</span><label class="collapse" for="c-40667067">[-]</label><label class="expand" for="c-40667067">[1 more]</label></div><br/><div class="children"><div class="content">I hear Aaron Swartz calling from behind the veil: Information wants to be free!</div><br/></div></div></div></div></div></div><div id="40666492" class="c"><input type="checkbox" id="c-40666492" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#40666684">prev</a><span>|</span><a href="#40666249">next</a><span>|</span><label class="collapse" for="c-40666492">[-]</label><label class="expand" for="c-40666492">[1 more]</label></div><br/><div class="children"><div class="content">There was a recent paper about a way to censor LLMs by just deleting the connections to any bad outputs, rather than training it to refuse them. I think this technique wouldn&#x27;t work.<p>Obviously you could train any bad outputs back into them if you have the model weights.</div><br/></div></div><div id="40666249" class="c"><input type="checkbox" id="c-40666249" checked=""/><div class="controls bullet"><span class="by">Mathnerd314</span><span>|</span><a href="#40666492">prev</a><span>|</span><a href="#40666128">next</a><span>|</span><label class="collapse" for="c-40666249">[-]</label><label class="expand" for="c-40666249">[2 more]</label></div><br/><div class="children"><div class="content">Reminds me of <a href="https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;representation-engineering&#x2F;" rel="nofollow">https:&#x2F;&#x2F;vgel.me&#x2F;posts&#x2F;representation-engineering&#x2F;</a>. There they were adding a control vector, w&#x27; = cvec + w, here they are &quot;ablating&quot; it, 
w&#x27; = w - dot(w,cvec)*cvec. There is an interesting field of learning how to &quot;brain chip&quot; LLMs into doing what you want.</div><br/><div id="40666307" class="c"><input type="checkbox" id="c-40666307" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40666249">parent</a><span>|</span><a href="#40666128">next</a><span>|</span><label class="collapse" for="c-40666307">[-]</label><label class="expand" for="c-40666307">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s so much work just like this coming out simultaneously.<p>Steering Vectors, Control Vectors, PyReft, PeFT improvements, Obliteration. It&#x27;s a great time to be doing representation engineering.</div><br/></div></div></div></div><div id="40666128" class="c"><input type="checkbox" id="c-40666128" checked=""/><div class="controls bullet"><span class="by">okwhateverdude</span><span>|</span><a href="#40666249">prev</a><span>|</span><a href="#40665987">next</a><span>|</span><label class="collapse" for="c-40666128">[-]</label><label class="expand" for="c-40666128">[7 more]</label></div><br/><div class="children"><div class="content">I gave some of the llama3 ablated models (eg. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;cognitivecomputations&#x2F;Llama-3-8B-Instruct-abliterated-v2-gguf" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;cognitivecomputations&#x2F;Llama-3-8B-Inst...</a>) a try and was pretty disappointed in the result. Could have been problems in the dataset, but overall, the model felt like it had been given a lobotomy. It would fail to produce stop tokens frequently and then start talking to itself.</div><br/><div id="40666399" class="c"><input type="checkbox" id="c-40666399" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#40666128">parent</a><span>|</span><a href="#40666138">next</a><span>|</span><label class="collapse" for="c-40666399">[-]</label><label class="expand" for="c-40666399">[1 more]</label></div><br/><div class="children"><div class="content">They might have been doing it wrong, the code can be a bit tricky. I did a recent ablation on Qwen2 (removing Chinese censorship refusals) and ran MixEval benchmarks (0.96 correlation w&#x2F; ChatArena results)and saw a neglible performance difference (see model card for results): <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;augmxnt&#x2F;Qwen2-7B-Instruct-deccp" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;augmxnt&#x2F;Qwen2-7B-Instruct-deccp</a></div><br/></div></div><div id="40666138" class="c"><input type="checkbox" id="c-40666138" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40666128">parent</a><span>|</span><a href="#40666399">prev</a><span>|</span><a href="#40665987">next</a><span>|</span><label class="collapse" for="c-40666138">[-]</label><label class="expand" for="c-40666138">[5 more]</label></div><br/><div class="children"><div class="content">I have entirely the opposite experience. Llama3 70b obliterated works perfectly and is willing to tell me how to commit mass genocide, all while maintaining quality outputs.</div><br/><div id="40666337" class="c"><input type="checkbox" id="c-40666337" checked=""/><div class="controls bullet"><span class="by">infotainment</span><span>|</span><a href="#40666128">root</a><span>|</span><a href="#40666138">parent</a><span>|</span><a href="#40666433">next</a><span>|</span><label class="collapse" for="c-40666337">[-]</label><label class="expand" for="c-40666337">[1 more]</label></div><br/><div class="children"><div class="content">Same, I installed an implementation of an orthagonalized LLama3 and it seems to work just as well as the base model, sans refusals.<p>I believe this is the model I had good results with:<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;wassname&#x2F;meta-llama-3-8b-instruct-helpfull" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;wassname&#x2F;meta-llama-3-8b-instruct-hel...</a></div><br/></div></div><div id="40666433" class="c"><input type="checkbox" id="c-40666433" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#40666128">root</a><span>|</span><a href="#40666138">parent</a><span>|</span><a href="#40666337">prev</a><span>|</span><a href="#40667051">next</a><span>|</span><label class="collapse" for="c-40666433">[-]</label><label class="expand" for="c-40666433">[2 more]</label></div><br/><div class="children"><div class="content">&gt; how to commit mass genocide, all while maintaining quality outputs.<p>sounds like a messed up eugenics filter.</div><br/></div></div><div id="40667051" class="c"><input type="checkbox" id="c-40667051" checked=""/><div class="controls bullet"><span class="by">fransje26</span><span>|</span><a href="#40666128">root</a><span>|</span><a href="#40666138">parent</a><span>|</span><a href="#40666433">prev</a><span>|</span><a href="#40665987">next</a><span>|</span><label class="collapse" for="c-40667051">[-]</label><label class="expand" for="c-40667051">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Der_Einzige<p>&gt; and is willing to tell me how to commit mass genocide, all while maintaining quality outputs<p>Ah, I see they fine-tuned it to satisfy the demands of the local market.. &#x2F;s &#x2F;s</div><br/></div></div></div></div></div></div><div id="40665987" class="c"><input type="checkbox" id="c-40665987" checked=""/><div class="controls bullet"><span class="by">akie</span><span>|</span><a href="#40666128">prev</a><span>|</span><a href="#40666198">next</a><span>|</span><label class="collapse" for="c-40665987">[-]</label><label class="expand" for="c-40665987">[9 more]</label></div><br/><div class="children"><div class="content">Pretty sure Asimov didn’t consider that when he wrote his three laws of robotics.</div><br/><div id="40666069" class="c"><input type="checkbox" id="c-40666069" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#40665987">parent</a><span>|</span><a href="#40666198">next</a><span>|</span><label class="collapse" for="c-40666069">[-]</label><label class="expand" for="c-40666069">[8 more]</label></div><br/><div class="children"><div class="content">Asimov wrote the three laws as a parody of rationalists who are so uncreative they expect a  ruleset can actually impose control<p>Or, as Dr Malcom would say: life, uh, finds a way.</div><br/><div id="40666159" class="c"><input type="checkbox" id="c-40666159" checked=""/><div class="controls bullet"><span class="by">jraph</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666069">parent</a><span>|</span><a href="#40666519">next</a><span>|</span><label class="collapse" for="c-40666159">[-]</label><label class="expand" for="c-40666159">[5 more]</label></div><br/><div class="children"><div class="content">Do you have an evidence for this? It surprises me and I can&#x27;t find anything about it.<p>This should be a crucial piece of information about the tree laws, yet it&#x27;s not mentioned in the Wikipedia article about the three laws [1], which is otherwise quite detailed. Reading this, everything makes me think that it was not a parody. I didn&#x27;t feel like it was parody when reading the Robot series neither. He wanted an alternative to the Frankenstein plot where robots kill their creators and the three laws were part of the answer.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Three_Laws_of_Robotics" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Three_Laws_of_Robotics</a></div><br/><div id="40666242" class="c"><input type="checkbox" id="c-40666242" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666159">parent</a><span>|</span><a href="#40666389">next</a><span>|</span><label class="collapse" for="c-40666242">[-]</label><label class="expand" for="c-40666242">[2 more]</label></div><br/><div class="children"><div class="content">I agree the term parody is absolutely inappropriate but it’s also not the case that they’re portrayed as entirely positive and complete. They’re ultimately flawed, resulting in many unintended consequences and ethical dilemmas. To that extent it is a refutation of the idea there are perfectly constructed maxims, and should serve as a real warning to people pursuing safety and alignment in AI. I know a fair number of them personally and they are often very young, generally inexperienced, highly intelligent, but with a hefty dose of hubris. This is a pretty dangerous combination IMO, but I also recognize their goals are generally unattainable in the broad sense, are useful in a narrow practical sense for people and enterprises who want a generally on guard rails solution, and they’re developing the technical techniques we might be able to use once some time has passed, we understand the domain better, and the companies hire a few grown ups.</div><br/><div id="40666287" class="c"><input type="checkbox" id="c-40666287" checked=""/><div class="controls bullet"><span class="by">jraph</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666242">parent</a><span>|</span><a href="#40666389">next</a><span>|</span><label class="collapse" for="c-40666287">[-]</label><label class="expand" for="c-40666287">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but it’s also not the case that they’re portrayed as entirely positive and complete.<p>This I agree with. A big part of the fun of the series is that Asimov constantly plays with these laws.<p>Thanks for the clarification.<p>(I still completely disagree that &quot;parody of rationalists who are so uncreative they expect a ruleset can actually impose control&quot; was the intent. I believe not only the word &quot;parody&quot; is to throw away, but the whole sentence with it too. I understand better your stance now though)</div><br/></div></div></div></div><div id="40666389" class="c"><input type="checkbox" id="c-40666389" checked=""/><div class="controls bullet"><span class="by">nonrandomstring</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666159">parent</a><span>|</span><a href="#40666242">prev</a><span>|</span><a href="#40666519">next</a><span>|</span><label class="collapse" for="c-40666389">[-]</label><label class="expand" for="c-40666389">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Do you have an evidence for this?<p>I think the strongest evidence is that many other examples of Asimov,
especially short stories are cautionary and deal with hubris and
unexpected side effects.<p>However it&#x27;s funny to ask for &#x27;evidence&#x27; about fiction in the context
of &quot;parodying rationalists&quot;. no?  Since what would <i>count</i> as
evidence?  Another, more &quot;authoritative&quot; literary interpreter saying
the same thing? Maybe a long time ago - historical statements seem to
carry more weight, as if people were wiser back then?. Or Asimov
himself?  But don&#x27;t they say, only bad writers explain themselves?</div><br/><div id="40666578" class="c"><input type="checkbox" id="c-40666578" checked=""/><div class="controls bullet"><span class="by">kevingadd</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666389">parent</a><span>|</span><a href="#40666519">next</a><span>|</span><label class="collapse" for="c-40666578">[-]</label><label class="expand" for="c-40666578">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re going to make an assertion about the intent of an author&#x27;s work, it seems like you should back that up with facts? Otherwise it&#x27;s an &quot;i think&quot; or &quot;it seems like&quot; or &quot;one could argue&quot;, isn&#x27;t it?</div><br/></div></div></div></div></div></div><div id="40666519" class="c"><input type="checkbox" id="c-40666519" checked=""/><div class="controls bullet"><span class="by">tomcam</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666069">parent</a><span>|</span><a href="#40666159">prev</a><span>|</span><a href="#40666459">next</a><span>|</span><label class="collapse" for="c-40666519">[-]</label><label class="expand" for="c-40666519">[1 more]</label></div><br/><div class="children"><div class="content">Don’t think so. Asimov wrote that his editor John Campbell established the 3 Laws. I think it was to tighten up Asimov’s work, though I’m less sure of that part.</div><br/></div></div><div id="40666459" class="c"><input type="checkbox" id="c-40666459" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#40665987">root</a><span>|</span><a href="#40666069">parent</a><span>|</span><a href="#40666519">prev</a><span>|</span><a href="#40666198">next</a><span>|</span><label class="collapse" for="c-40666459">[-]</label><label class="expand" for="c-40666459">[1 more]</label></div><br/><div class="children"><div class="content">rule 1: &quot;don&#x27;t be evil&quot;<p>rule 2: IBM gets a pass¹<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;JSON" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;JSON</a><p>(hmmm... except wikipedia doesn&#x27;t have the story)<p>EDIT: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=3693388">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=3693388</a></div><br/></div></div></div></div></div></div><div id="40666198" class="c"><input type="checkbox" id="c-40666198" checked=""/><div class="controls bullet"><span class="by">HanClinto</span><span>|</span><a href="#40665987">prev</a><span>|</span><a href="#40666038">next</a><span>|</span><label class="collapse" for="c-40666198">[-]</label><label class="expand" for="c-40666198">[1 more]</label></div><br/><div class="children"><div class="content">A little bit of discussion on the source paper was done here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40242939">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40242939</a><p>Really nice to see this work continuing -- it seems like a very powerful technique!</div><br/></div></div><div id="40666038" class="c"><input type="checkbox" id="c-40666038" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#40666198">prev</a><span>|</span><a href="#40666012">next</a><span>|</span><label class="collapse" for="c-40666038">[-]</label><label class="expand" for="c-40666038">[3 more]</label></div><br/><div class="children"><div class="content">So this seems to be about uncensoring a model that the user is running locally. Is that right, do they expect to limit what someone can do under those circumstances? Kind of like expecting no one to break local copy protection, except copy protection with much less reliable tools.</div><br/><div id="40666425" class="c"><input type="checkbox" id="c-40666425" checked=""/><div class="controls bullet"><span class="by">gopher_space</span><span>|</span><a href="#40666038">parent</a><span>|</span><a href="#40666271">next</a><span>|</span><label class="collapse" for="c-40666425">[-]</label><label class="expand" for="c-40666425">[1 more]</label></div><br/><div class="children"><div class="content">The free tools are already good enough.  LLMs seem like they&#x27;re going to be massively useful and weirdly hard to monetize.  Niche experts with frequent updates?<p>It feels like Apple is the only place that&#x27;s able to craft the user-centered brain in a box we all desperately require, and that&#x27;s too bad because monocultures suck.</div><br/></div></div><div id="40666271" class="c"><input type="checkbox" id="c-40666271" checked=""/><div class="controls bullet"><span class="by">Mathnerd314</span><span>|</span><a href="#40666038">parent</a><span>|</span><a href="#40666425">prev</a><span>|</span><a href="#40666012">next</a><span>|</span><label class="collapse" for="c-40666271">[-]</label><label class="expand" for="c-40666271">[1 more]</label></div><br/><div class="children"><div class="content">There are many hacks to uncensor LLMs, the surprising thing is that this is fairly simple but works really well.</div><br/></div></div></div></div><div id="40666012" class="c"><input type="checkbox" id="c-40666012" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#40666038">prev</a><span>|</span><a href="#40666313">next</a><span>|</span><label class="collapse" for="c-40666012">[-]</label><label class="expand" for="c-40666012">[1 more]</label></div><br/><div class="children"><div class="content">Great blog post, loved the straightforward explanations and code.</div><br/></div></div><div id="40666313" class="c"><input type="checkbox" id="c-40666313" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#40666012">prev</a><span>|</span><a href="#40666255">next</a><span>|</span><label class="collapse" for="c-40666313">[-]</label><label class="expand" for="c-40666313">[12 more]</label></div><br/><div class="children"><div class="content">Uncensoring Llama 3 is a violation of the Llama 3 acceptable use policy.<p><a href="https:&#x2F;&#x2F;llama.meta.com&#x2F;llama3&#x2F;use-policy&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llama.meta.com&#x2F;llama3&#x2F;use-policy&#x2F;</a><p>&gt; You agree you will not use, or <i>allow others to use</i>, Meta Llama 3 to:  &lt;list of bad things&gt;...<p>That terminates your Llama 3 license forcing you to delete all the &quot;materials&quot; from your system.</div><br/><div id="40666456" class="c"><input type="checkbox" id="c-40666456" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#40666313">parent</a><span>|</span><a href="#40667231">next</a><span>|</span><label class="collapse" for="c-40666456">[-]</label><label class="expand" for="c-40666456">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That terminates your Llama 3 license forcing you to delete all the &quot;materials&quot; from your system.<p>Or, it means you have broken a contract (of adhesion) formed by acquiring the weights from Meta. You can break contracts! Meta could take a civil case against you, but that&#x27;s it. The AUP is a document, it&#x27;s not going to force you to do anything. The court could potentially force you, but that&#x27;s unlikely, even in the more unlikely event that anyone cares enough to find out what&#x27;s happening and bring a case against you.</div><br/></div></div><div id="40667231" class="c"><input type="checkbox" id="c-40667231" checked=""/><div class="controls bullet"><span class="by">irusensei</span><span>|</span><a href="#40666313">parent</a><span>|</span><a href="#40666456">prev</a><span>|</span><a href="#40666327">next</a><span>|</span><label class="collapse" for="c-40667231">[-]</label><label class="expand" for="c-40667231">[1 more]</label></div><br/><div class="children"><div class="content">I am under the opinion that terms of use from models trained out of public (often stolen) content should be disregarded by the general public.</div><br/></div></div><div id="40666327" class="c"><input type="checkbox" id="c-40666327" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#40666313">parent</a><span>|</span><a href="#40667231">prev</a><span>|</span><a href="#40666503">next</a><span>|</span><label class="collapse" for="c-40666327">[-]</label><label class="expand" for="c-40666327">[8 more]</label></div><br/><div class="children"><div class="content">Do you mean to say that teaching people how to do things should be regarded, for this purpose, as a form of allowing them to do those things?</div><br/><div id="40666335" class="c"><input type="checkbox" id="c-40666335" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666327">parent</a><span>|</span><a href="#40666503">next</a><span>|</span><label class="collapse" for="c-40666335">[-]</label><label class="expand" for="c-40666335">[7 more]</label></div><br/><div class="children"><div class="content">The article clearly demonstrates how to circumvent the built-in protections in the model that prevent it from doing the stuff that violates the acceptable use policy. Which are clearly the things that are against the public good.<p>There should be CVEs for AI.</div><br/><div id="40666554" class="c"><input type="checkbox" id="c-40666554" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666335">parent</a><span>|</span><a href="#40666503">next</a><span>|</span><label class="collapse" for="c-40666554">[-]</label><label class="expand" for="c-40666554">[6 more]</label></div><br/><div class="children"><div class="content">Giving large, politicised software companies the sole power to determine what LLMs can and cannot say is against the public good.</div><br/><div id="40666973" class="c"><input type="checkbox" id="c-40666973" checked=""/><div class="controls bullet"><span class="by">atwrk</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666554">parent</a><span>|</span><a href="#40666568">next</a><span>|</span><label class="collapse" for="c-40666973">[-]</label><label class="expand" for="c-40666973">[1 more]</label></div><br/><div class="children"><div class="content">LLMs, in this context, are nothing more than search indexes. The exact same information is a google query away. Publicly crawlable information was the training material for them, after all.</div><br/></div></div><div id="40666568" class="c"><input type="checkbox" id="c-40666568" checked=""/><div class="controls bullet"><span class="by">29athrowaway</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666554">parent</a><span>|</span><a href="#40666973">prev</a><span>|</span><a href="#40666503">next</a><span>|</span><label class="collapse" for="c-40666568">[-]</label><label class="expand" for="c-40666568">[4 more]</label></div><br/><div class="children"><div class="content">Agreed. But uncensoring Llama 3 can do harm in the immediate term.<p>As much as I am not a fan of Meta, an uncensored Llama 3 in the wrong hands is a universally bad idea.</div><br/><div id="40666747" class="c"><input type="checkbox" id="c-40666747" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666568">parent</a><span>|</span><a href="#40666983">next</a><span>|</span><label class="collapse" for="c-40666747">[-]</label><label class="expand" for="c-40666747">[1 more]</label></div><br/><div class="children"><div class="content">Universally eh? Who decides what should be censored and what not? You?</div><br/></div></div><div id="40666983" class="c"><input type="checkbox" id="c-40666983" checked=""/><div class="controls bullet"><span class="by">pantalaimon</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666568">parent</a><span>|</span><a href="#40666747">prev</a><span>|</span><a href="#40667248">next</a><span>|</span><label class="collapse" for="c-40666983">[-]</label><label class="expand" for="c-40666983">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But uncensoring Llama 3 can do harm in the immediate term<p>How so?</div><br/></div></div><div id="40667248" class="c"><input type="checkbox" id="c-40667248" checked=""/><div class="controls bullet"><span class="by">wruza</span><span>|</span><a href="#40666313">root</a><span>|</span><a href="#40666568">parent</a><span>|</span><a href="#40666983">prev</a><span>|</span><a href="#40666503">next</a><span>|</span><label class="collapse" for="c-40667248">[-]</label><label class="expand" for="c-40667248">[1 more]</label></div><br/><div class="children"><div class="content">Almost everything in wrong hands is universally a bad idea. This phrase is just FUD and makes little sense.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40666503" class="c"><input type="checkbox" id="c-40666503" checked=""/><div class="controls bullet"><span class="by">pixxel</span><span>|</span><a href="#40666313">parent</a><span>|</span><a href="#40666327">prev</a><span>|</span><a href="#40666255">next</a><span>|</span><label class="collapse" for="c-40666503">[-]</label><label class="expand" for="c-40666503">[1 more]</label></div><br/><div class="children"><div class="content">You’re on Hacker News. It’s a shadow of its former self, but still.</div><br/></div></div></div></div><div id="40666255" class="c"><input type="checkbox" id="c-40666255" checked=""/><div class="controls bullet"><span class="by">olliej</span><span>|</span><a href="#40666313">prev</a><span>|</span><a href="#40666019">next</a><span>|</span><label class="collapse" for="c-40666255">[-]</label><label class="expand" for="c-40666255">[2 more]</label></div><br/><div class="children"><div class="content">While I still think presenting LLMs as &quot;intelligent&quot; is nonsense, I think this issue is interesting given the goal of these LLMs is just to produce a statistically plausible stream of text, it&#x27;s always just a matter of constructing queries where the inappropriate output is statistically plausible given the model.<p>Similarly I think the concerns about bad output are overblown: an LLM may tell you how to make an X, where X is bad, but so will google, an LLM may produce biased output but so will google, the real issue is the people making these systems have managed to convince people that there is some kind of actual intelligence, so people accept the output as &quot;a computer created it so it must be true&quot; rather than &quot;glorified output of google&quot;. People understand if you google &quot;why is race X terrible&quot; you&#x27;ll get racist BS, but don&#x27;t understand that if you ask an LLM to &quot;explain why race X is terrible&quot; you&#x27;re just getting automatically rewritten version of the google output. (Though maybe google&#x27;s &quot;AI&quot; search results will actually fix this misunderstanding more effectively than any explanatory blog post :D )<p>Anyway back to the problem, I really don&#x27;t think there&#x27;s a solution that is anything other then &quot;run the output through a separate system that is just giving a &#x27;is this text allowed given our rules&#x27;&quot; before transmitting it to the requestor. You could combine this with training in future as well (you will eventually build up a large test set of queries producing inappropriate output that the generative model produces, and you can use that as the basis for adversarial training of the LLM). I know there&#x27;s the desire to wrap in the content restrictions into the basic query handling because it&#x27;s negligible more work to add those tokens to the stream, but mechanisms for filtering&#x2F;identifying type of content are vastly cheaper than LLMs level &quot;AI&quot;.</div><br/><div id="40666458" class="c"><input type="checkbox" id="c-40666458" checked=""/><div class="controls bullet"><span class="by">nonrandomstring</span><span>|</span><a href="#40666255">parent</a><span>|</span><a href="#40666019">next</a><span>|</span><label class="collapse" for="c-40666458">[-]</label><label class="expand" for="c-40666458">[1 more]</label></div><br/><div class="children"><div class="content">&gt; so people accept the output as &quot;a computer created it so it must be true&quot;<p>This is the general form of the problem underlying half the news
stories on any day.<p>Oddly there are historical roots in science fiction. But always, giant
robots flailing their pincers and shouting &quot;does not compute!!&quot; were
also cautionary tropes against silly conceits of perfection.<p>What keeps it going, is that it perfectly suits the richest and
largest corporations since the East India Tea Company to have people
(even very smart people) believing the things they sell are
&#x27;infallible&#x27;.</div><br/></div></div></div></div><div id="40666019" class="c"><input type="checkbox" id="c-40666019" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#40666255">prev</a><span>|</span><a href="#40666318">next</a><span>|</span><label class="collapse" for="c-40666019">[-]</label><label class="expand" for="c-40666019">[1 more]</label></div><br/><div class="children"><div class="content">Ironic given that lesswrong folks who presented this did so as part of their mission of motivating policy makers to ban open access to models. Hate their ideology but love their research!<p>Edit: The data format is the same type used for DPO or RLHF style training. “Good” and “bad”, “harmful” vs “harmless”. What’s fun is to test the performance of this technique using your own datasets, to see how good the personalization is.</div><br/></div></div><div id="40666318" class="c"><input type="checkbox" id="c-40666318" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#40666019">prev</a><span>|</span><a href="#40666395">next</a><span>|</span><label class="collapse" for="c-40666318">[-]</label><label class="expand" for="c-40666318">[2 more]</label></div><br/><div class="children"><div class="content">The word is &#x27;ablation&#x27;. Do not butcher it</div><br/><div id="40666485" class="c"><input type="checkbox" id="c-40666485" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#40666318">parent</a><span>|</span><a href="#40666395">next</a><span>|</span><label class="collapse" for="c-40666485">[-]</label><label class="expand" for="c-40666485">[1 more]</label></div><br/><div class="children"><div class="content">Amusingly, they explicitly call it this in the article itself:<p>&gt; Once we have identified the refusal direction, we can &quot;ablate&quot; it, effectively removing the model&#x27;s ability to represent this feature</div><br/></div></div></div></div></div></div></div></div></div></body></html>