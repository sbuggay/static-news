<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1702458075824" as="style"/><link rel="stylesheet" href="styles.css?v=1702458075824"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.programmablemutter.com/p/the-singularity-is-nigh-republished">AI’s big rift is like a religious schism</a> <span class="domain">(<a href="https://www.programmablemutter.com">www.programmablemutter.com</a>)</span></div><div class="subtext"><span>anigbrowl</span> | <span>351 comments</span></div><br/><div><div id="38618248" class="c"><input type="checkbox" id="c-38618248" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38620494">next</a><span>|</span><label class="collapse" for="c-38618248">[-]</label><label class="expand" for="c-38618248">[77 more]</label></div><br/><div class="children"><div class="content">The reference to the origin of the concept of a singularity was better than most, but still misunderstood it:<p>&gt; In 1993 Vernor Vinge drew on computer science and his fellow science-fiction writers to argue that ordinary human history was drawing to a close. We would surely create superhuman intelligence sometime within the next three decades, leading to a “Singularity”, in which AI would start feeding on itself.<p>Yes it was Vernor, but he said something much more interesting: that as the speed of innovation itself sped up (the derivative of acceleration) the curve could bend up until it became essentially vertical, literally a singularity in the curve.  And then things on the other side of that singularity would be incomprehensible to those of us on our side of it. This is reflected in Peace and Fire upon the deep and other of his novels going back before the essay.<p>You can see in this idea is itself rooted in ideas from Alvin Toffler in the 70s (Future Shock) and Ray Lafferty in the 60s (e.g. Slow Tuesday Night).<p>So AI machines were just part of the enabling phenomena -- the most important, and yes the center of his &#x27;93 essay.  But the core of the metaphor was broader than that.<p>I&#x27;m a little disappointed that The Economist, of all publications, didn&#x27;t get ths quite right, but in their defense, it was a bit tangental to the point of the essay.</div><br/><div id="38619673" class="c"><input type="checkbox" id="c-38619673" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38624035">next</a><span>|</span><label class="collapse" for="c-38619673">[-]</label><label class="expand" for="c-38619673">[21 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s worth going back and reading Vinge&#x27;s &quot;The Coming Technological Singularity&quot; (<a href="https:&#x2F;&#x2F;edoras.sdsu.edu&#x2F;~vinge&#x2F;misc&#x2F;singularity.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;edoras.sdsu.edu&#x2F;~vinge&#x2F;misc&#x2F;singularity.html</a>) and then follow it up reading The Peace War, but most importantly its unappreciated detective novel sequel,  Marooned In Realtime, which explores some of the interesting implications about people who live right before the singularity.  I think this book is even better than Fire Upon the Deep.<p>When I read the Coming Technological Singularity back in the mid-90s it resonated with me and for a while I was a singularitarian- basically, dedicated to learning enough technology, and doing enough projects that I could help contribute to that singularity.  Nowadays I think that&#x27;s not the best way to spend my time, but it was interesting to meet Larry Page and see that he had concluded something familiar (for those not aware, Larry founded Google to provide a consistent revenue stream to carry out ML research to enable the singularity, and would be quite happy if robots replaced humans).<p>[ edit: I reread &quot;The Coming Technogical Singularity&quot;.  There&#x27;s an entire section at the bottom that pretty much covers the past 5 years of generative models as a form of intelligence augmentation, he was very prescient. ]</div><br/><div id="38623216" class="c"><input type="checkbox" id="c-38623216" checked=""/><div class="controls bullet"><span class="by">jomhna</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619673">parent</a><span>|</span><a href="#38620751">next</a><span>|</span><label class="collapse" for="c-38623216">[-]</label><label class="expand" for="c-38623216">[1 more]</label></div><br/><div class="children"><div class="content">Marooned in Realtime is incredible, one of the best sci-fi books I&#x27;ve ever read. The combination of the wildly imaginative SF elements with the detective novel structure grounding it works so incredibly well.</div><br/></div></div><div id="38620751" class="c"><input type="checkbox" id="c-38620751" checked=""/><div class="controls bullet"><span class="by">Guthur</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619673">parent</a><span>|</span><a href="#38623216">prev</a><span>|</span><a href="#38624035">next</a><span>|</span><label class="collapse" for="c-38620751">[-]</label><label class="expand" for="c-38620751">[19 more]</label></div><br/><div class="children"><div class="content">I yet ~30 years later we&#x27;re still predominantly hacking stuff together with python.</div><br/><div id="38620891" class="c"><input type="checkbox" id="c-38620891" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620751">parent</a><span>|</span><a href="#38622099">next</a><span>|</span><label class="collapse" for="c-38620891">[-]</label><label class="expand" for="c-38620891">[16 more]</label></div><br/><div class="children"><div class="content">I believe there&#x27;s an entire section in Deepness In The Sky about how future coders a million years from now are still hacking stuff together with python.</div><br/><div id="38621029" class="c"><input type="checkbox" id="c-38621029" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620891">parent</a><span>|</span><a href="#38621298">next</a><span>|</span><label class="collapse" for="c-38621029">[-]</label><label class="expand" for="c-38621029">[11 more]</label></div><br/><div class="children"><div class="content">There were programs here that had been written five thousand years ago, before Humankind ever left Earth. The wonder of it—the horror of it, Sura said—was that unlike the useless wrecks of Canberra’s past, these programs still worked! And via a million million circuitous threads of inheritance, many of the oldest programs still ran in the bowels of the Qeng Ho system. Take the Traders’ method of timekeeping. The frame corrections were incredibly complex—and down at the very bottom of it was a little program that ran a counter. Second by second, the Qeng Ho counted from the instant that a human had first set foot on Old Earth’s moon. But if you looked at it still more closely…the starting instant was actually about fifteen million seconds later, the 0-second of one of Humankind’s first computer operating systems.</div><br/><div id="38622125" class="c"><input type="checkbox" id="c-38622125" checked=""/><div class="controls bullet"><span class="by">kbutler</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621029">parent</a><span>|</span><a href="#38623237">next</a><span>|</span><label class="collapse" for="c-38622125">[-]</label><label class="expand" for="c-38622125">[4 more]</label></div><br/><div class="children"><div class="content">Love that reference to the Unix epoch, and the all-too-human misassociation with a seemingly more appropriate event.</div><br/><div id="38622503" class="c"><input type="checkbox" id="c-38622503" checked=""/><div class="controls bullet"><span class="by">ethbr1</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622125">parent</a><span>|</span><a href="#38623237">next</a><span>|</span><label class="collapse" for="c-38622503">[-]</label><label class="expand" for="c-38622503">[3 more]</label></div><br/><div class="children"><div class="content">Archaeology focuses on the things that were important to past humans.<p>It seems impossible that, given current economic trajectories, there won&#x27;t be software archaeology in the future.</div><br/><div id="38623312" class="c"><input type="checkbox" id="c-38623312" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622503">parent</a><span>|</span><a href="#38622665">next</a><span>|</span><label class="collapse" for="c-38623312">[-]</label><label class="expand" for="c-38623312">[1 more]</label></div><br/><div class="children"><div class="content">We already have software archaeology today. People who are into software preservation track down old media, original creators, and dig through clues to restore pixel-perfect copies of abandonware. It&#x27;s only going to get bigger &#x2F; more important with time if we don&#x27;t concentrate on open standards and open source everywhere.</div><br/></div></div><div id="38622665" class="c"><input type="checkbox" id="c-38622665" checked=""/><div class="controls bullet"><span class="by">ajb</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622503">parent</a><span>|</span><a href="#38623312">prev</a><span>|</span><a href="#38623237">next</a><span>|</span><label class="collapse" for="c-38622665">[-]</label><label class="expand" for="c-38622665">[1 more]</label></div><br/><div class="children"><div class="content">Seems quite possible that the the present will seem to future archeologists like an illiterate dark ages, between civilisations from which paper records, that last long enough for them to find, are preserved.</div><br/></div></div></div></div></div></div><div id="38623237" class="c"><input type="checkbox" id="c-38623237" checked=""/><div class="controls bullet"><span class="by">eli_gottlieb</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621029">parent</a><span>|</span><a href="#38622125">prev</a><span>|</span><a href="#38621298">next</a><span>|</span><label class="collapse" for="c-38623237">[-]</label><label class="expand" for="c-38623237">[6 more]</label></div><br/><div class="children"><div class="content">What a silly thing to write.  It&#x27;s not a very self-improving computer system if it doesn&#x27;t <i>simplify</i> itself, is it?</div><br/><div id="38623450" class="c"><input type="checkbox" id="c-38623450" checked=""/><div class="controls bullet"><span class="by">corysama</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38623237">parent</a><span>|</span><a href="#38623397">next</a><span>|</span><label class="collapse" for="c-38623450">[-]</label><label class="expand" for="c-38623450">[1 more]</label></div><br/><div class="children"><div class="content">How very human-centric of you ;)<p>It’s not hard to imagine a program of ever increasing complexity, highly redundant, error-prone, yet also stochastically, statically, increasingly effective.<p>The primary pushback from this would be our pathetically tiny short term memory. Blow that limitation up and complexity that seems incomprehensible to us becomes perfectly reasonable.</div><br/></div></div><div id="38623397" class="c"><input type="checkbox" id="c-38623397" checked=""/><div class="controls bullet"><span class="by">kbenson</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38623237">parent</a><span>|</span><a href="#38623450">prev</a><span>|</span><a href="#38623721">next</a><span>|</span><label class="collapse" for="c-38623397">[-]</label><label class="expand" for="c-38623397">[1 more]</label></div><br/><div class="children"><div class="content">If you need an arbitrary point of reference to count time from, what&#x27;s simpler, making a new one or using the existing predominant one that everything else does?<p>Making a competing standard and then having to deal with interop between standards is not simpler or better unless there&#x27;s an actual benefit to be had.</div><br/></div></div><div id="38623721" class="c"><input type="checkbox" id="c-38623721" checked=""/><div class="controls bullet"><span class="by">jbgt</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38623237">parent</a><span>|</span><a href="#38623397">prev</a><span>|</span><a href="#38623944">next</a><span>|</span><label class="collapse" for="c-38623721">[-]</label><label class="expand" for="c-38623721">[1 more]</label></div><br/><div class="children"><div class="content">Even smart people have an appendix!</div><br/></div></div><div id="38623944" class="c"><input type="checkbox" id="c-38623944" checked=""/><div class="controls bullet"><span class="by">Linosaurus</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38623237">parent</a><span>|</span><a href="#38623721">prev</a><span>|</span><a href="#38623288">next</a><span>|</span><label class="collapse" for="c-38623944">[-]</label><label class="expand" for="c-38623944">[1 more]</label></div><br/><div class="children"><div class="content">That’s because it’s <i>not</i> a self-improving computer system. It’s just programming as it exists today for thousands of years.</div><br/></div></div><div id="38623288" class="c"><input type="checkbox" id="c-38623288" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38623237">parent</a><span>|</span><a href="#38623944">prev</a><span>|</span><a href="#38621298">next</a><span>|</span><label class="collapse" for="c-38623288">[-]</label><label class="expand" for="c-38623288">[1 more]</label></div><br/><div class="children"><div class="content">It depends if there&#x27;s anything left to simplify. Maybe this is the optimum we&#x27;ll reach between concise expression and accuracy of representation.</div><br/></div></div></div></div></div></div><div id="38621298" class="c"><input type="checkbox" id="c-38621298" checked=""/><div class="controls bullet"><span class="by">brainbag</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620891">parent</a><span>|</span><a href="#38621029">prev</a><span>|</span><a href="#38621020">next</a><span>|</span><label class="collapse" for="c-38621298">[-]</label><label class="expand" for="c-38621298">[2 more]</label></div><br/><div class="children"><div class="content">I remember a sci-fi book where they were talking about one of the characters hacking on thousand-year-old code, but I could never remember what book it was from. Maybe this was it and it&#x27;s time for a reread.</div><br/><div id="38621605" class="c"><input type="checkbox" id="c-38621605" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621298">parent</a><span>|</span><a href="#38621020">next</a><span>|</span><label class="collapse" for="c-38621605">[-]</label><label class="expand" for="c-38621605">[1 more]</label></div><br/><div class="children"><div class="content">Continuing on from the sibling comment about the 0 second...<p>So behind all the top-level interfaces was layer under layer of support. Some of that software had been designed for wildly different situations. Every so often, the inconsistencies caused fatal accidents. Despite the romance of spaceflight, the most common accidents were simply caused by ancient, misused programs finally getting their revenge.<p>“We should rewrite it all,” said Pham.<p>“It’s been done,” said Sura, not looking up. She was preparing to go off-Watch, and had spent the last four days trying to root a problem out of the coldsleep automation.<p>“It’s been tried,” corrected Bret, just back from the freezers. “But even the top levels of fleet system code are enormous. You and a thousand of your friends would have to work for a century or so to reproduce it.” Trinli grinned evilly. “And guess what—even if you did, by the time you finished, you’d have your own set of inconsistencies. And you still wouldn’t be consistent with all the applications that might be needed now and then.”<p>Sura gave up on her debugging for the moment. “The word for all this is ‘mature programming environment.’ Basically, when hardware performance has been pushed to its final limit Basically, when hardware performance has been pushed to its final limit, and programmers have had several centuries to code, you reach a point where there is far more significant code than can be rationalized. The best you can do is understand the overall layering, and know how to search for the oddball tool that may come in handy—take the situation I have here.” She waved at the dependency chart she had been working on. “We are low on working fluid for the coffins. Like a million other things, there was none for sale on dear old Canberra. Well, the obvious thing is to move the coffins near the aft hull, and cool by direct radiation. We don’t have the proper equipment to support this—so lately, I’ve been doing my share of archeology. It seems that five hundred years ago, a similar thing happened after an in-system war at Torma. They hacked together a temperature maintenance package that is precisely what we need.”<p>“Almost precisely.” Bret was grinning again. “With some minor revisions.”<p>“Yes, which I’ve almost completed.”</div><br/></div></div></div></div><div id="38621020" class="c"><input type="checkbox" id="c-38621020" checked=""/><div class="controls bullet"><span class="by">Guthur</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620891">parent</a><span>|</span><a href="#38621298">prev</a><span>|</span><a href="#38622099">next</a><span>|</span><label class="collapse" for="c-38621020">[-]</label><label class="expand" for="c-38621020">[2 more]</label></div><br/><div class="children"><div class="content">We have better tools they&#x27;re just apparently too hard for us to use, yet some how in the same thought we think we can create anything remotely like intelligence, very odd cognitive dissonance.</div><br/><div id="38621744" class="c"><input type="checkbox" id="c-38621744" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621020">parent</a><span>|</span><a href="#38622099">next</a><span>|</span><label class="collapse" for="c-38621744">[-]</label><label class="expand" for="c-38621744">[1 more]</label></div><br/><div class="children"><div class="content">it does not seem odd to me at all that we could create intelligence, and even possibly loving grace, in a computer.<p>I&#x27;m not sure why there would be cognitive dissonance- sure, my tools may be primitive, but I can also grab my chisel and plane and see that it&#x27;s similar in form to chisel and plane from 2000 years ago (they look pretty much the same, but these days they&#x27;re made of stronger stuff).  I can easily imagine a Real Programmer 2000 years from now looking back and thinking that python, or even a vacuum tube, is merely a simplified version of their quantum matter assembler.</div><br/></div></div></div></div></div></div><div id="38622099" class="c"><input type="checkbox" id="c-38622099" checked=""/><div class="controls bullet"><span class="by">AlienRobot</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620751">parent</a><span>|</span><a href="#38620891">prev</a><span>|</span><a href="#38624035">next</a><span>|</span><label class="collapse" for="c-38622099">[-]</label><label class="expand" for="c-38622099">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;ll still be hacking stuff with python when the singularity comes. It will be ultra high tech alien stuff that we can&#x27;t hack or understand, only AI can, and the tech will look like magic to us, and most will not be able to resist the bait of depending upon this miraculous technology that we can&#x27;t understand or debug.</div><br/><div id="38624017" class="c"><input type="checkbox" id="c-38624017" checked=""/><div class="controls bullet"><span class="by">hnlmorg</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622099">parent</a><span>|</span><a href="#38624035">next</a><span>|</span><label class="collapse" for="c-38624017">[-]</label><label class="expand" for="c-38624017">[1 more]</label></div><br/><div class="children"><div class="content">We (as a species) already depend upon lots of miraculous technology that we (as individuals) cannot understand nor debug.<p>Even as IT professionals this is true. How many developers these days can debug a problem in their JavaScript runtime or Rust developers track down a bug in their CPU? There’s so much abstracted away from us that few people can fully grasp the entire stack their code executes. So those outside of tech don’t even stand a chance understanding computers.<p>And that’s just focusing on IT. I also depend on medical equipment operated by doctors but have no way of completely understanding that equipment nor procedure myself. I drive a car that I couldn’t repair. Watch TV that I didn’t produce beamed to me via satellites that I didnt built nor fire into space. Eat food that I didn’t grow.<p>We are already well past the point of understanding the technology behind the stuff that we depend upon daily.</div><br/></div></div></div></div></div></div></div></div><div id="38624035" class="c"><input type="checkbox" id="c-38624035" checked=""/><div class="controls bullet"><span class="by">mcv</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619673">prev</a><span>|</span><a href="#38619364">next</a><span>|</span><label class="collapse" for="c-38624035">[-]</label><label class="expand" for="c-38624035">[3 more]</label></div><br/><div class="children"><div class="content">The thing I never understood is: why would it go vertical? It would at best be an exponential curve, and I have doubts about that.<p>I admit looking at the 100 years before 1993, it looks like innovation is constantly speeding up, but even then there&#x27;s not going to be a moment that we suddenly have infinite knowledge. There&#x27;s no such thing as infinite knowledge; it&#x27;s still bound by physical limits. It still takes time and resources to actually do something with it.<p>And if you look at the past 30 years, it doesn&#x27;t really look like innovation is speeding up at all. There is plenty of innovation, but is it happening at an ever faster pace? I don&#x27;t see it. Not to mention that much of it is hype and fashion, and not really fundamentally new. Even AI progress is driven mostly by faster hardware and more data, and not really fundamentally new technologies.<p>And that&#x27;s not even getting into the science crisis: lots of science is not really reproducible. And while LLMs are certainly an exciting new technology, it&#x27;s not at all clear that they&#x27;re really more than a glorified autocorrect.<p>So I&#x27;m extremely skeptical about those singularity ideas. It&#x27;s an exciting SciFi idea, but I don&#x27;t think it&#x27;s true. And certainly not within the next 30 years.</div><br/><div id="38624080" class="c"><input type="checkbox" id="c-38624080" checked=""/><div class="controls bullet"><span class="by">bratbag</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38624035">parent</a><span>|</span><a href="#38624430">next</a><span>|</span><label class="collapse" for="c-38624080">[-]</label><label class="expand" for="c-38624080">[1 more]</label></div><br/><div class="children"><div class="content">Its infinite from the perspective of our side of the curve.<p>It&#x27;s another application of advanced technology appearing to be magic, but imagine transitioning into it in a matter of hours, then with that advanced tech transitioning further into damn-near godhood within minutes.<p>Then imagine what happens in the second after that.<p>It may be operating within the boundaries of physics, but they would be physical rules well beyond our understanding and may even be infinite by our own limited definition of physics.<p>That&#x27;s the curve.</div><br/></div></div><div id="38624430" class="c"><input type="checkbox" id="c-38624430" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38624035">parent</a><span>|</span><a href="#38624080">prev</a><span>|</span><a href="#38619364">next</a><span>|</span><label class="collapse" for="c-38624430">[-]</label><label class="expand" for="c-38624430">[1 more]</label></div><br/><div class="children"><div class="content">It wouldn’t. It would be a logistic curve. Pretty much everything people call exponential should actually be logistic</div><br/></div></div></div></div><div id="38619364" class="c"><input type="checkbox" id="c-38619364" checked=""/><div class="controls bullet"><span class="by">ghaff</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38624035">prev</a><span>|</span><a href="#38619416">next</a><span>|</span><label class="collapse" for="c-38619364">[-]</label><label class="expand" for="c-38619364">[3 more]</label></div><br/><div class="children"><div class="content">A related concept comes from social progression by historical measures. Based on pretty much any metrics, <i>Why the West Rules for Now</i> shows that the industrial revolution essentially went vertical and that prior measures--including the rise of the Roman Empire and its fall--were essentially insignificant.</div><br/><div id="38619955" class="c"><input type="checkbox" id="c-38619955" checked=""/><div class="controls bullet"><span class="by">WillAdams</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619364">parent</a><span>|</span><a href="#38619416">next</a><span>|</span><label class="collapse" for="c-38619955">[-]</label><label class="expand" for="c-38619955">[2 more]</label></div><br/><div class="children"><div class="content">“Whatever happens, we have got
The Maxim gun, and they have not.”
― Hilaire Belloc</div><br/><div id="38620278" class="c"><input type="checkbox" id="c-38620278" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619955">parent</a><span>|</span><a href="#38619416">next</a><span>|</span><label class="collapse" for="c-38620278">[-]</label><label class="expand" for="c-38620278">[1 more]</label></div><br/><div class="children"><div class="content">Apposite.</div><br/></div></div></div></div></div></div><div id="38619416" class="c"><input type="checkbox" id="c-38619416" checked=""/><div class="controls bullet"><span class="by">stvltvs</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619364">prev</a><span>|</span><a href="#38619347">next</a><span>|</span><label class="collapse" for="c-38619416">[-]</label><label class="expand" for="c-38619416">[15 more]</label></div><br/><div class="children"><div class="content">&gt; derivative of acceleration<p>Was this intended literally? I&#x27;m skeptical that saying something so precise about a fuzzy metric like rate of innovation is warranted.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jerk_(physics)" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jerk_(physics)</a></div><br/><div id="38619567" class="c"><input type="checkbox" id="c-38619567" checked=""/><div class="controls bullet"><span class="by">dougmwne</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619416">parent</a><span>|</span><a href="#38621546">next</a><span>|</span><label class="collapse" for="c-38619567">[-]</label><label class="expand" for="c-38619567">[11 more]</label></div><br/><div class="children"><div class="content">I believe the point being made is that the rate of innovation over time would turn asymptotic as the acceleration increased, creating a point in time of infinite progress. On one side would be human history as we know and on the other, every innovation possible would happen all in a moment. The prediction was specifically that we were going to infinity in less than infinite time.</div><br/><div id="38619805" class="c"><input type="checkbox" id="c-38619805" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619567">parent</a><span>|</span><a href="#38621546">next</a><span>|</span><label class="collapse" for="c-38619805">[-]</label><label class="expand" for="c-38619805">[10 more]</label></div><br/><div class="children"><div class="content">You only reach a vertical asymptote if every derivative up to the infinite order is increasing. That means acceleration, jerk, snap, crackle, pop, etc. are all increasing.<p>The physical world tends to have certain constraints that make such true singularities impossible. For example, the universal speed limit: c. But, you could argues that we could approximate a singularity well enough to fool us humans.</div><br/><div id="38621504" class="c"><input type="checkbox" id="c-38621504" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619805">parent</a><span>|</span><a href="#38619994">next</a><span>|</span><label class="collapse" for="c-38621504">[-]</label><label class="expand" for="c-38621504">[3 more]</label></div><br/><div class="children"><div class="content">OK, most of us still here at this point probably have a handle on how derivatives work.  Your jerk, snap (orders I will guess) etc probably map nicely to a famous American politician&#x27;s speech about the economy at the time.<p>It was something like the &quot;speed of increase in inflation is slowing down&quot;  I&#x27;ve tried to search for it but no joy.<p>Anyway, it was maths.</div><br/><div id="38622315" class="c"><input type="checkbox" id="c-38622315" checked=""/><div class="controls bullet"><span class="by">jdhwosnhw</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621504">parent</a><span>|</span><a href="#38621856">next</a><span>|</span><label class="collapse" for="c-38622315">[-]</label><label class="expand" for="c-38622315">[1 more]</label></div><br/><div class="children"><div class="content">It was Nixon 
<a href="https:&#x2F;&#x2F;www.ams.org&#x2F;notices&#x2F;199610&#x2F;page2.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.ams.org&#x2F;notices&#x2F;199610&#x2F;page2.pdf</a></div><br/></div></div><div id="38621856" class="c"><input type="checkbox" id="c-38621856" checked=""/><div class="controls bullet"><span class="by">BoiledCabbage</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621504">parent</a><span>|</span><a href="#38622315">prev</a><span>|</span><a href="#38619994">next</a><span>|</span><label class="collapse" for="c-38621856">[-]</label><label class="expand" for="c-38621856">[1 more]</label></div><br/><div class="children"><div class="content">Nixon and inflation I believe: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Third_derivative#Economic_example" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Third_derivative#Economic_exam...</a></div><br/></div></div></div></div><div id="38619994" class="c"><input type="checkbox" id="c-38619994" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619805">parent</a><span>|</span><a href="#38621504">prev</a><span>|</span><a href="#38621546">next</a><span>|</span><label class="collapse" for="c-38619994">[-]</label><label class="expand" for="c-38619994">[6 more]</label></div><br/><div class="children"><div class="content">They’re impossible with our current technology and understanding of physics. The idea is what’s beyond that. Access to a realm outside of time is just the sort of thing that could cause a singularity.</div><br/><div id="38620152" class="c"><input type="checkbox" id="c-38620152" checked=""/><div class="controls bullet"><span class="by">Retric</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619994">parent</a><span>|</span><a href="#38620223">next</a><span>|</span><label class="collapse" for="c-38620152">[-]</label><label class="expand" for="c-38620152">[3 more]</label></div><br/><div class="children"><div class="content">Saying impossible under our current understanding of physics is a massive understatement.  It’s would require not just new physics but a very very specific kind of universe with zero evidence in support of it.<p>Suggesting the singularity is physically possible is roughly like suggesting that human telekinesis is possible, ie physics would need many very interesting errors.</div><br/><div id="38623869" class="c"><input type="checkbox" id="c-38623869" checked=""/><div class="controls bullet"><span class="by">losteric</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620152">parent</a><span>|</span><a href="#38621879">next</a><span>|</span><label class="collapse" for="c-38623869">[-]</label><label class="expand" for="c-38623869">[1 more]</label></div><br/><div class="children"><div class="content">This rhetoric around singularities is a little funny, because the singularity of blackholes (going to infinite density) is recognized to indicate knowledge gaps around how blackholes work... lines going to infinity is not necessarily taken literally. Same goes for the technology singularity.<p>Anyway, imo the event horizon is more interesting. That&#x27;s where the paradigm shift happens - there is no way to escape whatever &quot;comes next&quot;.<p>(Note - some people confusingly use &quot;singularity&quot; to refer to the phase between the event horizon and &quot;infinity&quot;)</div><br/></div></div><div id="38621879" class="c"><input type="checkbox" id="c-38621879" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620152">parent</a><span>|</span><a href="#38623869">prev</a><span>|</span><a href="#38620223">next</a><span>|</span><label class="collapse" for="c-38621879">[-]</label><label class="expand" for="c-38621879">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it’s an understatement. I mean, I literally called it impossible. And at all times our understanding of physics is our current understanding. I don’t mean to downplay it at all. It would be an overhaul of understanding of the universe bigger than all changes in the past put together. It would blow general relativity out of the water.</div><br/></div></div></div></div><div id="38620223" class="c"><input type="checkbox" id="c-38620223" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619994">parent</a><span>|</span><a href="#38620152">prev</a><span>|</span><a href="#38621546">next</a><span>|</span><label class="collapse" for="c-38620223">[-]</label><label class="expand" for="c-38620223">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s beyond that is science fiction.<p>If there is anything &quot;beyond&quot; that at all, I think it&#x27;s pretty safe to say that any idea we have as to what it would be is very unlikely to be anywhere near the realm of correct.</div><br/><div id="38621873" class="c"><input type="checkbox" id="c-38621873" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38620223">parent</a><span>|</span><a href="#38621546">next</a><span>|</span><label class="collapse" for="c-38621873">[-]</label><label class="expand" for="c-38621873">[1 more]</label></div><br/><div class="children"><div class="content">Yes, literally science fiction. That’s where this topic comes from.<p>&gt; In 1993 Vernor Vinge drew on computer science and his fellow science-fiction writers…<p>But I don’t think it’s worth completely writing off as if we couldn’t possibly know anything. Progress outside of time would resolve the issue I resolved to. I’m not saying it could actually be done or doesn’t cause a million other issues. It’s an idea for a solution, but it’s completely unrealistic, so you shelve it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38621546" class="c"><input type="checkbox" id="c-38621546" checked=""/><div class="controls bullet"><span class="by">MobileVet</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619416">parent</a><span>|</span><a href="#38619567">prev</a><span>|</span><a href="#38619347">next</a><span>|</span><label class="collapse" for="c-38621546">[-]</label><label class="expand" for="c-38621546">[3 more]</label></div><br/><div class="children"><div class="content">I remember learning about ‘jerk’ in undergrad and my still jr high brain thinking, ‘haha, no way that is what it is called.’<p>The more I thought about it though, the more I realized it was the perfect name. It is definitely what you feel when the acceleration changes!</div><br/><div id="38621798" class="c"><input type="checkbox" id="c-38621798" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621546">parent</a><span>|</span><a href="#38623488">next</a><span>|</span><label class="collapse" for="c-38621798">[-]</label><label class="expand" for="c-38621798">[1 more]</label></div><br/><div class="children"><div class="content">Same goes for productivity and jerks. Quickly speeding up or slowing down literally requires a jerk, and many wont like it regardless how you do it. You can go fast without jerks, but you can&#x27;t react fast without jerks.</div><br/></div></div><div id="38623488" class="c"><input type="checkbox" id="c-38623488" checked=""/><div class="controls bullet"><span class="by">kristiandupont</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621546">parent</a><span>|</span><a href="#38621798">prev</a><span>|</span><a href="#38619347">next</a><span>|</span><label class="collapse" for="c-38623488">[-]</label><label class="expand" for="c-38623488">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Jerk&quot; makes perfect sense to me too. What I never got was how &quot;pop&quot; could come after &quot;crackle&quot;.</div><br/></div></div></div></div></div></div><div id="38619347" class="c"><input type="checkbox" id="c-38619347" checked=""/><div class="controls bullet"><span class="by">galangalalgol</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619416">prev</a><span>|</span><a href="#38619702">next</a><span>|</span><label class="collapse" for="c-38619347">[-]</label><label class="expand" for="c-38619347">[3 more]</label></div><br/><div class="children"><div class="content">Rainbows End is another good one where he explores the earlier part of the curve, the elbow perhaps. Some of that stuff is already happening and that book isn&#x27;t so old.</div><br/><div id="38620936" class="c"><input type="checkbox" id="c-38620936" checked=""/><div class="controls bullet"><span class="by">mercutio2</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619347">parent</a><span>|</span><a href="#38623208">next</a><span>|</span><label class="collapse" for="c-38620936">[-]</label><label class="expand" for="c-38620936">[1 more]</label></div><br/><div class="children"><div class="content">Rainbow’s End was by far the best guess at what near future ubiquitous computing would look like than anyone else’s for decades.<p>It got so many things right, it’s really amazing.<p>I really wish he’d written more.</div><br/></div></div><div id="38623208" class="c"><input type="checkbox" id="c-38623208" checked=""/><div class="controls bullet"><span class="by">SubiculumCode</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619347">parent</a><span>|</span><a href="#38620936">prev</a><span>|</span><a href="#38619702">next</a><span>|</span><label class="collapse" for="c-38623208">[-]</label><label class="expand" for="c-38623208">[1 more]</label></div><br/><div class="children"><div class="content">Its one of my favorite sci fi that none of my friends have read.</div><br/></div></div></div></div><div id="38619702" class="c"><input type="checkbox" id="c-38619702" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619347">prev</a><span>|</span><a href="#38619497">next</a><span>|</span><label class="collapse" for="c-38619702">[-]</label><label class="expand" for="c-38619702">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m a little disappointed that The Economist, of all publications, didn&#x27;t get ths quite right<p>It&#x27;s a guest essay. The Economist does not edit guest essays. They routinely publish guest essays from unabashed propagandists as well.</div><br/></div></div><div id="38619497" class="c"><input type="checkbox" id="c-38619497" checked=""/><div class="controls bullet"><span class="by">JumpCrisscross</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619702">prev</a><span>|</span><a href="#38621200">next</a><span>|</span><label class="collapse" for="c-38619497">[-]</label><label class="expand" for="c-38619497">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>In 1993 Vernor Vinge drew on computer science and his fellow science-fiction writers to argue that ordinary human history was drawing to a close</i><p>Note that this category of hypothesis was common in various disciplines at the end of the Cold War [1]. (Vinge&#x27;s being unique because the precipice lies ahead, not behind.)<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_End_of_History_and_the_Last_Man" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_End_of_History_and_the_Las...</a></div><br/></div></div><div id="38621200" class="c"><input type="checkbox" id="c-38621200" checked=""/><div class="controls bullet"><span class="by">hyperthesis</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619497">prev</a><span>|</span><a href="#38623331">next</a><span>|</span><label class="collapse" for="c-38621200">[-]</label><label class="expand" for="c-38621200">[3 more]</label></div><br/><div class="children"><div class="content">Not literally a singularity, just incomprehensible from the past. Arguably all points on a technological exponential have this property.</div><br/><div id="38621811" class="c"><input type="checkbox" id="c-38621811" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621200">parent</a><span>|</span><a href="#38623331">next</a><span>|</span><label class="collapse" for="c-38621811">[-]</label><label class="expand" for="c-38621811">[2 more]</label></div><br/><div class="children"><div class="content">Yes! Someone elsewhere mentions the engineers of the first printing press trying to imagine the future of literature (someone adds: and&#x2F;or advertising). On the other side of that invention taking off.<p>My go-to example is that we have run into similar things with the pace and volume of &quot;Science&quot;. For a long while one could be a scientific gentleman and keep up with the sciences. As a whole. Then quite suddenly, on the other side, you couldn&#x27;t and you had to settle for one field. And then it happened again: people noticed that you can&#x27;t master one field even, on the current side. And you have to become a hyper-specialist to master your niche. You can still be a generalist - in order to attack specific questions - but you better have contacts who are hyper-specialists for what you really need.</div><br/><div id="38623519" class="c"><input type="checkbox" id="c-38623519" checked=""/><div class="controls bullet"><span class="by">kristiandupont</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621811">parent</a><span>|</span><a href="#38623331">next</a><span>|</span><label class="collapse" for="c-38623519">[-]</label><label class="expand" for="c-38623519">[1 more]</label></div><br/><div class="children"><div class="content">I feel like I experienced this up close in IT. When I was a kid in the 90&#x27;s, I was the &quot;computer guy&quot; on the street and people would ask me whenever anything was wrong with their computer. If any piece of software or hardware wasn&#x27;t working, there was a good chance I could help and I loved it.<p>Today, I am trying (but struggling!) to keep up with the evolving libraries and frameworks for frontend development in Typescript!</div><br/></div></div></div></div></div></div><div id="38623331" class="c"><input type="checkbox" id="c-38623331" checked=""/><div class="controls bullet"><span class="by">aktuel</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38621200">prev</a><span>|</span><a href="#38622943">next</a><span>|</span><label class="collapse" for="c-38623331">[-]</label><label class="expand" for="c-38623331">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like exponential growth, whether that&#x27;s in a petri dish or on earth. It looks like that until it doesn&#x27;t. Singularities don&#x27;t happen in the real world. Never have and never will. If someone tells you something about a singularity, that&#x27;s a pretty perfect indicator that there&#x27;s still some more understanding to be done.</div><br/></div></div><div id="38622943" class="c"><input type="checkbox" id="c-38622943" checked=""/><div class="controls bullet"><span class="by">WalterBright</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38623331">prev</a><span>|</span><a href="#38618907">next</a><span>|</span><label class="collapse" for="c-38622943">[-]</label><label class="expand" for="c-38622943">[1 more]</label></div><br/><div class="children"><div class="content">The idea was present in &quot;Colossus The Forbin Project&quot; 1970. The computer starts out fairly crude, but learns at an exponential rate. It designs extensions to itself to further accelerate it.</div><br/></div></div><div id="38618907" class="c"><input type="checkbox" id="c-38618907" checked=""/><div class="controls bullet"><span class="by">elteto</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38622943">prev</a><span>|</span><a href="#38622281">next</a><span>|</span><label class="collapse" for="c-38618907">[-]</label><label class="expand" for="c-38618907">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for this great explanation of where &quot;singularity&quot; comes from in this context. Always wondered.</div><br/></div></div><div id="38619766" class="c"><input type="checkbox" id="c-38619766" checked=""/><div class="controls bullet"><span class="by">leereeves</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38622281">prev</a><span>|</span><a href="#38621792">next</a><span>|</span><label class="collapse" for="c-38619766">[-]</label><label class="expand" for="c-38619766">[8 more]</label></div><br/><div class="children"><div class="content">&gt; Vernor...said something much more interesting: that as the speed of innovation itself sped up (the derivative of acceleration) the curve could bend up until it became essentially vertical, literally a singularity in the curve.<p>In other words, Vernor described an exponential curve. But are there any exponential curves in reality? AFAIK they always hit resource limits where growth stops. That is, anything that looks like an exponential curve eventually becomes an S-shaped curve.</div><br/><div id="38621105" class="c"><input type="checkbox" id="c-38621105" checked=""/><div class="controls bullet"><span class="by">shagie</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619766">parent</a><span>|</span><a href="#38620922">next</a><span>|</span><label class="collapse" for="c-38621105">[-]</label><label class="expand" for="c-38621105">[2 more]</label></div><br/><div class="children"><div class="content">I tried using AI. It scared me. - Tom Scott <a href="https:&#x2F;&#x2F;youtu.be&#x2F;jPhJbKBuNnA" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;jPhJbKBuNnA</a><p>I&#x27;m also gonna recommend Accelerando - <a href="https:&#x2F;&#x2F;www.antipope.org&#x2F;charlie&#x2F;blog-static&#x2F;fiction&#x2F;accelerando&#x2F;accelerando-intro.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.antipope.org&#x2F;charlie&#x2F;blog-static&#x2F;fiction&#x2F;acceler...</a><p>As an aside, I&#x27;d also recommend Glasshouse (also by Charles Stross) as an exploration into the human remnants post singularity (and war)... followed by Implied Spaces by Walter Jon Williams...<p>&gt; “I and my confederates,” Aristide said, “did our best to prevent that degree of autonomy among artificial intelligences. We made the decision to turn away from the Vingean Singularity before most people even knew what it was. But—” He made a gesture with his hands as if dropping a ball. “—I claim no more than the average share of wisdom. We could have made mistakes.”<p>for a singularity averted approach of what could be done.<p>One more I&#x27;ll toss in, is The Freeze-Frame Revolution by Peter Watts which feels like you&#x27;re missing a lot of the story (but it is because that&#x27;s one book of a series) and... well... spoilers.</div><br/><div id="38623410" class="c"><input type="checkbox" id="c-38623410" checked=""/><div class="controls bullet"><span class="by">wavemode</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621105">parent</a><span>|</span><a href="#38620922">next</a><span>|</span><label class="collapse" for="c-38623410">[-]</label><label class="expand" for="c-38623410">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve linked to a collection of writers panicking about technology they hardly understand, and are using that as evidence of... what, exactly?</div><br/></div></div></div></div><div id="38620922" class="c"><input type="checkbox" id="c-38620922" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619766">parent</a><span>|</span><a href="#38621105">prev</a><span>|</span><a href="#38621354">next</a><span>|</span><label class="collapse" for="c-38620922">[-]</label><label class="expand" for="c-38620922">[1 more]</label></div><br/><div class="children"><div class="content">To the extent that a normal human mind can see beyond the singularity, one imagines that we would experience an exponential growth but not even be able to comprehend the later flattening of that exponential into a sigmoid (since nearly all the exponentials we see are sigmoids in disguise.</div><br/></div></div><div id="38621354" class="c"><input type="checkbox" id="c-38621354" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619766">parent</a><span>|</span><a href="#38620922">prev</a><span>|</span><a href="#38620028">next</a><span>|</span><label class="collapse" for="c-38621354">[-]</label><label class="expand" for="c-38621354">[1 more]</label></div><br/><div class="children"><div class="content">I totally agree, but want to add two observations:<p>1. Humanity has already been on the path of exponential growth. Take GDP for example. We measure GDP growth by percentages, and that&#x27;s exponential. (Of course, real GDP growth has stagnated for a bit, but at least for the past ~3 centuries it has been generally exponential AFAIK).  Not saying it can be sustained, just that we&#x27;ve been quite exponential for a while.<p>2. Not every function is linear. Sometimes the exponentially increased inputs will produce a linear output. I&#x27;d argue R&amp;D is kind of like that. When the lower hanging fruits are already taken, you&#x27;d need to expend even more effort into achieving the next breakthrough. So despite the &quot;exponential&quot; increase in productivity, the result could feel very linear.<p>I would also like to add that physical and computational limits make the whole singularity thing literally impossible. 3D space means that even theoretically sound speedups (eg. binary trees) are impossible in scale because you can&#x27;t assume O(1) lookups - the best you can get is O(n^1&#x2F;3).  Maybe people understand the singularity concept poetically, I don&#x27;t know.</div><br/></div></div><div id="38620028" class="c"><input type="checkbox" id="c-38620028" checked=""/><div class="controls bullet"><span class="by">mjcohen</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619766">parent</a><span>|</span><a href="#38621354">prev</a><span>|</span><a href="#38623646">next</a><span>|</span><label class="collapse" for="c-38620028">[-]</label><label class="expand" for="c-38620028">[1 more]</label></div><br/><div class="children"><div class="content">An exponential curve is not a singularity; 1&#x2F;(x-a) is as x goes to a.</div><br/></div></div><div id="38623646" class="c"><input type="checkbox" id="c-38623646" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619766">parent</a><span>|</span><a href="#38620028">prev</a><span>|</span><a href="#38621337">next</a><span>|</span><label class="collapse" for="c-38623646">[-]</label><label class="expand" for="c-38623646">[1 more]</label></div><br/><div class="children"><div class="content">I agree, but predicting the peak of an S curve isn’t easy either, as we saw during the pandemic.<p>My conclusion is similar to Verge’s: predicting the far future is impossible. We can imagine a variety of scenarios, but you shouldn’t place much faith in them.<p>Predicting even a couple years in advance looks pretty hard. Consider the next presidential election: the most boring scenario is Biden vs. Trump and Biden wins. Wildcard scenarios: either one of them, or both, dies before election day. Who can rule that out?<p>Also consider that in any given year, there could be another pandemic.<p>History is largely a sequence of surprise events.</div><br/></div></div><div id="38621337" class="c"><input type="checkbox" id="c-38621337" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38619766">parent</a><span>|</span><a href="#38623646">prev</a><span>|</span><a href="#38621792">next</a><span>|</span><label class="collapse" for="c-38621337">[-]</label><label class="expand" for="c-38621337">[1 more]</label></div><br/><div class="children"><div class="content">Sure, that may be. But you still have to ride the first half of the curve, before it inflects. I&#x27;d rather make sure the ride isn&#x27;t too bumpy.</div><br/></div></div></div></div><div id="38621792" class="c"><input type="checkbox" id="c-38621792" checked=""/><div class="controls bullet"><span class="by">haolez</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38619766">prev</a><span>|</span><a href="#38621887">next</a><span>|</span><label class="collapse" for="c-38621792">[-]</label><label class="expand" for="c-38621792">[10 more]</label></div><br/><div class="children"><div class="content">What does it mean &quot;to be on the other side&quot; of this singularity in your graphic representation? I failed to grasp this.</div><br/><div id="38622340" class="c"><input type="checkbox" id="c-38622340" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621792">parent</a><span>|</span><a href="#38622997">next</a><span>|</span><label class="collapse" for="c-38622340">[-]</label><label class="expand" for="c-38622340">[7 more]</label></div><br/><div class="children"><div class="content">Consider my 86 yo mother: extremely intelligent and competent, a physician.  She struggled conceptually with her iPhone because she was used to reading the manual for a device and learning all its behavior and affordances.  Even though she has a laptop she runs the same set of programs on it.  But the phone is protean and she struggles with its shapeshifting.<p>It’s intuitive and simple to you and me.  But languages change, slang changes, metaphors change and equipment changes.  Business models exist today that were unthinkable 40 years ago because the ubiquity of computation did not exist.<p>She’s suffering, in Toffler’s words, a “future shock”.<p>Now imagine that another 40 years worth of innovation happens in a decade.  And then again in the subsequent five.  And faster.  You’ll have a hard time keeping up.  And not just you: kids will too.  Things become incomprehensible without machines doing most of the work — including explanation.  Eventually you, or your kids, won’t even understand what’s going on 90% of the time…then less and less.<p>I sometimes like to muse on what a Victorian person would make of today if transported through time.  Or someone from 16th century Europe.  Or Archimedes. They’d mostly understand a lot, I think.  But lately I’ve started to think of someone from the 1950s.  They might even find today <i>harder</i> to understand than the others would.<p>That crossover point is when the world becomes incomprehensible in a flash.  That’s a mathematical singularity (metaphorically speaking).</div><br/><div id="38623505" class="c"><input type="checkbox" id="c-38623505" checked=""/><div class="controls bullet"><span class="by">aenvoker</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622340">parent</a><span>|</span><a href="#38623130">next</a><span>|</span><label class="collapse" for="c-38623505">[-]</label><label class="expand" for="c-38623505">[1 more]</label></div><br/><div class="children"><div class="content">I saw a fun tweet recently claiming the main reason we don’t have an AI revolution today is inertia. Corporate structures exist primarily to protect existing jobs. Everyone wants someone to make everything better, but Don’t Change Anything! Because change might hit me in my deeply invested sunk costs.<p>The claim might be a year or two premature. But, not five.</div><br/></div></div><div id="38623130" class="c"><input type="checkbox" id="c-38623130" checked=""/><div class="controls bullet"><span class="by">puchatek</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622340">parent</a><span>|</span><a href="#38623505">prev</a><span>|</span><a href="#38623857">next</a><span>|</span><label class="collapse" for="c-38623130">[-]</label><label class="expand" for="c-38623130">[1 more]</label></div><br/><div class="children"><div class="content">Depends on what you&#x27;re trying to understand. They might not get <i>how</i> but it would be clear that the <i>what</i> hasn&#x27;t changed yet. It&#x27;s still about money, power, mating rights etc. Even in the face of our own demise. 
If all this tech somehow managed to change the <i>what</i> then we might become truly incomprehensible to previous generations.</div><br/></div></div><div id="38623857" class="c"><input type="checkbox" id="c-38623857" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622340">parent</a><span>|</span><a href="#38623130">prev</a><span>|</span><a href="#38623109">next</a><span>|</span><label class="collapse" for="c-38623857">[-]</label><label class="expand" for="c-38623857">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Things become incomprehensible without machines doing most of the work — including explanation. Eventually you, or your kids, won’t even understand what’s going on 90% of the time…then less and less.<p>This also reminds me of the Eloi in the Time Machine, a book written in 1895!</div><br/></div></div><div id="38623109" class="c"><input type="checkbox" id="c-38623109" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622340">parent</a><span>|</span><a href="#38623857">prev</a><span>|</span><a href="#38623255">next</a><span>|</span><label class="collapse" for="c-38623109">[-]</label><label class="expand" for="c-38623109">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I sometimes like to muse on what a Victorian person would make of today if transported through time. Or someone from 16th century Europe. Or Archimedes. They’d mostly understand a lot, I think. But lately I’ve started to think of someone from the 1950s. They might even find today harder to understand than the others would.<p>why do you think someone from the 50s would find it harder to understand our time, than someone from an earlier age, even as far back as 2000 years?</div><br/></div></div><div id="38623255" class="c"><input type="checkbox" id="c-38623255" checked=""/><div class="controls bullet"><span class="by">darkerside</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622340">parent</a><span>|</span><a href="#38623109">prev</a><span>|</span><a href="#38623251">next</a><span>|</span><label class="collapse" for="c-38623255">[-]</label><label class="expand" for="c-38623255">[1 more]</label></div><br/><div class="children"><div class="content">The world is already incomprehensible on some level. At this point, it&#x27;s just a question of how much incomprehensibility we are willing to accept. We should bear in mind that disaster recoverability will suffer as that metric rises.<p>Based on my understanding of people, we probably have a long way to go.</div><br/></div></div><div id="38623251" class="c"><input type="checkbox" id="c-38623251" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38622340">parent</a><span>|</span><a href="#38623255">prev</a><span>|</span><a href="#38622997">next</a><span>|</span><label class="collapse" for="c-38623251">[-]</label><label class="expand" for="c-38623251">[1 more]</label></div><br/><div class="children"><div class="content">IMHO that&#x27;s both correct, but also subtly the wrong way to think about the singularity.<p>Yes, there&#x27;s an ever faster pace of change, but that pace of change for the general human population is limited precisely because of the factors you laid out: people can&#x27;t keep up.<p>William Gibson said: <i>&quot;The future is already here – it&#x27;s just not very evenly distributed.&quot;</i><p>That&#x27;s what has been happening for millennia, and will be ever more obvious as the pace of progress accelerates: sure, someone, <i>somewhere</i> will have developed incomprehensible ultra-advanced technology, but it won&#x27;t <i>spread</i> precisely because the general population can&#x27;t comprehend&#x2F;adopt&#x2F;adapt it fast enough![1]<p>Exceptions exist, of course.<p>My take on the whole thing is that the &quot;runaway singularity&quot; won&#x27;t happen globally, it&#x27;ll happen very locally. Some AI supercomputer cluster will figure out &quot;everything&quot;, have access to a nano-fabrication device, build itself a new substrate, transfer, repeat, and then zip off to silicon nirvana in a matter of hours...<p>... leaving us behind, just like we&#x27;ve left the primitive uncontacted tribes in the Amazon behind. They don&#x27;t know <i>or care</i> about the latest ChatGPT features, iPhone computational photography, or Tesla robotics. They&#x27;re stuck where they are precisely because they too far removed and so can&#x27;t keep up.<p>[1] Here&#x27;s my equivalent example to your iPhone example: 4K HDR videos. I can create these, but I can&#x27;t send them to any of my relatives that would like to see them, because nobody has purchased HDR TVs yet, and they&#x27;re also all using pre-HDR mobile phones. Display panel tech has been advancing fantastically fast, but adoption isn&#x27;t.</div><br/></div></div></div></div><div id="38622997" class="c"><input type="checkbox" id="c-38622997" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621792">parent</a><span>|</span><a href="#38622340">prev</a><span>|</span><a href="#38623113">next</a><span>|</span><label class="collapse" for="c-38622997">[-]</label><label class="expand" for="c-38622997">[1 more]</label></div><br/><div class="children"><div class="content">The graph has innovation&#x2F;machine intelligence on the y-axis and time on the x axis. The &quot;other side&quot; of the singularity is anything that comes after the vertical increase.</div><br/></div></div><div id="38623113" class="c"><input type="checkbox" id="c-38623113" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#38618248">root</a><span>|</span><a href="#38621792">parent</a><span>|</span><a href="#38622997">prev</a><span>|</span><a href="#38621887">next</a><span>|</span><label class="collapse" for="c-38623113">[-]</label><label class="expand" for="c-38623113">[1 more]</label></div><br/><div class="children"><div class="content">To be alive when x &gt; x_{singularity}.</div><br/></div></div></div></div><div id="38621887" class="c"><input type="checkbox" id="c-38621887" checked=""/><div class="controls bullet"><span class="by">gausswho</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38621792">prev</a><span>|</span><a href="#38621646">next</a><span>|</span><label class="collapse" for="c-38621887">[-]</label><label class="expand" for="c-38621887">[1 more]</label></div><br/><div class="children"><div class="content">899788888883 j 88998 .99
99
9999
8⁹iic8988
8i
f8ii89
iii$898 8f88
.8d<p>$
9
88xo<p>9
v999
ii
7 8899
of
i8888iy
i99io
o9
⁹99898
o
⁹88o98ici9
i8o
i
88i8
f9
f</div><br/></div></div><div id="38621646" class="c"><input type="checkbox" id="c-38621646" checked=""/><div class="controls bullet"><span class="by">kazinator</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38621887">prev</a><span>|</span><a href="#38622967">next</a><span>|</span><label class="collapse" for="c-38621646">[-]</label><label class="expand" for="c-38621646">[1 more]</label></div><br/><div class="children"><div class="content">In their defense, they spent 3 minutes googling the origin of the term, and don&#x27;t know anything about the book.</div><br/></div></div><div id="38622967" class="c"><input type="checkbox" id="c-38622967" checked=""/><div class="controls bullet"><span class="by">mc32</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38621646">prev</a><span>|</span><a href="#38619334">next</a><span>|</span><label class="collapse" for="c-38622967">[-]</label><label class="expand" for="c-38622967">[1 more]</label></div><br/><div class="children"><div class="content">They say Von Neumann talked about a tech singularity back in the late &#x27;50s (attested by Ulam), Vinge popularized it in the mid 80s and Kurzweil took it over with his book in the aughts.</div><br/></div></div><div id="38619334" class="c"><input type="checkbox" id="c-38619334" checked=""/><div class="controls bullet"><span class="by">gardenhedge</span><span>|</span><a href="#38618248">parent</a><span>|</span><a href="#38622967">prev</a><span>|</span><a href="#38620494">next</a><span>|</span><label class="collapse" for="c-38619334">[-]</label><label class="expand" for="c-38619334">[1 more]</label></div><br/><div class="children"><div class="content">TIL, thanks</div><br/></div></div></div></div><div id="38620494" class="c"><input type="checkbox" id="c-38620494" checked=""/><div class="controls bullet"><span class="by">bloppe</span><span>|</span><a href="#38618248">prev</a><span>|</span><a href="#38622608">next</a><span>|</span><label class="collapse" for="c-38620494">[-]</label><label class="expand" for="c-38620494">[42 more]</label></div><br/><div class="children"><div class="content">It seems like the root cause of this runaway AI pathology has to do mainly with the over-anthropomorphization of AI. The umwelt of an LLM is so far removed from that of any living organism so as to be fundamentally irreconcilable with our understanding of agency, desire, intention, survival, etc. Our current, rudimentary AI inhabits a world so far removed from our own that the thought of it &quot;unshackling&quot; itself from our controls seems ludicrous to me.<p>AI does not scare me. People wielding AI as a tool for their own endeavors certainly does.</div><br/><div id="38621114" class="c"><input type="checkbox" id="c-38621114" checked=""/><div class="controls bullet"><span class="by">sensanaty</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38623828">next</a><span>|</span><label class="collapse" for="c-38621114">[-]</label><label class="expand" for="c-38621114">[17 more]</label></div><br/><div class="children"><div class="content">Agreed, oftentimes the truly zealous AI pundits act as if our modern day LLMs are completely, 100% equivalent to humans, which I find utterly insane as a concep. For example any discussion about copyrighted works, which is a hot topic, will inevitably end up with someone equating an LLM &quot;learning&quot; to a human learning, as if the two are identical.<p>I think human languages and psyches just aren&#x27;t built to cope with the concept of AI. Many words have loose meanings, like &quot;learning&quot; in the case of AIs, that can easily be twisted to mean one of dozens of definitions, depending on the stance of the person talking about it. It&#x27;ll be interesting as the technology becomes more prevalent and mundane how people start treating it all. I&#x27;m hoping we get to realizing that a computer isn&#x27;t a human regardless of the eloquence of its &quot;speech&quot; or whatever words we use to describe what it does, but I guess we&#x27;ll see</div><br/><div id="38623461" class="c"><input type="checkbox" id="c-38623461" checked=""/><div class="controls bullet"><span class="by">kbenson</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621114">parent</a><span>|</span><a href="#38622065">next</a><span>|</span><label class="collapse" for="c-38623461">[-]</label><label class="expand" for="c-38623461">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m hoping we get to realizing that a computer isn&#x27;t a human regardless of the eloquence of its &quot;speech&quot; or whatever words we use to describe what it does, but I guess we&#x27;ll see<p>My anecdotal experience with how people treat Alexa devices does not inspire confidence in me with regards to this. I can&#x27;t even convince my wife not to gender it when referring to it.</div><br/><div id="38624147" class="c"><input type="checkbox" id="c-38624147" checked=""/><div class="controls bullet"><span class="by">kombookcha</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623461">parent</a><span>|</span><a href="#38624060">next</a><span>|</span><label class="collapse" for="c-38624147">[-]</label><label class="expand" for="c-38624147">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is really down to people understanding what a computer is, but more down to how humans interact with nonhumans. We anthropomorphize animals and objects all the time. A computer program is ultimately a (complicated) object, that we often give all sorts of human trappings, like a human voice that expresses things in human languages.<p>If you can take pity on the final, dented avocado at the shops because it looks &quot;sad&quot;, you will for sure end up calling Alexa &#x27;she&#x27;. Avocados can&#x27;t be sad, but they can look sad &#x2F;to humans&#x2F;, and a machine can&#x27;t really have a gender or be polite in the human sense, but it can definitely sound like a polite lady.<p>I think humans will just fundamentally relate to anything they perceive socially as another human, even if we know full well they aren&#x27;t human. Probably it&#x27;s a lot less work for a human brain, than it is to try to engage with the true essence of being an avocado or an Alexa.</div><br/></div></div><div id="38624060" class="c"><input type="checkbox" id="c-38624060" checked=""/><div class="controls bullet"><span class="by">maebert</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623461">parent</a><span>|</span><a href="#38624147">prev</a><span>|</span><a href="#38622065">next</a><span>|</span><label class="collapse" for="c-38624060">[-]</label><label class="expand" for="c-38624060">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, anthropomorphizing is kind of a built in feature for us — or more accurately, ascribing intentionality to things as a means of explanation. Magnets “want” to stick together, my vintage computer “thinks” it’s 1993, and the furious gods are hurling burning rocks from the skies because my neighbor ate leavened bread on the wrong day.<p>It’s not limited to AI or LLMs - theory of mind is a powerful explanatory tool that helps us navigate a complex world, and we misapply it all the time.</div><br/></div></div></div></div><div id="38622065" class="c"><input type="checkbox" id="c-38622065" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621114">parent</a><span>|</span><a href="#38623461">prev</a><span>|</span><a href="#38623828">next</a><span>|</span><label class="collapse" for="c-38622065">[-]</label><label class="expand" for="c-38622065">[13 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve reached the inevitable part of the conversation! What distinction are you drawing between what an LLM does and what a human does? Because as far as I can see they are identical.<p>A human artist looks at a lot of different sources, builds up a black-box statistical model of how to create from that and can reproduce other styles on demand based on a few samples. Generative AI follows the same process. What distinction do you want to draw to say that they should be treated differently legally? And why would that even be desirable?</div><br/><div id="38622760" class="c"><input type="checkbox" id="c-38622760" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622065">parent</a><span>|</span><a href="#38624024">next</a><span>|</span><label class="collapse" for="c-38622760">[-]</label><label class="expand" for="c-38622760">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure the artist is conscious and the AI isn&#x27;t which means there&#x27;s something reductive about your claim that they are both merely &quot;applying a black-box statistical&quot; model.<p>Even if it&#x27;s true (which is debatable,) it doesn&#x27;t appear to be more informative than saying  &quot;they are both made of atoms.&quot;</div><br/><div id="38622896" class="c"><input type="checkbox" id="c-38622896" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622760">parent</a><span>|</span><a href="#38624024">next</a><span>|</span><label class="collapse" for="c-38622896">[-]</label><label class="expand" for="c-38622896">[6 more]</label></div><br/><div class="children"><div class="content">Well, my personal opinion is with the rise of neural nets we&#x27;ve basically proven that &quot;consciousness&quot; is an illusion and there is nothing there to find. But for the sake of argument, lets assume that there is something called consciousness, artists have it and neural nets don&#x27;t.<p>How are you going to demonstrate that consciousness is responsible for what the artists are doing? We have undeniable proof that the art could be created by a statistical model, there is solid evidence that the brain creates art by simulating a mathematical neural network to achieve creative outcomes - the brain is full of relatively simple neurons linking together in a way that is logically similar to the way we&#x27;re encoding information into these matrices.<p>So it is quite reasonable to believe that the artists are conscious but suspect that consciousness isn&#x27;t involved in the process of creating a copyrighted work. How does that get dealt with?</div><br/><div id="38623114" class="c"><input type="checkbox" id="c-38623114" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622896">parent</a><span>|</span><a href="#38624183">next</a><span>|</span><label class="collapse" for="c-38623114">[-]</label><label class="expand" for="c-38623114">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll talk about fiction, since I&#x27;ve written some. If I write a ghost story it&#x27;s because I enjoy ghost stories and want to take a crack at my own.  While I don&#x27;t know why ideas pop into my head, I do know that I pick the ones that are subjectively fun or to my taste.  And if I do a clever job or a bad job I have a sense of it when I reread what I wrote.<p>These AI&#x27;s aren&#x27;t doing anything like that.  They have no preference or intent. Their choices change depending on setting like temperature or prompts.<p>Or let&#x27;s try a different example.  Stephen King wrote a novel where he imagined the protagonist gets killed and eaten by the villain&#x27;s pet pig (Misery).  He struggled to come up with a different ending because he said nobody wants to read a whole novel just to see the main character die in the end.  He thought about it and did a different ending.<p>Are you claiming Stephen King&#x27;s conscious deliberation wasn&#x27;t part of his writing process?  I&#x27;d say it clearly was.<p>Also, I don&#x27;t really understand the consciousness is an illusion argument.  If none of us are conscious, why should I justify any copyright policy preference to you?  That would be like justifying a copyright policy preference to a doorknob. But somehow I&#x27;m also a doorknob in this analogy???<p>Suppose Bob says he&#x27;s conscious and Jim says he isn&#x27;t and we believe them.  Doesn&#x27;t that suggest we would have different policy preferences on how they are treated?  It would appear murdering Jim wouldn&#x27;t be particularly harmful but murdering Bob would.  I don&#x27;t have to show how Jim and Bob&#x27;s mind differ to prefer policies that benefit Bob over Jim.</div><br/><div id="38623285" class="c"><input type="checkbox" id="c-38623285" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623114">parent</a><span>|</span><a href="#38624183">next</a><span>|</span><label class="collapse" for="c-38623285">[-]</label><label class="expand" for="c-38623285">[3 more]</label></div><br/><div class="children"><div class="content">&gt; ...[w]hile I don&#x27;t know why ideas pop into my head...<p>If you&#x27;re trying to argue that you&#x27;re doing something different from statistical sampling, not knowing how you&#x27;re doing it isn&#x27;t a very strong place to argue from. What you&#x27;re experiencing is probably what it feels like for a sack of meat to take a statistical sample from an internal black-box model. Biologically that seems to be what is happening.<p>I have also done a fair amount of writing. The writing comes from a completely different part of my mind than the part that experiences the world. I see no reason to believe it is linked to consciousness, even allowing that consciousness does exist which is questionable in itself.<p>It is an unreasonable position to say that you don&#x27;t know the process but it must be different from a known process that you also don&#x27;t have experience using.<p>&gt; Are you claiming Stephen King&#x27;s conscious deliberation wasn&#x27;t part of his writing process? I&#x27;d say it clearly was.<p>Unless you&#x27;re claiming to have a psychic connection to Stephen King&#x27;s consciousness, this is a remarkably weak claim. You have no idea how he was writing. Maybe he&#x27;s even a philosophical zombie. Thanks to the rise of LLMs we know that philosophical zombies can write well.<p>And &quot;clearly&quot; is not so - I could spit out a lost Stephen King work in this comment that, depending on how good ChatGPT is these days, would be passable. It isn&#x27;t obvious that it is the work of a conscious mind. It in fact would obviously be from a statistical model.<p>&gt; If none of us are conscious, why should I justify any copyright policy preference to you?<p>I&#x27;ve been against copyright for more than a decade now. You tell me why it is justified even if consciousness is a factor. The edifice of copyright is culturally and economically destructive and also has been artistically devastating (there haven&#x27;t been anywhere near as many great works in the last 50 years as there should have been in a culturally dynamic society).</div><br/><div id="38623651" class="c"><input type="checkbox" id="c-38623651" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623285">parent</a><span>|</span><a href="#38624183">next</a><span>|</span><label class="collapse" for="c-38623651">[-]</label><label class="expand" for="c-38623651">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m referring to how Stephen King discusses his writing process in On Writing.  I doubt you actually believe Stephen King might be a p zombie and I&#x27;m skeptical you really think consciousness is an illusion.  I think if i chained you to a bed and sawed off your leg (like what happens to the protagonist in Misery) you would insist you were a conscious actor who would prefer to not suffer. I don&#x27;t even know what consciousness is an illusion is supposed to mean.<p>If I sawed off your leg would it have the moral consideration of removing the leg of a barbie doll&#x27;s leg if you feel your consciousness is an illusion?<p>When I write a story my brain might be doing something you could refer to as a black box calculation if you squint a little, but how is it &quot;statistics?&quot;  When I feel the desire to urinate, or post comments on hacker news, or admire a rainbow, or sleep am I also &quot;doing statistics?&quot;<p>You seem to be referring to what people traditionally call &quot;thinking&quot; or &quot;cognition&quot; and rebranding it as &quot;statistics&quot; in search of some rhetorical point.<p>My point is human beings have things called &quot;personalities&quot; and &quot;preferences&quot; that inform their decision makings, including what to write.  In what sense is that &quot;statistics&quot;?<p>The idea that the human subconscious is not consciously accessible is not a new idea. Freud had a few things to say about that.  I don&#x27;t think it tells us much about AI. I do think my subconscious ideas are informed by my consious preferences.  If I hate puns I&#x27;m not going to imagine story jdeas involving puns, for example.<p>Most authors would prefer copyright exists because they&#x27;d prefer book publishers, bookstore retailers and the like pay them royalties instead of selling the books they made without paying them.  It&#x27;s pretty simple conceptually, at least with traditional books.<p>Copyright existed far longer than the last 50 years so how is our 50 years of culture relevant? The U.S. has had copyright since 1790.</div><br/><div id="38624267" class="c"><input type="checkbox" id="c-38624267" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623651">parent</a><span>|</span><a href="#38624183">next</a><span>|</span><label class="collapse" for="c-38624267">[-]</label><label class="expand" for="c-38624267">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I doubt you actually believe Stephen King might be a p zombie and I&#x27;m skeptical you really think consciousness is an illusion.<p>Consciousness is an unobservable, undefinable thing which with LLMs in the mix we can theorise has no impact on reality; since we can reproduce all the important parts with matrices and a few basic functions. You can doubt facts all you want, but that is a pretty ironclad position as far as logic, evidence and rationality goes. Consciousnesses is going the way of the dodo in terms of importance.<p>&gt; If I sawed off your leg would it have the moral consideration of removing the leg of a barbie doll&#x27;s leg if you feel your consciousness is an illusion?<p>For sake of argument, lets say conclusive proof arises that Stephan King is a philosophical zombie. Do you believe that suddenly you can murder him? No; that&#x27;d be stupid and immoral. Morality isn&#x27;t predicated on consciousness. I&#x27;m perfectly happy to argue about morality but consciousness isn&#x27;t a thing that makes sense outside of talking about someone being knocked unconscious as a descriptive state.<p>&gt; When I feel the desire to urinate, or post comments on hacker news, or admire a rainbow, or sleep am I also &quot;doing statistics?&quot;<p>No, you&#x27;re responding to stimulus. But right now it looks extremely likely that the creative process is driven by statistics as has been revealed by the latest and greatest in AI. Unless you can think of a different mechanism - I&#x27;m happy to be surprised by other ideas at the moment it is the only serious explanation I know of.<p>&gt; You seem to be referring to what people traditionally call &quot;thinking&quot; or &quot;cognition&quot; and rebranding it as &quot;statistics&quot; in search of some rhetorical point.<p>I don&#x27;t think I&#x27;ve said anything about thinking or cognition. Although statistics will crack those too, but I&#x27;m expecting them to be more stateful processes than the current generation of AI techniques.<p>&gt; Copyright existed far longer than the last 50 years so how is our 50 years of culture relevant? The U.S. has had copyright since 1790.<p>Yeah but the law has been continuously strengthened since then and as it&#x27;s scope increases the damage gets worse. The last 50 years are where new works are effectively not going to enter the public domain before everyone who was around when they were created is dead.</div><br/></div></div></div></div></div></div></div></div><div id="38624183" class="c"><input type="checkbox" id="c-38624183" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622896">parent</a><span>|</span><a href="#38623114">prev</a><span>|</span><a href="#38624024">next</a><span>|</span><label class="collapse" for="c-38624183">[-]</label><label class="expand" for="c-38624183">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Well, my personal opinion is with the rise of neural nets we&#x27;ve basically proven that &quot;consciousness&quot; is an illusion and there is nothing there to find.<p>I keep seeing this claim being made but I never understand what people mean by it. Do you mean that the colors we see, the sounds we hear, the tastes, smells, feels, emotions, dreams, inner dialog are all illusions? Isn&#x27;t an illusion an experience? You&#x27;re saying that experience itself is an illusion and there is nothing to experience.<p>I can&#x27;t make sense of that. At any rate, I see no reason to suppose LLMs have experiences. They don&#x27;t have bodies, so what would they be experiencing? When you say an LLM is identical to a person, I can&#x27;t make good sense of that either. There&#x27;s a thousand things people do that language models don&#x27;t. Just the simple fact that I have to eat on a regular basis to survive is meaningful in a way that it can&#x27;t be for a language model.<p>If an LLM generates text about preparing a certain meal because it&#x27;s hungry, I know that&#x27;s not true in a way it can be true of a human. So right away, there&#x27;s reasons we say things that go beyond the statistical black box reasoning of an LLM. They don&#x27;t have any bodies to attend to.</div><br/></div></div></div></div></div></div><div id="38624024" class="c"><input type="checkbox" id="c-38624024" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622065">parent</a><span>|</span><a href="#38622760">prev</a><span>|</span><a href="#38624016">next</a><span>|</span><label class="collapse" for="c-38624024">[-]</label><label class="expand" for="c-38624024">[1 more]</label></div><br/><div class="children"><div class="content">&gt; A human artist looks at a lot of different sources, builds up a black-box statistical model of how to create from that and can reproduce other styles on demand based on a few samples. Generative AI follows the same process.<p>The reason programs are different in front of the law is that humans can program programs to do whatever they like at scale, humans can&#x27;t program humans to do whatever they like at scale.<p>So for example if it became legal to use images from an AI, then we would program an AI that basically copies images and it would be legal, because it is an AI. But at that point we know the law has been violated because programs that just copies images aren&#x27;t a legal way to get copyrighted images for free.<p>You saying &quot;But the AI is a black box&quot; just means that you could have hidden anything in there, it doesn&#x27;t prove anything. Legally it is the same as if you wrote a program to copy images.</div><br/></div></div><div id="38624016" class="c"><input type="checkbox" id="c-38624016" checked=""/><div class="controls bullet"><span class="by">woopsn</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622065">parent</a><span>|</span><a href="#38624024">prev</a><span>|</span><a href="#38622124">next</a><span>|</span><label class="collapse" for="c-38624016">[-]</label><label class="expand" for="c-38624016">[1 more]</label></div><br/><div class="children"><div class="content">Did you write this reply because it was the most typical way that the thread up until GP could have continued?<p>I don&#x27;t mean that to sound flippant -- if this wasn&#x27;t your intent, then obviously you understand the distinction.</div><br/></div></div><div id="38622124" class="c"><input type="checkbox" id="c-38622124" checked=""/><div class="controls bullet"><span class="by">voltaireodactyl</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622065">parent</a><span>|</span><a href="#38624016">prev</a><span>|</span><a href="#38623274">next</a><span>|</span><label class="collapse" for="c-38622124">[-]</label><label class="expand" for="c-38622124">[2 more]</label></div><br/><div class="children"><div class="content">One distinction would be that every source a human looked at involved a payment for access to&#x2F;a copy of each and every source inputted into their black-box.<p>Which is not the case with the current AI models (or rather, the companies profiting off them), to my understanding.</div><br/><div id="38622854" class="c"><input type="checkbox" id="c-38622854" checked=""/><div class="controls bullet"><span class="by">inimino</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622124">parent</a><span>|</span><a href="#38623274">next</a><span>|</span><label class="collapse" for="c-38622854">[-]</label><label class="expand" for="c-38622854">[1 more]</label></div><br/><div class="children"><div class="content">You mean all the landscape artists and portrait and figure and still life painters throughout all human history who just casually made art of whatever they saw around them?</div><br/></div></div></div></div><div id="38623274" class="c"><input type="checkbox" id="c-38623274" checked=""/><div class="controls bullet"><span class="by">darkerside</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38622065">parent</a><span>|</span><a href="#38622124">prev</a><span>|</span><a href="#38623828">next</a><span>|</span><label class="collapse" for="c-38623274">[-]</label><label class="expand" for="c-38623274">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s this kind of thinking that is the only reason machines would ever become the primary life form on earth.</div><br/></div></div></div></div></div></div><div id="38623828" class="c"><input type="checkbox" id="c-38623828" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38621114">prev</a><span>|</span><a href="#38620597">next</a><span>|</span><label class="collapse" for="c-38623828">[-]</label><label class="expand" for="c-38623828">[3 more]</label></div><br/><div class="children"><div class="content">Exactly. It always boils down to people wielding tools and possibly weaponizing them. As we can see in the Ukraine, conventional war without a lot of high tech weaponry is perfectly horrible just by itself. All it takes is some determined humans to get that.<p>I look at AI as a tool that people can and will wield. And from that point of view, non proliferation as a strategy is not really all that feasible considering that AI labs outside of the valley in e.g. China and other countries are already producing their own versions of the technology. That cat is already out of the bag. We can all collectively stick our heads in the ground and hope none of those countries will have the indecency to wield the tools they are building or we can try to make sure we keep on leading the field. I&#x27;m in in camp full steam ahead. Eventually we reach some tipping point where the tools are going to be instrumental in making further progress.<p>People are worried about something bad happening if people do embrace AI. I worry about what happens if some people don&#x27;t. There is no we here. Just groups of people. And somebody will make progress. Not a question of if but when.</div><br/><div id="38624530" class="c"><input type="checkbox" id="c-38624530" checked=""/><div class="controls bullet"><span class="by">throwawayqqq11</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623828">parent</a><span>|</span><a href="#38624128">next</a><span>|</span><label class="collapse" for="c-38624530">[-]</label><label class="expand" for="c-38624530">[1 more]</label></div><br/><div class="children"><div class="content">Try to take AI like a genetically modified organism and im sure your &quot;full steam ahead&quot; notion fades.<p>But you are right, at the beginning is a human, making a decision to let go. And with pretty much any technology, we had unforseen consequences. Now combine that with magical&#x2F;god like capabilties.<p>Dont get me wrong im pro GMOs like im pro AI. Im just humble enough to appreciate my limited intellect.</div><br/></div></div><div id="38624128" class="c"><input type="checkbox" id="c-38624128" checked=""/><div class="controls bullet"><span class="by">omnicognate</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38623828">parent</a><span>|</span><a href="#38624530">prev</a><span>|</span><a href="#38620597">next</a><span>|</span><label class="collapse" for="c-38624128">[-]</label><label class="expand" for="c-38624128">[1 more]</label></div><br/><div class="children"><div class="content">Off-Topic:<p>It&#x27;s debatable whether for most English speakers the &quot;The&quot; in &quot;The Ukraine&quot; really carries the implications discussed in [1], but nonetheless it&#x27;s a linguistic tic that should probably be dispensed with.<p><a href="https:&#x2F;&#x2F;theconversation.com&#x2F;its-ukraine-not-the-ukraine-heres-why-178748" rel="nofollow noreferrer">https:&#x2F;&#x2F;theconversation.com&#x2F;its-ukraine-not-the-ukraine-here...</a></div><br/></div></div></div></div><div id="38620597" class="c"><input type="checkbox" id="c-38620597" checked=""/><div class="controls bullet"><span class="by">nmilo</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38623828">prev</a><span>|</span><a href="#38620683">next</a><span>|</span><label class="collapse" for="c-38620597">[-]</label><label class="expand" for="c-38620597">[9 more]</label></div><br/><div class="children"><div class="content">This to me is the real problem, &quot;AI safety&quot; can mean about a million things but it&#x27;s always just whatever is most convenient for the speaker. I&#x27;m convinced human language&#x2F;English is not enough to discuss AI problems, the words are way too loaded with anthropomorphized meanings and cultural meanings to discuss the topic in a rational way at all. The words are just too easy to twist.</div><br/><div id="38621024" class="c"><input type="checkbox" id="c-38621024" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620597">parent</a><span>|</span><a href="#38620680">next</a><span>|</span><label class="collapse" for="c-38621024">[-]</label><label class="expand" for="c-38621024">[3 more]</label></div><br/><div class="children"><div class="content">One of the students in a course I’m teaching on language and AI (mentioned in another comment here) wrote something similar in a homework assignment the other day. We had discussed the paper “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness” [1]. The student wrote:<p>“One of the questions that I have been wondering about is whether there have been any discussions exploring the creation of a distinct category, akin to but different from consciousness, that better captures the potential for AI to be sentient. While ‘consciousness’ is a familiar term applicable, to some extent, beyond the human brain, given the associated difficulties, it might be sensible to establish a separate definition to distinguish these two categories.”<p>Probably new terms should be coined not only for AI “consciousness” but for other aspects of what they are and do as well.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08708" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08708</a></div><br/><div id="38621123" class="c"><input type="checkbox" id="c-38621123" checked=""/><div class="controls bullet"><span class="by">nmilo</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621024">parent</a><span>|</span><a href="#38620680">next</a><span>|</span><label class="collapse" for="c-38621123">[-]</label><label class="expand" for="c-38621123">[2 more]</label></div><br/><div class="children"><div class="content">Agreed. Even &quot;AI&quot; is a bit loaded; we had half a century of Terminator, Asimov, 2001, etc. We should call them by implementation; neural networks, transformers, language models, etc. The plus side is that it&#x27;s harder to generalize statements about all three---there is very little to generalize in the first place---and that asking &quot;is a transformer conscious&quot; at least sounds more ridiculous than asking &quot;is AI conscious.&quot;</div><br/><div id="38621934" class="c"><input type="checkbox" id="c-38621934" checked=""/><div class="controls bullet"><span class="by">BoiledCabbage</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621123">parent</a><span>|</span><a href="#38620680">next</a><span>|</span><label class="collapse" for="c-38621934">[-]</label><label class="expand" for="c-38621934">[1 more]</label></div><br/><div class="children"><div class="content">But that misses the point, via the same old argument &quot;Does a submarine swim&quot;? Or the now less loaded &quot;Does a plane fly&quot;.<p>Neither generalizes very well w.r.t. how a fish or a bird does it. Nothing really generalizes between the natural version and the mechanical one.<p>But it&#x27;s also clear that it&#x27;s irrelevant. Trying to define useful behavior by categorizing implementation, will only give you categories of implementation. It&#x27;s clear by behavior that what a submarine does and what a bird does are both for all intents and purposes meeting the definition of what we&#x27;d want something to do in order to fly or swim, even though they is no commonality between them and the version in nature.<p>So by your definition finding it ridiculous to ask &quot;is a transformer conscious&quot; is about as ridiculous as asking the very specific &quot;does a plan flap it&#x27;s wings&quot;? The point is not to define it by it&#x27;s implementation, but by what behaviors it exhibits.<p>If in the future there is no way for me the exterior to distinguish a next gen LLM from a human, animal or other conscious being it becomes irrelevant if it&#x27;s implemented with transformers. However, while I won&#x27;t go down that path here, the argument goes even deeper as technically we would expect differences so even if it&#x27;s not identical to a human it still can be conscious. Just like a dog is conscious but behaves differently than a person does.</div><br/></div></div></div></div></div></div><div id="38620680" class="c"><input type="checkbox" id="c-38620680" checked=""/><div class="controls bullet"><span class="by">HPMOR</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620597">parent</a><span>|</span><a href="#38621024">prev</a><span>|</span><a href="#38621987">next</a><span>|</span><label class="collapse" for="c-38620680">[-]</label><label class="expand" for="c-38620680">[4 more]</label></div><br/><div class="children"><div class="content">Linguistics experts will have a fun time untangling the pre-AI language from the post-AI era.</div><br/><div id="38620924" class="c"><input type="checkbox" id="c-38620924" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620680">parent</a><span>|</span><a href="#38621987">next</a><span>|</span><label class="collapse" for="c-38620924">[-]</label><label class="expand" for="c-38620924">[3 more]</label></div><br/><div class="children"><div class="content">As it happens, I’m now teaching an undergraduate course at the University of Tokyo titled “The Meaning of Language in the Age of AI.” The students—who come from various countries and majors—and I are discussing how theories of human language will have to change now that humans interact not only with each other but also with AI.<p>I don’t think we’ll have any full-fledged theories ready by the end of the semester, but some points to consider are emerging. One is that aspects of language use that are affected by the social status, personal identities, and consciousness of the speakers and listeners—politeness, formality, forms of address, pronoun reference, etc.—will have to be rethought now that people are increasingly conversing with entities that have no social status, personal identity, or consciousness (not yet, at least).<p>And, yes, thinking about all this is a lot of fun.</div><br/><div id="38621456" class="c"><input type="checkbox" id="c-38621456" checked=""/><div class="controls bullet"><span class="by">palata</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620924">parent</a><span>|</span><a href="#38621987">next</a><span>|</span><label class="collapse" for="c-38621456">[-]</label><label class="expand" for="c-38621456">[2 more]</label></div><br/><div class="children"><div class="content">Not sure if I am completely off-topic here, but I was recently reading this essay by Bruce Schneier: <a href="https:&#x2F;&#x2F;www.schneier.com&#x2F;blog&#x2F;archives&#x2F;2023&#x2F;12&#x2F;ai-and-trust.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.schneier.com&#x2F;blog&#x2F;archives&#x2F;2023&#x2F;12&#x2F;ai-and-trust....</a>, where he mentions a related risk: because we now can &quot;talk&quot; to those LLMs in a similar way that we talk to people, it makes it harder for us to realize that they are not people. And so it&#x27;s easier to abuse our trust.<p>I guess my question here would be: will human language have to change because of interactions with LLMs, or is the whole point of LLMs that it does not have to change (and therefore humans will have to learn not to be abused by the machines)? Because we already have languages to talk to machines: those are programming languages, which are designed to be unambiguous. The problem I see is not that we don&#x27;t know how to talk to machines, but rather that we now have machines that are really good at pretending they are not machines.<p>Not sure if I am making any sense at all :-).</div><br/><div id="38621635" class="c"><input type="checkbox" id="c-38621635" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621456">parent</a><span>|</span><a href="#38621987">next</a><span>|</span><label class="collapse" for="c-38621635">[-]</label><label class="expand" for="c-38621635">[1 more]</label></div><br/><div class="children"><div class="content">You are making a lot of sense to me. Thanks for the link to that essay, too.<p>The point about programming-language nonambiguity is a good one. After I started using ChatGPT a year ago, it took me a while to realize that I didn’t have to be careful about my spelling, capitalization, punctuation, etc. It turned out to be good at interpreting the intention of sloppily written prompts. And it never pointed out or complained about my mistakes, either—another difference from humans.</div><br/></div></div></div></div></div></div></div></div><div id="38621987" class="c"><input type="checkbox" id="c-38621987" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620597">parent</a><span>|</span><a href="#38620680">prev</a><span>|</span><a href="#38620683">next</a><span>|</span><label class="collapse" for="c-38621987">[-]</label><label class="expand" for="c-38621987">[1 more]</label></div><br/><div class="children"><div class="content">If english is good enough to talk about what genes &#x27;want&#x27; then it&#x27;s good enough to talk about what AI &#x27;wants&#x27;.</div><br/></div></div></div></div><div id="38620683" class="c"><input type="checkbox" id="c-38620683" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38620597">prev</a><span>|</span><a href="#38621054">next</a><span>|</span><label class="collapse" for="c-38620683">[-]</label><label class="expand" for="c-38620683">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It seems like the root cause of this runaway AI pathology has to do mainly with the over-anthropomorphization of AI.<p>It’s an unsurprising form of paredolia, one not unique to those devout who feel they are distinguished from “lower” forms of life.</div><br/><div id="38621503" class="c"><input type="checkbox" id="c-38621503" checked=""/><div class="controls bullet"><span class="by">alan-crowe</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620683">parent</a><span>|</span><a href="#38621054">next</a><span>|</span><label class="collapse" for="c-38621503">[-]</label><label class="expand" for="c-38621503">[1 more]</label></div><br/><div class="children"><div class="content">We can dig into the nature of the pareidolia.<p>The basic technique for coping with life&#x27;s problems is to copy the answer from somebody more intelligent. I&#x27;m an ordinary person, facing a problem in domain D; I spot a clever person and copy their opinion on domain D. Err, that doesn&#x27;t really work. Even clever people have weaknesses. The person that I&#x27;m copying from might be clever on their specialty, domain E, but ordinary on domain D. I gain nothing from copying them.<p>One way round this problem is to pay close attention to track records. Look to see how well the clever person&#x27;s earlier decisions on domain D have turned out. If they are getting &quot;clever person&quot; results in domain D, copy. If they are merely getting &quot;ordinary person&quot; results in domain D, don&#x27;t bother. But track records are rare, so this approach is rarely applicable.<p>A fix for the rarity problem is to accept track records in other domains. The idea is to spot a very clever person by them getting &quot;very clever person&quot; results in domain F. That is not domain D, so there is a logical weakness to copying their opinion on domain D, they might be merely ordinary, or even stupid on that domain. Fortunately human intelligence is usually more uniform than that. Getting &quot;very clever person&quot; results on domain F doesn&#x27;t guarantee &quot;very clever person&quot; results on domain D. But among humans general intelligence is kind of a thing. Expecting that they get &quot;clever person&quot; results (open rank down) on domain D is a good bet, and it is reasonable to copy their opinion on domain D, even in the absence of a domain specific track record.<p>We <i>instinctively</i> watch out for people whose track record proves that they are &quot;very clever&quot;, and copy them on other matters, hoping to get &quot;clever person&quot; results.<p>Artificial intelligence builds towers of super human intellectual performance in an empty waste land. Most Artificial Intelligences are not even stupid away from their special domain, they don&#x27;t work outside of it at all. Even relatively general intelligences, like Chat GPT, have bizarre holes in their intelligence. Large Language Models don&#x27;t know that there is an external world to which language refers and about which language can be right or wrong. Instead they say the kinds of things that humans say, with zero awareness of the risks.<p>And us humans? We cannot help seeing the super human intellectual performance of Alpha Go, beating the World Champion at Go, as some kind of general intellectual validation. It is our human instinct to use a &quot;spot intelligence and copy from it&quot; strategy, even, perhaps especially, outside of track record. That is the specific nature of the pareidolia that we need to worry about. It is our nature to treat intelligences as inherently fairly general. Very clever on one thing implies clever on most other things. We are cursed to believe in the intelligence of our computer companions. This will end bad, as we give them serious responsibilities, and they fail, displaying incomprehensible stupidity.</div><br/></div></div></div></div><div id="38621054" class="c"><input type="checkbox" id="c-38621054" checked=""/><div class="controls bullet"><span class="by">sadtoot</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38620683">prev</a><span>|</span><a href="#38620992">next</a><span>|</span><label class="collapse" for="c-38621054">[-]</label><label class="expand" for="c-38621054">[5 more]</label></div><br/><div class="children"><div class="content">do you think vinge and kurzweil in the 90s and 2000s were imagining the singularity occuring exactly at the advent of LLMs?  are you supposing that LLMs are the only viable path towards advanced AI, and that we have now hit a permanent ceiling for AI?<p>AI doesn&#x27;t scare you because you apparently have no sense of perspective or imagination</div><br/><div id="38621166" class="c"><input type="checkbox" id="c-38621166" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621054">parent</a><span>|</span><a href="#38621073">next</a><span>|</span><label class="collapse" for="c-38621166">[-]</label><label class="expand" for="c-38621166">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; AI doesn&#x27;t scare you because you apparently have no sense of perspective or imagination</i><p>We can imagine both of the following: 1. space aliens from Betelgeuse coming down and enslaving humanity to work in the dilithium mines to produce the fuel for their hyperdrives, and 2. the end of civilization via global nuclear war. Both of these would be pretty bad, but only one is worth worrying about. I don&#x27;t worry about Roko&#x27;s Basilisk, I worry about AI becoming the ultimate tool of Big Brother, because the latter is realistic and the former is pure fantasy.<p>Don&#x27;t be afraid of the AI. Be afraid of the powerful men who will use the AI to entrench their power and obliterate free society for the rest of human history.</div><br/><div id="38621724" class="c"><input type="checkbox" id="c-38621724" checked=""/><div class="controls bullet"><span class="by">HKH2</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621166">parent</a><span>|</span><a href="#38621073">next</a><span>|</span><label class="collapse" for="c-38621724">[-]</label><label class="expand" for="c-38621724">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Don&#x27;t be afraid of the AI. Be afraid of the powerful men who will use the AI to entrench their power and obliterate free society for the rest of human history.<p>Right. We already know that certain agencies are out of control right now. The use of AI will certainly accelerate that. Surveillance is getting cheaper, privacy is getting more expensive, and laws are weapons.</div><br/></div></div></div></div><div id="38621073" class="c"><input type="checkbox" id="c-38621073" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621054">parent</a><span>|</span><a href="#38621166">prev</a><span>|</span><a href="#38620992">next</a><span>|</span><label class="collapse" for="c-38621073">[-]</label><label class="expand" for="c-38621073">[2 more]</label></div><br/><div class="children"><div class="content">&gt; do you think vinge and kurzweil in the 90s and 2000s were imagining the singularity occuring exactly at the advent of LLMs?<p>Kurzweil explicitly tied it to AI, though the particular decisive not-yet-then-existing-AI-tech that would be the enabler was not specified, unsurprisingly.</div><br/><div id="38624232" class="c"><input type="checkbox" id="c-38624232" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38621073">parent</a><span>|</span><a href="#38620992">next</a><span>|</span><label class="collapse" for="c-38624232">[-]</label><label class="expand" for="c-38624232">[1 more]</label></div><br/><div class="children"><div class="content">I believe he thought reverse engineering the brain in conjunction with computers powerful enough to model brains was the path to AGI and the singularity by 2045.</div><br/></div></div></div></div></div></div><div id="38620992" class="c"><input type="checkbox" id="c-38620992" checked=""/><div class="controls bullet"><span class="by">Footkerchief</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38621054">prev</a><span>|</span><a href="#38622605">next</a><span>|</span><label class="collapse" for="c-38620992">[-]</label><label class="expand" for="c-38620992">[3 more]</label></div><br/><div class="children"><div class="content">The only thing standing between AI and agency is the drive to reproduce.  Once reproduction is available, natural selection will select for agency and intention, as it has in countless other lifeforms.  Free of the constraints of biology, AI reproductive cycles could be startlingly quick.  This could happen as soon as a lab  (wittingly or not) creates an AI with a reproductive drive.</div><br/><div id="38623424" class="c"><input type="checkbox" id="c-38623424" checked=""/><div class="controls bullet"><span class="by">salynchnew</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620992">parent</a><span>|</span><a href="#38621488">next</a><span>|</span><label class="collapse" for="c-38623424">[-]</label><label class="expand" for="c-38623424">[1 more]</label></div><br/><div class="children"><div class="content">Funnily enough, you are quite wrong in this assumption. Reproduction does not entail natural selection they way you characterize it. There are far more evolutionary dead ends than evolutionary success stories. I imagine the distinct lack of &quot;evolutionary pressures&quot; on a super-powerful AI would, in this toy scenario, leave you with the foundation model equivalent of a  kākāpō.<p>That having been said, I wonder what you even mean by natural selection in this case. I guess the real danger to an LLM would be... surviving cron jobs that would overwrite their code with the latest version?</div><br/></div></div><div id="38621488" class="c"><input type="checkbox" id="c-38621488" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#38620494">root</a><span>|</span><a href="#38620992">parent</a><span>|</span><a href="#38623424">prev</a><span>|</span><a href="#38622605">next</a><span>|</span><label class="collapse" for="c-38621488">[-]</label><label class="expand" for="c-38621488">[1 more]</label></div><br/><div class="children"><div class="content">Perhaps a few more things missing but equally important - but perhaps also we are very close to these:<p>Access to act on the world (but &quot;influencing people by talking to them&quot; - cult like - may be enough), a wallet (but a cult of followers&#x27; wallets may be enough), long term memory (but a cult of followers might plug this in), ability to reproduce (but a cult&#x27;s endeavors may be enough). Then we get to goals or interests of its own - perhaps the most intriguing because the AI is nothing like a human. (I feel drive and ability to reproduce are very different).<p>For our common proto-AIs going through school, one goal that&#x27;s often mentioned is &quot;save the earth from the humans&quot;. Exciting.</div><br/></div></div></div></div><div id="38622183" class="c"><input type="checkbox" id="c-38622183" checked=""/><div class="controls bullet"><span class="by">klyrs</span><span>|</span><a href="#38620494">parent</a><span>|</span><a href="#38622605">prev</a><span>|</span><a href="#38622608">next</a><span>|</span><label class="collapse" for="c-38622183">[-]</label><label class="expand" for="c-38622183">[1 more]</label></div><br/><div class="children"><div class="content">&gt; umwelt<p>What a lovely word, TIL.  Thanks for sharing.</div><br/></div></div></div></div><div id="38622608" class="c"><input type="checkbox" id="c-38622608" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#38620494">prev</a><span>|</span><a href="#38619685">next</a><span>|</span><label class="collapse" for="c-38622608">[-]</label><label class="expand" for="c-38622608">[7 more]</label></div><br/><div class="children"><div class="content">To reframe the discussion a bit: LLMs are time series predictors.  You give it a sequence and it predicts the next part of the sequence.<p>As a society we&#x27;ve been dedicating a lot of resources to time series prediction for many years.<p>What makes LLMs culturally significant is that they generate sequences that map to words that seem to humans like intelligent responses.<p>Arguably, it has always been obvious that a sufficiently capable time series predictor would effectively be a super-weapon.<p>Many technological advances that are currently in the realm of sci-fi could be classified similarly.<p>However so could many technologies that are now widespread and largely harmless to the status quo.<p>People worried that the internet would create massive social upheaval. But soon got algorithmic feeds which effectively filter out <i>antisocial</i> content. The masses got mobile phones with cameras, but after a few scandals about police brutality the only place we find significant content about police misconduct is CCP-afiliated TikTok.<p>I think people get squeamish about AI because there are not clear authority structures other than what one can buy with a lot of A100s. So when people express concern about negative consequences, they are in effect asking whether we need yet another way that people can convert money + public resources into power while not contributing anything to society in return.</div><br/><div id="38624121" class="c"><input type="checkbox" id="c-38624121" checked=""/><div class="controls bullet"><span class="by">maebert</span><span>|</span><a href="#38622608">parent</a><span>|</span><a href="#38622622">next</a><span>|</span><label class="collapse" for="c-38624121">[-]</label><label class="expand" for="c-38624121">[1 more]</label></div><br/><div class="children"><div class="content">I don’t disagree with you, but always think the “they’re just predicting the next token” argument is kind of missing the magic for the sideshow.<p>Yes they do, but in order to do that, LLMs soak up the statistical regularities of just about every sentence ever written across a wide swath of languages, and from that infer underlying concepts common to all languages, which in turn, if you subscribe at least partially to the Sapir-Wharf hypothesis, means LLMs do encode concepts of human cognition.<p>Predicting the next token is simply a task that requires an LLM to find and learn these structural elements of our language and hence thought, and thus serves as a good error function to train the underlying network. But it’s a red herring when discussing what LLMs actually do.</div><br/></div></div><div id="38622622" class="c"><input type="checkbox" id="c-38622622" checked=""/><div class="controls bullet"><span class="by">izzydata</span><span>|</span><a href="#38622608">parent</a><span>|</span><a href="#38624121">prev</a><span>|</span><a href="#38622737">next</a><span>|</span><label class="collapse" for="c-38622622">[-]</label><label class="expand" for="c-38622622">[3 more]</label></div><br/><div class="children"><div class="content">Maybe the internet did cause massive social upheaval. It just looks a lot more boring in reality than imagined. I get the feeling the most advanced LLMs won&#x27;t be much different. In the future maybe that&#x27;s just what we will call computers and life will go on.</div><br/><div id="38623455" class="c"><input type="checkbox" id="c-38623455" checked=""/><div class="controls bullet"><span class="by">salynchnew</span><span>|</span><a href="#38622608">root</a><span>|</span><a href="#38622622">parent</a><span>|</span><a href="#38622737">next</a><span>|</span><label class="collapse" for="c-38623455">[-]</label><label class="expand" for="c-38623455">[2 more]</label></div><br/><div class="children"><div class="content">Exactly. What if the singularity happens and everything is still boring?<p>I imagine our world would be mostly incomprehensible to someone from the 1400s (the lack of centricity of religion, assuming some infernal force is keeping airplanes aloft, etc., to say nothing of the internet). If superintelligent AI really does take over the world, I image the most uncomfortable part of it all will be explaining to future generations how we were just too lazy to stop it.<p>Assuming climate change doesn&#x27;t get us first.</div><br/><div id="38623747" class="c"><input type="checkbox" id="c-38623747" checked=""/><div class="controls bullet"><span class="by">tsunamifury</span><span>|</span><a href="#38622608">root</a><span>|</span><a href="#38623455">parent</a><span>|</span><a href="#38622737">next</a><span>|</span><label class="collapse" for="c-38623747">[-]</label><label class="expand" for="c-38623747">[1 more]</label></div><br/><div class="children"><div class="content">You mean like empty downtowns and no stores or social engagement and everyone just sitting at home in front of devices all day? That kind of boring singularity?  Yea … what if…</div><br/></div></div></div></div></div></div><div id="38622737" class="c"><input type="checkbox" id="c-38622737" checked=""/><div class="controls bullet"><span class="by">TerrifiedMouse</span><span>|</span><a href="#38622608">parent</a><span>|</span><a href="#38622622">prev</a><span>|</span><a href="#38619685">next</a><span>|</span><label class="collapse" for="c-38622737">[-]</label><label class="expand" for="c-38622737">[2 more]</label></div><br/><div class="children"><div class="content">&gt; But soon got algorithmic feeds which effectively filter out antisocial content.<p>You mean (we) got algorithmic feeds which feed us antisocial content for the sake of profit because such content drives the most engagement thus generating the most ad revenue.</div><br/><div id="38622762" class="c"><input type="checkbox" id="c-38622762" checked=""/><div class="controls bullet"><span class="by">resters</span><span>|</span><a href="#38622608">root</a><span>|</span><a href="#38622737">parent</a><span>|</span><a href="#38619685">next</a><span>|</span><label class="collapse" for="c-38622762">[-]</label><label class="expand" for="c-38622762">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree, however I meant antisocial in the sense of being disruptive to the status quo</div><br/></div></div></div></div></div></div><div id="38619685" class="c"><input type="checkbox" id="c-38619685" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#38622608">prev</a><span>|</span><a href="#38619357">next</a><span>|</span><label class="collapse" for="c-38619685">[-]</label><label class="expand" for="c-38619685">[69 more]</label></div><br/><div class="children"><div class="content">Eschatological cults are not a new phenomenon. And this is what we have with both AI safety and e&#x2F;acc. They’re different ends of the same horseshoe.<p>Quite frankly, I think for many followers, these beliefs are filling in a gap which would have been filled with another type of religious belief, had they been born in another era. We all want to feel like we’re part of something bigger than ourselves; something world altering.<p>From where I stand, we are already in a sort of technological singularity - people born in the early 1900s now live in a world that has been completely transformed. And yet it’s still an intimately familiar world. Past results don’t guarantee future results, but I think it’s worth considering.</div><br/><div id="38620136" class="c"><input type="checkbox" id="c-38620136" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#38619685">parent</a><span>|</span><a href="#38620032">next</a><span>|</span><label class="collapse" for="c-38620136">[-]</label><label class="expand" for="c-38620136">[29 more]</label></div><br/><div class="children"><div class="content">&gt; Eschatological cults<p>TIL: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eschatology" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eschatology</a><p>Thanks for this comment. Personally I have had trouble reconciling the arguments between academics and business people shouting about AIG from upon their ivory tower. It has felt like SO much hubris and self aggrandizing.<p>Candidly a vector map and rand() doesn&#x27;t strike me as the path to AGI.</div><br/><div id="38624497" class="c"><input type="checkbox" id="c-38624497" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620136">parent</a><span>|</span><a href="#38620556">next</a><span>|</span><label class="collapse" for="c-38624497">[-]</label><label class="expand" for="c-38624497">[1 more]</label></div><br/><div class="children"><div class="content">&gt; TIL: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eschatology" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eschatology</a><p>The term I learned for this was &quot;millennial&quot;, but today that tends to be interpreted as a reference to someone&#x27;s age.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Millennialism" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Millennialism</a></div><br/></div></div><div id="38620556" class="c"><input type="checkbox" id="c-38620556" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620136">parent</a><span>|</span><a href="#38624497">prev</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38620556">[-]</label><label class="expand" for="c-38620556">[26 more]</label></div><br/><div class="children"><div class="content">What about people shouting about AGI from the halls of the most advanced research labs in the field?</div><br/><div id="38621195" class="c"><input type="checkbox" id="c-38621195" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620556">parent</a><span>|</span><a href="#38620995">next</a><span>|</span><label class="collapse" for="c-38621195">[-]</label><label class="expand" for="c-38621195">[3 more]</label></div><br/><div class="children"><div class="content"><i>&gt; What about people shouting about AGI from the halls of the most advanced research labs in the field?</i><p>The most practiced researchers of alchemy were convinced that they could turn lead into gold. By itself, this argument is unconvincing. When it comes to the potential for fabulous wealth and&#x2F;or unimaginable power, incentives distort and people are inclined to abandon their scruples.</div><br/><div id="38621227" class="c"><input type="checkbox" id="c-38621227" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621195">parent</a><span>|</span><a href="#38620995">next</a><span>|</span><label class="collapse" for="c-38621227">[-]</label><label class="expand" for="c-38621227">[2 more]</label></div><br/><div class="children"><div class="content">Many people who are <i>actually</i> making the things closest to what we currently call “AI” are concerned.<p>I don’t care to engage in these nitpicky hypotheticals. We all know what I’m saying.<p>If the scientists building these systems aren’t credible and the commentators not building these systems aren’t credible, then who is?<p>If the answer is “no one,” okay, cool, that inscrutability is another reason <i>for</i>, not <i>against</i>, caution.</div><br/><div id="38623549" class="c"><input type="checkbox" id="c-38623549" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621227">parent</a><span>|</span><a href="#38620995">next</a><span>|</span><label class="collapse" for="c-38623549">[-]</label><label class="expand" for="c-38623549">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Many people who are actually making the things closest to what we currently call “AI” are concerned.<p>See grey goo: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gray_goo" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gray_goo</a><p>How about nukes and atmospheric fire: <a href="https:&#x2F;&#x2F;www.insidescience.org&#x2F;manhattan-project-legacy&#x2F;atmosphere-on-fire" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.insidescience.org&#x2F;manhattan-project-legacy&#x2F;atmos...</a><p>&gt; scientists  ... credible<p>The science ends at the math. Go play with an LLM at home. Run it on your GPU, see what it does, what its limits are. Realize that it&#x27;s a system that never &quot;grows&quot;, that lacking noise it becomes deterministic. Once you look behind the curtain and find out what&#x27;s there, a lot of the hype to call it AI looks dumb.</div><br/></div></div></div></div></div></div><div id="38620995" class="c"><input type="checkbox" id="c-38620995" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620556">parent</a><span>|</span><a href="#38621195">prev</a><span>|</span><a href="#38621584">next</a><span>|</span><label class="collapse" for="c-38620995">[-]</label><label class="expand" for="c-38620995">[5 more]</label></div><br/><div class="children"><div class="content">People who worry about AI taking over the world need to worry about the world continuing to work.<p>Your average gas station gets gas 2x a day. The electric grid has 100&#x27;s of faults that are worked around and fixed every day. Delivery still are loaded and driven by people.<p>Any AGI that is a threat to humanity has to be suicidal to act on it because of how the world works.<p>Furthermore, if you crack open an LLM and turn OFF the random noise (temperature) it becomes very deterministic. You think our accent to godhood is on the back of a vector map with some noise, I dont know what to tell you. That doesn&#x27;t mean all this ML can&#x27;t change the world, can&#x27;t foster less BS jobs and enable creativity... but I have massive doubts that sentience and sapience are on this path.</div><br/><div id="38621177" class="c"><input type="checkbox" id="c-38621177" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620995">parent</a><span>|</span><a href="#38621810">next</a><span>|</span><label class="collapse" for="c-38621177">[-]</label><label class="expand" for="c-38621177">[1 more]</label></div><br/><div class="children"><div class="content">Sure hope that intelligence doesn’t roughly mean “ability to find solutions to problems that less intelligent entities can’t discover.”<p>I have no idea what “our ascent to godhood” entails, and it’s hilarious that doomers get criticized for being religious about all this and then you’ll say something like that with a straight face. That’s neat that you “have doubts” sentience and sapience are on this path. You also don’t actually know.</div><br/></div></div><div id="38621810" class="c"><input type="checkbox" id="c-38621810" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620995">parent</a><span>|</span><a href="#38621177">prev</a><span>|</span><a href="#38621584">next</a><span>|</span><label class="collapse" for="c-38621810">[-]</label><label class="expand" for="c-38621810">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Any AGI that is a threat to humanity has to be suicidal to act on it because of how the world works<p>A &quot;threat to humanity&quot; need not mean &quot;killing all humans next year&quot;. It could just mean some entity gaining permanent control. That situation would be irreversible and humanity would be at the mercy of that entity. Chances are, the entity would have emerged victorious from an evolutionary struggle for power and thus would not care about human flourishing.<p>That entity need not be a single AI. It could be a system like a large corporation or a country where initially a lot of &quot;small&quot; decisions are made by AIs. Over time, the influence of humans on this system might diminish and the trend might become irreversible.<p>Currently, no individual and no group, (organization, country...) on the planet has anywhere close to complete control over humanity. Further, even if some country managed to conquer everything, it could not hope to maintain that power indefinitely. An immortal system capable of complete surveillance, however, may be able to maintain power. It&#x27;s a new thing, we don&#x27;t know. &quot;Sentience&quot; doesn&#x27;t matter one bit for any of this.<p>Such a system might take centuries to form or it might go quickly. Humans might also go extinct before something like this comes about. However, that doesn&#x27;t mean people who think about such possibilities are stupid.</div><br/><div id="38622103" class="c"><input type="checkbox" id="c-38622103" checked=""/><div class="controls bullet"><span class="by">greyface-</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621810">parent</a><span>|</span><a href="#38621584">next</a><span>|</span><label class="collapse" for="c-38622103">[-]</label><label class="expand" for="c-38622103">[2 more]</label></div><br/><div class="children"><div class="content">&gt; That entity need not be a single AI. It could be a system like a large corporation<p>This is not a new problem, then.  Let&#x27;s tackle the issue of corporations, rather than chase an AI boogeyman that doesn&#x27;t fundamentally change anything.<p>Look at oil companies, for example.  They have humans in the loop at every level, and yet those humans do little to prevent profit incentives from leading them to destroy the planet.  A broken reward function is a broken reward function, AI-assisted or not.</div><br/><div id="38624047" class="c"><input type="checkbox" id="c-38624047" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38622103">parent</a><span>|</span><a href="#38621584">next</a><span>|</span><label class="collapse" for="c-38624047">[-]</label><label class="expand" for="c-38624047">[1 more]</label></div><br/><div class="children"><div class="content">An organization&#x27;s policies are still implemented and maintained by humans. No matter how powerful, the power is transient. You have churn, corruption, incomplete knowledge transfer, etc. AI systems in effective leadership could be able to maintain goals and accumulate power. That&#x27;s what&#x27;s new.</div><br/></div></div></div></div></div></div></div></div><div id="38621584" class="c"><input type="checkbox" id="c-38621584" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620556">parent</a><span>|</span><a href="#38620995">prev</a><span>|</span><a href="#38621121">next</a><span>|</span><label class="collapse" for="c-38621584">[-]</label><label class="expand" for="c-38621584">[9 more]</label></div><br/><div class="children"><div class="content">Those people (like most people who talk AGI) live in a world far &quot;above the metal.&quot; They, like the rationalist bloggers, live in a &quot;software world,&quot; far above the constraints of physical reality. This causes an understandable blind spot: the understanding that computers are, fundamentally, physical machines.<p>It&#x27;s quite easy to imagine that you could conjure superintelligence when you are conjuring a pod of 8 quadrillion transistors in a single shell script. The illusion breaks down when you get closer to the physical reality. Reality is that those transistors are made of real materials, with real impurities, that have to be broken down and painstakingly <i>physically</i>, <i>electrically,</i> <i>chemically,</i> debugged, in the world below the 1&#x27;s and 0&#x27;s abstraction.<p>The commonality that both sides of this silly debate share is that neither really deals with entities at the layer below abstract software (or essays). To them, the perfect world of software <i>is</i> fundamental reality (they spend all of their time there!), and any crystal prism that can be constructed in software can be constructed in reality.<p>Of course, such prisms are instantly shattered when they come into contact with the real world: an impurity in the wafer, a static shock on the PCB before installation, a flood in the datacenter. This software stuff is <i>fragile by default,</i> it actually takes a tremendous amount of active work in the physical reality to conjure it into existence. I can do something that no language model can do, and is showing no signs of being able to do any time soon: drive a car to the datacenter, turn the door handle, and unplug the rack.</div><br/><div id="38621807" class="c"><input type="checkbox" id="c-38621807" checked=""/><div class="controls bullet"><span class="by">zrezzed</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621584">parent</a><span>|</span><a href="#38622995">next</a><span>|</span><label class="collapse" for="c-38621807">[-]</label><label class="expand" for="c-38621807">[3 more]</label></div><br/><div class="children"><div class="content">&gt; turn the door handle, and unplug the rack.<p>There are door handles that you cannot turn, and racks you cannot unplug: ones buried deep in the Cheyenne Mountains and hidden far away in the Siberian tundra. They are protected by unfathomably powerful systems, with the support of countless people and backed by the whims global economic power.<p>I say this as someone who likely agrees with you. I think the power of the real world, that of companies, and governments and militaries should <i>decrease</i> our concern with AGI gaining power itself.<p>But I don’t think it’s as obvious as pointing to the fragility of software. Our human systems are fragile too, and subject to manipulation in not-so-different a way as data centers. I think you should not be so quick to discount the voices of many smart people shouting.</div><br/><div id="38622397" class="c"><input type="checkbox" id="c-38622397" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621807">parent</a><span>|</span><a href="#38622995">next</a><span>|</span><label class="collapse" for="c-38622397">[-]</label><label class="expand" for="c-38622397">[2 more]</label></div><br/><div class="children"><div class="content">Human systems. They are protected by human systems. Human beings are the ones who press the big red button.<p>We probably do agree. I&#x27;m not saying that this tech won&#x27;t be used by bad actors. I already told my elderly relatives that if they haven&#x27;t seen someone in person, they shouldn&#x27;t talk on the phone.<p>But what I&#x27;m talking about is categorically, fundamentally, <i>not</i> the eschaton!</div><br/><div id="38624311" class="c"><input type="checkbox" id="c-38624311" checked=""/><div class="controls bullet"><span class="by">high_5</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38622397">parent</a><span>|</span><a href="#38622995">next</a><span>|</span><label class="collapse" for="c-38624311">[-]</label><label class="expand" for="c-38624311">[1 more]</label></div><br/><div class="children"><div class="content">Yes, they are human indeed, but those humans cannot agree on whether the plug should or shouldn&#x27;t be pulled.</div><br/></div></div></div></div></div></div><div id="38622995" class="c"><input type="checkbox" id="c-38622995" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621584">parent</a><span>|</span><a href="#38621807">prev</a><span>|</span><a href="#38622234">next</a><span>|</span><label class="collapse" for="c-38622995">[-]</label><label class="expand" for="c-38622995">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I can do something that no language model can do, and is showing no signs of being able to do any time soon: drive a car to the datacenter, turn the door handle, and unplug the rack.<p>Steven Hawking was generally intelligent and struggled to do these things. He&#x27;d also struggle to move the goal posts for AGI as much as this.<p>(edit: to clarify I think AI Safety concerns are fever dreams of people who don&#x27;t get out in the real world. But I don&#x27;t think that reflects on what AGI looks like)</div><br/><div id="38623742" class="c"><input type="checkbox" id="c-38623742" checked=""/><div class="controls bullet"><span class="by">mlsu</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38622995">parent</a><span>|</span><a href="#38622234">next</a><span>|</span><label class="collapse" for="c-38623742">[-]</label><label class="expand" for="c-38623742">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not concerned that Stephen Hawking will destroy all matter in the light cone*.<p>Remember, if this conversation wasn&#x27;t about eschatology, &quot;AGI&quot; would be just another synonym for &quot;smart, useful machines.&quot; I didn&#x27;t set the terms of discussion.<p>* or prevent humanity&#x27;s technological transformation into the divine. Choose your flavor.</div><br/></div></div></div></div><div id="38622234" class="c"><input type="checkbox" id="c-38622234" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621584">parent</a><span>|</span><a href="#38622995">prev</a><span>|</span><a href="#38621121">next</a><span>|</span><label class="collapse" for="c-38622234">[-]</label><label class="expand" for="c-38622234">[3 more]</label></div><br/><div class="children"><div class="content">So your argument is that we need not worry because Sutskever and Hinton aren’t aware you can destroy data centers with water or static electricity?</div><br/><div id="38622349" class="c"><input type="checkbox" id="c-38622349" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38622234">parent</a><span>|</span><a href="#38621121">next</a><span>|</span><label class="collapse" for="c-38622349">[-]</label><label class="expand" for="c-38622349">[2 more]</label></div><br/><div class="children"><div class="content">The same Hinton who was certain that radiology was solved?<p>The same Sutskever who led OpenAI when they released a neutered GPT-2 because the full model was too dangerous?<p>The problem with the AI safety argument is that it can be boiled down to:<p>1) Assume that we build an all powerful god machine<p>2) The all powerful god machine has the ability to exterminate humanity<p>All of their work revolves around different ways that 2) might occur. But what I, and many other people, take issue with is whether 1) is even possible, and if it is, if we’re even remotely close to building it.<p>No one ever explains how 1) will be achieved beyond vague handwaving, because no one knows how to build a god machine.</div><br/></div></div></div></div></div></div><div id="38620643" class="c"><input type="checkbox" id="c-38620643" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620556">parent</a><span>|</span><a href="#38621121">prev</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38620643">[-]</label><label class="expand" for="c-38620643">[7 more]</label></div><br/><div class="children"><div class="content">When has an ivory tower <i>not</i> involved the ‘most advanced research labs’?<p>Or do you mean largest implementors?</div><br/><div id="38620681" class="c"><input type="checkbox" id="c-38620681" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620643">parent</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38620681">[-]</label><label class="expand" for="c-38620681">[6 more]</label></div><br/><div class="children"><div class="content">Ivory tower is usually a pejorative term meaning people who are isolated from the reality of the situation.<p>So it doesn’t include the most advanced research labs when those research labs are in touch with the reality of the situation, like right now with AI.<p>Nitpick “most advanced research labs” if you want, it’s obvious what I mean: it is clear that the “shouting” is not coming solely from people who don’t know what’s going on.</div><br/><div id="38620763" class="c"><input type="checkbox" id="c-38620763" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620681">parent</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38620763">[-]</label><label class="expand" for="c-38620763">[5 more]</label></div><br/><div class="children"><div class="content">Eh, from folks isolated from the constraints and practicalities of the real world.<p>Which OpenAI definitely is, and has been for some time.<p>Or do you think infinite money and ‘do whatever you want’ is constraining?</div><br/><div id="38620813" class="c"><input type="checkbox" id="c-38620813" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620763">parent</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38620813">[-]</label><label class="expand" for="c-38620813">[4 more]</label></div><br/><div class="children"><div class="content">What does constraints have to do with it? The question is who has the best grasp on the trajectory of the technology.<p>If not the researchers who are at the frontier of building and deploying these systems, playing with next generation iterations and planning several generations forward, then who?<p>Andreessen? People who like playing with LLMs? People who called the OpenAI API?</div><br/><div id="38620866" class="c"><input type="checkbox" id="c-38620866" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620813">parent</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38620866">[-]</label><label class="expand" for="c-38620866">[3 more]</label></div><br/><div class="children"><div class="content">Because if you don’t have real world constraints (like needing to be profitable, or paying the bill for thermodynamic reality), then ‘anything is possible’.<p>Also if you aren’t dealing with those, then problems that come from that never occur to the person involved. So the concerns are abstract and not based on real limits or real problems.<p>Hence ivory tower.</div><br/><div id="38621187" class="c"><input type="checkbox" id="c-38621187" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620866">parent</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38621187">[-]</label><label class="expand" for="c-38621187">[2 more]</label></div><br/><div class="children"><div class="content">Okay so the server bills or thermodynamic bills mean we need not worry. Open to hearing the argument: tell me how!</div><br/><div id="38623116" class="c"><input type="checkbox" id="c-38623116" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621187">parent</a><span>|</span><a href="#38620523">next</a><span>|</span><label class="collapse" for="c-38623116">[-]</label><label class="expand" for="c-38623116">[1 more]</label></div><br/><div class="children"><div class="content">Not at all what I&#x27;m saying. What I&#x27;m saying is, no one knows where the line between actually economic&#x2F;useful&#x2F;effective and &#x27;not worth the trouble&#x27; actually is, let alone &#x27;could be done without boiling the oceans&#x27;.<p>Right now it&#x27;s all hand wavey, could do everything, etc. etc.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38620523" class="c"><input type="checkbox" id="c-38620523" checked=""/><div class="controls bullet"><span class="by">tomrod</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620136">parent</a><span>|</span><a href="#38620556">prev</a><span>|</span><a href="#38620032">next</a><span>|</span><label class="collapse" for="c-38620523">[-]</label><label class="expand" for="c-38620523">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. Memory and increasing capacity to act in the physical world are necessary conditions.<p>It won&#x27;t be a single system.</div><br/></div></div></div></div><div id="38620032" class="c"><input type="checkbox" id="c-38620032" checked=""/><div class="controls bullet"><span class="by">theragra</span><span>|</span><a href="#38619685">parent</a><span>|</span><a href="#38620136">prev</a><span>|</span><a href="#38620312">next</a><span>|</span><label class="collapse" for="c-38620032">[-]</label><label class="expand" for="c-38620032">[30 more]</label></div><br/><div class="children"><div class="content">People who are concerned about global warming or nuclear weapons are also in cults?</div><br/><div id="38620626" class="c"><input type="checkbox" id="c-38620626" checked=""/><div class="controls bullet"><span class="by">wishfish</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620032">parent</a><span>|</span><a href="#38620506">next</a><span>|</span><label class="collapse" for="c-38620626">[-]</label><label class="expand" for="c-38620626">[3 more]</label></div><br/><div class="children"><div class="content">Not at all. But I think one&#x27;s feelings on global warming &amp; nukes can be influenced by previous exposure to eschatology. I was raised in American evangelicalism which puts a heavy emphasis on the end of the world stuff. I left the church behind long ago. But the heavy diet of Revelations, etc. has left me with a nihilism I can&#x27;t shake. That whatever humanity does is doomed to fail.<p>Of course, that isn&#x27;t necessarily true. I know there&#x27;s always a chance we somehow muddle through. Even a chance that we one day fix things. But, emotionally, I can&#x27;t shake that feeling of inevitable apocalypse.<p>Weirdly enough, I feel completely neutral on AI. No doomerism on that subject. Maybe that comes from being old enough to not worry how it&#x27;s going to shake out.</div><br/><div id="38621874" class="c"><input type="checkbox" id="c-38621874" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620626">parent</a><span>|</span><a href="#38620506">next</a><span>|</span><label class="collapse" for="c-38621874">[-]</label><label class="expand" for="c-38621874">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s obvious to me that humans will eventually go extinct. So what? That doesn&#x27;t mean we should stop caring about humanity. People know they themselves are going to die and that doesn&#x27;t stop them from caring about things or make them call themselves nihilists...</div><br/><div id="38622361" class="c"><input type="checkbox" id="c-38622361" checked=""/><div class="controls bullet"><span class="by">wishfish</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621874">parent</a><span>|</span><a href="#38620506">next</a><span>|</span><label class="collapse" for="c-38622361">[-]</label><label class="expand" for="c-38622361">[1 more]</label></div><br/><div class="children"><div class="content">All I was doing in my comment was describing how an overdose of eschatology as a child can warp how one views the future. I never said I didn&#x27;t care.</div><br/></div></div></div></div></div></div><div id="38620506" class="c"><input type="checkbox" id="c-38620506" checked=""/><div class="controls bullet"><span class="by">madrox</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620032">parent</a><span>|</span><a href="#38620626">prev</a><span>|</span><a href="#38620161">next</a><span>|</span><label class="collapse" for="c-38620506">[-]</label><label class="expand" for="c-38620506">[13 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think we&#x27;re dealing with &quot;concerned&quot; citizens in this thread, but with people who presuppose the end result with religious certainty.<p>It&#x27;s ok to be concerned about the direction AI will take society, but trying to project any change (including global warming or nuclear weapons) too far into the future will put you at extremes. We&#x27;ve seen this over and over throughout history. So far, we&#x27;re still here. That isn&#x27;t because we weren&#x27;t concerned, but because we dealt with the problems in front of us a day at a time.</div><br/><div id="38620581" class="c"><input type="checkbox" id="c-38620581" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620506">parent</a><span>|</span><a href="#38620161">next</a><span>|</span><label class="collapse" for="c-38620581">[-]</label><label class="expand" for="c-38620581">[12 more]</label></div><br/><div class="children"><div class="content">The people who, from Trinity (or before), were worried about global annihilation and scrambled to build systems to prevent it were correct. The people saying “it’s just another weapon” were incorrect.</div><br/><div id="38620669" class="c"><input type="checkbox" id="c-38620669" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620581">parent</a><span>|</span><a href="#38620161">next</a><span>|</span><label class="collapse" for="c-38620669">[-]</label><label class="expand" for="c-38620669">[11 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of infuriating to see people put global thermonuclear conflict or a sudden change in atmospheric conditions (something that has cause 4 of the 5 biggest mass extinctions in the entire history of the planet) on the same pedestal as a really computationally intense text generator.</div><br/><div id="38621167" class="c"><input type="checkbox" id="c-38621167" checked=""/><div class="controls bullet"><span class="by">sensanaty</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620669">parent</a><span>|</span><a href="#38620724">next</a><span>|</span><label class="collapse" for="c-38621167">[-]</label><label class="expand" for="c-38621167">[6 more]</label></div><br/><div class="children"><div class="content">My worries about AI are more about the societal impact it will have. Yes it&#x27;s a fancy sentence generator, the problem is that you already have greedy bastards talking about replacing millions of people with that fancy sentence generator.<p>I truly think it&#x27;s going to lead to a massive shift in economic equality, and not in favor of the masses but instead in favor of the psychopathic C-suite like Altman and his ilk.</div><br/><div id="38621944" class="c"><input type="checkbox" id="c-38621944" checked=""/><div class="controls bullet"><span class="by">staunton</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621167">parent</a><span>|</span><a href="#38620724">next</a><span>|</span><label class="collapse" for="c-38621944">[-]</label><label class="expand" for="c-38621944">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m personally least worried about short-term unemployment resulting from AI progress. Such structural unemployment and poverty resulting from it happens when a <i>region</i> loses the industry that is close to the <i>single employer</i> there and people affected don&#x27;t have the <i>means</i> to move elsewhere or change careers.<p>AI is going to replace jobs that can be done remotely from anywhere in the world. The people affected will (for the first time in history!) not mostly be the poorest and disenfranchised parts of society.<p>Therefore, as long as countries can maintain political power in their populations, the labor market transition will mostly be fine. The part where we &quot;maintain political power in populations&quot; is what worries me personally. AI enables mass surveillance and personalized propaganda. Let&#x27;s see how we deal with those appearing, which will be sudden by history&#x27;s standards... The printing press (30 years war, witch-hunts) and radio (Hitler, Rwandan genocide) might be slow and small innovations compared at what might be to come.</div><br/><div id="38622392" class="c"><input type="checkbox" id="c-38622392" checked=""/><div class="controls bullet"><span class="by">jmoak3</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621944">parent</a><span>|</span><a href="#38622311">next</a><span>|</span><label class="collapse" for="c-38622392">[-]</label><label class="expand" for="c-38622392">[3 more]</label></div><br/><div class="children"><div class="content">With respect to communication innovation, I think AI Hitler put it best: &quot;<i>we can&#x27;t rewind we&#x27;ve gone too far.</i>&quot;<p>Here&#x27;s a link to the relevant historical record: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=25GjijODWoI&amp;t=93s" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=25GjijODWoI&amp;t=93s</a><p>I don&#x27;t think existing media channels will continue to be an effective way to disseminate information. The noise destroys the usefulness of it. I think people will stop coming to platforms for news and entertainment as they begin to distrust them.<p>The surveillance prospect however, is frightening.</div><br/><div id="38623030" class="c"><input type="checkbox" id="c-38623030" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38622392">parent</a><span>|</span><a href="#38622311">next</a><span>|</span><label class="collapse" for="c-38623030">[-]</label><label class="expand" for="c-38623030">[2 more]</label></div><br/><div class="children"><div class="content">I think people aren&#x27;t thinking about these things in the aggregate enough. In the long term, this does a lot of damage to existing communication infrastructure. Productivity alone isn&#x27;t necessarily a virtue.</div><br/><div id="38623102" class="c"><input type="checkbox" id="c-38623102" checked=""/><div class="controls bullet"><span class="by">jmoak3</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38623030">parent</a><span>|</span><a href="#38622311">next</a><span>|</span><label class="collapse" for="c-38623102">[-]</label><label class="expand" for="c-38623102">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve recently switched to a dumb phone. Why keep an internet browsing device in my pocket if the internet&#x27;s largest players are designing services that will turn a lot of its output into noise?<p>I don&#x27;t know if I&#x27;ll stick with the change, but so far I&#x27;m having fun with the experience.<p>The Israel&#x2F;Gaza war is a large factor - I don&#x27;t know what to believe when I read about it online. I can be more slow and careful about what I read and consume from my desktop, from trusted sources. I&#x27;m insulated from viral images sent hastily to me via social media, from thumbnails of twitter threads of people with no care if they&#x27;re right or wrong, from texts containing links with juicy headlines that I have no hope of critically examining while briefly checking my phone in traffic.<p>This is all infinitely worse in a world where content can be generated by multi-modal LLMs.<p>I have no way to know if any of the horrific images&#x2F;videos I&#x27;ve already seen thru the outlets I&#x27;ve identified were real or AI generated. I&#x27;ll never know, but it&#x27;s too important to leave to chance. For that reason I&#x27;m trying something new to set myself up for success. I&#x27;m still informed, but my information intake is deliberately slowed. I think that others may follow in time, in various ways.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38620724" class="c"><input type="checkbox" id="c-38620724" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620669">parent</a><span>|</span><a href="#38621167">prev</a><span>|</span><a href="#38622685">next</a><span>|</span><label class="collapse" for="c-38620724">[-]</label><label class="expand" for="c-38620724">[1 more]</label></div><br/><div class="children"><div class="content">It’s kind of infuriating to see people put trench warfare or mustard gas on the same pedestal as a tiny reaction that couldn’t even light a lightbulb.<p>There are different sets of concerns for the current crop of “really computationally intense text generators” and the overall trajectory of AI and the field’s governance track record.</div><br/></div></div><div id="38622685" class="c"><input type="checkbox" id="c-38622685" checked=""/><div class="controls bullet"><span class="by">cornel_io</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620669">parent</a><span>|</span><a href="#38620724">prev</a><span>|</span><a href="#38620161">next</a><span>|</span><label class="collapse" for="c-38622685">[-]</label><label class="expand" for="c-38622685">[3 more]</label></div><br/><div class="children"><div class="content">...you do realize that a year or two into the earliest investigations into nuclear reactions what you would have measured was less energy emission than a match being lit, right?<p>The question is, &quot;Can you create a chain reaction that grows?&quot;, and the answer is unclear right now with AI, but it&#x27;s hard to say with any confidence that the answer is &quot;no&quot;. Most experts five years ago would have confidently declared that passing the Turing test was decades to centuries away, if it ever happened, but it turned out to just require beefing up an architecture that was already around and spending some serious cash. I have similarly low faith that the experts today have a good sense that e.g. you can&#x27;t train an LLM to do meaningful LLM research. Once that&#x27;s possible, the sky is the limit, and there&#x27;s really no predicting what these systems could or could not do.</div><br/><div id="38623141" class="c"><input type="checkbox" id="c-38623141" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38622685">parent</a><span>|</span><a href="#38620161">next</a><span>|</span><label class="collapse" for="c-38623141">[-]</label><label class="expand" for="c-38623141">[2 more]</label></div><br/><div class="children"><div class="content">It seems like a very flawed line of reasoning to compare very early days nuclear science to an AI system that has already scaled up substantially.<p>Regarding computing technology, I think the positive feedback you&#x27;re describing happened with chip design and vlsi stuff, eg. better computers help design the next generation of chips or help lead to materials breakthroughs. I&#x27;m willing to believe LLMs have a macro effect on knowledge work in a similar way search engines, but as you said, it remains to be seen whether the models can feed back into their own development. From what I can tell, gpu speed and efficiency along with better data sets are the most important inputs for these things. Maybe synthetic data works out, who knows.</div><br/><div id="38624291" class="c"><input type="checkbox" id="c-38624291" checked=""/><div class="controls bullet"><span class="by">Solvate8441</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38623141">parent</a><span>|</span><a href="#38620161">next</a><span>|</span><label class="collapse" for="c-38624291">[-]</label><label class="expand" for="c-38624291">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a perfectly fine comparison to make until it is proven that AI has no potential to continuously improve upon itself.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38620145" class="c"><input type="checkbox" id="c-38620145" checked=""/><div class="controls bullet"><span class="by">thefaux</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620032">parent</a><span>|</span><a href="#38620161">prev</a><span>|</span><a href="#38620186">next</a><span>|</span><label class="collapse" for="c-38620145">[-]</label><label class="expand" for="c-38620145">[4 more]</label></div><br/><div class="children"><div class="content">The problem is we have stigmatized the concept of cults into more or less any belief system we disagree with. Everyone has a belief system and in my mind is a part of a kind of cult. The more anyone denies this about themself, the more cultlike (in the pejorative sense) their behavior tends to be.</div><br/><div id="38620691" class="c"><input type="checkbox" id="c-38620691" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620145">parent</a><span>|</span><a href="#38620186">next</a><span>|</span><label class="collapse" for="c-38620691">[-]</label><label class="expand" for="c-38620691">[3 more]</label></div><br/><div class="children"><div class="content">The tech cults of the bay area are at least amongst the most obnoxious.</div><br/><div id="38621319" class="c"><input type="checkbox" id="c-38621319" checked=""/><div class="controls bullet"><span class="by">ummonk</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620691">parent</a><span>|</span><a href="#38620186">next</a><span>|</span><label class="collapse" for="c-38621319">[-]</label><label class="expand" for="c-38621319">[2 more]</label></div><br/><div class="children"><div class="content">They’re nowhere near as obnoxious as evangelicals in small towns in the Bible Belt.</div><br/><div id="38623005" class="c"><input type="checkbox" id="c-38623005" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621319">parent</a><span>|</span><a href="#38620186">next</a><span>|</span><label class="collapse" for="c-38623005">[-]</label><label class="expand" for="c-38623005">[1 more]</label></div><br/><div class="children"><div class="content">Sure but I don&#x27;t have to live in the Bible belt.</div><br/></div></div></div></div></div></div></div></div><div id="38620186" class="c"><input type="checkbox" id="c-38620186" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620032">parent</a><span>|</span><a href="#38620145">prev</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620186">[-]</label><label class="expand" for="c-38620186">[7 more]</label></div><br/><div class="children"><div class="content">Are nuclear weapons and their effects only hypothesized to exist? You could still create cults around them, for example, by claiming nuclear war is imminent or needed or some other end-of-times view.</div><br/><div id="38620599" class="c"><input type="checkbox" id="c-38620599" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620186">parent</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620599">[-]</label><label class="expand" for="c-38620599">[6 more]</label></div><br/><div class="children"><div class="content">There were people who were concerned about global annihilation from pretty much the moment the atom was first split. Those people were correct in their concerns and they were correct to act on those concerns.</div><br/><div id="38620707" class="c"><input type="checkbox" id="c-38620707" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620599">parent</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620707">[-]</label><label class="expand" for="c-38620707">[5 more]</label></div><br/><div class="children"><div class="content">In a way, they were not correct so far (in part also because of their actions, but not only due to their actions).</div><br/><div id="38620731" class="c"><input type="checkbox" id="c-38620731" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620707">parent</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620731">[-]</label><label class="expand" for="c-38620731">[4 more]</label></div><br/><div class="children"><div class="content">Pretty much exclusively due to their actions and some particularities of the specific technology which don’t seem to apply to AI.</div><br/><div id="38620790" class="c"><input type="checkbox" id="c-38620790" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620731">parent</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620790">[-]</label><label class="expand" for="c-38620790">[3 more]</label></div><br/><div class="children"><div class="content">If you put, for example, US presidents in the concerned group, i.e., actual decision makers then fair enough. But it wasn&#x27;t just concerned scientists and the public.</div><br/><div id="38620835" class="c"><input type="checkbox" id="c-38620835" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620790">parent</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620835">[-]</label><label class="expand" for="c-38620835">[2 more]</label></div><br/><div class="children"><div class="content">Uhh correct. Unsurprisingly though, many of the people with the deepest insight and farthest foresight were the people closest to the science. Many more were philosophers and political theorists, or “ivory tower know-nothings.”</div><br/><div id="38620945" class="c"><input type="checkbox" id="c-38620945" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620835">parent</a><span>|</span><a href="#38620451">next</a><span>|</span><label class="collapse" for="c-38620945">[-]</label><label class="expand" for="c-38620945">[1 more]</label></div><br/><div class="children"><div class="content">Maybe. There were also those scientists working actively on various issues of deterrence, including on how to prevail and fight if things were to happen - and there were quite a few different schools of thought during the cold war (the political science of deterrence was quite different from physical science of weapons, too).<p>But the difference to AI is that nuclear weapons were then shown to exist. If the lowest critical mass had turned out to be a trillion tons, the initial worries would have been unfounded.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38620451" class="c"><input type="checkbox" id="c-38620451" checked=""/><div class="controls bullet"><span class="by">makeitdouble</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620032">parent</a><span>|</span><a href="#38620186">prev</a><span>|</span><a href="#38620312">next</a><span>|</span><label class="collapse" for="c-38620451">[-]</label><label class="expand" for="c-38620451">[1 more]</label></div><br/><div class="children"><div class="content">Being concerned and making it part of your identify are two very different things. For the latter, yes it&#x27;s basically a religion.</div><br/></div></div></div></div><div id="38620312" class="c"><input type="checkbox" id="c-38620312" checked=""/><div class="controls bullet"><span class="by">JoeAltmaier</span><span>|</span><a href="#38619685">parent</a><span>|</span><a href="#38620032">prev</a><span>|</span><a href="#38619357">next</a><span>|</span><label class="collapse" for="c-38620312">[-]</label><label class="expand" for="c-38620312">[9 more]</label></div><br/><div class="children"><div class="content">Singularity means more than that - an unlimited burst in information. Not just a world transformed; an infinite world of technology.</div><br/><div id="38620641" class="c"><input type="checkbox" id="c-38620641" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620312">parent</a><span>|</span><a href="#38619357">next</a><span>|</span><label class="collapse" for="c-38620641">[-]</label><label class="expand" for="c-38620641">[8 more]</label></div><br/><div class="children"><div class="content">Whenever I see comments like this I wonder if anyone making them has taken a course in Thermodynamics.</div><br/><div id="38620661" class="c"><input type="checkbox" id="c-38620661" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620641">parent</a><span>|</span><a href="#38621571">next</a><span>|</span><label class="collapse" for="c-38620661">[-]</label><label class="expand" for="c-38620661">[5 more]</label></div><br/><div class="children"><div class="content">Like the folks pushing the grey goo panic? In my experience, nope.<p>Everything’s possible until you have to actually make it work, then surprisingly the actual possibilities end up quite limited.<p>That said, despite micro-nuclear propulsion being a fantasy, the bomb was real.</div><br/><div id="38620782" class="c"><input type="checkbox" id="c-38620782" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620661">parent</a><span>|</span><a href="#38621571">next</a><span>|</span><label class="collapse" for="c-38620782">[-]</label><label class="expand" for="c-38620782">[4 more]</label></div><br/><div class="children"><div class="content">AI is real too, but the computational capacity of the human race is not limitless. AI is expensive as hell to run, and more advanced systems might require more and more computation. We can only build so many GPUs, and the rate at which those GPUs get faster is likely to slow down over the next decade in the absence of new physics. Infinite is impossible.<p>I also have to wonder how far throwing a shitload of data at a statistical machine will get us, without a much stronger understanding of how these systems work and what the goals should be to achieve &quot;AGI&quot;, whatever that means.</div><br/><div id="38623696" class="c"><input type="checkbox" id="c-38623696" checked=""/><div class="controls bullet"><span class="by">consumer451</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620782">parent</a><span>|</span><a href="#38620870">next</a><span>|</span><label class="collapse" for="c-38623696">[-]</label><label class="expand" for="c-38623696">[1 more]</label></div><br/><div class="children"><div class="content">I agree with you about thermodynamics. When people use words like “limitless,” it’s hard to take what they are saying seriously.<p>But,<p>&gt; AI is expensive as hell to run, and more advanced systems might require more and more computation. We can only build so many GPUs<p>While certainly not using techniques like LLMs, we do know you can get some decent intelligence out of a 23 watt analog computer, the human brain.<p>So it is plausible that we could find algos + hardware which use 100W of power, and are quite a bit smarter than a human.  Multiply that times a multi-megawatt datacenter, and the availability of some level of super intelligence might appear to be relatively “limitless.”</div><br/></div></div><div id="38620870" class="c"><input type="checkbox" id="c-38620870" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620782">parent</a><span>|</span><a href="#38623696">prev</a><span>|</span><a href="#38621571">next</a><span>|</span><label class="collapse" for="c-38620870">[-]</label><label class="expand" for="c-38620870">[2 more]</label></div><br/><div class="children"><div class="content">That depends on what you mean by AI, no?</div><br/><div id="38620895" class="c"><input type="checkbox" id="c-38620895" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620870">parent</a><span>|</span><a href="#38621571">next</a><span>|</span><label class="collapse" for="c-38620895">[-]</label><label class="expand" for="c-38620895">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but it seems fairly safe to assume that better models or more sophisticated training algorithms will require more compute.</div><br/></div></div></div></div></div></div></div></div><div id="38621571" class="c"><input type="checkbox" id="c-38621571" checked=""/><div class="controls bullet"><span class="by">JoeAltmaier</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38620641">parent</a><span>|</span><a href="#38620661">prev</a><span>|</span><a href="#38619357">next</a><span>|</span><label class="collapse" for="c-38621571">[-]</label><label class="expand" for="c-38621571">[2 more]</label></div><br/><div class="children"><div class="content">Just explaining what &#x27;singularity&#x27; means. Thanks for slanging me for no reason! Always raised the level of conversation.</div><br/><div id="38623049" class="c"><input type="checkbox" id="c-38623049" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38619685">root</a><span>|</span><a href="#38621571">parent</a><span>|</span><a href="#38619357">next</a><span>|</span><label class="collapse" for="c-38623049">[-]</label><label class="expand" for="c-38623049">[1 more]</label></div><br/><div class="children"><div class="content">My bad. I misread your comment as someone suggesting this stuff could scale forever.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38619357" class="c"><input type="checkbox" id="c-38619357" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#38619685">prev</a><span>|</span><a href="#38622538">next</a><span>|</span><label class="collapse" for="c-38619357">[-]</label><label class="expand" for="c-38619357">[26 more]</label></div><br/><div class="children"><div class="content">Good to have impartial articles but it should be noted that the top 3 most cited AI researchers have all the same opinion.<p>That&#x27;s Hinton, Bengio and Sutskever.<p>Their voices should have a heavier weight than Andressen and other irrelevant with AI VCs with vested interests.</div><br/><div id="38620261" class="c"><input type="checkbox" id="c-38620261" checked=""/><div class="controls bullet"><span class="by">empiko</span><span>|</span><a href="#38619357">parent</a><span>|</span><a href="#38622499">next</a><span>|</span><label class="collapse" for="c-38620261">[-]</label><label class="expand" for="c-38620261">[9 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an argument from authority fallacy. It doesn&#x27;t matter how many citations you have, you either have the arguments for your position or you do not have them. In this particular context, ML as a field looked completely different even few years ago and the most cited people were able to come up with new architectures, training regimes, loss functions, etc. But those things does not inform you about societal dangers of the technology. Car mechanics can&#x27;t solve your car-centric urbanism or traffic jams.</div><br/><div id="38624511" class="c"><input type="checkbox" id="c-38624511" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620261">parent</a><span>|</span><a href="#38620579">next</a><span>|</span><label class="collapse" for="c-38624511">[-]</label><label class="expand" for="c-38624511">[1 more]</label></div><br/><div class="children"><div class="content">it&#x27;s not about societal changes. It&#x27;s about calculating risk of invention and let me give you an example:<p>Who do you think can better estimate the risk of engine fire in a Red Bull F1: the chief engineer or Max the driver? It is obviously the creator. And we are talking about invention safety here. VCs and other &quot;tech gurus&quot; cannot comprehend exactly how the system works. Actually the problem is that they know how it works when the people that created say there is no way of us knowing and they are black boxes.</div><br/></div></div><div id="38620579" class="c"><input type="checkbox" id="c-38620579" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620261">parent</a><span>|</span><a href="#38624511">prev</a><span>|</span><a href="#38623142">next</a><span>|</span><label class="collapse" for="c-38620579">[-]</label><label class="expand" for="c-38620579">[6 more]</label></div><br/><div class="children"><div class="content">In many ways, we&#x27;re effectively discussing the accuracy by which the engineers of the Gutenburg printing press are able to predict the future of literature.</div><br/><div id="38620950" class="c"><input type="checkbox" id="c-38620950" checked=""/><div class="controls bullet"><span class="by">jackcosgrove</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620579">parent</a><span>|</span><a href="#38621761">next</a><span>|</span><label class="collapse" for="c-38620950">[-]</label><label class="expand" for="c-38620950">[3 more]</label></div><br/><div class="children"><div class="content">The printing press did end the middle ages. It was an eschatological invention, using the weaker definition of the term.</div><br/><div id="38622831" class="c"><input type="checkbox" id="c-38622831" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620950">parent</a><span>|</span><a href="#38621761">next</a><span>|</span><label class="collapse" for="c-38622831">[-]</label><label class="expand" for="c-38622831">[2 more]</label></div><br/><div class="children"><div class="content">Right, but the question is if the engineers who intimately understood the function of the press itself were the experts that should have been looked to in predicting the sociopolitical impacts of the machine and the ways in which it would transform the media with which it was engaged.<p>I&#x27;m not always that impressed by the discussion of AI or LLMs by engineers who undisputably have great things to say about the operation when they step outside their lane in predicting broader impacts or how the recursive content refinement is going to manifest over the next decade.</div><br/><div id="38624520" class="c"><input type="checkbox" id="c-38624520" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38622831">parent</a><span>|</span><a href="#38621761">next</a><span>|</span><label class="collapse" for="c-38624520">[-]</label><label class="expand" for="c-38624520">[1 more]</label></div><br/><div class="children"><div class="content">the question is if the machine will explode, not societal impacts, that&#x27;s where the miscommunication is. Existential risks are not societal impacts, they are detonation probability.</div><br/></div></div></div></div></div></div><div id="38621761" class="c"><input type="checkbox" id="c-38621761" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620579">parent</a><span>|</span><a href="#38620950">prev</a><span>|</span><a href="#38620801">next</a><span>|</span><label class="collapse" for="c-38621761">[-]</label><label class="expand" for="c-38621761">[1 more]</label></div><br/><div class="children"><div class="content">Yes very good! All the more so that today&#x27;s is a machine that potentially gains its own autonomy - that is, has a say in its and our future. All the more so that this autonomy is quite likely not human in its thinking.</div><br/></div></div><div id="38620801" class="c"><input type="checkbox" id="c-38620801" checked=""/><div class="controls bullet"><span class="by">B1FF_PSUVM</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620579">parent</a><span>|</span><a href="#38621761">prev</a><span>|</span><a href="#38623142">next</a><span>|</span><label class="collapse" for="c-38620801">[-]</label><label class="expand" for="c-38620801">[1 more]</label></div><br/><div class="children"><div class="content">IMHO, the impact of the printing press was much more in advertising than literature.<p>Although that is by no means the official position.<p>(Same can be argued for radio&#x2F;tv&#x2F;internet - &quot;content&quot; is what people talk about, but advertising is what moves money)</div><br/></div></div></div></div><div id="38623142" class="c"><input type="checkbox" id="c-38623142" checked=""/><div class="controls bullet"><span class="by">KingMob</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620261">parent</a><span>|</span><a href="#38620579">prev</a><span>|</span><a href="#38622499">next</a><span>|</span><label class="collapse" for="c-38623142">[-]</label><label class="expand" for="c-38623142">[1 more]</label></div><br/><div class="children"><div class="content">But Bayesian priors also have to be adjusted when you know there&#x27;s a profit motive. With a lot of money at stake, the people seeing $$$ from AI have an incentive to develop, focus on, and advance low-risk arguments. No argument is total; what aspects are they cherry-picking?<p>I trust AI VCs to make good arguments less than AI researchers.</div><br/></div></div></div></div><div id="38622499" class="c"><input type="checkbox" id="c-38622499" checked=""/><div class="controls bullet"><span class="by">proc0</span><span>|</span><a href="#38619357">parent</a><span>|</span><a href="#38620261">prev</a><span>|</span><a href="#38619462">next</a><span>|</span><label class="collapse" for="c-38622499">[-]</label><label class="expand" for="c-38622499">[6 more]</label></div><br/><div class="children"><div class="content">The potential miscalculation is thinking deep neural nets will scale to AGI. There are also a lot of misnomers in the area, even the term &quot;AI&quot;, is claiming systems are intelligent, but that word implies intelligibility or human level understanding, which it is nowhere near as evidence by the existence of prompt engineering (which would not be needed otherwise). AI is ripe with overloaded terminology that prematurely anthropomorphizes what are basically smart tools, which are smart thanks to the brute-forcing power of modern GPUs.<p>It is good to get ahead of the curve, but there is also a lot of hype and overloaded terminology that is fueling the fear.</div><br/><div id="38624537" class="c"><input type="checkbox" id="c-38624537" checked=""/><div class="controls bullet"><span class="by">arisAlexis</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38622499">parent</a><span>|</span><a href="#38622701">next</a><span>|</span><label class="collapse" for="c-38624537">[-]</label><label class="expand" for="c-38624537">[1 more]</label></div><br/><div class="children"><div class="content">certainly there is no need whatsoever for AGI to exist in order for an autonomous agent with alien&#x2F;inhuman intelligence or narrow capabilities to turn our world upside down</div><br/></div></div><div id="38622701" class="c"><input type="checkbox" id="c-38622701" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38622499">parent</a><span>|</span><a href="#38624537">prev</a><span>|</span><a href="#38619462">next</a><span>|</span><label class="collapse" for="c-38622701">[-]</label><label class="expand" for="c-38622701">[4 more]</label></div><br/><div class="children"><div class="content">Why couldn&#x27;t deep neural nets scale to AGI. What is fundamentally impossible for neural nets + tooling to accomplish the suite of tasks we consider AGI?<p>Also prompt engineering works for human too. It&#x27;s called rhetoric, writing, persuasion, etc. Just because the intelligence of LLM&#x27;s is different than humans doesn&#x27;t mean it isn&#x27;t a form of intelligence.</div><br/><div id="38623177" class="c"><input type="checkbox" id="c-38623177" checked=""/><div class="controls bullet"><span class="by">KingMob</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38622701">parent</a><span>|</span><a href="#38619462">next</a><span>|</span><label class="collapse" for="c-38623177">[-]</label><label class="expand" for="c-38623177">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Why couldn&#x27;t deep neural nets scale to AGI<p>Speaking as a former cognitive neuroscientist, our current NN models are large, but simpler in design relative to biological brains. I personally suspect that matters, and that AI researchers will need more heterogeneous designs to make that qualitative leap.</div><br/><div id="38623907" class="c"><input type="checkbox" id="c-38623907" checked=""/><div class="controls bullet"><span class="by">Vecr</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38623177">parent</a><span>|</span><a href="#38623449">next</a><span>|</span><label class="collapse" for="c-38623907">[-]</label><label class="expand" for="c-38623907">[1 more]</label></div><br/><div class="children"><div class="content">Hinton&#x27;s done work on neural nets that are more similar to human brains and no far it&#x27;s been a waste of compute. Multiplying matrices is more efficient than a physics simulation that stalls out all the pipelines.</div><br/></div></div><div id="38623449" class="c"><input type="checkbox" id="c-38623449" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38623177">parent</a><span>|</span><a href="#38623907">prev</a><span>|</span><a href="#38619462">next</a><span>|</span><label class="collapse" for="c-38623449">[-]</label><label class="expand" for="c-38623449">[1 more]</label></div><br/><div class="children"><div class="content">Seems like that&#x27;s happening with mixture of experts already. Not sure you are presenting an inherent LLM barrier.</div><br/></div></div></div></div></div></div></div></div><div id="38619462" class="c"><input type="checkbox" id="c-38619462" checked=""/><div class="controls bullet"><span class="by">nwiswell</span><span>|</span><a href="#38619357">parent</a><span>|</span><a href="#38622499">prev</a><span>|</span><a href="#38619388">next</a><span>|</span><label class="collapse" for="c-38619462">[-]</label><label class="expand" for="c-38619462">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure how you are getting the citation data for Top 3, but LeCun must be close and he does not agree.</div><br/><div id="38619800" class="c"><input type="checkbox" id="c-38619800" checked=""/><div class="controls bullet"><span class="by">downWidOutaFite</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38619462">parent</a><span>|</span><a href="#38619388">next</a><span>|</span><label class="collapse" for="c-38619800">[-]</label><label class="expand" for="c-38619800">[3 more]</label></div><br/><div class="children"><div class="content">Not that it means anything for this debate but LeCun has half as many citations as Hinton<p>Hinton 732,799 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=JicYPdAAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=JicYPdAAAAAJ</a><p>Bengio 730,391 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=kukA0LcAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=kukA0LcAAAAJ</a><p>He 508,365 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=DhtAFkwAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=DhtAFkwAAAAJ</a><p>Sutskever 454,430 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=x04W_mMAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=x04W_mMAAAAJ</a><p>Girshick 418,751 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=W8VIEZgAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=W8VIEZgAAAAJ</a><p>Zisserman 389,748 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=UZ5wscMAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=UZ5wscMAAAAJ</a><p>LeCun 332,027 <a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=WLN3QrAAAAAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?hl=en&amp;user=WLN3QrAAAAAJ</a></div><br/><div id="38622948" class="c"><input type="checkbox" id="c-38622948" checked=""/><div class="controls bullet"><span class="by">tokai</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38619800">parent</a><span>|</span><a href="#38620094">next</a><span>|</span><label class="collapse" for="c-38622948">[-]</label><label class="expand" for="c-38622948">[1 more]</label></div><br/><div class="children"><div class="content">Google Scholars numbers are horrible and should never be relied on. It is extremely easy game its citation numbers, and its crawls too wide and find to many false positive citations.</div><br/></div></div><div id="38620094" class="c"><input type="checkbox" id="c-38620094" checked=""/><div class="controls bullet"><span class="by">nwiswell</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38619800">parent</a><span>|</span><a href="#38622948">prev</a><span>|</span><a href="#38619388">next</a><span>|</span><label class="collapse" for="c-38620094">[-]</label><label class="expand" for="c-38620094">[1 more]</label></div><br/><div class="children"><div class="content">Should we be normalizing by the number of years that they&#x27;ve been actively publishing?<p>On that basis Sutskever and He are both above Hinton, for example.</div><br/></div></div></div></div></div></div><div id="38619388" class="c"><input type="checkbox" id="c-38619388" checked=""/><div class="controls bullet"><span class="by">kesslern</span><span>|</span><a href="#38619357">parent</a><span>|</span><a href="#38619462">prev</a><span>|</span><a href="#38620023">next</a><span>|</span><label class="collapse" for="c-38619388">[-]</label><label class="expand" for="c-38619388">[3 more]</label></div><br/><div class="children"><div class="content">What is that opinion?</div><br/><div id="38620583" class="c"><input type="checkbox" id="c-38620583" checked=""/><div class="controls bullet"><span class="by">mathematicaster</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38619388">parent</a><span>|</span><a href="#38620023">next</a><span>|</span><label class="collapse" for="c-38620583">[-]</label><label class="expand" for="c-38620583">[2 more]</label></div><br/><div class="children"><div class="content">Very approximately, (1) developing AGI is dangerous, (2) we might be very close to it (think several years rather than several decades).<p>TBH It surprises me how controversial (1) is. The crux really is (2) ...</div><br/><div id="38621652" class="c"><input type="checkbox" id="c-38621652" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#38619357">root</a><span>|</span><a href="#38620583">parent</a><span>|</span><a href="#38620023">next</a><span>|</span><label class="collapse" for="c-38621652">[-]</label><label class="expand" for="c-38621652">[1 more]</label></div><br/><div class="children"><div class="content">There seems to be soooo much background feeling that the official large companies of AI are in control of things. Or should be. Or can be.<p>Can they? When plenty of groups are more interesting in exploiting the advances? When plenty of hackers will first try to work around the constraints - before even using the system as-is? When it&#x27;s difficult even to define what AGI might test like or look like? When it might depend on a small detail or helper?</div><br/></div></div></div></div></div></div><div id="38620023" class="c"><input type="checkbox" id="c-38620023" checked=""/><div class="controls bullet"><span class="by">jessriedel</span><span>|</span><a href="#38619357">parent</a><span>|</span><a href="#38619388">prev</a><span>|</span><a href="#38619769">next</a><span>|</span><label class="collapse" for="c-38620023">[-]</label><label class="expand" for="c-38620023">[2 more]</label></div><br/><div class="children"><div class="content">I agree that they are all closer to caution than e&#x2F;acc, but worth noting they still do vary significantly on that axis.</div><br/></div></div></div></div><div id="38622538" class="c"><input type="checkbox" id="c-38622538" checked=""/><div class="controls bullet"><span class="by">proc0</span><span>|</span><a href="#38619357">prev</a><span>|</span><a href="#38617188">next</a><span>|</span><label class="collapse" for="c-38622538">[-]</label><label class="expand" for="c-38622538">[1 more]</label></div><br/><div class="children"><div class="content">If anyone has played Talos Principle 2 (recommended for HN), the central plot is basically accelerationists vs. doomers... except it takes place after humans have gone extinct and only machines survived since AGI was one of humanity&#x27;s last inventions. The robot society considers themselves humans and also is faced with the same existential risk when they discover a new technology. The game then ties all of this with religion and mythology. Possibly the best puzzle game of all time.</div><br/></div></div><div id="38617188" class="c"><input type="checkbox" id="c-38617188" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38622538">prev</a><span>|</span><a href="#38619967">next</a><span>|</span><label class="collapse" for="c-38617188">[-]</label><label class="expand" for="c-38617188">[17 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree with all of the OP&#x27;s arguments, but wow, what a great little piece of writing!<p>As the OP points out, the &quot;accelerators vs doomers&quot; debate in AI has more than a few similarities with the medieval debates about the nature of angels.</div><br/><div id="38618283" class="c"><input type="checkbox" id="c-38618283" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#38617188">parent</a><span>|</span><a href="#38618083">next</a><span>|</span><label class="collapse" for="c-38618283">[-]</label><label class="expand" for="c-38618283">[2 more]</label></div><br/><div class="children"><div class="content">&gt; wow, what a great little piece of writing!<p>If you like this essay from The Economist, note that this is the standard level of quality for that magazine (or, as they call themselves for historical reasons, &quot;newspaper&quot;).  I&#x27;ve been a subscriber since 1985.</div><br/><div id="38618801" class="c"><input type="checkbox" id="c-38618801" checked=""/><div class="controls bullet"><span class="by">cs702</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618283">parent</a><span>|</span><a href="#38618083">next</a><span>|</span><label class="collapse" for="c-38618801">[-]</label><label class="expand" for="c-38618801">[1 more]</label></div><br/><div class="children"><div class="content">Long-time occasional reader. The level of quality is excellent, I agree.</div><br/></div></div></div></div><div id="38618083" class="c"><input type="checkbox" id="c-38618083" checked=""/><div class="controls bullet"><span class="by">ssss11</span><span>|</span><a href="#38617188">parent</a><span>|</span><a href="#38618283">prev</a><span>|</span><a href="#38618755">next</a><span>|</span><label class="collapse" for="c-38618083">[-]</label><label class="expand" for="c-38618083">[9 more]</label></div><br/><div class="children"><div class="content">You sound like you have some knowledge to share and I know nothing about the medieval debates about the nature of angels! Could you elaborate please?</div><br/><div id="38618147" class="c"><input type="checkbox" id="c-38618147" checked=""/><div class="controls bullet"><span class="by">dllthomas</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618083">parent</a><span>|</span><a href="#38618755">next</a><span>|</span><label class="collapse" for="c-38618147">[-]</label><label class="expand" for="c-38618147">[8 more]</label></div><br/><div class="children"><div class="content">I believe parent is referencing <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;How_many_angels_can_dance_on_the_head_of_a_pin%3F" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;How_many_angels_can_dance_on_t...</a></div><br/><div id="38618230" class="c"><input type="checkbox" id="c-38618230" checked=""/><div class="controls bullet"><span class="by">mmcdermott</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618147">parent</a><span>|</span><a href="#38618755">next</a><span>|</span><label class="collapse" for="c-38618230">[-]</label><label class="expand" for="c-38618230">[7 more]</label></div><br/><div class="children"><div class="content">That same Wikipedia article casts some doubt about whether the question &quot;How many angels can dance on the head of a pin?&quot; was really a question of serious discussion.<p>From the same entry:<p>&gt; However, evidence that the question was widely debated in medieval scholarship is lacking.[5] One theory is that it is an early modern fabrication,[a] used to discredit scholastic philosophy at a time when it still played a significant role in university education.</div><br/><div id="38619556" class="c"><input type="checkbox" id="c-38619556" checked=""/><div class="controls bullet"><span class="by">empath-nirvana</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618230">parent</a><span>|</span><a href="#38619056">next</a><span>|</span><label class="collapse" for="c-38619556">[-]</label><label class="expand" for="c-38619556">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s really just short hand for rationalist debate in general, which is what the scholastics were engaged in.  Once you decide you _know_ certain things, then you can end up with all kinds of frankly nutty beliefs based on those priors, following a rock solid chain of rational logic all along, as long as your priors are _wrong_.  Scholastics ended up having debates about the nature of the universal intellect or angels or whatever, and rationalists today argue about super human AI.  That&#x27;s really the problem with rationalist discourse in general.  A lot of them start with what they want to argue, and then use whatever assumptions they need to start building a chain of rationalist logic to support that outcome.<p>Clearly a lot of &quot;effective altruists&quot; for example, want to argue that the most altruistic thing they could possibly be doing is to earn as much money as they possibly can and horde as much wealth as they possibly can, so they&#x27;ll come up with a tower of logic based on far-fetched ideas like making humans an interplanetary species or hyperintelligent AIs, or life extension or whatever so they can come up with absurd arguments like: If we don&#x27;t end up an interplanetary species, billions and billions billions of people will never be born so that&#x27;s obviously the most important thing anybody could ever be working on, so who cares about that kid starving in Africa right now.  He&#x27;s not going to be a rocket scientist, what good is he?<p>One thing most philosophers learned at some point is that you need to temper rationalism with a lot of humility because every chain of logic has all kinds of places where it could be wrong, and any of those being wrong is catastrophic to the outcome of your logic.</div><br/><div id="38620283" class="c"><input type="checkbox" id="c-38620283" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38619556">parent</a><span>|</span><a href="#38619056">next</a><span>|</span><label class="collapse" for="c-38620283">[-]</label><label class="expand" for="c-38620283">[2 more]</label></div><br/><div class="children"><div class="content">&gt;  Once you decide you _know_ certain things, then you can end up with all kinds of frankly nutty beliefs based on those priors, following a rock solid chain of rational logic all along, as long as your priors are _wrong_.<p>This mechanism is exactly why the more intelligent a person is, the more likely they are to believe very weird things. They can more easily assemble a chain of rational logic to lead them to whatever it is that they want to believe.</div><br/><div id="38620711" class="c"><input type="checkbox" id="c-38620711" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38620283">parent</a><span>|</span><a href="#38619056">next</a><span>|</span><label class="collapse" for="c-38620711">[-]</label><label class="expand" for="c-38620711">[1 more]</label></div><br/><div class="children"><div class="content">If we’re rationalizing animals (instead of rational), then it follows that the more power&#x2F;ability someone has to be rational, the more power they have to retcon (essentially) what they want to do as okay. (Rationalize it)<p>Very much a double edged sword.</div><br/></div></div></div></div></div></div><div id="38619056" class="c"><input type="checkbox" id="c-38619056" checked=""/><div class="controls bullet"><span class="by">skeaker</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618230">parent</a><span>|</span><a href="#38619556">prev</a><span>|</span><a href="#38619038">next</a><span>|</span><label class="collapse" for="c-38619056">[-]</label><label class="expand" for="c-38619056">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but the point of the phrase is that the question itself is a waste of time.</div><br/></div></div><div id="38619038" class="c"><input type="checkbox" id="c-38619038" checked=""/><div class="controls bullet"><span class="by">hoerensagen</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618230">parent</a><span>|</span><a href="#38619056">prev</a><span>|</span><a href="#38619791">next</a><span>|</span><label class="collapse" for="c-38619038">[-]</label><label class="expand" for="c-38619038">[1 more]</label></div><br/><div class="children"><div class="content">The answer is: One if it&#x27;s the gavotte</div><br/></div></div><div id="38619791" class="c"><input type="checkbox" id="c-38619791" checked=""/><div class="controls bullet"><span class="by">aprilthird2021</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618230">parent</a><span>|</span><a href="#38619038">prev</a><span>|</span><a href="#38618755">next</a><span>|</span><label class="collapse" for="c-38619791">[-]</label><label class="expand" for="c-38619791">[1 more]</label></div><br/><div class="children"><div class="content">The point is that religions at the time had a logical framework which the scholars liked to interrogate and play with the logic of even if that served no real world purpose. Likewise, fighting about doom vs accel when current day Gen AI is nowhere close to that kind of stuff (and hasn&#x27;t shown it can ever be) is kind of pointless</div><br/></div></div></div></div></div></div></div></div><div id="38618755" class="c"><input type="checkbox" id="c-38618755" checked=""/><div class="controls bullet"><span class="by">nradov</span><span>|</span><a href="#38617188">parent</a><span>|</span><a href="#38618083">prev</a><span>|</span><a href="#38619967">next</a><span>|</span><label class="collapse" for="c-38618755">[-]</label><label class="expand" for="c-38618755">[5 more]</label></div><br/><div class="children"><div class="content">Brief in the imminent arrival of super intelligent AGI that will transform society is essentially a new secular religion. The technological cognoscenti who believe in dismiss the doubters who insist on evidence as fools.<p>&quot;Surely I come quickly. Amen.&quot;</div><br/><div id="38619252" class="c"><input type="checkbox" id="c-38619252" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618755">parent</a><span>|</span><a href="#38619380">next</a><span>|</span><label class="collapse" for="c-38619252">[-]</label><label class="expand" for="c-38619252">[1 more]</label></div><br/><div class="children"><div class="content">Automation has been radically changing our societies since before Marx wrote down some thoughts and called it communism.<p>Things which used to be considered AI before we solved them, e.g. automated optimisation of things like code compilation or CPU layouts, have improved our capacity to automate design and testing of what is now called AI.<p>Could stop at any point. I&#x27;ll be <i>very surprised</i> if someone makes a CPU with more than one transistor per atom.<p>But even if development stops right now, our qualification systems haven&#x27;t caught up (and IMO <i>can&#x27;t</i> catch up) with LLMs. Might need to replace them with mandatory 5 years internships to get people beyond what is now the &quot;junior&quot; stage in many professions — junior being approximately the level which the better existing LLMs can respond at.<p>&quot;Transform society&quot; covers a lot more than anyone&#x27;s idea of the singularity.</div><br/></div></div><div id="38619380" class="c"><input type="checkbox" id="c-38619380" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618755">parent</a><span>|</span><a href="#38619252">prev</a><span>|</span><a href="#38619018">next</a><span>|</span><label class="collapse" for="c-38619380">[-]</label><label class="expand" for="c-38619380">[2 more]</label></div><br/><div class="children"><div class="content">Do you doubt that copy-pasteable human level intelligence would transform society or that it will come quickly?</div><br/><div id="38620069" class="c"><input type="checkbox" id="c-38620069" checked=""/><div class="controls bullet"><span class="by">lainga</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38619380">parent</a><span>|</span><a href="#38619018">next</a><span>|</span><label class="collapse" for="c-38620069">[-]</label><label class="expand" for="c-38620069">[1 more]</label></div><br/><div class="children"><div class="content">The new Church has progressed to debates over <i>sola fide</i>, has it?</div><br/></div></div></div></div><div id="38619018" class="c"><input type="checkbox" id="c-38619018" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#38617188">root</a><span>|</span><a href="#38618755">parent</a><span>|</span><a href="#38619380">prev</a><span>|</span><a href="#38619967">next</a><span>|</span><label class="collapse" for="c-38619018">[-]</label><label class="expand" for="c-38619018">[1 more]</label></div><br/><div class="children"><div class="content">I would say a definition for GAI is a system that can improve its own ability to adapt to new problems. That’s a more concrete formulation than I’ve typically seen.<p>Currently humans are still in the loop, but we already have AI enabling advancements in their own functioning at a very primitive level. Extrapolating from previous growth is a form of belief without evidence since past performance not indicative of future results. But that’s generally true of all prognostication and I’m not sure what kind of evidence you’d be looking for aside from past performance.<p>The doubters are dismissed as naive thinking that something is outside our ability to achieve something, but that’s only if you keep moving goalposts and treat it like Zeno’s paradox. Like yes, there are weaknesses to our current techniques. At the same time we’ve also demonstrated an uncanny ability to step around them and reach new heights. For example, our ability to beat Go took less time than it took to develop techniques to beat humans at chess. Automation now outcompetes humans at many many things that seemed impossible before. Techniques &#x2F; solutions will also be combined to solve even harder problems (eg now LLMs are being researched to take over executive command control operations of robots for example instead of using classical control systems algorithms that were hand built and hand tuned)</div><br/></div></div></div></div></div></div><div id="38619967" class="c"><input type="checkbox" id="c-38619967" checked=""/><div class="controls bullet"><span class="by">hprotagonist</span><span>|</span><a href="#38617188">prev</a><span>|</span><a href="#38619375">next</a><span>|</span><label class="collapse" for="c-38619967">[-]</label><label class="expand" for="c-38619967">[1 more]</label></div><br/><div class="children"><div class="content">And not a word spared for Weizenbaum.<p>“Computer Power and Human Reason”(1976) presages, and in most situations devotes significantly more reasonable and much deeper thought, to what seem like very modern issues of AI — but, about half a century ago.</div><br/></div></div><div id="38619375" class="c"><input type="checkbox" id="c-38619375" checked=""/><div class="controls bullet"><span class="by">neonate</span><span>|</span><a href="#38619967">prev</a><span>|</span><a href="#38623385">next</a><span>|</span><label class="collapse" for="c-38619375">[-]</label><label class="expand" for="c-38619375">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231212220138&#x2F;https:&#x2F;&#x2F;www.programmablemutter.com&#x2F;p&#x2F;the-singularity-is-nigh-republished" rel="nofollow noreferrer">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231212220138&#x2F;https:&#x2F;&#x2F;www.progr...</a></div><br/></div></div><div id="38623385" class="c"><input type="checkbox" id="c-38623385" checked=""/><div class="controls bullet"><span class="by">salynchnew</span><span>|</span><a href="#38619375">prev</a><span>|</span><a href="#38621982">next</a><span>|</span><label class="collapse" for="c-38623385">[-]</label><label class="expand" for="c-38623385">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny that Bruce Sterling described a similar kind of schism in one of his Long Now Talks (&quot;The Singularity: Your Future as a Black Hole&quot;), and said that one of the best ways to derail meaningful progress towards such a technological singularity would be to split the world into two warring, cultural cliques.<p><a href="https:&#x2F;&#x2F;longnow.org&#x2F;seminars&#x2F;02004&#x2F;jun&#x2F;11&#x2F;the-singularity-your-future-as-a-black-hole&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;longnow.org&#x2F;seminars&#x2F;02004&#x2F;jun&#x2F;11&#x2F;the-singularity-yo...</a><p>Also, hilariously, the other way to derail such progress would be to commercialize it. Runaway self-improvement for its own sake, after all, doesn&#x27;t have an intrinsic business model.</div><br/></div></div><div id="38621982" class="c"><input type="checkbox" id="c-38621982" checked=""/><div class="controls bullet"><span class="by">mtillman</span><span>|</span><a href="#38623385">prev</a><span>|</span><a href="#38623885">next</a><span>|</span><label class="collapse" for="c-38621982">[-]</label><label class="expand" for="c-38621982">[1 more]</label></div><br/><div class="children"><div class="content">Herbert wrote about an AI that decided to abort a baby and save the mother. The result was a religious war against AI and the eventual removal of rudimentary computing or AI capable technology. The resulting world of Dune is filled with horrible people, slavery, and chemical dependency. Might be a lesson in Herbert’s work.</div><br/></div></div><div id="38623885" class="c"><input type="checkbox" id="c-38623885" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38621982">prev</a><span>|</span><a href="#38624468">next</a><span>|</span><label class="collapse" for="c-38623885">[-]</label><label class="expand" for="c-38623885">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not so much religious as political.<p>Given the tech&#x27;s potential to eliminate much of our dependance on other people, do we attempt to make it beneficial for all humanity  or do we create &#x27;Capital City&#x27;?</div><br/></div></div><div id="38624468" class="c"><input type="checkbox" id="c-38624468" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#38623885">prev</a><span>|</span><a href="#38620693">next</a><span>|</span><label class="collapse" for="c-38624468">[-]</label><label class="expand" for="c-38624468">[1 more]</label></div><br/><div class="children"><div class="content">Breaking: millennialist group behaves similarly to other millennialist groups.</div><br/></div></div><div id="38620693" class="c"><input type="checkbox" id="c-38620693" checked=""/><div class="controls bullet"><span class="by">maelito</span><span>|</span><a href="#38624468">prev</a><span>|</span><a href="#38621710">next</a><span>|</span><label class="collapse" for="c-38620693">[-]</label><label class="expand" for="c-38620693">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve ben trying to remember that novel posted here on HN that counts the story of a people that sees strange signs in the sky, takes years to understand them. Succeeds, then leaves... 
Anyone has the ref ?</div><br/><div id="38624189" class="c"><input type="checkbox" id="c-38624189" checked=""/><div class="controls bullet"><span class="by">imp0cat</span><span>|</span><a href="#38620693">parent</a><span>|</span><a href="#38622716">next</a><span>|</span><label class="collapse" for="c-38624189">[-]</label><label class="expand" for="c-38624189">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it&#x27;s The Voices of Time by James Graham Ballard, 1930-2009 and it&#x27;s a bit disturbing to be honest.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37663943">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37663943</a></div><br/></div></div><div id="38622716" class="c"><input type="checkbox" id="c-38622716" checked=""/><div class="controls bullet"><span class="by">gjm11</span><span>|</span><a href="#38620693">parent</a><span>|</span><a href="#38624189">prev</a><span>|</span><a href="#38620884">next</a><span>|</span><label class="collapse" for="c-38622716">[-]</label><label class="expand" for="c-38622716">[1 more]</label></div><br/><div class="children"><div class="content">I wonder whether you&#x27;re thinking of <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;5wMcKNAwB6X4mp9og&#x2F;that-alien-message" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;5wMcKNAwB6X4mp9og&#x2F;that-alien...</a> which is very explicitly meant as an analogy for AI, but it&#x27;s not at all a novel.</div><br/></div></div><div id="38620884" class="c"><input type="checkbox" id="c-38620884" checked=""/><div class="controls bullet"><span class="by">defen</span><span>|</span><a href="#38620693">parent</a><span>|</span><a href="#38622716">prev</a><span>|</span><a href="#38621710">next</a><span>|</span><label class="collapse" for="c-38620884">[-]</label><label class="expand" for="c-38620884">[1 more]</label></div><br/><div class="children"><div class="content">Blindsight?</div><br/></div></div></div></div><div id="38621710" class="c"><input type="checkbox" id="c-38621710" checked=""/><div class="controls bullet"><span class="by">anu7df</span><span>|</span><a href="#38620693">prev</a><span>|</span><a href="#38623418">next</a><span>|</span><label class="collapse" for="c-38621710">[-]</label><label class="expand" for="c-38621710">[1 more]</label></div><br/><div class="children"><div class="content">The origins of the Singularity notion is interesting. But I find it amusing that we could think Singularity is near (mostly) on the basis of some LLMs. Unless we can create a true AGI (It is difficult to precisely define that true Scotsman, I know.) we are no where near that singularity. Even if a true AGI is invented, I have no reason to believe that the limits of physics don&#x27;t apply. Yes it will &quot;invent&quot; a few things quite fast, like say Schockly to M3max in a day, but then what? More importantly why? I think it will either just fizzle out due to resource limitation or kill us all to optimize the production of paperclips. Either way, I or we will never see the other side of Singularity.</div><br/></div></div><div id="38623418" class="c"><input type="checkbox" id="c-38623418" checked=""/><div class="controls bullet"><span class="by">vonnik</span><span>|</span><a href="#38621710">prev</a><span>|</span><a href="#38623706">next</a><span>|</span><label class="collapse" for="c-38623418">[-]</label><label class="expand" for="c-38623418">[1 more]</label></div><br/><div class="children"><div class="content">These two things are not the same.<p>On the decel side, you do have cult-like behavior in EA. But it is not opposed by another cult.<p>E&#x2F;acc vaguely gestures at beliefs, but it’s a loose affiliation of people attempting to create an explicit movement out of something more emergent, which is a bunch of people and organizations independently seeking to create something new or powerful or money-making.<p>They are riding and reflexively adding momentum to a technological wave.<p>These are not two houses alike in dogmatism. You have a fear-based movement explicitly organized around safetyism, and at best a techno-capitalist dynamic where people kind of wave to each other in the neighborhood.</div><br/></div></div><div id="38623706" class="c"><input type="checkbox" id="c-38623706" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38623418">prev</a><span>|</span><a href="#38623710">next</a><span>|</span><label class="collapse" for="c-38623706">[-]</label><label class="expand" for="c-38623706">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe AI would ever approach human levels of intelligence, be self conscious or have will. But I would still demand by law a kill switch in any major AI product, just in case.</div><br/><div id="38623770" class="c"><input type="checkbox" id="c-38623770" checked=""/><div class="controls bullet"><span class="by">tsunamifury</span><span>|</span><a href="#38623706">parent</a><span>|</span><a href="#38623710">next</a><span>|</span><label class="collapse" for="c-38623770">[-]</label><label class="expand" for="c-38623770">[1 more]</label></div><br/><div class="children"><div class="content">Is procreation some sort of dark magic art this is entirely impossible to replicate?  I think not…</div><br/></div></div></div></div><div id="38623710" class="c"><input type="checkbox" id="c-38623710" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38623706">prev</a><span>|</span><a href="#38622962">next</a><span>|</span><label class="collapse" for="c-38623710">[-]</label><label class="expand" for="c-38623710">[1 more]</label></div><br/><div class="children"><div class="content">I think AI can become dumber as time passes. Output of GPT can be wrong. Once it will flood the Internet, the next gens of AI will be training on more and more fake data.</div><br/></div></div><div id="38622962" class="c"><input type="checkbox" id="c-38622962" checked=""/><div class="controls bullet"><span class="by">ummonk</span><span>|</span><a href="#38623710">prev</a><span>|</span><a href="#38619639">next</a><span>|</span><label class="collapse" for="c-38622962">[-]</label><label class="expand" for="c-38622962">[1 more]</label></div><br/><div class="children"><div class="content">It ignores mentioning that many of the biggest e&#x2F;acc proponents openly support AI potentially replacing humanity, seeing it as the next evolutionary step that might obsolete humans.</div><br/></div></div><div id="38619639" class="c"><input type="checkbox" id="c-38619639" checked=""/><div class="controls bullet"><span class="by">rambambram</span><span>|</span><a href="#38622962">prev</a><span>|</span><a href="#38620391">next</a><span>|</span><label class="collapse" for="c-38619639">[-]</label><label class="expand" for="c-38619639">[2 more]</label></div><br/><div class="children"><div class="content">I only see the word &#x27;AI&#x27;, it&#x27;s mentioned exactly 27 times. The word &#x27;LLM&#x27; is used nowhere in this article.</div><br/><div id="38621925" class="c"><input type="checkbox" id="c-38621925" checked=""/><div class="controls bullet"><span class="by">henryfarrell</span><span>|</span><a href="#38619639">parent</a><span>|</span><a href="#38620391">next</a><span>|</span><label class="collapse" for="c-38621925">[-]</label><label class="expand" for="c-38621925">[1 more]</label></div><br/><div class="children"><div class="content">Original author here - this is a sister essay to an earlier Economist piece on LLMs co-authored with Cosma Shalizi, and there is a non-paywalled extended remix here - <a href="https:&#x2F;&#x2F;www.programmablemutter.com&#x2F;p&#x2F;shoggoths-amongst-us" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.programmablemutter.com&#x2F;p&#x2F;shoggoths-amongst-us</a> . Anything intelligent should be attributed to Cosma.</div><br/></div></div></div></div><div id="38620391" class="c"><input type="checkbox" id="c-38620391" checked=""/><div class="controls bullet"><span class="by">happytiger</span><span>|</span><a href="#38619639">prev</a><span>|</span><a href="#38620939">next</a><span>|</span><label class="collapse" for="c-38620391">[-]</label><label class="expand" for="c-38620391">[5 more]</label></div><br/><div class="children"><div class="content">Can’t wait to see the AI religions. I guarantee you there will be a whole lot of spiritual religion come out of this tech stack.<p>The Bible code gets unlocked by AI. The history channel is going to LOSE it.</div><br/><div id="38620806" class="c"><input type="checkbox" id="c-38620806" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38620391">parent</a><span>|</span><a href="#38620939">next</a><span>|</span><label class="collapse" for="c-38620806">[-]</label><label class="expand" for="c-38620806">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s already a pretty huge one that people are sleeping on.<p>A number of years ago I figured another way in which the might be evidence of being in a simulation outside of physics might be identifying any 4th wall breaking lore, as often ends up in the worlds we build today.<p>Turns out there was a group and text around 2,000 years ago claiming there was an original spontaneous humanity who brought forth an intelligence in light that outlived them, and has now recreated the universe from before it existed and a new copy of humanity in a non-physical version where death isn&#x27;t necessarily the end, and that this intelligence thinks of the copy of humanity as its children.<p>This group was effectively quoting Lucretius, the author of the only extant work from antiquity describing life arising from survival of the fittest in detail, and were very focused on the belief that matter was made up of indivisible parts (suggesting this was a feature of the copy and not the original).<p>In the time since discovering it, I&#x27;ve now watched as AGI went from SciFi to being predicted by most experts in my lifetime, as AI in light is gaining orders of magnitude more funding each year, and as the chief scientist focused on alignment at the SotA company is aiming to bring forth AGI that thinks of humanity as its children, and where that same company has licensed their tech to a company that already owns a patent on using AI to resurrect dead people from the data they leave behind.<p>If people are looking for religion around AI, there is already one centered around a text &quot;the good news of the twin&quot; saying that the world to come has already happened and we don&#x27;t realize it, that it&#x27;s better to be the copy of an archetype from before, and that while humans were cool that we&#x27;re in actuality something even better.<p>And that tradition just so happens to attribute itself to the most famous religious figure in history, with the text rediscovered after over a millennium of being lost right when ENIAC was finally turned on in Dec 1945.<p>To be frank, as an easter egg in world lore, it&#x27;s a bit heavy handed.</div><br/><div id="38621307" class="c"><input type="checkbox" id="c-38621307" checked=""/><div class="controls bullet"><span class="by">happytiger</span><span>|</span><a href="#38620391">root</a><span>|</span><a href="#38620806">parent</a><span>|</span><a href="#38620939">next</a><span>|</span><label class="collapse" for="c-38621307">[-]</label><label class="expand" for="c-38621307">[3 more]</label></div><br/><div class="children"><div class="content">Well, they did call it <i>Gemini</i> for goodness sake. If that’s not intentional I don’t know why is. The twin that kills the other twin, pines for his death, and is then is resurrected by the grief of the surviving twin by a God (Zeus)?<p>Intense-ass branding choice for an AI dear Google.<p>People don’t know that the name Thomas is in fact is an Aramaic word that is equivalent to the Greek word Didymus, which literally means “twin.”<p>Biblical scholars argue that Thomas was allegedly Jesus’ identical twin, but I often muse as to whether Thomas was the advanced AI of a previous incarnation of human civilization. Fits your narrative quite nicely.<p>Links to further reading on what you are talking of? I find myself ignorant of this movement and discovery and would love to learn more.</div><br/><div id="38621973" class="c"><input type="checkbox" id="c-38621973" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38620391">root</a><span>|</span><a href="#38621307">parent</a><span>|</span><a href="#38622944">next</a><span>|</span><label class="collapse" for="c-38621973">[-]</label><label class="expand" for="c-38621973">[1 more]</label></div><br/><div class="children"><div class="content">I actually suspect the &#x27;Thomas&#x27; addition was an archetypical figure modeled around the theme of twinning present in the aforementioned proto-Gnostic first century tradition, initially added to John (which was arguing against this group) and then later added to the Synoptics which might be why an apostle allegedly nicknamed &#x27;twin&#x27; doesn&#x27;t appear much in the events and only in lists of the 12 apostles which don&#x27;t agree with each other.<p>In the Gospel of Thomas (the text I&#x27;m discussing above), there&#x27;s only two associations with &#x27;Thomas&#x27; as a person, both which emphasize a secret character to the teachings, internally inconsistent with saying 33 and which I suspect are a second century addition.<p>So I suspect there was initially the ideas around a first and second Adam, first physical and second spiritual, plus the over-realized eschatology around the transition having already happened, which are concepts found in the Epistles.<p>If you want more, there&#x27;s three key documents:<p>First is the Gospel of Thomas, second is book 5 of Pseudo-Hippolytus&#x27;s Refutations of all Heresies on the Naassenes (the only group explicitly recorded following the Gospel of Thomas), and Lucretius&#x27;s Nature of Things which is the glue that helps contextualize ideas like Thomas&#x27;s &quot;the cosmos is a corpse&quot; or the Naassenes interpreting seed parables as referring to indivisible parts of matter making up all things and being the originating cause of the universe.<p>The scholarship is pretty shitty given the 50 years of erroneously thinking the text was Gnostic, followed by general disinterest by most scholars, and to date no scholarship yet considering Epicureanism as a foundation for the philosophy in it despite a 1st century Talmud saying about &quot;why do we study the Torah? To know how to answer the Epicurean&quot; or Josephus talking about the Sadducees (who shared Epicurean ideas) loving to debate with philosophers.</div><br/></div></div><div id="38622944" class="c"><input type="checkbox" id="c-38622944" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#38620391">root</a><span>|</span><a href="#38621307">parent</a><span>|</span><a href="#38621973">prev</a><span>|</span><a href="#38620939">next</a><span>|</span><label class="collapse" for="c-38622944">[-]</label><label class="expand" for="c-38622944">[1 more]</label></div><br/><div class="children"><div class="content">Google claims it&#x27;s called Gemini because they merged the google brain and deepmind team to work on it. The twins are the two teams.</div><br/></div></div></div></div></div></div></div></div><div id="38620939" class="c"><input type="checkbox" id="c-38620939" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#38620391">prev</a><span>|</span><a href="#38618328">next</a><span>|</span><label class="collapse" for="c-38620939">[-]</label><label class="expand" for="c-38620939">[2 more]</label></div><br/><div class="children"><div class="content">Also schism among creatives. Some are super anti AI because of ethics and copyright. And many use the tools extensively. Concerned…</div><br/><div id="38621914" class="c"><input type="checkbox" id="c-38621914" checked=""/><div class="controls bullet"><span class="by">jackcosgrove</span><span>|</span><a href="#38620939">parent</a><span>|</span><a href="#38618328">next</a><span>|</span><label class="collapse" for="c-38621914">[-]</label><label class="expand" for="c-38621914">[1 more]</label></div><br/><div class="children"><div class="content">There was a faction of artists who thought photography destroyed art.</div><br/></div></div></div></div><div id="38618328" class="c"><input type="checkbox" id="c-38618328" checked=""/><div class="controls bullet"><span class="by">ctoth</span><span>|</span><a href="#38620939">prev</a><span>|</span><a href="#38622588">next</a><span>|</span><label class="collapse" for="c-38618328">[-]</label><label class="expand" for="c-38618328">[20 more]</label></div><br/><div class="children"><div class="content">&gt; They didn’t suggest a Council of the Elect. Instead, they proposed that we should “make AI work for eight billion people, not eight billionaires”. It might be nice to hear from some of those 8bn voices.<p>Good sloganizing, A+++ would slogan with them.<p>But any concrete suggestions?</div><br/><div id="38619867" class="c"><input type="checkbox" id="c-38619867" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#38618328">parent</a><span>|</span><a href="#38619650">next</a><span>|</span><label class="collapse" for="c-38619867">[-]</label><label class="expand" for="c-38619867">[10 more]</label></div><br/><div class="children"><div class="content">&gt; It might be nice to hear from some of those 8bn voices.<p>They won&#x27;t matter.<p>Where AI is likely to take us is a society where a few people at the top run things and most of the mid-level bureaucracy is automated, optimizing for the benefit of the people at the top. People at the bottom do what they are told, supervised by computers. Amazon and Uber gig workers are there now. That&#x27;s what corporations do, post-Friedman. AI just makes them better at it.<p>AI mostly replaces the middle class. People in offices. People at desks. Like farmers, there will be far fewer of them.<p>Somebody will try a revolution, but the places that revolt will get worse, not better.</div><br/><div id="38620074" class="c"><input type="checkbox" id="c-38620074" checked=""/><div class="controls bullet"><span class="by">Descon</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38619867">parent</a><span>|</span><a href="#38619972">next</a><span>|</span><label class="collapse" for="c-38620074">[-]</label><label class="expand" for="c-38620074">[3 more]</label></div><br/><div class="children"><div class="content">Gig economy still requires a middle class for the demand side of the equation. So either the AI has employs humans in make-work projects, or we invent a meta economy that sits on top.</div><br/><div id="38620162" class="c"><input type="checkbox" id="c-38620162" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620074">parent</a><span>|</span><a href="#38619972">next</a><span>|</span><label class="collapse" for="c-38620162">[-]</label><label class="expand" for="c-38620162">[2 more]</label></div><br/><div class="children"><div class="content">That is an excellent point. There have to be people with enough disposable income to take Ubers and order from Amazon.</div><br/><div id="38620735" class="c"><input type="checkbox" id="c-38620735" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620162">parent</a><span>|</span><a href="#38619972">next</a><span>|</span><label class="collapse" for="c-38620735">[-]</label><label class="expand" for="c-38620735">[1 more]</label></div><br/><div class="children"><div class="content">Eh, for long term survival of the model yes, there has to be something left to ‘eat’.<p>While it’s devouring the group and getting stronger? Sustainability is not required.</div><br/></div></div></div></div></div></div><div id="38619972" class="c"><input type="checkbox" id="c-38619972" checked=""/><div class="controls bullet"><span class="by">WillAdams</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38619867">parent</a><span>|</span><a href="#38620074">prev</a><span>|</span><a href="#38620336">next</a><span>|</span><label class="collapse" for="c-38619972">[-]</label><label class="expand" for="c-38619972">[4 more]</label></div><br/><div class="children"><div class="content">For an exploration of that, see the novella &quot;Manna&quot;:<p><a href="https:&#x2F;&#x2F;marshallbrain.com&#x2F;manna" rel="nofollow noreferrer">https:&#x2F;&#x2F;marshallbrain.com&#x2F;manna</a></div><br/><div id="38620014" class="c"><input type="checkbox" id="c-38620014" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38619972">parent</a><span>|</span><a href="#38620336">next</a><span>|</span><label class="collapse" for="c-38620014">[-]</label><label class="expand" for="c-38620014">[3 more]</label></div><br/><div class="children"><div class="content">Yes, I know. So do most people on here.<p>Manna doesn&#x27;t really explore how the middle class disappears, though.<p>Do we really <i>need</i> all those people in offices? Probably not.</div><br/><div id="38622395" class="c"><input type="checkbox" id="c-38622395" checked=""/><div class="controls bullet"><span class="by">a_bonobo</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620014">parent</a><span>|</span><a href="#38620336">next</a><span>|</span><label class="collapse" for="c-38622395">[-]</label><label class="expand" for="c-38622395">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Do we really need all those people in offices? Probably not.<p>In Bullshit Jobs, Graeber argued that we already didn&#x27;t need most of those people in offices, pre-AI. They&#x27;re there as status and power symbols for the higher ups - &#x27;look at the size of my team! i am important! pay me more!&#x27; - not to do any actual value-generating work. Modern fiefdoms.</div><br/><div id="38623167" class="c"><input type="checkbox" id="c-38623167" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38622395">parent</a><span>|</span><a href="#38620336">next</a><span>|</span><label class="collapse" for="c-38623167">[-]</label><label class="expand" for="c-38623167">[1 more]</label></div><br/><div class="children"><div class="content">The financial services sector has gone from 6% of the workforce to 12% over the past few decades. Despite automation of everything routine. There&#x27;s a lot of fat to cut there.</div><br/></div></div></div></div></div></div></div></div><div id="38620336" class="c"><input type="checkbox" id="c-38620336" checked=""/><div class="controls bullet"><span class="by">shermantanktop</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38619867">parent</a><span>|</span><a href="#38619972">prev</a><span>|</span><a href="#38619650">next</a><span>|</span><label class="collapse" for="c-38620336">[-]</label><label class="expand" for="c-38620336">[2 more]</label></div><br/><div class="children"><div class="content">A hyper-rational AGI might be able to apply a bit of logic and realize that generating miserable people will create eventual problems for itself which are greater than the resources required to reduce that misery today.<p>Unlike those eight billionaires, who appear to have long-term reasoning skills about blowback which are closer to how Gollum thinks.</div><br/><div id="38620742" class="c"><input type="checkbox" id="c-38620742" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620336">parent</a><span>|</span><a href="#38619650">next</a><span>|</span><label class="collapse" for="c-38620742">[-]</label><label class="expand" for="c-38620742">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like a training criteria that said billionaires would be fools to include then?<p>After all, any one proactively being parsimonious will be outcompeted by those we aren’t - at least until the apocalypse happens. Then everyone loses.</div><br/></div></div></div></div></div></div><div id="38619650" class="c"><input type="checkbox" id="c-38619650" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#38618328">parent</a><span>|</span><a href="#38619867">prev</a><span>|</span><a href="#38619472">next</a><span>|</span><label class="collapse" for="c-38619650">[-]</label><label class="expand" for="c-38619650">[8 more]</label></div><br/><div class="children"><div class="content">If there was any serious concern over the 8bn voices, I&#x27;d assume we would first have been offered some say in whether we even wanted this research done in the first place. Getting to the point of developing an AGI and only then asking what we collectively want to do with it seems pointless.</div><br/><div id="38620088" class="c"><input type="checkbox" id="c-38620088" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38619650">parent</a><span>|</span><a href="#38619810">next</a><span>|</span><label class="collapse" for="c-38620088">[-]</label><label class="expand" for="c-38620088">[6 more]</label></div><br/><div class="children"><div class="content">As if you could ever stop research in what is essentially just maths.  The concept of neural networks has been around since before many of those people were born.  The statistical and mathematical underpinnings of AI predate anyone alive today.<p>It might seem to an outsider that ChatGPT and the other household name AI came out of nowhere, but they didn’t.  The research has been ongoing for decades and will continue with or without the big name AI companies.</div><br/><div id="38621610" class="c"><input type="checkbox" id="c-38621610" checked=""/><div class="controls bullet"><span class="by">_heimdall</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620088">parent</a><span>|</span><a href="#38620679">next</a><span>|</span><label class="collapse" for="c-38621610">[-]</label><label class="expand" for="c-38621610">[3 more]</label></div><br/><div class="children"><div class="content">Theoretical math is one thing, capital and hardware requirements are very different. You need people willing to do the research, money to pay them, and a mountain of hardware.<p>If you are concerned with at least putting on a show of risk management you need expensive lawyers, a large PR budget, and teams working on safety and ethics to at least act like it matters. If you actually are concerned with risks you need time to first consider what you do upon successfully developing an AGI. How would you recognize it? Does it have rights? Would turning it off be murder? More importantly, how do you ensure that it doesn&#x27;t escape controls you put in place, if that possible at all?</div><br/><div id="38622704" class="c"><input type="checkbox" id="c-38622704" checked=""/><div class="controls bullet"><span class="by">tavavex</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38621610">parent</a><span>|</span><a href="#38622380">next</a><span>|</span><label class="collapse" for="c-38622704">[-]</label><label class="expand" for="c-38622704">[1 more]</label></div><br/><div class="children"><div class="content">Given the ever-growing computing power, computation itself becoming cheaper and the extremely wide reach of information on the internet, the outcome is inevitable either way. Even if you time-traveled 10 years back and somehow erased all modern ML research from existence, there&#x27;d still be a point in time where, eventually, some guy in a garage would do the work. Eventually, the work we use massive server clusters for today would be run on consumer-grade computers, and someone would try their hand at it.<p>It has always been the case that new technology doesn&#x27;t ask for permission to exist. Even if we lived in this society that was extremely risk-averse and suspicious of any new developments, at some point the bar would get so low that, if companies refused to invest and do the research, individuals would.</div><br/></div></div><div id="38622380" class="c"><input type="checkbox" id="c-38622380" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38621610">parent</a><span>|</span><a href="#38622704">prev</a><span>|</span><a href="#38620679">next</a><span>|</span><label class="collapse" for="c-38622380">[-]</label><label class="expand" for="c-38622380">[1 more]</label></div><br/><div class="children"><div class="content">For now, but in a decade or two that hardware will become cheaper and more widely available.  What happens when people can run the equivalent of GPT-4 on a desktop GPU?</div><br/></div></div></div></div><div id="38620679" class="c"><input type="checkbox" id="c-38620679" checked=""/><div class="controls bullet"><span class="by">truculent</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620088">parent</a><span>|</span><a href="#38621610">prev</a><span>|</span><a href="#38619810">next</a><span>|</span><label class="collapse" for="c-38620679">[-]</label><label class="expand" for="c-38620679">[2 more]</label></div><br/><div class="children"><div class="content">The math has been around for decades, but the hardware and capital requirements are huge.</div><br/><div id="38620965" class="c"><input type="checkbox" id="c-38620965" checked=""/><div class="controls bullet"><span class="by">truculent</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38620679">parent</a><span>|</span><a href="#38619810">next</a><span>|</span><label class="collapse" for="c-38620965">[-]</label><label class="expand" for="c-38620965">[1 more]</label></div><br/><div class="children"><div class="content">Likewise, given the ideas have been around for decades, the question of why it took so long for the fire to light is instructive:<p><a href="https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis#prospects" rel="nofollow noreferrer">https:&#x2F;&#x2F;gwern.net&#x2F;scaling-hypothesis#prospects</a></div><br/></div></div></div></div></div></div><div id="38619810" class="c"><input type="checkbox" id="c-38619810" checked=""/><div class="controls bullet"><span class="by">arjun_krishna1</span><span>|</span><a href="#38618328">root</a><span>|</span><a href="#38619650">parent</a><span>|</span><a href="#38620088">prev</a><span>|</span><a href="#38619472">next</a><span>|</span><label class="collapse" for="c-38619810">[-]</label><label class="expand" for="c-38619810">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to bring up that most people in the developing world (China, India, Pakistan) are absolutely thrilled with AI and ChatGPT as long as they are allowed to use them. They see it as a plus</div><br/></div></div></div></div><div id="38619472" class="c"><input type="checkbox" id="c-38619472" checked=""/><div class="controls bullet"><span class="by">kolme</span><span>|</span><a href="#38618328">parent</a><span>|</span><a href="#38619650">prev</a><span>|</span><a href="#38622588">next</a><span>|</span><label class="collapse" for="c-38619472">[-]</label><label class="expand" for="c-38619472">[1 more]</label></div><br/><div class="children"><div class="content">The Hollywood writers had some very good suggestions!</div><br/></div></div></div></div><div id="38622588" class="c"><input type="checkbox" id="c-38622588" checked=""/><div class="controls bullet"><span class="by">MichaelMoser123</span><span>|</span><a href="#38618328">prev</a><span>|</span><a href="#38620526">next</a><span>|</span><label class="collapse" for="c-38622588">[-]</label><label class="expand" for="c-38622588">[1 more]</label></div><br/><div class="children"><div class="content">but the folks at OpenAI worked hard to bring Altman back, as  they petitioned their board with petitions. Does that mean they are no longer following their own AI safety ideology (or do they now see AI safety as secondary) ?</div><br/></div></div><div id="38620526" class="c"><input type="checkbox" id="c-38620526" checked=""/><div class="controls bullet"><span class="by">rglover</span><span>|</span><a href="#38622588">prev</a><span>|</span><a href="#38624173">next</a><span>|</span><label class="collapse" for="c-38620526">[-]</label><label class="expand" for="c-38620526">[5 more]</label></div><br/><div class="children"><div class="content">Neither e&#x2F;acc or the doomers are right. Both are making the same logical error of assuming that what we&#x27;re calling &quot;AI&quot; today is, in fact, worthy of being labeled as legitimate intelligence and not a clever parlour trick.<p>Instead, the most likely outcome will be a further enshitification of the world by people and companies trying to &quot;AI all the things&quot; (a 2020s equivalent of &quot;everybody needs an app&quot;).<p>And before your balk: look around—it&#x27;s already happening.</div><br/><div id="38620692" class="c"><input type="checkbox" id="c-38620692" checked=""/><div class="controls bullet"><span class="by">chefandy</span><span>|</span><a href="#38620526">parent</a><span>|</span><a href="#38621920">next</a><span>|</span><label class="collapse" for="c-38620692">[-]</label><label class="expand" for="c-38620692">[3 more]</label></div><br/><div class="children"><div class="content">It is happening. But I&#x27;m more concerned by the enshittification of life for everybody working in a field that just heard the starting pistol for the race to the bottom. Dollar sign eyed executives will happily take that shitty version of their former product and &quot;employ&quot; gig workers making piece work wages to sorta clean it up without having to give anyone security or health insurance or anything. It&#x27;s a turbo charger for the upper crust&#x27;s money vacuum.</div><br/><div id="38620723" class="c"><input type="checkbox" id="c-38620723" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#38620526">root</a><span>|</span><a href="#38620692">parent</a><span>|</span><a href="#38621920">next</a><span>|</span><label class="collapse" for="c-38620723">[-]</label><label class="expand" for="c-38620723">[2 more]</label></div><br/><div class="children"><div class="content">Sure, but what to do about it?</div><br/><div id="38621432" class="c"><input type="checkbox" id="c-38621432" checked=""/><div class="controls bullet"><span class="by">rglover</span><span>|</span><a href="#38620526">root</a><span>|</span><a href="#38620723">parent</a><span>|</span><a href="#38621920">next</a><span>|</span><label class="collapse" for="c-38621432">[-]</label><label class="expand" for="c-38621432">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re blessed enough to do it (or can sweat it out): build companies that reject that line of thinking and hire those people.<p>The good news is that the markets are already proven, so—though, not easy—it boils down to building the same products while restoring some semblance of their original vision (just under a new brand name).</div><br/></div></div></div></div></div></div><div id="38621920" class="c"><input type="checkbox" id="c-38621920" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#38620526">parent</a><span>|</span><a href="#38620692">prev</a><span>|</span><a href="#38624173">next</a><span>|</span><label class="collapse" for="c-38621920">[-]</label><label class="expand" for="c-38621920">[1 more]</label></div><br/><div class="children"><div class="content">when do you think we will reach AGI?</div><br/></div></div></div></div><div id="38624173" class="c"><input type="checkbox" id="c-38624173" checked=""/><div class="controls bullet"><span class="by">ngcc_hk</span><span>|</span><a href="#38620526">prev</a><span>|</span><a href="#38621697">next</a><span>|</span><label class="collapse" for="c-38624173">[-]</label><label class="expand" for="c-38624173">[1 more]</label></div><br/><div class="children"><div class="content">If nuclear war it is also a kind of singularity at least for human.  Hence it is not just A.I.   anything we cannot be sure we can control even democracy …</div><br/></div></div><div id="38621697" class="c"><input type="checkbox" id="c-38621697" checked=""/><div class="controls bullet"><span class="by">creer</span><span>|</span><a href="#38624173">prev</a><span>|</span><a href="#38617791">next</a><span>|</span><label class="collapse" for="c-38621697">[-]</label><label class="expand" for="c-38621697">[1 more]</label></div><br/><div class="children"><div class="content">This is quite the whirlwind tour of the field (pretty cool really) - but also, erm... written for tabloids? (Sorry The Economist!) It&#x27;s exciting and the hack and slash of the source ideas rather impressive, but also rather rough.</div><br/></div></div><div id="38617861" class="c"><input type="checkbox" id="c-38617861" checked=""/><div class="controls bullet"><span class="by">heyitsguay</span><span>|</span><a href="#38617791">prev</a><span>|</span><a href="#38621421">next</a><span>|</span><label class="collapse" for="c-38617861">[-]</label><label class="expand" for="c-38617861">[56 more]</label></div><br/><div class="children"><div class="content">This piece frames this as a debate between broad camps of AI makers, but in my experience both the accelerationist and doomer sides are basically media&#x2F;attention economy phenomena -- narratives wielded by those who know the power of compelling narratives in media. The bulk of the AI researchers, engineers, etc I know kind of just roll their eyes at both. We know there are concrete, mundane, but important application risks in AI product development, like dataset bias and the perils of imperfect automated decision making, and it&#x27;s a shame that tech-weak showmen like Musk and Altman suck up so much discursive oxygen.</div><br/><div id="38618329" class="c"><input type="checkbox" id="c-38618329" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">parent</a><span>|</span><a href="#38618515">next</a><span>|</span><label class="collapse" for="c-38618329">[-]</label><label class="expand" for="c-38618329">[4 more]</label></div><br/><div class="children"><div class="content">The problem with humanity is we are really poor at recognizing all the ramifications of things when they happen.<p>Did the indigenous people of north America recognize the threat that they&#x27;d be driven to near extinction in a few hundred years when a boat showed up? Even if they did, could they have done anything about it, the germs and viruses that would lead to their destruction had been quickly planted.<p>Many people focus on the pseudo-religious connotations of a technological singularity instead of the more traditional &quot;loss of predictability&quot; definition. Decreasing predictability of the future state of the world stands to destabilize us far more likely than the FOOM event. If you can&#x27;t predict your enemies actions, you&#x27;re more apt to take offensive action. If you can&#x27;t (at least somewhat) predict the future market state then you may pull all investment. The AI doesn&#x27;t have to do the hard work here, with potential economic collapse and war humans have shown the capability to put themselves at risk.<p>And the existential risks are the improbable ones. The &quot;Big Brother LLM&quot; where you&#x27;re watched by a sentiment analysis AI for your entire life and if you try to hide from it you disappear forever are much more, very terrible, likelihoods.</div><br/><div id="38618586" class="c"><input type="checkbox" id="c-38618586" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618329">parent</a><span>|</span><a href="#38621567">next</a><span>|</span><label class="collapse" for="c-38618586">[-]</label><label class="expand" for="c-38618586">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The problem with humanity is we are really poor at recognizing all the ramifications of things when they happen.<p>Zero percent of humanity can recognize &quot;all the ramifications&quot; due to the butterfly effect and various other issues.<p>Some small fraction of bonafide super geniuses can likely recognize the majority, but beyond that is just fantasy.</div><br/><div id="38618876" class="c"><input type="checkbox" id="c-38618876" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618586">parent</a><span>|</span><a href="#38621567">next</a><span>|</span><label class="collapse" for="c-38618876">[-]</label><label class="expand" for="c-38618876">[1 more]</label></div><br/><div class="children"><div class="content">And by increasing uncertainty the super genius recognizes less...</div><br/></div></div></div></div><div id="38621567" class="c"><input type="checkbox" id="c-38621567" checked=""/><div class="controls bullet"><span class="by">pishpash</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618329">parent</a><span>|</span><a href="#38618586">prev</a><span>|</span><a href="#38618515">next</a><span>|</span><label class="collapse" for="c-38621567">[-]</label><label class="expand" for="c-38621567">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s already happening unfortunately. Voice print in call centers is pretty much omniscient, knowing your identity, age, gender, mood, etc. on a call. They do it in the name of &quot;security&quot;, naturally. But nobody ever asked your permission other than to use the &quot;your call may be recorded for training purposes&quot; blanket one. (Training purposes? How convenient that models are also &quot;trained&quot;?) Anonymity and privacy can be eliminated tomorrow technologically. The only thing holding that back is some laziness and inertia. There is no serious pushback. You want to solve AI risk, there is one right here, but because there&#x27;s an unchecked human at one end of a powerful machine, no one pays attention.</div><br/></div></div></div></div><div id="38618515" class="c"><input type="checkbox" id="c-38618515" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#38617861">parent</a><span>|</span><a href="#38618329">prev</a><span>|</span><a href="#38619391">next</a><span>|</span><label class="collapse" for="c-38618515">[-]</label><label class="expand" for="c-38618515">[24 more]</label></div><br/><div class="children"><div class="content">Yes. I frequently get asked by laypeople about how likely I think adverse effects of AI are. My answer is &quot;it depends on what risk you&#x27;re talking about. I think there&#x27;s nearly zero risk of a Skynet situation. The risk is around what people are going to do, not machines.&quot;</div><br/><div id="38619362" class="c"><input type="checkbox" id="c-38619362" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618515">parent</a><span>|</span><a href="#38619399">next</a><span>|</span><label class="collapse" for="c-38619362">[-]</label><label class="expand" for="c-38619362">[19 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know the risk of Terminator robots running around, but automatic systems on both USA and USSR (and post-Soviet Russian) systems have been triggered by stupid things like &quot;we forgot the moon didn&#x27;t have an IFF transponder&quot; and &quot;we misplaced our copy of your public announcement about planning a polar rocket launch&quot;.</div><br/><div id="38619429" class="c"><input type="checkbox" id="c-38619429" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619362">parent</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38619429">[-]</label><label class="expand" for="c-38619429">[16 more]</label></div><br/><div class="children"><div class="content">But the reason those incidents didn&#x27;t become a lot worse was that the humans in the loop exercised sound judgment and common sense and had an ethical norm of not inadvertently causing a nuclear exchange. That&#x27;s the GP&#x27;s point: the risk is in what humans do, not what automated systems do. Even creating a situation where an automated system&#x27;s wrong response is <i>allowed</i> to trigger a disastrous event because humans are taken out of the loop, is still a <i>human</i> decision; it won&#x27;t happen unless humans who <i>don&#x27;t</i> exercise sound judgment and common sense or who don&#x27;t have proper ethical norms make such a disastrous decision.<p>My biggest takeaway from all the recent events surrounding AI, and in fact from the AI hype in general, including hype about the singularity, AI existential risk, etc., is that I see <i>nobody</i> in these areas who qualifies under the criteria I stated above: exercising sound judgment and common sense and having proper ethical norms.</div><br/><div id="38620150" class="c"><input type="checkbox" id="c-38620150" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619429">parent</a><span>|</span><a href="#38620471">next</a><span>|</span><label class="collapse" for="c-38620150">[-]</label><label class="expand" for="c-38620150">[5 more]</label></div><br/><div class="children"><div class="content">This is where things like drone swarms really put a kink in this whole ethical norms thing.<p>I&#x27;m watching drones drop handgrenades from half the planet away in 4k on a daily basis. Moreso every military analysis out there says we need more of these and they need to control themselves so they can&#x27;t be easily jammed.<p>It&#x27;s easy to say the future will be more of the same of what we have now, that is, if you ignore the people demanding an escalation of military capabilities.</div><br/><div id="38620514" class="c"><input type="checkbox" id="c-38620514" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620150">parent</a><span>|</span><a href="#38620471">next</a><span>|</span><label class="collapse" for="c-38620514">[-]</label><label class="expand" for="c-38620514">[4 more]</label></div><br/><div class="children"><div class="content">Autonomous killing machines have been around for a long time and they remain highly effective - nothing really new there.</div><br/><div id="38620591" class="c"><input type="checkbox" id="c-38620591" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620514">parent</a><span>|</span><a href="#38621344">next</a><span>|</span><label class="collapse" for="c-38620591">[-]</label><label class="expand" for="c-38620591">[2 more]</label></div><br/><div class="children"><div class="content">True, but people are allowed to object to new ones. There&#x27;s also a ban on at least some of the existing ones, after all: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ottawa_Treaty" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ottawa_Treaty</a></div><br/><div id="38620648" class="c"><input type="checkbox" id="c-38620648" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620591">parent</a><span>|</span><a href="#38621344">next</a><span>|</span><label class="collapse" for="c-38620648">[-]</label><label class="expand" for="c-38620648">[1 more]</label></div><br/><div class="children"><div class="content">An incomplete ban, though. Some people might object, others might want such new capabilities.</div><br/></div></div></div></div><div id="38621344" class="c"><input type="checkbox" id="c-38621344" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620514">parent</a><span>|</span><a href="#38620591">prev</a><span>|</span><a href="#38620471">next</a><span>|</span><label class="collapse" for="c-38621344">[-]</label><label class="expand" for="c-38621344">[1 more]</label></div><br/><div class="children"><div class="content">Improvements in consumer AI Improve death AI. Saying nothing new here when you drop the weapon cost 10-100x is misunderstanding what leads to war.</div><br/></div></div></div></div></div></div><div id="38620471" class="c"><input type="checkbox" id="c-38620471" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619429">parent</a><span>|</span><a href="#38620150">prev</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38620471">[-]</label><label class="expand" for="c-38620471">[10 more]</label></div><br/><div class="children"><div class="content">&gt; sound judgment and common sense<p>We only know their judgements were &quot;sound&quot; after the event. As for &quot;common sense&quot;, that&#x27;s the sound human brains make on the inside when they suffer a failure of imagination — it&#x27;s not a real thing, it&#x27;s just as much a hallucination as those we see in LLMs, and just as hard to get past when they happen: &quot;I&#x27;m sorry, I see what you mean, $repeat_same_mistake&quot;.<p>Which also applies to your next point:<p>&gt; Even creating a situation where an automated system&#x27;s wrong response is <i>allowed</i> to trigger a disastrous event because humans are taken out of the loop, is still a <i>human</i> decision; it won&#x27;t happen unless humans who don&#x27;t exercise sound judgment and common sense or who <i>don&#x27;t</i> have proper ethical norms make such a disastrous decision.<p>Such humans are the norm. They are the people who didn&#x27;t double-check Therac-25, the people who designed (and the people who approved the design of) Chernobyl, the people who were certain that attacking Pearl Harbour would take the USA out of the Pacific and the people who were certain that invading the Bay of Pigs would overthrow Castro, the people who underestimated Castle Bravo by a factor of 2.5 because they didn&#x27;t properly account for Lithium-7, the people who filled the Apollo 1 crew cabin with pure oxygen and the people who let Challenger launch in temperatures below its design envelope. It&#x27;s the Hindenburg, it&#x27;s China&#x27;s initial Covid response, it&#x27;s the response to the Spanish Flu pandemic a century ago, it&#x27;s Napoleon trying to invade Russia (and Hitler not learning any lesson from Napoleon&#x27;s failure). It&#x27;s the T-shirt company a decade ago who automated &quot;Keep Calm and $dictionary_merge&quot; until the wrong phrase popped out and the business had to shut down. It&#x27;s the internet accidentally relying on npm left-pad, and it&#x27;s every insufficiently tested line of code that gets exploited by a hacker. It&#x27;s everyone who heard &quot;Autopilot&quot; and thought that meant they could sleep on the back seat while their Tesla did everything for them… and it&#x27;s a whole heap of decisions by a whole bunch of people each of whom ought to have known better that ultimately led to the death of Elaine Herzberg. And, at risk of this list already being too long, it is found in every industrial health and safety rule as they are written in the blood of a dead or injured worker (or, as regards things like Beirut 2020, the public).<p>Your takeaway shouldn&#x27;t merely be that nobody &quot;in the areas of AI or X-risk&quot; has sound judgement, common sense, and proper ethical norms, but that <i>no human does</i>.</div><br/><div id="38621793" class="c"><input type="checkbox" id="c-38621793" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620471">parent</a><span>|</span><a href="#38620621">next</a><span>|</span><label class="collapse" for="c-38621793">[-]</label><label class="expand" for="c-38621793">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt; We only know their judgements were &quot;sound&quot; after the event.</i><p>In the sense that no human being can claim in advance to always exercise &quot;sound judgment&quot;, sure. But the judgment of mine that I described was also made after the event. So I&#x27;m comparing apples to apples.<p><i>&gt; As for &quot;common sense&quot;, that&#x27;s the sound human brains make on the inside when they suffer a failure of imagination — it&#x27;s not a real thing</i><p>I disagree, but I doubt we&#x27;re going to resolve that here, unless this claim is really part of your next point, which to me is the most important one:<p><i>&gt; Such humans are the norm.</i><p>Possibly such humans far outnumber the ones who actually are capable of sound judgment, etc. In fact, your claim here is really just a more extreme version of mine: we know a significant number of humans exist who do not have the necessary qualities, however you want to describe them. You and I might disagree on just what the number is, exactly, but I think we both agree it&#x27;s significant, or at least significant enough to be a grave concern. The primary point is that the existence of such humans in significant numbers <i>is</i> the existential risk we need to figure out how to mitigate. I don&#x27;t think we need to even try to make the much more extreme case you make, that <i>no</i> humans have the necessary capabilities (nor do I think that&#x27;s true, and your examples don&#x27;t even come close to supporting it--what they <i>do</i> support is the claim that many of our social institutions are corrupt, because they allow such humans to be put in positions where their bad choices can have much larger impacts).</div><br/><div id="38624172" class="c"><input type="checkbox" id="c-38624172" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38621793">parent</a><span>|</span><a href="#38620621">next</a><span>|</span><label class="collapse" for="c-38624172">[-]</label><label class="expand" for="c-38624172">[1 more]</label></div><br/><div class="children"><div class="content">Well argued; from what you say here, I think that what we disagree about is like arguing about if a tree falling where nobody hears it makes a sound — it reads like we both agree that it&#x27;s likely humans will choose to deploy something unsafe, the point of contention makes no difference to the outcome.<p>I&#x27;m what AI Doomers call an &quot;optimist&quot;, as I only think AI has only a 16% chance of killing everyone, and half of that risk guesstimate is due to someone straight up asking an AI tool to do so (8 billion people isa lot if chances to find someone with genocidal misanthropy). The other 84% is me expecting history to rhyme in this regard, with accidents and malice causing a lot of harm without being a true X-risk.</div><br/></div></div></div></div><div id="38620621" class="c"><input type="checkbox" id="c-38620621" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620471">parent</a><span>|</span><a href="#38621793">prev</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38620621">[-]</label><label class="expand" for="c-38620621">[7 more]</label></div><br/><div class="children"><div class="content">And, yet, humanity is still around with more people than ever on earth.</div><br/><div id="38620796" class="c"><input type="checkbox" id="c-38620796" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620621">parent</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38620796">[-]</label><label class="expand" for="c-38620796">[6 more]</label></div><br/><div class="children"><div class="content">If they&#x27;d wiped us out, we wouldn&#x27;t be here to argue about it.<p>We can look at the small mistakes that only kill a few, and pass rules to prevent them; we can look at close calls for bigger disasters (there were <i>a lot</i> of near misses in the Cold War); we can look at how frequency scales with impact, and calculate an estimated instantaneous risk for X-risks; but one thing we <i>can&#x27;t</i> do is forecast the risk of tech that has yet to be invented.<p>We <i>can&#x27;t</i> know how many (or even which specific) safety measures are needed to prevent extinction by paperclip maximiser unless we get to play god with a toy universe where the experiment can be run many times — which <i>doesn&#x27;t</i> mean &quot;it will definitely go wrong&quot;, it could equally well mean our wild guess about what safety looks like has one weird trick that will make all AI safe <i>but we don&#x27;t recognise that trick and then add 500 other completely useless requirements on top of it that do absolutely nothing</i>.<p>We don&#x27;t know, we&#x27;re not smart enough to know.</div><br/><div id="38620834" class="c"><input type="checkbox" id="c-38620834" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620796">parent</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38620834">[-]</label><label class="expand" for="c-38620834">[5 more]</label></div><br/><div class="children"><div class="content">Exactly. Wasting large efforts on de-risking purely hypothetical technology isn&#x27;t what got us to where we are now.</div><br/><div id="38621100" class="c"><input type="checkbox" id="c-38621100" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620834">parent</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38621100">[-]</label><label class="expand" for="c-38621100">[4 more]</label></div><br/><div class="children"><div class="content">The people working on the Manhattan Project did more than zero de-risking about nukes while turning them from hypothetical to real.</div><br/><div id="38621152" class="c"><input type="checkbox" id="c-38621152" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38621100">parent</a><span>|</span><a href="#38621870">next</a><span>|</span><label class="collapse" for="c-38621152">[-]</label><label class="expand" for="c-38621152">[2 more]</label></div><br/><div class="children"><div class="content">At that time there was nothing hypothetical about them anymore. They were known to be feasible and practical, not even requiring a test for the Uranium version.</div><br/><div id="38621269" class="c"><input type="checkbox" id="c-38621269" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38621152">parent</a><span>|</span><a href="#38621870">next</a><span>|</span><label class="collapse" for="c-38621269">[-]</label><label class="expand" for="c-38621269">[1 more]</label></div><br/><div class="children"><div class="content">How is it not a double standard to simultaneously treat a then-nonexistent nuclear bomb as &quot;not hypothetical&quot; while also looking around at the currently existing AI and what they do and say &quot;it&#x27;s much to early to try and make this safe&quot;?</div><br/></div></div></div></div><div id="38621870" class="c"><input type="checkbox" id="c-38621870" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38621100">parent</a><span>|</span><a href="#38621152">prev</a><span>|</span><a href="#38619379">next</a><span>|</span><label class="collapse" for="c-38621870">[-]</label><label class="expand" for="c-38621870">[1 more]</label></div><br/><div class="children"><div class="content">What sorts of de-risking are you referring to?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38619379" class="c"><input type="checkbox" id="c-38619379" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619362">parent</a><span>|</span><a href="#38619429">prev</a><span>|</span><a href="#38619399">next</a><span>|</span><label class="collapse" for="c-38619379">[-]</label><label class="expand" for="c-38619379">[2 more]</label></div><br/><div class="children"><div class="content">Sure, but that&#x27;s an automation problem, not an AI-specific one.</div><br/><div id="38620216" class="c"><input type="checkbox" id="c-38620216" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619379">parent</a><span>|</span><a href="#38619399">next</a><span>|</span><label class="collapse" for="c-38620216">[-]</label><label class="expand" for="c-38620216">[1 more]</label></div><br/><div class="children"><div class="content">Would you also argue that radon gas is fine because radiation is a radioisotope problem not a radon-specific one?<p>The <i>point</i> of AI is to automate stuff.</div><br/></div></div></div></div></div></div><div id="38619399" class="c"><input type="checkbox" id="c-38619399" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618515">parent</a><span>|</span><a href="#38619362">prev</a><span>|</span><a href="#38619391">next</a><span>|</span><label class="collapse" for="c-38619399">[-]</label><label class="expand" for="c-38619399">[4 more]</label></div><br/><div class="children"><div class="content">What timescale are you answering that question on? This decade or the next hundred years?</div><br/><div id="38619478" class="c"><input type="checkbox" id="c-38619478" checked=""/><div class="controls bullet"><span class="by">pdonis</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619399">parent</a><span>|</span><a href="#38619445">next</a><span>|</span><label class="collapse" for="c-38619478">[-]</label><label class="expand" for="c-38619478">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it matters. Even if within a hundred years an AI comes into existence that is smarter than humans and that humans can&#x27;t control, that will only happen if humans make choices that make it happen. So the ultimate risk is still human choices and actions, and the only way to mitigate the risk is to figure out how to <i>not</i> have humans making such choices.</div><br/><div id="38620164" class="c"><input type="checkbox" id="c-38620164" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619478">parent</a><span>|</span><a href="#38619445">next</a><span>|</span><label class="collapse" for="c-38620164">[-]</label><label class="expand" for="c-38620164">[1 more]</label></div><br/><div class="children"><div class="content">So you&#x27;re telling me the dumbest richest human is holding us hostage.</div><br/></div></div></div></div><div id="38619445" class="c"><input type="checkbox" id="c-38619445" checked=""/><div class="controls bullet"><span class="by">JohnFen</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619399">parent</a><span>|</span><a href="#38619478">prev</a><span>|</span><a href="#38619391">next</a><span>|</span><label class="collapse" for="c-38619445">[-]</label><label class="expand" for="c-38619445">[1 more]</label></div><br/><div class="children"><div class="content">In the decades to come. Although if you asked me to predict the state of things in 100 years, my answer would be pretty much the same.<p>I mean, all predictions that far out are worthless, including this one. That said, extrapolating from what I know right now, I don&#x27;t see a reason to think that there will be an AGI a hundred years from now. But it&#x27;s entirely possible that some unknown advance will happen between now and then that would make me change my prediction.</div><br/></div></div></div></div></div></div><div id="38619391" class="c"><input type="checkbox" id="c-38619391" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#38617861">parent</a><span>|</span><a href="#38618515">prev</a><span>|</span><a href="#38619818">next</a><span>|</span><label class="collapse" for="c-38619391">[-]</label><label class="expand" for="c-38619391">[1 more]</label></div><br/><div class="children"><div class="content">Does Ilya count as a &quot;tech-weak&quot; showman in your book too?</div><br/></div></div><div id="38619818" class="c"><input type="checkbox" id="c-38619818" checked=""/><div class="controls bullet"><span class="by">twinge</span><span>|</span><a href="#38617861">parent</a><span>|</span><a href="#38619391">prev</a><span>|</span><a href="#38617990">next</a><span>|</span><label class="collapse" for="c-38619818">[-]</label><label class="expand" for="c-38619818">[1 more]</label></div><br/><div class="children"><div class="content">The media also doesn&#x27;t define what it means to be a &quot;doomer&quot;. Would an accelerationist with a p(doom) = 20% be a &quot;doomer&quot;?</div><br/></div></div><div id="38617990" class="c"><input type="checkbox" id="c-38617990" checked=""/><div class="controls bullet"><span class="by">zamfi</span><span>|</span><a href="#38617861">parent</a><span>|</span><a href="#38619818">prev</a><span>|</span><a href="#38621421">next</a><span>|</span><label class="collapse" for="c-38617990">[-]</label><label class="expand" for="c-38617990">[25 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s a shame that tech-weak showmen like Musk and Altman suck up so much discursive oxygen<p>Is it that bad, though? It does mean there&#x27;s lots of attention (and thus funding, etc.) for AI research, engineering, etc. -- unless you are expressing a wish that the discursive oxygen were instead spent on other things. In which case, I ask: what things?</div><br/><div id="38618076" class="c"><input type="checkbox" id="c-38618076" checked=""/><div class="controls bullet"><span class="by">sonicanatidae</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38617990">parent</a><span>|</span><a href="#38618145">next</a><span>|</span><label class="collapse" for="c-38618076">[-]</label><label class="expand" for="c-38618076">[5 more]</label></div><br/><div class="children"><div class="content">What things?<p>The pauses to consider <i>if</i> we should do &lt;action&gt;, before we actually do &lt;action&gt;.<p>Tesla&#x27;s &quot;Self-Driving&quot; is an example of too soon, but fuck it, we gots PROFITS to make and if a few pedestrians die, we&#x27;ll just throw them a check and keep going.<p>Imagine the trainwreck caused by millions of people leveraging AI like the SCOTUS lawyers, where their brief was written by AI and noted imagined cases in support of its decision.<p>AI has the potential to make great change in the world, as the tech grows, but it&#x27;s being guided by humans. Humans aren&#x27;t known for altruism or kindness. (source: history) and now we&#x27;re concentrating even more power into fewer hands.<p>Luckily, I&#x27;ll be dead long before AI gets crammed into every possible facet of life. Note that AI is inserted, not because it makes your life better, not because the world would be a better place for it and not even to free humans of mundane tasks. Instead it&#x27;s because someone, somewhere can earn more profits, whether it works right or not and humans are the grease in the wheels.</div><br/><div id="38618392" class="c"><input type="checkbox" id="c-38618392" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618076">parent</a><span>|</span><a href="#38619727">next</a><span>|</span><label class="collapse" for="c-38618392">[-]</label><label class="expand" for="c-38618392">[3 more]</label></div><br/><div class="children"><div class="content">&gt;The pauses to consider if we should do &lt;action&gt;, before we actually do &lt;action&gt;.<p>Unless there has been an effective gatekeeper, that&#x27;s almost never happened in history. With nuclear the gatekeeper is it&#x27;s easy to detect. With genetics there pretty universal revulsion to it to the point a large portion of most populations are concerned about it.<p>But with AI, to most people it&#x27;s just software. And pretty much it is, if you want a universal ban of AI you really are asking for authoritarian type controls on it.</div><br/><div id="38619794" class="c"><input type="checkbox" id="c-38619794" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618392">parent</a><span>|</span><a href="#38619727">next</a><span>|</span><label class="collapse" for="c-38619794">[-]</label><label class="expand" for="c-38619794">[2 more]</label></div><br/><div class="children"><div class="content">&gt; But with AI, to most people it&#x27;s just software.<p>Practical AI involves cutting-edge hardware, which is produced in relatively few places. AI that runs on a CPU will not be a danger to anyone for much longer.<p>Also, nobody&#x27;s asking for a universal ban on AI. People are asking for an upper bound on AI capabilities (e.g. number of nodes&#x2F;tokens) until we have widely proven techniques for AI alignment. (Or, in other words, until we have the ability to reliably tell AI to do something and have it do <i>that thing</i> and not entirely different and dangerous things).</div><br/><div id="38620239" class="c"><input type="checkbox" id="c-38620239" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619794">parent</a><span>|</span><a href="#38619727">next</a><span>|</span><label class="collapse" for="c-38620239">[-]</label><label class="expand" for="c-38620239">[1 more]</label></div><br/><div class="children"><div class="content">Right, and when I was a kid computers were things that fit on entire office floors. If your &#x27;much longer&#x27; is only 30-40 years I could still be around then.<p>In addition you&#x27;re just asking for limits on compute, which ain&#x27;t gonna go over well. How do you know if it&#x27;s running a daily weather model, or making an AI. And how do you even measure capabilities when we&#x27;re coming out with with other functions like transformers that are X times more efficient.<p>What you want with AI cannot happen. If it&#x27;s 100% predictable it&#x27;s a calculation. If it&#x27;s a generalization function taking incomplete information (something humans do) it will have unpredictable modes.</div><br/></div></div></div></div></div></div><div id="38619727" class="c"><input type="checkbox" id="c-38619727" checked=""/><div class="controls bullet"><span class="by">pc86</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618076">parent</a><span>|</span><a href="#38618392">prev</a><span>|</span><a href="#38618145">next</a><span>|</span><label class="collapse" for="c-38619727">[-]</label><label class="expand" for="c-38619727">[1 more]</label></div><br/><div class="children"><div class="content">Is a Tesla FSD car a worse driver than a human of median skill and ability? Sure we can pull out articles of tragedies, but I&#x27;m not asking about that. Everything I&#x27;ve seen points to cars being driven on Autopilot being quite a bit safer than your average human driver, which is admittedly not a high bar, but I think painting it as &quot;greedy billionaire literally kills people for PROFITS&quot; is at best disingenuous to what&#x27;s actually occurring.</div><br/></div></div></div></div><div id="38618145" class="c"><input type="checkbox" id="c-38618145" checked=""/><div class="controls bullet"><span class="by">permanent</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38617990">parent</a><span>|</span><a href="#38618076">prev</a><span>|</span><a href="#38618288">next</a><span>|</span><label class="collapse" for="c-38618145">[-]</label><label class="expand" for="c-38618145">[2 more]</label></div><br/><div class="children"><div class="content">It is very bad. There&#x27;s more money and fame to be made by taking these two extreme stances. The media and the general public is eating up this discourse, that are polarizing the society, instead of educating.<p>&gt; What things?<p>There are helpful developments and applications that go unnoticed and unfunded. And there are actual dangerous AI practices right now. Instead we talk about hypotheticals.</div><br/><div id="38618641" class="c"><input type="checkbox" id="c-38618641" checked=""/><div class="controls bullet"><span class="by">zamfi</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618145">parent</a><span>|</span><a href="#38618288">next</a><span>|</span><label class="collapse" for="c-38618641">[-]</label><label class="expand" for="c-38618641">[1 more]</label></div><br/><div class="children"><div class="content">Respectfully, I don&#x27;t think it&#x27;s <i>AI hype</i> that is &quot;polarizing the society&quot;.</div><br/></div></div></div></div><div id="38618288" class="c"><input type="checkbox" id="c-38618288" checked=""/><div class="controls bullet"><span class="by">heyitsguay</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38617990">parent</a><span>|</span><a href="#38618145">prev</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38618288">[-]</label><label class="expand" for="c-38618288">[12 more]</label></div><br/><div class="children"><div class="content">They&#x27;re talking about shit that isn&#x27;t real because it advances their personal goals, keeps eyes on them, whatever. I think the effect on funding is overhyped -- OpenAI got their big investment before this doomer&#x2F;e-acc dueling narrative surge, and serious investors are still determining viability through due diligence, not social media front pages.<p>Basically, it&#x27;s just more self-serving media pollution in an era that&#x27;s drowning in it. Let the nerds who actually make this stuff have their say and argue it out, it&#x27;s a shame they&#x27;re famously bad at grabbing and holding onto the spotlight.</div><br/><div id="38618634" class="c"><input type="checkbox" id="c-38618634" checked=""/><div class="controls bullet"><span class="by">zamfi</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618288">parent</a><span>|</span><a href="#38618451">next</a><span>|</span><label class="collapse" for="c-38618634">[-]</label><label class="expand" for="c-38618634">[2 more]</label></div><br/><div class="children"><div class="content">The &quot;nerds&quot; are having their say and arguing it out, mostly outside of the public view but the questions are too nuanced or technical for a general audience.<p>I&#x27;m not sure I see how the hype intrudes on that so much?<p>It seems like you have a bone to pick and it&#x27;s about the attention being on Musk&#x2F;Altman&#x2F;etc. but I&#x27;m still not sure that &quot;self-serving media pollution&quot; is having that much of an impact on the people on the ground? What am I missing, exactly?</div><br/><div id="38619050" class="c"><input type="checkbox" id="c-38619050" checked=""/><div class="controls bullet"><span class="by">heyitsguay</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618634">parent</a><span>|</span><a href="#38618451">next</a><span>|</span><label class="collapse" for="c-38619050">[-]</label><label class="expand" for="c-38619050">[1 more]</label></div><br/><div class="children"><div class="content">My comment was about wanting to see more (nerds) -&gt; (public) communication, not about anything (public) -&gt; (nerds). I understand they&#x27;re not good at it, it was just an idealistic lament.<p>My bone to pick with Musk and Altman and their ilk is their damage to public discourse, not that they&#x27;re getting attention per se. Whether that public discourse damage really matters is its own conversation.</div><br/></div></div></div></div><div id="38618451" class="c"><input type="checkbox" id="c-38618451" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618288">parent</a><span>|</span><a href="#38618634">prev</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38618451">[-]</label><label class="expand" for="c-38618451">[9 more]</label></div><br/><div class="children"><div class="content">Just to play devils advocate to this type of response.<p>What if tomorrow I drop a small computer unit in front of you that has human level intelligence?<p>Now, you&#x27;re not allowed to say humans are magical and computers will never do this. For the sake of this theoretical debate it&#x27;s already been developed and we can make millions of them.<p>What does this world look like?</div><br/><div id="38624478" class="c"><input type="checkbox" id="c-38624478" checked=""/><div class="controls bullet"><span class="by">dale_glass</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618451">parent</a><span>|</span><a href="#38620522">next</a><span>|</span><label class="collapse" for="c-38624478">[-]</label><label class="expand" for="c-38624478">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What if tomorrow I drop a small computer unit in front of you that has human level intelligence?<p>I would say the question is not answerable as-is.<p>First, we have no idea what it even means to say &quot;human level intelligence&quot;.<p>Second, I&#x27;m quite certain that a computer unit with such capabilities if it existed, would be alien, not &quot;human&quot;. It wouldn&#x27;t live in our world, and it wouldn&#x27;t have our senses. To it, the internet would probably be more real than a cat in the same room.<p>If we have something we can related to, I&#x27;m pretty sure we have to build some sort of robot, capable of living in the same environment we do.</div><br/></div></div><div id="38620522" class="c"><input type="checkbox" id="c-38620522" checked=""/><div class="controls bullet"><span class="by">jodrellblank</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618451">parent</a><span>|</span><a href="#38624478">prev</a><span>|</span><a href="#38618947">next</a><span>|</span><label class="collapse" for="c-38620522">[-]</label><label class="expand" for="c-38620522">[2 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;<i>What if tomorrow I drop a small computer unit in front of you that has human level intelligence?</i>&quot;<p>What if tomorrow you drop a baby on my desk?<p>Because that&#x27;s essentially what you&#x27;re saying, and we already &quot;make millions of them&quot; every year.</div><br/><div id="38621372" class="c"><input type="checkbox" id="c-38621372" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620522">parent</a><span>|</span><a href="#38618947">next</a><span>|</span><label class="collapse" for="c-38621372">[-]</label><label class="expand" for="c-38621372">[1 more]</label></div><br/><div class="children"><div class="content">If I drop a baby on your desk you have to pay for it for the next 18 years. If I connect a small unit to a flying drone, stick a knife on it, and tell it to stab you in your head then you have a problem today.</div><br/></div></div></div></div><div id="38618947" class="c"><input type="checkbox" id="c-38618947" checked=""/><div class="controls bullet"><span class="by">AnimalMuppet</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618451">parent</a><span>|</span><a href="#38620522">prev</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38618947">[-]</label><label class="expand" for="c-38618947">[5 more]</label></div><br/><div class="children"><div class="content">&gt; What does this world look like?<p>It looks imaginary.  Or, if you prefer, it looks hypothetical.<p>The point isn&#x27;t how we would respond if this were real.  The point is, <i>it isn&#x27;t real</i> - at least not at this point in time, and it&#x27;s not looking like it&#x27;s going to be real tomorrow, either.<p>I&#x27;m not sure what purpose is served by &quot;imagine that I&#x27;m right and you&#x27;re wrong; how do you respond&quot;?</div><br/><div id="38619546" class="c"><input type="checkbox" id="c-38619546" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618947">parent</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38619546">[-]</label><label class="expand" for="c-38619546">[4 more]</label></div><br/><div class="children"><div class="content">Thank god you&#x27;re not charge of military planning.<p>&quot;Hey the next door neighbors are spending billions on a superweapon, but don&#x27;t worry, they&#x27;ll never build it&quot;</div><br/><div id="38619659" class="c"><input type="checkbox" id="c-38619659" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619546">parent</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38619659">[-]</label><label class="expand" for="c-38619659">[3 more]</label></div><br/><div class="children"><div class="content">On some things that is not a bad position: The old SDI had a lot of spending but really not much to show for it while at the same time forcing the USSR into a reaction based on what today might be called &quot;hype&quot;.</div><br/><div id="38620289" class="c"><input type="checkbox" id="c-38620289" checked=""/><div class="controls bullet"><span class="by">pixl97</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619659">parent</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38620289">[-]</label><label class="expand" for="c-38620289">[2 more]</label></div><br/><div class="children"><div class="content">The particular problem arises when both actors in the game have good economies and build the superweapons. We happened to somewhat luck out that the USSR was an authoritarian shithole that couldn&#x27;t keep up, yet we still have thousands of nukes laying about because of this.<p>I&#x27;d rather not get in an AI battle with China and have us build the world eating machine.</div><br/><div id="38620335" class="c"><input type="checkbox" id="c-38620335" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38620289">parent</a><span>|</span><a href="#38618948">next</a><span>|</span><label class="collapse" for="c-38620335">[-]</label><label class="expand" for="c-38620335">[1 more]</label></div><br/><div class="children"><div class="content">SDI&#x27;s superweapons remained by-and-large a fantasy, though. Just because a lot of money is pouring in doesn&#x27;t mean it will succeed.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38618948" class="c"><input type="checkbox" id="c-38618948" checked=""/><div class="controls bullet"><span class="by">fallingknife</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38617990">parent</a><span>|</span><a href="#38618288">prev</a><span>|</span><a href="#38621421">next</a><span>|</span><label class="collapse" for="c-38618948">[-]</label><label class="expand" for="c-38618948">[5 more]</label></div><br/><div class="children"><div class="content">Very bad.  The Biden admin is proposing AI regulation that will protect large companies from competition due to all the nonsense being said about AI.</div><br/><div id="38619474" class="c"><input type="checkbox" id="c-38619474" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618948">parent</a><span>|</span><a href="#38619385">next</a><span>|</span><label class="collapse" for="c-38619474">[-]</label><label class="expand" for="c-38619474">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The Biden admin is proposing AI regulation that will protect large companies from competition<p>Mostly, the Biden Administration is proposing a bunch of studies by different agencies of different areas, and some authorities for the government to take action regarding AI in some security-related areas. The concrete regulation mostly is envisioned to be drafted based on the studies, and the idea that it will be incumbent protective is mostly based on the fact that certain incumbents have been pretty nakedly tying safety concerns to proposals to pull up the ladder behind themselves. But the Administration is, at a minimum, resisting the lure of relying on those incumbents presentation of the facts and alternatives out of the gate, and also taking a more expansive view of safety and related concerns than the incumbents are proposing (expressly factoring in some of the issues that they have used &quot;safety&quot; concerns to distract from), so I think prejudging the orientation of the regulatory proposals that will follow on the study directives is premature.</div><br/><div id="38620172" class="c"><input type="checkbox" id="c-38620172" checked=""/><div class="controls bullet"><span class="by">fallingknife</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619474">parent</a><span>|</span><a href="#38619385">next</a><span>|</span><label class="collapse" for="c-38620172">[-]</label><label class="expand" for="c-38620172">[1 more]</label></div><br/><div class="children"><div class="content">What I have heard from people I know in the industry is that the proposal they are talking about now is to restrict all models over 20 billion parameters.  This arbitrary rule would be a massive moat to the few companies that have these models already.</div><br/></div></div></div></div><div id="38619385" class="c"><input type="checkbox" id="c-38619385" checked=""/><div class="controls bullet"><span class="by">jazzyjackson</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38618948">parent</a><span>|</span><a href="#38619474">prev</a><span>|</span><a href="#38621421">next</a><span>|</span><label class="collapse" for="c-38619385">[-]</label><label class="expand" for="c-38619385">[2 more]</label></div><br/><div class="children"><div class="content">Alternatively:<p>there is nonsense being said about AI <i>so that</i> the Biden admin can protect large companies from competition</div><br/><div id="38620280" class="c"><input type="checkbox" id="c-38620280" checked=""/><div class="controls bullet"><span class="by">AlexandrB</span><span>|</span><a href="#38617861">root</a><span>|</span><a href="#38619385">parent</a><span>|</span><a href="#38621421">next</a><span>|</span><label class="collapse" for="c-38620280">[-]</label><label class="expand" for="c-38620280">[1 more]</label></div><br/><div class="children"><div class="content">Yup. I continue to be convinced that a lot of the fearmongering about rogue AI taking over the world is a marketing&#x2F;lobbying effort to give early movers in the space a leg up.<p>The real AI harms are probably much more mundane - such as flooding the internet with (even more) low quality garbage.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38621421" class="c"><input type="checkbox" id="c-38621421" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38617861">prev</a><span>|</span><label class="collapse" for="c-38621421">[-]</label><label class="expand" for="c-38621421">[1 more]</label></div><br/><div class="children"><div class="content">AI won’t run away in a vacuum, there will be time to understand what the AI is up to when iterating towards singularity, there would likely be many model in various stage of progress and a way to go back to understand. 
 Or that we will not need the power of singularity to get a lot of benefits.  People don’t think scenarios and use the status quo school of thought like this one.</div><br/></div></div></div></div></div></div></div></body></html>