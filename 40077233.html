<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1713517254413" as="style"/><link rel="stylesheet" href="styles.css?v=1713517254413"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://docs.hatchet.run/blog/multi-tenant-queues">Multi-tenant queues in Postgres</a> <span class="domain">(<a href="https://docs.hatchet.run">docs.hatchet.run</a>)</span></div><div class="subtext"><span>abelanger</span> | <span>32 comments</span></div><br/><div><div id="40081224" class="c"><input type="checkbox" id="c-40081224" checked=""/><div class="controls bullet"><span class="by">ndriscoll</span><span>|</span><a href="#40081007">next</a><span>|</span><label class="collapse" for="c-40081224">[-]</label><label class="expand" for="c-40081224">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve got something wonky going on with that query plan for the 2nd partition by attempt. In particular the seq scan on tasks to do the `(tasks.id = eligible_tasks.id)` hash join seems odd. The filter on queued status in `CTE eligible_tasks` (and not in the last join) also seems weird. Is that plan for the same query in the article?<p>If you add an index on `group_key, id WHERE status = &#x27;queued&#x27;` and remove the 2nd `WHERE tasks.&quot;status&quot; = &#x27;QUEUED&#x27;` (I believe that&#x27;s redundant?), you might get a better plan. You&#x27;d want something to make sure you&#x27;re not preferring one group_key as well.<p>I think you should be able to solve your problem with workers having zero tasks by moving the LIMIT into the second CTE?<p>It&#x27;s also useful in practice to have something like a worker_id and timestamp and not just set status to RUNNING in case a worker gets stuck&#x2F;dies and you need to unclaim the work.</div><br/><div id="40081751" class="c"><input type="checkbox" id="c-40081751" checked=""/><div class="controls bullet"><span class="by">abelanger</span><span>|</span><a href="#40081224">parent</a><span>|</span><a href="#40081007">next</a><span>|</span><label class="collapse" for="c-40081751">[-]</label><label class="expand" for="c-40081751">[4 more]</label></div><br/><div class="children"><div class="content">Ah, great catch - I just pushed an update to match the query from the article. I wasn&#x27;t looking at much except for the WindowAgg line, thank you!<p>I tried with a similar indexing strategy - it did make a very noticeable difference, breaking at about 40000 enqueued tasks instead of 25000. I left indexing out of the article because it can open up a different can of worms with performance degradation over a longer time horizon.<p>I also tried with an `ORDER BY RANDOM()` across group keys first, which does help with fairness but breaks the &quot;deterministic&quot; element.</div><br/><div id="40081810" class="c"><input type="checkbox" id="c-40081810" checked=""/><div class="controls bullet"><span class="by">ndriscoll</span><span>|</span><a href="#40081224">root</a><span>|</span><a href="#40081751">parent</a><span>|</span><a href="#40081007">next</a><span>|</span><label class="collapse" for="c-40081810">[-]</label><label class="expand" for="c-40081810">[3 more]</label></div><br/><div class="children"><div class="content">With your updated plan, you have<p>&gt; Hash Cond: (tasks.id = t1.id)<p>&gt;          -&gt;  Seq Scan on tasks  (cost=0.00..254.60 rows=48 width=14) (actual time=0.566..10.550 rows=10000 loops=1)<p>&gt;                Filter: (status = &#x27;QUEUED&#x27;::&quot;TaskStatus&quot;)<p>So it&#x27;s still doing a seq scan on tasks when I&#x27;d expect it to join using the PK. It must be getting tripped up by the redundant filter on queued status. Try removing that.<p>I ninja edited my previous comment, but if you move the LIMIT to the 2nd CTE, that should fix your issue with workers not getting work. If you do that and add the other index I think in principle it <i>should</i> be able to do everything by maintaining a priority queue of the heads of each partition (which are each pre-sorted by the index now). idk if pg does that though. If it does, then that portion of the query should be streamed, so you don&#x27;t need to try to limit it early to avoid a sort of 10k elements when you only need 10. Then if you remove the redundant QUEUED check, it should be doing everything else through indexes.<p>Basically, if doing this manually I&#x27;d expect the &quot;good&quot; solution to do this in a way where starting from an index, each row is streamed (i.e. no full sort) with logn complexity. So I look at it from a perspective of &quot;how do I get the database to do what I&#x27;d do by hand?&quot;</div><br/><div id="40081984" class="c"><input type="checkbox" id="c-40081984" checked=""/><div class="controls bullet"><span class="by">abelanger</span><span>|</span><a href="#40081224">root</a><span>|</span><a href="#40081810">parent</a><span>|</span><a href="#40081007">next</a><span>|</span><label class="collapse" for="c-40081984">[-]</label><label class="expand" for="c-40081984">[2 more]</label></div><br/><div class="children"><div class="content">I created a gist with these recommendations - certainly an improvement, but I don&#x27;t think it gets around the `WindowAgg` running across all 10k rows: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;abelanger5&#x2F;5c1a75755072239716cb587a2a9c9838" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;abelanger5&#x2F;5c1a75755072239716cb587a2...</a>. Does this accurately implement your suggestions?<p>Happy to try out other suggestions, but I haven&#x27;t found a way to get a window function or `JOIN LATERAL` to scale in near-constant time for this queueing strategy.</div><br/><div id="40082182" class="c"><input type="checkbox" id="c-40082182" checked=""/><div class="controls bullet"><span class="by">ndriscoll</span><span>|</span><a href="#40081224">root</a><span>|</span><a href="#40081984">parent</a><span>|</span><a href="#40081007">next</a><span>|</span><label class="collapse" for="c-40082182">[-]</label><label class="expand" for="c-40082182">[1 more]</label></div><br/><div class="children"><div class="content">It looks like now it does still only pull 100 rows out of the sort (so moving the limit into the 2nd cte didn&#x27;t hurt). It isn&#x27;t doing <i>all</i> 10000 rows now though, which is interesting. By any chance, do you have 9200 different tenants? If so that makes sense. What I suggested would work when you have a small number of tenants with queued work (it scales n log n with tenants with queued work, but log n with amount of tasks that a single tenant has queued). So if you&#x27;re currently testing with many tenants queueing at once, you could see how it behaves with like 20 tenants where one has 9k items and the others have ~50 each. Then it sort of depends on how your distribution looks in practice to know whether that&#x27;s acceptable.<p>You could also probably do tricks where individual workers filter to specific tenant IDs in the first CTE (e.g. filter group_key mod num_workers = worker_id) to reduce that cardinality if you expect it to be large. Or you could e.g. select 100 random group_keys as a first step and use that to filter the window, but then that needs another partial index on just `group_key where status = queued`.<p>Edit: I see it&#x27;s still doing a seq scan on tasks. There is a PK on id, right? It knows there&#x27;s only 100 rows from the rest of the query so it seems odd to me that it would decide to scan the table. You could try putting a hash index on id if it&#x27;s refusing to use the btree index I guess. Or it might change its mind if you add 1M SUCCEEDED tasks or something.<p>Another thing to consider is that out of the box, pg&#x27;s default config for the planner is tuned to like 20 year old hardware. You need to tweak the io costs for SSDs and tell it you have more RAM if you haven&#x27;t done that. See e.g. <a href="https:&#x2F;&#x2F;pgtune.leopard.in.ua&#x2F;" rel="nofollow">https:&#x2F;&#x2F;pgtune.leopard.in.ua&#x2F;</a> for better starting values.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="40081007" class="c"><input type="checkbox" id="c-40081007" checked=""/><div class="controls bullet"><span class="by">plandis</span><span>|</span><a href="#40081224">prev</a><span>|</span><a href="#40083408">next</a><span>|</span><label class="collapse" for="c-40081007">[-]</label><label class="expand" for="c-40081007">[3 more]</label></div><br/><div class="children"><div class="content">At a previous job we did something similar but ended up having workers first poll another table to determine which tenant to query against. We called these items tokens and they represented a finite amount of dedicated thread time for processing a specific tenants’ queue.<p>What this looked like was a worker thread would first query the token table for which tenant to process eligible tasks from, and then update the token to take a timed lock and during that time would solely process eligible tasks from a specific tenant.<p>This has some nice properties:<p>1. You can scale different tenants using different amounts of tokens which means different but controlled amounts of thread time.<p>2. It allows for locality on your worker thread. Within a specific tenant the processing was usually similar so any shared resources could be cached and reused after polling for additional eligible tasks from the tenants queue.</div><br/><div id="40084431" class="c"><input type="checkbox" id="c-40084431" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#40081007">parent</a><span>|</span><a href="#40084547">next</a><span>|</span><label class="collapse" for="c-40084431">[-]</label><label class="expand" for="c-40084431">[1 more]</label></div><br/><div class="children"><div class="content">Reminded me of the token bucket[1] algorithm. Good point about locality.<p>[1]: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Token_bucket" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Token_bucket</a></div><br/></div></div><div id="40084547" class="c"><input type="checkbox" id="c-40084547" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#40081007">parent</a><span>|</span><a href="#40084431">prev</a><span>|</span><a href="#40083408">next</a><span>|</span><label class="collapse" for="c-40084547">[-]</label><label class="expand" for="c-40084547">[1 more]</label></div><br/><div class="children"><div class="content">I like this approach a lot, but I’m unsure about time based vs number of items based fairness. I guess it really depends on the application.</div><br/></div></div></div></div><div id="40083408" class="c"><input type="checkbox" id="c-40083408" checked=""/><div class="controls bullet"><span class="by">nosefrog</span><span>|</span><a href="#40081007">prev</a><span>|</span><a href="#40080684">next</a><span>|</span><label class="collapse" for="c-40083408">[-]</label><label class="expand" for="c-40083408">[4 more]</label></div><br/><div class="children"><div class="content">Is a fair queue worth it vs spinning up more capacity? I&#x27;ve worked on multiple projects where we&#x27;ve ended up ripping out a queue and just spinning up more machines to handle the load synchronously instead.</div><br/><div id="40084524" class="c"><input type="checkbox" id="c-40084524" checked=""/><div class="controls bullet"><span class="by">strken</span><span>|</span><a href="#40083408">parent</a><span>|</span><a href="#40084000">next</a><span>|</span><label class="collapse" for="c-40084524">[-]</label><label class="expand" for="c-40084524">[1 more]</label></div><br/><div class="children"><div class="content">Is it a choice? Most projects I&#x27;ve worked on had times when they became overwhelmed with requests; a queue handles this case, but more capacity just makes it rarer. Ideally you want enough capacity to handle X% of requests within Y milliseconds <i>and</i> a queue to deal with the leftovers, and I suppose if your X is low enough then a fair queue becomes a necessity.</div><br/></div></div><div id="40084000" class="c"><input type="checkbox" id="c-40084000" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#40083408">parent</a><span>|</span><a href="#40084524">prev</a><span>|</span><a href="#40083743">next</a><span>|</span><label class="collapse" for="c-40084000">[-]</label><label class="expand" for="c-40084000">[1 more]</label></div><br/><div class="children"><div class="content">More capacity won&#x27;t address operations that the originator isn&#x27;t willing to (or can&#x27;t) hang around to wait for and&#x2F;or that are long-running enough that restarts due to failures might be needed. That&#x27;s the most immediate reason: Tasks where no amount of capacity will remove the need to have some form of queueing mechanism.<p>For complex enough workflows, queues are also often helpful at addressing potentially failing stages in ways that are easier to debug. But in that case you want your queue to be closer to a state machine where actually <i>waiting</i> in the queue for much time is the exception. You can <i>just</i> build a state machine for that too, ensuring the inputs to the stage about to execute are recorded in a durable, restartable way. But sometimes you may need more copies of the same type of job, and soon you have something that looks and smells much like a queue anyway.<p>Then lastly, spikes. But they only really help well enough if you still spin up more machines aggressively enough that the wait time doesn&#x27;t get long enough to be perceived as just as bad as or worse than an immediate error, so it does make sense to ask your question.<p>A queue also doesn&#x27;t need to be complex. If it gets complex, that increases the reasons to ask your question for that specific system. If it potentially grows large, as well (sometimes the solution to that is simply to refuse to queue if the queue exceeds a certain size or the processing time goes above a certain threshold).<p>Queues are great when appropriate, but they <i>do</i> often get used as a &quot;solution&quot; to a scaling problem that hasn&#x27;t been sufficiently analyzed, which sounds like might have been the case in your examples.</div><br/></div></div><div id="40083743" class="c"><input type="checkbox" id="c-40083743" checked=""/><div class="controls bullet"><span class="by">datascienced</span><span>|</span><a href="#40083408">parent</a><span>|</span><a href="#40084000">prev</a><span>|</span><a href="#40080684">next</a><span>|</span><label class="collapse" for="c-40083743">[-]</label><label class="expand" for="c-40083743">[1 more]</label></div><br/><div class="children"><div class="content">I guess a queue handles spikes, and is OK for async-allowed operations such as generating a PDF and email it over say loading a web page.<p>A queue may give you time to scale up too?</div><br/></div></div></div></div><div id="40080684" class="c"><input type="checkbox" id="c-40080684" checked=""/><div class="controls bullet"><span class="by">jperras</span><span>|</span><a href="#40083408">prev</a><span>|</span><a href="#40082047">next</a><span>|</span><label class="collapse" for="c-40080684">[-]</label><label class="expand" for="c-40080684">[3 more]</label></div><br/><div class="children"><div class="content">Is the `FOR UPDATE SKIP LOCKED` in the CTE necessary? Granted my understanding of Postgres row-level locking and their interaction with CTEs may be a bit lacking, but according to the docs[1]:<p><pre><code>  The sub-statements in WITH are executed concurrently with each other and with the main query. Therefore, when using data-modifying statements in WITH, the order in which the specified updates actually happen is unpredictable. All the statements are executed with the same snapshot (see Chapter 13), so they cannot “see” one another&#x27;s effects on the target tables.

</code></pre>
1. <a href="https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;queries-with.html#QUERIES-WITH-MODIFYING" rel="nofollow">https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;queries-with.html#QU...</a></div><br/><div id="40080851" class="c"><input type="checkbox" id="c-40080851" checked=""/><div class="controls bullet"><span class="by">mslot</span><span>|</span><a href="#40080684">parent</a><span>|</span><a href="#40082047">next</a><span>|</span><label class="collapse" for="c-40080851">[-]</label><label class="expand" for="c-40080851">[2 more]</label></div><br/><div class="children"><div class="content">Read committed mode (PostgreSQL&#x27;s default) can get pretty funky.<p>If two transactions concurrently perform a SELECT (may be in a CTE) followed by an UPDATE, then they might see and try to update the same rows. That&#x27;s often undesirable, for instance in the example of a queue where messages are supposed to arrive ~once. Serializable mode would &quot;solve&quot; the problem by letting one transaction fail, and expects the application to retry or otherwise deal with the consequences.<p>FOR UPDATE is a precision tool for working around read committed limitations. It ensures rows are locked by whichever transaction reads them first, such that the second reader blocks and (here&#x27;s the funky part) when the first transaction is done it actually reads the latest row version instead of the one that was in the snapshot. That&#x27;s semantically a bit weird, but nonetheless very useful, and actually matches how updates work in PostgreSQL.<p>The biggest issue with SELECT..FOR UPDATE is that it blocks waiting for concurrent updaters to finish, even if the rows no longer match its filter after the update. The SKIP LOCKED avoids all that by simply skipping the locked rows in the SELECT. Semantically even weirder, but very useful for queues.</div><br/><div id="40080932" class="c"><input type="checkbox" id="c-40080932" checked=""/><div class="controls bullet"><span class="by">jperras</span><span>|</span><a href="#40080684">root</a><span>|</span><a href="#40080851">parent</a><span>|</span><a href="#40082047">next</a><span>|</span><label class="collapse" for="c-40080932">[-]</label><label class="expand" for="c-40080932">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I see - the default transactional isolation level is what I wasn&#x27;t accounting for.<p>Thanks for the explanation! Very much appreciated.</div><br/></div></div></div></div></div></div><div id="40082047" class="c"><input type="checkbox" id="c-40082047" checked=""/><div class="controls bullet"><span class="by">time0ut</span><span>|</span><a href="#40080684">prev</a><span>|</span><a href="#40081870">next</a><span>|</span><label class="collapse" for="c-40082047">[-]</label><label class="expand" for="c-40082047">[1 more]</label></div><br/><div class="children"><div class="content">Very cool. Bookmarked in case I ever need to do this.<p>I have implemented a transactional outbox in postgres using a simpler version of this plus a trigger to notify listening workers. It worked well and vastly outpaced the inserting processes. It easily handled several million tasks per hour without issue.<p>It is also nice the article showed the correct CTE based form of the query. It is possible to naively write the query without it and sometimes get way more tasks than you asked for when there are concurrent workers. I discovered that pretty quickly but it had me pulling my hair out…</div><br/></div></div><div id="40081870" class="c"><input type="checkbox" id="c-40081870" checked=""/><div class="controls bullet"><span class="by">jvolkman</span><span>|</span><a href="#40082047">prev</a><span>|</span><a href="#40082353">next</a><span>|</span><label class="collapse" for="c-40081870">[-]</label><label class="expand" for="c-40081870">[1 more]</label></div><br/><div class="children"><div class="content">I have my queue workers maintain a list of task owners they&#x27;ve already processed, and prefer to get a task from an owner they&#x27;ve least-recently seen (or haven&#x27;t seen) using `ORDER BY array_position(:seen_owner_ids, owner_id) desc`. Each new task&#x27;s owner_id is inserted into the front of the list (and removed elsewhere if it exists).<p>But I have a relatively small number of possible `owner_id` values at any given time.</div><br/></div></div><div id="40082353" class="c"><input type="checkbox" id="c-40082353" checked=""/><div class="controls bullet"><span class="by">phibz</span><span>|</span><a href="#40081870">prev</a><span>|</span><a href="#40083343">next</a><span>|</span><label class="collapse" for="c-40082353">[-]</label><label class="expand" for="c-40082353">[5 more]</label></div><br/><div class="children"><div class="content">Why not something like Kafka or Redis?</div><br/><div id="40083631" class="c"><input type="checkbox" id="c-40083631" checked=""/><div class="controls bullet"><span class="by">teraflop</span><span>|</span><a href="#40082353">parent</a><span>|</span><a href="#40084562">next</a><span>|</span><label class="collapse" for="c-40083631">[-]</label><label class="expand" for="c-40083631">[1 more]</label></div><br/><div class="children"><div class="content">The most straightforward reason is that if you need a transactional database anyway, then moving the queue into the DB allows you to <i>atomically</i> en&#x2F;dequeue messages at the same time as making other updates. Which can massively simplify your architecture because it eliminates an enormous category of possible failure modes. (Or it can massively improve your system correctness, if you didn&#x27;t realize those failure modes were possible.)</div><br/></div></div><div id="40084562" class="c"><input type="checkbox" id="c-40084562" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#40082353">parent</a><span>|</span><a href="#40083631">prev</a><span>|</span><a href="#40083173">next</a><span>|</span><label class="collapse" for="c-40084562">[-]</label><label class="expand" for="c-40084562">[1 more]</label></div><br/><div class="children"><div class="content">Using Kafka as a work queue is widely documented as a mistake. Using a single database results in a lot of operational simplicity and you get to skip a lot of distributed systems when all your state is in a single system</div><br/></div></div><div id="40083173" class="c"><input type="checkbox" id="c-40083173" checked=""/><div class="controls bullet"><span class="by">dewey</span><span>|</span><a href="#40082353">parent</a><span>|</span><a href="#40084562">prev</a><span>|</span><a href="#40082794">next</a><span>|</span><label class="collapse" for="c-40083173">[-]</label><label class="expand" for="c-40083173">[1 more]</label></div><br/><div class="children"><div class="content">Also most of the times you already have a database so why not use that instead of adding another service to the pile.</div><br/></div></div><div id="40082794" class="c"><input type="checkbox" id="c-40082794" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#40082353">parent</a><span>|</span><a href="#40083173">prev</a><span>|</span><a href="#40083343">next</a><span>|</span><label class="collapse" for="c-40082794">[-]</label><label class="expand" for="c-40082794">[1 more]</label></div><br/><div class="children"><div class="content">Because that’s sane, easy, and boring.</div><br/></div></div></div></div><div id="40083343" class="c"><input type="checkbox" id="c-40083343" checked=""/><div class="controls bullet"><span class="by">mind-blight</span><span>|</span><a href="#40082353">prev</a><span>|</span><a href="#40080309">next</a><span>|</span><label class="collapse" for="c-40083343">[-]</label><label class="expand" for="c-40083343">[1 more]</label></div><br/><div class="children"><div class="content">Super cool! I was looking at the self hosted quickstart, and it looks like Docker compare installs both hatchet and RabitMQ. Does hatchet use rabbit alongside Postgres?</div><br/></div></div><div id="40080309" class="c"><input type="checkbox" id="c-40080309" checked=""/><div class="controls bullet"><span class="by">ucarion</span><span>|</span><a href="#40083343">prev</a><span>|</span><a href="#40083044">next</a><span>|</span><label class="collapse" for="c-40080309">[-]</label><label class="expand" for="c-40080309">[3 more]</label></div><br/><div class="children"><div class="content">This is pretty fancy stuff! Sorry if I&#x27;m just not reading carefully enough, but does this approach account for tenants whose messages take longer to process, as opposed to a tenant that sends a larger volume of messages?</div><br/><div id="40080562" class="c"><input type="checkbox" id="c-40080562" checked=""/><div class="controls bullet"><span class="by">abelanger</span><span>|</span><a href="#40080309">parent</a><span>|</span><a href="#40083044">next</a><span>|</span><label class="collapse" for="c-40080562">[-]</label><label class="expand" for="c-40080562">[2 more]</label></div><br/><div class="children"><div class="content">No, this assumes you&#x27;ve already split your tasks into quanta of approximately the same duration - so all tasks are weighted equally in terms of execution time. If each of the tasks have different weights you might be looking at something like deficit round-robin [1], which I&#x27;ve looked at implementations for in RabbitMQ and we&#x27;re thinking about how to implement in PG as well [2].<p>[1] <a href="https:&#x2F;&#x2F;www.cs.bu.edu&#x2F;fac&#x2F;matta&#x2F;Teaching&#x2F;cs655-papers&#x2F;DRR.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.bu.edu&#x2F;fac&#x2F;matta&#x2F;Teaching&#x2F;cs655-papers&#x2F;DRR.pd...</a><p>[2] <a href="https:&#x2F;&#x2F;nithril.github.io&#x2F;amqp&#x2F;2015&#x2F;07&#x2F;05&#x2F;fair-consuming-with-rabbitmq&#x2F;" rel="nofollow">https:&#x2F;&#x2F;nithril.github.io&#x2F;amqp&#x2F;2015&#x2F;07&#x2F;05&#x2F;fair-consuming-wit...</a></div><br/><div id="40082416" class="c"><input type="checkbox" id="c-40082416" checked=""/><div class="controls bullet"><span class="by">ucarion</span><span>|</span><a href="#40080309">root</a><span>|</span><a href="#40080562">parent</a><span>|</span><a href="#40083044">next</a><span>|</span><label class="collapse" for="c-40082416">[-]</label><label class="expand" for="c-40082416">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense!<p>My gut tells me that it would often make sense to jump straight to shuffle sharding, where you&#x27;d converge on fair solutions dynamically, in a lot of cases. I&#x27;m looking forward to that follow-on article!</div><br/></div></div></div></div></div></div><div id="40083044" class="c"><input type="checkbox" id="c-40083044" checked=""/><div class="controls bullet"><span class="by">wbeckler</span><span>|</span><a href="#40080309">prev</a><span>|</span><a href="#40080165">next</a><span>|</span><label class="collapse" for="c-40083044">[-]</label><label class="expand" for="c-40083044">[2 more]</label></div><br/><div class="children"><div class="content">I love your animations! How did you do those?</div><br/><div id="40083060" class="c"><input type="checkbox" id="c-40083060" checked=""/><div class="controls bullet"><span class="by">abelanger</span><span>|</span><a href="#40083044">parent</a><span>|</span><a href="#40080165">next</a><span>|</span><label class="collapse" for="c-40083060">[-]</label><label class="expand" for="c-40083060">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! I&#x27;ve been using <a href="https:&#x2F;&#x2F;jitter.video">https:&#x2F;&#x2F;jitter.video</a> with the Lottie exporter. It also has a Figma plugin so you can reuse components.</div><br/></div></div></div></div><div id="40080165" class="c"><input type="checkbox" id="c-40080165" checked=""/><div class="controls bullet"><span class="by">andrewstuart</span><span>|</span><a href="#40083044">prev</a><span>|</span><a href="#40082789">next</a><span>|</span><label class="collapse" for="c-40080165">[-]</label><label class="expand" for="c-40080165">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if Postgres RBAC row based access control is another solution to this.</div><br/></div></div></div></div></div></div></div></body></html>