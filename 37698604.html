<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695978061139" as="style"/><link rel="stylesheet" href="styles.css?v=1695978061139"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2309.16039">Llama 2 Long</a>Â <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>apsec112</span> | <span>19 comments</span></div><br/><div><div id="37701005" class="c"><input type="checkbox" id="c-37701005" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#37698955">next</a><span>|</span><label class="collapse" for="c-37701005">[-]</label><label class="expand" for="c-37701005">[1 more]</label></div><br/><div class="children"><div class="content">This research takes us closer to Anthropic Claude like models that can handle 100K tokens - being able to reference book sized context is awesome and being able to run that locally in a way that benchmarks in a similar way will be awesome. What&#x27;s the bottleneck to get to 100K? Is it just compute power&#x2F;cost? It&#x27;s funny that OpenAI don&#x27;t offer an API people can use that can do 100K tokens yet!</div><br/></div></div><div id="37698955" class="c"><input type="checkbox" id="c-37698955" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#37701005">prev</a><span>|</span><a href="#37700095">next</a><span>|</span><label class="collapse" for="c-37698955">[-]</label><label class="expand" for="c-37698955">[1 more]</label></div><br/><div class="children"><div class="content">Great news. No URLs for the models yet and nothing on <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;meta-llama" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;meta-llama</a> . But the appendix has a really cool explanation of the RoPE algorithm and a visualization of what it means to rotate the angle of the token embeddings set of values. It really gives you an intuitive understanding of why the cosine similarity varies periodically so that every $x tokens are more like each other. With the increased base frequency in this version of RoPE that just means more spurrious &quot;similiarity&#x27; between tokens ever $x tokens along in positional encoding. I&#x27;ve never seen anyone exploit this to achieve a desired result yet but it seems straightforwards: pre-tokenize your input and then massage it so the tokens you want most linked $x tokens apart.</div><br/></div></div><div id="37700095" class="c"><input type="checkbox" id="c-37700095" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#37698955">prev</a><span>|</span><a href="#37700056">next</a><span>|</span><label class="collapse" for="c-37700095">[-]</label><label class="expand" for="c-37700095">[6 more]</label></div><br/><div class="children"><div class="content">This is very cool, and I was reading the paper when I noticed they call it an open-source model, but wouldn&#x27;t a better term be &quot;open-weight model&quot;, since to make the model weights you&#x27;d need the document sources and lots of compute? Or did they actually open source it fully so people can pull the same sources in to build the same weights and I didn&#x27;t get the memo?</div><br/><div id="37700243" class="c"><input type="checkbox" id="c-37700243" checked=""/><div class="controls bullet"><span class="by">gojomo</span><span>|</span><a href="#37700095">parent</a><span>|</span><a href="#37700474">next</a><span>|</span><label class="collapse" for="c-37700243">[-]</label><label class="expand" for="c-37700243">[4 more]</label></div><br/><div class="children"><div class="content">Many clearly think of the weights as a source-like artifact.<p>Also, the model&#x27;s architecture is sufficiently documented, &amp; supported by open code, for others to fine-tune it &amp; run it for generation, which to me justifies describing the model as &quot;open&quot;, even if the entire process for creating a new set of high-quality weights isn&#x27;t. (That&#x27;s in contrast to &quot;Open&quot;AI&#x27;s GPT4, about which many architectural details are undisclosed.)<p>Note also: as I understand it, there&#x27;s so much paralellism, algorithmic randomization, &amp; even floating-point-implementation-instability in how such models are trained that even having the exact same training corpus wouldn&#x27;t be enough to ensure the same final weights. That would require both Facebook &amp; reproducers to do every calculation, in every stage of prepration, in identical order on identical hardware - a constraint that&#x27;d make typical parallel&#x2F;distributed training optimizations (subject to all sorts of CPU&#x2F;IO&#x2F;network jitter) impossible.</div><br/><div id="37700956" class="c"><input type="checkbox" id="c-37700956" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#37700095">root</a><span>|</span><a href="#37700243">parent</a><span>|</span><a href="#37700266">next</a><span>|</span><label class="collapse" for="c-37700956">[-]</label><label class="expand" for="c-37700956">[1 more]</label></div><br/><div class="children"><div class="content">I suspect a lot of people like me are feeling quite a bit &quot;nerd sniped&quot; while trying to read the paper. I am just suggesting open-weights are more fit term than the open source term I saw when trying to read the paper, I was not talking about EXACT reproducible builds, which I agree are probably a stretch with current tech (but obviously not impossible with new tech but possibly slower and not obviously needed anyway). I also know about OpenLLaMA and others but that&#x27;s not Meta&#x27;s work, and it&#x27;s not the work in the paper. I&#x27;m pretty sure a lot of the core principles Llama uses are drawn from OpenAI&#x27;s published research but I agree that there is a lack of openness recently, e.g people having to speculate about even core things like how they presumably use mixture of experts, but &quot;open source&quot; is not the right term for them just because OpenAI is not being open right now. I agree mostly that the model itself is somewhat open but I really think we should consider &quot;open weights&quot; as a much better term, I think it&#x27;s much more reasonable than saying open source.</div><br/></div></div><div id="37700266" class="c"><input type="checkbox" id="c-37700266" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#37700095">root</a><span>|</span><a href="#37700243">parent</a><span>|</span><a href="#37700956">prev</a><span>|</span><a href="#37700474">next</a><span>|</span><label class="collapse" for="c-37700266">[-]</label><label class="expand" for="c-37700266">[2 more]</label></div><br/><div class="children"><div class="content">Yes it&#x27;s &quot;open&quot;, but if it doesn&#x27;t fit the OSI definition it&#x27;s rude to call it &quot;open source&quot;.</div><br/><div id="37700491" class="c"><input type="checkbox" id="c-37700491" checked=""/><div class="controls bullet"><span class="by">JohnKemeny</span><span>|</span><a href="#37700095">root</a><span>|</span><a href="#37700266">parent</a><span>|</span><a href="#37700474">next</a><span>|</span><label class="collapse" for="c-37700491">[-]</label><label class="expand" for="c-37700491">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Rude&quot; is a strangely chosen word here. Rude towards whom? I could maybe accept &quot;misleading&quot;, but as a person who don&#x27;t believe OSI has an ownership of the term &quot;open source&quot;, I don&#x27;t think it&#x27;s misleading either.</div><br/></div></div></div></div></div></div><div id="37700474" class="c"><input type="checkbox" id="c-37700474" checked=""/><div class="controls bullet"><span class="by">ninjin</span><span>|</span><a href="#37700095">parent</a><span>|</span><a href="#37700243">prev</a><span>|</span><a href="#37700056">next</a><span>|</span><label class="collapse" for="c-37700474">[-]</label><label class="expand" for="c-37700474">[1 more]</label></div><br/><div class="children"><div class="content">I find it confusing to release weights under a source code license too as Apache 2.0 that is used for plenty of open models spends a lot of time talking about code, which should be unrelated to weights (I am not a lawyer though). But perhaps one should look at weights as &quot;firmware&quot; necessary to run a model? I am not sure.<p>Although I am getting tired to keep mentioning it at this point, LLaMa is <i>not</i> open by any reasonable definition though. It is <i>available</i>. But as a sibling points out, it is rude of Facebook to call it something it blatantly is not, even if it is <i>more</i> open than say Clos^WOpenAI&#x27;s offerings. In addition, it is <i>very</i> rude towards the people behind Mistral, GPT-J, etc. who do honour the tradition of open science and open source to place your work both in your papers and marketing next to them. There is a great word to describe what Facebook is doing: appropriation.<p>Neither is releasing LLaMa the way that Facebook does honouring their &quot;commitment to open science&quot; as its license also violates the principles of open science. This was objectively false when they stated it six or so months ago and even after OSI and many others calling them out they still have not adjusted their messaging and I am frankly considering them bad faith actors at this point.<p>The best argument in favour of Facebook doing what they are doing is that their model is <i>more</i> open than the closed ones. Which is fair (pun not intended). But, calling your dog a cat because it is more akin to a cat than a fish is still misleading and rightfully surprises and upsets cat lovers, even if dogs are fine pets in their own right.</div><br/></div></div></div></div><div id="37700056" class="c"><input type="checkbox" id="c-37700056" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#37700095">prev</a><span>|</span><a href="#37699038">next</a><span>|</span><label class="collapse" for="c-37700056">[-]</label><label class="expand" for="c-37700056">[3 more]</label></div><br/><div class="children"><div class="content">So can you take this:<p><pre><code>  (...) experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences (...)
</code></pre>
...to the extreme and arrange training into passes with increasing context length and decreasing number of samples?</div><br/><div id="37700341" class="c"><input type="checkbox" id="c-37700341" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37700056">parent</a><span>|</span><a href="#37699038">next</a><span>|</span><label class="collapse" for="c-37700341">[-]</label><label class="expand" for="c-37700341">[2 more]</label></div><br/><div class="children"><div class="content">Yes, empirically I&#x27;ve noticed that if you do not exchange that sequence length gain for huge batch sizes up front, however, your performance will go down overall....</div><br/><div id="37700757" class="c"><input type="checkbox" id="c-37700757" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#37700056">root</a><span>|</span><a href="#37700341">parent</a><span>|</span><a href="#37699038">next</a><span>|</span><label class="collapse" for="c-37700757">[-]</label><label class="expand" for="c-37700757">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d imagine performance will suffer but how does it relate to gains in overall training costs? Ie. with same compute budget, which approach would produce better overall performance?<p>Expanding on this idea, do you think it makes sense to explore what could be called &quot;bootstrapping phase&quot; or &quot;progressive training&quot; for foundational model training:<p>- starting with small number of weights that are being increased with further training<p>- arranging training data with basics first - short sentences, logic, grammar, arithmetic, naive knowledge (&quot;bear is animal&quot; etc) that increases in complexity as training progresses.<p>- increasing context length - ideally implicit, based on increasing sample sizes</div><br/></div></div></div></div></div></div><div id="37699038" class="c"><input type="checkbox" id="c-37699038" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37700056">prev</a><span>|</span><a href="#37699603">next</a><span>|</span><label class="collapse" for="c-37699038">[-]</label><label class="expand" for="c-37699038">[1 more]</label></div><br/><div class="children"><div class="content">&gt;the 70B variant can already surpass gpt-3.5-turbo-16k&#x27;s overall performance on a suite of long-context tasks<p>Now I believe Meta when they say this, as I can believe they were careful about dataset contamination.</div><br/></div></div><div id="37699603" class="c"><input type="checkbox" id="c-37699603" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#37699038">prev</a><span>|</span><a href="#37700022">next</a><span>|</span><label class="collapse" for="c-37699603">[-]</label><label class="expand" for="c-37699603">[1 more]</label></div><br/><div class="children"><div class="content">Long-context tasks are not really the true gap between LLaMA and GPT series, but important result.</div><br/></div></div><div id="37700022" class="c"><input type="checkbox" id="c-37700022" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#37699603">prev</a><span>|</span><a href="#37699361">next</a><span>|</span><label class="collapse" for="c-37700022">[-]</label><label class="expand" for="c-37700022">[1 more]</label></div><br/><div class="children"><div class="content">Getting there.<p>Is there an online demo?</div><br/></div></div><div id="37699361" class="c"><input type="checkbox" id="c-37699361" checked=""/><div class="controls bullet"><span class="by">lettergram</span><span>|</span><a href="#37700022">prev</a><span>|</span><a href="#37699467">next</a><span>|</span><label class="collapse" for="c-37699361">[-]</label><label class="expand" for="c-37699361">[2 more]</label></div><br/><div class="children"><div class="content">Itâs been interesting watching the open source community get ROPE working well. I find it interesting that the method appears to be as good as retraining a large model.<p>To me, that reads like models can be much smaller, understanding of human language at a fraction the size. But we then need a technique (such as this) to expand the context window to interpret larger inputs.<p>This could dramatically democratize model development.</div><br/><div id="37699911" class="c"><input type="checkbox" id="c-37699911" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#37699361">parent</a><span>|</span><a href="#37699467">next</a><span>|</span><label class="collapse" for="c-37699911">[-]</label><label class="expand" for="c-37699911">[1 more]</label></div><br/><div class="children"><div class="content">The thing is, GPT-4 isn&#x27;t useful to me because it understands language. It&#x27;s useful to me because it understands a wide variety of advanced, complex phenomena and is able to synthesize these insights into novel solutions to all sorts of problems.<p>A lot of knowledge could be taken out and put in a vector search system which dynamically loads context into the input, but some knowledge seems worth embedding into the model. The question becomes what knowledge is critical in order to create a useful, large-context language model which can run on the edge.</div><br/></div></div></div></div><div id="37699467" class="c"><input type="checkbox" id="c-37699467" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#37699361">prev</a><span>|</span><label class="collapse" for="c-37699467">[-]</label><label class="expand" for="c-37699467">[2 more]</label></div><br/><div class="children"><div class="content">How does modified RoPE compare to the sliding window attention used by Mistral?</div><br/><div id="37699759" class="c"><input type="checkbox" id="c-37699759" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37699467">parent</a><span>|</span><label class="collapse" for="c-37699759">[-]</label><label class="expand" for="c-37699759">[1 more]</label></div><br/><div class="children"><div class="content">This is still full attention so it should be better in benchmarks, whereas the sliding window attention is more efficient so it allows higher context sizes.</div><br/></div></div></div></div></div></div></div></div></div></body></html>