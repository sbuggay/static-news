<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692349258435" as="style"/><link rel="stylesheet" href="styles.css?v=1692349258435"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://lwn.net/Articles/939981/">GIL removal and the Faster CPython project</a> <span class="domain">(<a href="https://lwn.net">lwn.net</a>)</span></div><div class="subtext"><span>signa11</span> | <span>42 comments</span></div><br/><div><div id="37171620" class="c"><input type="checkbox" id="c-37171620" checked=""/><div class="controls bullet"><span class="by">BiteCode_dev</span><span>|</span><a href="#37171551">next</a><span>|</span><label class="collapse" for="c-37171620">[-]</label><label class="expand" for="c-37171620">[4 more]</label></div><br/><div class="children"><div class="content">The article is well written, and a good history of the all ordeal. But please note it gives more weight to the &quot;against the GIL&quot; side of the story, and all that can go wrong.<p>It doesn&#x27;t highlight enough the other side of the coin:<p>- Sam&#x27;s work is very high quality, and he brought with the no-gil some unrelated perf improvements so that people don&#x27;t feel like they loose perf too much.<p>- Sam played the open source game perfectly, and is incredibly patient given what he is bringing on the able and how slow and flaccid the steering council reaction was (without the community pushing on it, it would still be collecting dust).<p>- Sub interpreters have yet to demonstrate any usefulness at all in Python. In fact, any serious metrics at all. This is the first attempt to be that well defined, and measured.<p>- The community feedback shown a great interest in this particular project.<p>- The steering council did conclude &quot;We intend to accept PEP 703, although we’re still working on the acceptance details.&quot;<p>I&#x27;m not a no-gil enthusiast. I would be fine with have it never be removed, and I think we should try sub-interpreter first.<p>But what&#x27;s fair is fair.</div><br/><div id="37172038" class="c"><input type="checkbox" id="c-37172038" checked=""/><div class="controls bullet"><span class="by">TylerE</span><span>|</span><a href="#37171620">parent</a><span>|</span><a href="#37171551">next</a><span>|</span><label class="collapse" for="c-37172038">[-]</label><label class="expand" for="c-37172038">[3 more]</label></div><br/><div class="children"><div class="content">What I’d liked to see is the performance patches without the no-Gil part.</div><br/><div id="37172106" class="c"><input type="checkbox" id="c-37172106" checked=""/><div class="controls bullet"><span class="by">Znafon</span><span>|</span><a href="#37171620">root</a><span>|</span><a href="#37172038">parent</a><span>|</span><a href="#37172147">next</a><span>|</span><label class="collapse" for="c-37172106">[-]</label><label class="expand" for="c-37172106">[1 more]</label></div><br/><div class="children"><div class="content">The performance patches have already been merged and there is ongoing working in the Faster CPython project.</div><br/></div></div><div id="37172147" class="c"><input type="checkbox" id="c-37172147" checked=""/><div class="controls bullet"><span class="by">formerly_proven</span><span>|</span><a href="#37171620">root</a><span>|</span><a href="#37172038">parent</a><span>|</span><a href="#37172106">prev</a><span>|</span><a href="#37171551">next</a><span>|</span><label class="collapse" for="c-37172147">[-]</label><label class="expand" for="c-37172147">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re already using a bunch of those in recent Python versions.</div><br/></div></div></div></div></div></div><div id="37171551" class="c"><input type="checkbox" id="c-37171551" checked=""/><div class="controls bullet"><span class="by">nullspace</span><span>|</span><a href="#37171620">prev</a><span>|</span><a href="#37171608">next</a><span>|</span><label class="collapse" for="c-37171551">[-]</label><label class="expand" for="c-37171551">[7 more]</label></div><br/><div class="children"><div class="content">What a great article from LWN. It was well-worth reading. As someone who was excited about the NoGIL from Sam Gross when it was first posted here, I think I&#x27;m beginning to change my mind after reading this article and reflecting on my own personal experiences.<p>My experience is with writing backend systems in several different languages (including Python) at various volume&#x2F;latency&#x2F;throughput levels. I&#x27;ve basically worked on only two types of systems -<p>1. one that exposes some sort of an endpoint to the network - it accepts requests of some kind, does computation and other network requests and sends response of some kind (including long polling, ws etc).<p>2. reads a message from a &quot;queue&quot; (could be database, could be based on polling another api etc) and does computation&#x2F;network calls and basically sends it to other queues.<p>Nothing else. Huge variance in specific requirements, but that&#x27;s it. For the first type of system, latency matters more. For the second system, throughput matters more.<p>For the first type of system, I want to be able to spin up threads in response to requests, without worrying that an endpoint is too computationally heavy and might block others. I want to be able to share connections to databases in a shared pool. NoGIL would be useful here.<p>For the second type of system, I can&#x27;t remember the last time where I wrote one where I had in-process parallelism&#x2F;concurrency with shared resources (even in langs where there&#x27;s no GIL). It would just get too confusing and hard to reason about. Any optimizations were mostly based on intelligent batching. For parallelism, you&#x27;d just have multiple _completely_ independent processes, probably across multiple machines.<p>I would absolutely be disappointed if NoGIL meant compromising on the quality of of the second type of system here. In practice, most of my mental bandwidth today goes towards making the second type of system better.</div><br/><div id="37172939" class="c"><input type="checkbox" id="c-37172939" checked=""/><div class="controls bullet"><span class="by">mirsadm</span><span>|</span><a href="#37171551">parent</a><span>|</span><a href="#37172071">next</a><span>|</span><label class="collapse" for="c-37172939">[-]</label><label class="expand" for="c-37172939">[1 more]</label></div><br/><div class="children"><div class="content">&gt;For the second type of system, I can&#x27;t remember the last time where I wrote one where I had in-process parallelism&#x2F;concurrency with shared resources (even in langs where there&#x27;s no GIL). It would just get too confusing and hard to reason about. Any optimizations were mostly based on intelligent batching. For parallelism, you&#x27;d just have multiple _completely_ independent processes, probably across multiple machines.<p>Interestingly I&#x27;m working on something like this right now and do have large shared resources which meant I had to abandon using a multiprocess strategy.<p>I don&#x27;t see why it would be confusing though, provided the shared resources are read only.</div><br/></div></div><div id="37172071" class="c"><input type="checkbox" id="c-37172071" checked=""/><div class="controls bullet"><span class="by">scarygliders</span><span>|</span><a href="#37171551">parent</a><span>|</span><a href="#37172939">prev</a><span>|</span><a href="#37171985">next</a><span>|</span><label class="collapse" for="c-37172071">[-]</label><label class="expand" for="c-37172071">[1 more]</label></div><br/><div class="children"><div class="content">&gt; For the second type of system, I can&#x27;t remember the last time where I wrote one where I had in-process parallelism&#x2F;concurrency with shared resources (even in langs where there&#x27;s no GIL). It would just get too confusing and hard to reason about. Any optimizations were mostly based on intelligent batching. For parallelism, you&#x27;d just have multiple _completely_ independent processes, probably across multiple machines.<p>For myself, the prospect of no-gil is interesting, in that something like my Captain&#x27;s Log application [0] can be free from it; for example, I currently use a QThread to implement a JournalParser, which is basically the program&#x27;s &quot;engine&quot; - the parser constantly reads in game events from a player journal file generated by the game Elite: Dangerous (and Odyssey), and depending on the particular event, fires off a related custom QSignal, which is then processed by whichever slot (receiving function) is listening for a given Signal.<p>There are other places in that application where no GIL might be quite handy.<p>In other words, I can see where having no GIL can be useful for GUI applications like mine.<p>[0] <a href="https:&#x2F;&#x2F;captainslog.scarygliders.net&#x2F;captains-log-2&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;captainslog.scarygliders.net&#x2F;captains-log-2&#x2F;</a></div><br/></div></div><div id="37171985" class="c"><input type="checkbox" id="c-37171985" checked=""/><div class="controls bullet"><span class="by">VagabundoP</span><span>|</span><a href="#37171551">parent</a><span>|</span><a href="#37172071">prev</a><span>|</span><a href="#37172362">next</a><span>|</span><label class="collapse" for="c-37171985">[-]</label><label class="expand" for="c-37171985">[1 more]</label></div><br/><div class="children"><div class="content">As a hobbyist who uses python I don&#x27;t think I&#x27;ll be directly using concurrency in my code, but I&#x27;m betting that over time the standard library and popular external libraries will.<p>And that will raise everyone&#x27;s code.</div><br/></div></div><div id="37172362" class="c"><input type="checkbox" id="c-37172362" checked=""/><div class="controls bullet"><span class="by">spacechild1</span><span>|</span><a href="#37171551">parent</a><span>|</span><a href="#37171985">prev</a><span>|</span><a href="#37171693">next</a><span>|</span><label class="collapse" for="c-37172362">[-]</label><label class="expand" for="c-37172362">[2 more]</label></div><br/><div class="children"><div class="content">The GIL is a bottleneck in applications that are CPU bound, e.g. machine learning, so naturally the NoGIL project is not that interesting to people writing server applications.<p>Of course, one may argue that you probably should not write CPU bound programs in Python in the first place, but that&#x27;s another story :)</div><br/><div id="37172590" class="c"><input type="checkbox" id="c-37172590" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#37171551">root</a><span>|</span><a href="#37172362">parent</a><span>|</span><a href="#37171693">next</a><span>|</span><label class="collapse" for="c-37172590">[-]</label><label class="expand" for="c-37172590">[1 more]</label></div><br/><div class="children"><div class="content">A lot of Java server based applications are multi threaded, not CPU bound, and tend to do a lot of things that generally can&#x27;t work in python because of the GIL. It&#x27;s too simplistic to think of this as something only of interest for CPU bound stuff. A lot of what Java applications do is of course the thread per connection style processing that older java applications still do (more modern ones would use non blocking IO and green threads). But there are also background threads doing useful work or more complex requests that fork off asynchronous work across multiple CPUs and then aggregate the results back as the response. Java apps tend to have vastly more threads than CPU cores. The exception is when things are CPU bound; then you want to minimize the context switching and end up with an number that is close to the number of CPU cores.<p>The GIL is not about the CPU but about enabling those kinds of things. With the current GIL in place it&#x27;s very simple: as soon as you hit the global lock, everything stops until it is released. It doesn&#x27;t matter how many CPU cores you have, they&#x27;ll be idling while one of them holds the lock. There&#x27;s barely any point in even trying to do that with the GIL in place. Forget about sharing data between threads. Mostly that&#x27;s done via queues or databases in python. Removing the GIL will revolutionize a few things in key use cases for python:<p>- data processing &amp; ETL<p>- event driven server systems<p>- machine learning and data science systems<p>They can all benefit from this and that&#x27;s the reason a lot of people are pushing for this. The short term performance losses are not inherent to removing the GIL but just a necessary evil while the python developers deal with fixing the bottlenecks and a few decades worth of technical debt.</div><br/></div></div></div></div><div id="37171693" class="c"><input type="checkbox" id="c-37171693" checked=""/><div class="controls bullet"><span class="by">YeBanKo</span><span>|</span><a href="#37171551">parent</a><span>|</span><a href="#37172362">prev</a><span>|</span><a href="#37171608">next</a><span>|</span><label class="collapse" for="c-37171693">[-]</label><label class="expand" for="c-37171693">[1 more]</label></div><br/><div class="children"><div class="content">To take advantage of NoGIL you don’t necessarily need to use parallelism directly. But let’s say your web server or async task executor can be more efficient at sharing context between threads.</div><br/></div></div></div></div><div id="37171608" class="c"><input type="checkbox" id="c-37171608" checked=""/><div class="controls bullet"><span class="by">w10-1</span><span>|</span><a href="#37171551">prev</a><span>|</span><a href="#37172768">next</a><span>|</span><label class="collapse" for="c-37171608">[-]</label><label class="expand" for="c-37171608">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t imagine a more difficult test case for open source.<p>The decision is entirely reasonable, to move forward in test mode, treat multiple interpreters as an interim experiment, but target being concurrent.  The constraint of running old and new code in the same VM is a tall one.  Surprisingly, LWN&#x27;s summary says nothing about testing, which is largely unsolved and could lead to releases with unknown but serious bugs.  Microsoft, Facebook&#x2F;Meta, and Conda have stepped up with resources and a super-majority of core contributors want to move forward, but it&#x27;s unclear what happens if things get hard and more people are needed.<p>Meanwhile a crazy number of projects in academia and industry from web sites to big data to AI depend on Python.  The potential to export costs to python developers might be measured in percentage of GDP.<p>It doesn&#x27;t sound like the problems are even known yet.  It might be fairer to commit to the Faster CPython approach of knowable improvement over the next 3+ years, but have the concurrent Python promotors do more than just prototype.  They should analyze the kinds of problems that could present, how they could be detected, and what can be done about it.  This should be reviewed by people with backgrounds in proving concurrency guarantees.  Then the questions can be fairly presented to the steering committee, when the unknowns are at least identified.<p>There&#x27;s not a lot of program-management scale decision-making in open-source, but most have been relatively simple questions of driving the market in a given direction (apache, eclipse, linux).  This has real technical unknowns.<p>And I can&#x27;t help feeling they should also address inter-language ABI&#x27;s at the same time.  A big issue is matching the expected execution model of C.  Java&#x27;s foreign-function and memory interfaces have been incubating for many years, and Swift is also getting better at wrapping C and C++, but FFI&#x27;s are notoriously (and likely unnecessarily) difficult.</div><br/></div></div><div id="37172768" class="c"><input type="checkbox" id="c-37172768" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37171608">prev</a><span>|</span><a href="#37172311">next</a><span>|</span><label class="collapse" for="c-37172768">[-]</label><label class="expand" for="c-37172768">[2 more]</label></div><br/><div class="children"><div class="content">I want to see python with automatic and transparent concurrency.<p>Ie. The programmer writes:<p><pre><code>    for file in glob.glob(&#x27;*.jpg&#x27;):
      data.append(load_file(file))
</code></pre>
Then python itself parallelizes that work.   It should do that by identifying loops of pure functions and pushing them off to other threads.  One should also be able to mark a function as pure.    Python itself would be responsible for making everything appear as if the code ran serially - for example by buffering&#x2F;reordering log statements.<p>Rationale:    Python is designed to be easy and simple.   Concurrency in general is not that.   Python should do only do concurrency in a way that can be made easy and simple.</div><br/><div id="37172891" class="c"><input type="checkbox" id="c-37172891" checked=""/><div class="controls bullet"><span class="by">lmeyerov</span><span>|</span><a href="#37172768">parent</a><span>|</span><a href="#37172311">next</a><span>|</span><label class="collapse" for="c-37172891">[-]</label><label class="expand" for="c-37172891">[1 more]</label></div><br/><div class="children"><div class="content">Not sure if intentional: That&#x27;s a super frustrating program for a traditional automatic parallelization compiler, e.g., polyhedral:<p>* glob is an i&#x2F;o operation: impure<p>* load_file is an i&#x2F;o operation: impure<p>* data.append serializes part of the execution order<p>---<p>Interestingly, GPT4 does pretty ok:<p>files = glob.glob(&#x27;*.jpg&#x27;)<p>with ThreadPoolExecutor() as executor:<p><pre><code>    data = list(executor.map(load_file, files))
</code></pre>
---<p>Separately, if just going for concurrency for i&#x2F;o, async&#x2F;await is pretty amazing:<p><pre><code>    tasks = [load_file(file) for file in glob.glob(&#x27;*.jpg&#x27;)]

    data = await asyncio.gather(*tasks)
</code></pre>
A lot less program transformation needed as it&#x27;s essentially sugar over promises, and avoids much of the need to restructure code beyond the coloring problem. So I&#x27;m not as much about the case for a compiler here..</div><br/></div></div></div></div><div id="37172311" class="c"><input type="checkbox" id="c-37172311" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37172768">prev</a><span>|</span><a href="#37171652">next</a><span>|</span><label class="collapse" for="c-37172311">[-]</label><label class="expand" for="c-37172311">[5 more]</label></div><br/><div class="children"><div class="content">&gt;<i>There was a question from Shannon about &quot;what people think is a acceptable slowdown for single-threaded code&quot;. To a large extent, that question went unanswered in the thread, but he had estimated an impact &quot;in the 15-20% range, but it could be more, depending on the impact on PEP 659&quot;.</i><p>So, let me get this straight, some of those working in the project to &quot;make CPython faster&quot;, think it&#x27;s acceptable to overnight make most existing Python code 15-20% slower?<p>I&#x27;d say max 5% and that if the gil removal was a benefit to other optimizations going forward (they say the opposite: the change with complicate and stall their other optimizations).<p>Meanwhile, Shannon had a fundraising proposal for him personally &quot;speeding up CPython&quot; 5x back in 2020. Now it&#x27;s a whole team working on speeding up CPython with much larger corporate support, and it seems their targets are quite smaller?</div><br/><div id="37172332" class="c"><input type="checkbox" id="c-37172332" checked=""/><div class="controls bullet"><span class="by">tda</span><span>|</span><a href="#37172311">parent</a><span>|</span><a href="#37172441">next</a><span>|</span><label class="collapse" for="c-37172332">[-]</label><label class="expand" for="c-37172332">[2 more]</label></div><br/><div class="children"><div class="content">But consider that in perhaps 99% of current python code high performance (of the pure python code) is not the primary concern. And now for the times when performance does matter you can get a 20x (?) speedup without rewriting in another language.</div><br/><div id="37172406" class="c"><input type="checkbox" id="c-37172406" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37172311">root</a><span>|</span><a href="#37172332">parent</a><span>|</span><a href="#37172441">next</a><span>|</span><label class="collapse" for="c-37172406">[-]</label><label class="expand" for="c-37172406">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d expect in many those cases people care for performance, they already use multiprocess (so the impact from a 20% hit GIL-free CPython would actually be a slowdown).<p>And for those that do not use multiprocess, rewriting their currently single threaded Python code for no-gil shared-concurrency will indeed bring a speedup, but would still be a significant rewrite.<p>Also there are people using Python for things like web serving, where performance might have been secondary to convenience when they picked Python, but still a sudden 15-20% slowdown will seriously impact their server budgets (or ability to update versions)...</div><br/></div></div></div></div><div id="37172441" class="c"><input type="checkbox" id="c-37172441" checked=""/><div class="controls bullet"><span class="by">ml10sx</span><span>|</span><a href="#37172311">parent</a><span>|</span><a href="#37172332">prev</a><span>|</span><a href="#37171652">next</a><span>|</span><label class="collapse" for="c-37172441">[-]</label><label class="expand" for="c-37172441">[2 more]</label></div><br/><div class="children"><div class="content">I agree that the &quot;faster CPython&quot; project is way oversold. I have compared real world code with numerical and string operations without any network or disk accesses. 3.12-beta only uses 20-25% less time than 3.8.<p>That&#x27;s the level of 2.7.<p>I seems that the old boys are desperate for some bullet point features for the next release to impress their corporate masters. So they use the work of Sam Gross, but they will slowly get the credit over time.</div><br/><div id="37172561" class="c"><input type="checkbox" id="c-37172561" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#37172311">root</a><span>|</span><a href="#37172441">parent</a><span>|</span><a href="#37171652">next</a><span>|</span><label class="collapse" for="c-37172561">[-]</label><label class="expand" for="c-37172561">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a big history of projects underfunded, overpromising, and getting tossed aside, not even halfway there, when it comes to a faster Python.<p>In a PL-fantasy-league they would just heavily sponsor Lars Bak to create a team and work on a new Python interpreter!</div><br/></div></div></div></div></div></div><div id="37171652" class="c"><input type="checkbox" id="c-37171652" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#37172311">prev</a><span>|</span><a href="#37171960">next</a><span>|</span><label class="collapse" for="c-37171652">[-]</label><label class="expand" for="c-37171652">[6 more]</label></div><br/><div class="children"><div class="content">It’s more important to prioritize single-threaded performance because it’s much harder to improve by throwing money at the problem.<p>With multithreaded performance, you can just add another core to (more than) offset whatever overheads there are from using process-based parallelism.<p>I think that this entire GIL vs No-GIL dichotomy is misguided. The biggest problem people have with multiprocessing is that you can’t share memory. So add virtual processes with an <i>explicit</i> mechanism for memory sharing. Then you can keep all of your single-threaded optimizations like refcounting without barriers because the objects for one thread will stay in that thread.</div><br/><div id="37172767" class="c"><input type="checkbox" id="c-37172767" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#37171652">parent</a><span>|</span><a href="#37172300">next</a><span>|</span><label class="collapse" for="c-37172767">[-]</label><label class="expand" for="c-37172767">[1 more]</label></div><br/><div class="children"><div class="content">The fallacy here is assuming this is a necessary trade off. You can have good single threaded performance without a GIL. Rust has this notion of zero cost abstractions. It does Threads just fine. Java does single threaded stuff just fine as well. Lots of other languages do. This is not science fiction. The locking gets optimized away. Or you can choose to have code without locks at all. Or fine-grained&#x2F;structured concurrency. Something being thread safe or not is basically just an API contract. And of course optimistic locking is also a thing. There are no good reasons why python wouldn&#x27;t be able to do similar things. But you need to get rid of the GIL first.<p>A lot of the performance reduction in python without the Gil is basically just unaddressed technical debt. That should be fixable over time. Adding a lot of locks is a stop gap solution. That indeed makes things slower. But the proper fix is probably rethinking how that stuff works internally in a lot of places or having API contracts that document the thread safety or lack thereof.<p>And also having faster python runtimes and compilers actually enables re-implementing a lot of things that currently depend on native libraries in python. A lot of native code interactions are precisely where you need locks. Unless you change how that works. The point of removing the GIL is getting systematic about finding and fixing those things. It will get better over time.</div><br/></div></div><div id="37172300" class="c"><input type="checkbox" id="c-37172300" checked=""/><div class="controls bullet"><span class="by">Galanwe</span><span>|</span><a href="#37171652">parent</a><span>|</span><a href="#37172767">prev</a><span>|</span><a href="#37171960">next</a><span>|</span><label class="collapse" for="c-37172300">[-]</label><label class="expand" for="c-37172300">[4 more]</label></div><br/><div class="children"><div class="content">Agree 100%.<p>If you need concurrency at the moment, you have already switched to using multiprocessing, so having a no-GIL multithreading is useless.<p>The only issue with Python&#x2F;multiprocessing, is that sometimes you don&#x27;t want queues, but shared mutable state. And as you said, placing Python objects in shared memory at the moment is convoluted, restrictive, and suboptimal.<p>Fixing _that_ should be the objective. What Python need is better support for placing native instances in shared memory.</div><br/><div id="37172470" class="c"><input type="checkbox" id="c-37172470" checked=""/><div class="controls bullet"><span class="by">Znafon</span><span>|</span><a href="#37171652">root</a><span>|</span><a href="#37172300">parent</a><span>|</span><a href="#37171960">next</a><span>|</span><label class="collapse" for="c-37172470">[-]</label><label class="expand" for="c-37172470">[3 more]</label></div><br/><div class="children"><div class="content">&gt; If you need concurrency at the moment, you have already switched to using multiprocessing, so having a no-GIL multithreading is useless.<p>&gt; The only issue with Python&#x2F;multiprocessing, is that sometimes you don&#x27;t want queues, but shared mutable state. And as you said, placing Python objects in shared memory at the moment is convoluted, restrictive, and suboptimal.<p>The PEP goes into the motivation behind this work, and using multiple process does not magically solves all the issues:<p>&gt; Multiprocessing, with communication via shared memory or UNIX sockets, adds much complexity and in effect rules out interacting with CUDA from different workers, severely restricting the design space.<p>&gt; I reimplemented parts of HMMER, a standard method for multiple-sequence alignment. I chose this method because it stresses both single-thread performance (scoring) and multi-threaded performance (searching a database of sequences). The GIL became the bottleneck when using only eight threads. This is a method where the current popular implementations rely on 64 or even 128 threads per process. I tried moving to subprocesses but was blocked by the prohibitive IPC costs.<p>&gt; NumPy does release the GIL in its inner loops (which do the heavy lifting), but that is not nearly enough. NumPy doesn’t offer a solution to utilize all CPU cores of a single machine well, and instead leaves that to Dask and other multiprocessing solutions. Those aren’t very efficient and are also more clumsy to use. That clumsiness comes mainly in the extra abstractions and layers the users need to concern themselves with when using, e.g., dask.array which wraps numpy.ndarray. It also shows up in oversubscription issues that the user must explicitly be aware of and manage via either environment variables or a third package, threadpoolctl. The main reason is that NumPy calls into BLAS for linear algebra - and those calls it has no control over, they do use all cores by default via either pthreads or OpenMP.<p>and it discusses the alternatives at <a href="https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0703&#x2F;#alternatives" rel="nofollow noreferrer">https:&#x2F;&#x2F;peps.python.org&#x2F;pep-0703&#x2F;#alternatives</a>.</div><br/><div id="37172718" class="c"><input type="checkbox" id="c-37172718" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#37171652">root</a><span>|</span><a href="#37172470">parent</a><span>|</span><a href="#37171960">next</a><span>|</span><label class="collapse" for="c-37172718">[-]</label><label class="expand" for="c-37172718">[2 more]</label></div><br/><div class="children"><div class="content">You don’t need <i>OS</i> processes for multiprocessing. You can use threads in the same OS process. See: Erlang.</div><br/><div id="37172869" class="c"><input type="checkbox" id="c-37172869" checked=""/><div class="controls bullet"><span class="by">Znafon</span><span>|</span><a href="#37171652">root</a><span>|</span><a href="#37172718">parent</a><span>|</span><a href="#37171960">next</a><span>|</span><label class="collapse" for="c-37172869">[-]</label><label class="expand" for="c-37172869">[1 more]</label></div><br/><div class="children"><div class="content">Would the work on sub-interpreters be interested for that then (<a href="https:&#x2F;&#x2F;lwn.net&#x2F;SubscriberLink&#x2F;941090&#x2F;8bcb029dbf548f26&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;lwn.net&#x2F;SubscriberLink&#x2F;941090&#x2F;8bcb029dbf548f26&#x2F;</a>) ?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37171960" class="c"><input type="checkbox" id="c-37171960" checked=""/><div class="controls bullet"><span class="by">VagabundoP</span><span>|</span><a href="#37171652">prev</a><span>|</span><a href="#37171777">next</a><span>|</span><label class="collapse" for="c-37171960">[-]</label><label class="expand" for="c-37171960">[3 more]</label></div><br/><div class="children"><div class="content">Excellent write up as usual from LWN.<p>I love the Python community. It&#x27;s really a leading light for open source software. And it shows what transparency and good governance can achieve.<p>Although I appreciate the engineering hours that Meta, Microsoft and others give, its pretty miserable still compared to the value that the whole tech industry (and beyond with data science) extract from Python and other open source software.</div><br/><div id="37172422" class="c"><input type="checkbox" id="c-37172422" checked=""/><div class="controls bullet"><span class="by">stevesimmons</span><span>|</span><a href="#37171960">parent</a><span>|</span><a href="#37172707">next</a><span>|</span><label class="collapse" for="c-37172422">[-]</label><label class="expand" for="c-37172422">[1 more]</label></div><br/><div class="children"><div class="content">We can all help to change that, within our own organisations!<p>I did my bit at JPMorgan 8 years ago, convincing the tech leadership team to sponsor PyCon UK, plus a recruiting stand and supporting a group of junior developers from all JPMorgan&#x27;s UK locations to attend. I left JPM 5 years ago now, and they are still PyCon UK&#x27;s headline sponsor.<p>Compared with the enormous benefit we got from Python and its open source ecosystem, it was a totally negligible cost.</div><br/></div></div><div id="37172707" class="c"><input type="checkbox" id="c-37172707" checked=""/><div class="controls bullet"><span class="by">lrtzyv</span><span>|</span><a href="#37171960">parent</a><span>|</span><a href="#37172422">prev</a><span>|</span><a href="#37171777">next</a><span>|</span><label class="collapse" for="c-37172707">[-]</label><label class="expand" for="c-37172707">[1 more]</label></div><br/><div class="children"><div class="content">They pretend to be transparent. All actual decisions are made in backrooms.
The mailing lists are censored and the inner circle cannot be criticized.<p>Real contributors are exploited by those who work for the right corporations, do very little and go for any clerical position of power.<p>Do not be misled by the LWN articles, which are very kind and always namedrop the deciders. It is selective reporting.</div><br/></div></div></div></div><div id="37171777" class="c"><input type="checkbox" id="c-37171777" checked=""/><div class="controls bullet"><span class="by">Mayzie</span><span>|</span><a href="#37171960">prev</a><span>|</span><a href="#37172265">next</a><span>|</span><label class="collapse" for="c-37171777">[-]</label><label class="expand" for="c-37171777">[4 more]</label></div><br/><div class="children"><div class="content">I understand the desire for multi-threaded CPython scripts, but why not do a PHP5 -&gt; PHP7 and focus on single-thread performance more, particularly around implementing a JIT? We know from PyPy (and other JIT-enabled languages like NodeJS) that performance improvements can be significant. I feel like that should be the primary focus for speedups, and have always wondered why the idea has never been more popular. Has a CPython JIT been ruled out for some reason I&#x27;m not familiar with?</div><br/><div id="37172364" class="c"><input type="checkbox" id="c-37172364" checked=""/><div class="controls bullet"><span class="by">josefx</span><span>|</span><a href="#37171777">parent</a><span>|</span><a href="#37171889">next</a><span>|</span><label class="collapse" for="c-37172364">[-]</label><label class="expand" for="c-37172364">[1 more]</label></div><br/><div class="children"><div class="content">PyPy breaks compatibility with CPython in significant ways, as far as I understand the current no-gil proposal only got through because it has an automated fallback for modules that are not no-gil compatible.</div><br/></div></div><div id="37171889" class="c"><input type="checkbox" id="c-37171889" checked=""/><div class="controls bullet"><span class="by">turndown</span><span>|</span><a href="#37171777">parent</a><span>|</span><a href="#37172364">prev</a><span>|</span><a href="#37172086">next</a><span>|</span><label class="collapse" for="c-37171889">[-]</label><label class="expand" for="c-37171889">[1 more]</label></div><br/><div class="children"><div class="content">You should look into the copy &amp; patch efforts underway for Python[0]; an actual JIT will probably never exist but I think c&amp;p has a shot of being mainlined in the next few years, such that Python could dynamically choose to either run the interpreter or a c&amp;p option.<p>0: <a href="https:&#x2F;&#x2F;github.com&#x2F;faster-cpython&#x2F;ideas&#x2F;issues&#x2F;588">https:&#x2F;&#x2F;github.com&#x2F;faster-cpython&#x2F;ideas&#x2F;issues&#x2F;588</a></div><br/></div></div><div id="37172086" class="c"><input type="checkbox" id="c-37172086" checked=""/><div class="controls bullet"><span class="by">dagw</span><span>|</span><a href="#37171777">parent</a><span>|</span><a href="#37171889">prev</a><span>|</span><a href="#37172265">next</a><span>|</span><label class="collapse" for="c-37172086">[-]</label><label class="expand" for="c-37172086">[1 more]</label></div><br/><div class="children"><div class="content"><i>Has a CPython JIT been ruled out for some reason</i><p>There have been several attempts at integrating a JIT into CPython. Google made a stab at it back around version 2.6 and there was a second attempt around the 2-&gt;3 change to build a JIT on top of LLVM. However none of these projects produced results that the core developers felt were good enough across the board and had problems with backwards compatibility and lot cross platform LLVM issues (as it was a pretty new project) so they were dropped after Google stopped funding the project.<p>That being said there is still work being done and people are hoping for at least some JITing in upcoming python. Pyston is project being worked on Guido Van Possum himself and both Microsoft Instagram have their own JITed versions of Python they use internally. Some initial parts of Pyston are even scheduled for being included in python 3.12 or 3.13</div><br/></div></div></div></div><div id="37172265" class="c"><input type="checkbox" id="c-37172265" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#37171777">prev</a><span>|</span><a href="#37172015">next</a><span>|</span><label class="collapse" for="c-37172265">[-]</label><label class="expand" for="c-37172265">[3 more]</label></div><br/><div class="children"><div class="content">Multi-threading gets you programs that are, at the very most, 8x faster. Is it really worth it? If this makes it even slightly more difficult to develop Python for normal people then it will get forked, and that sucks. If it can be done without any impact then great, but I&#x27;m still not sure it&#x27;s worth it.</div><br/><div id="37172415" class="c"><input type="checkbox" id="c-37172415" checked=""/><div class="controls bullet"><span class="by">josefx</span><span>|</span><a href="#37172265">parent</a><span>|</span><a href="#37172015">next</a><span>|</span><label class="collapse" for="c-37172415">[-]</label><label class="expand" for="c-37172415">[2 more]</label></div><br/><div class="children"><div class="content">&gt; at the very most, 8x faster.<p>Where does that number come from? My current desktop has 32 logical cores and it has been a few years since I bought that one.</div><br/><div id="37172964" class="c"><input type="checkbox" id="c-37172964" checked=""/><div class="controls bullet"><span class="by">alecmg</span><span>|</span><a href="#37172265">root</a><span>|</span><a href="#37172415">parent</a><span>|</span><a href="#37172015">next</a><span>|</span><label class="collapse" for="c-37172964">[-]</label><label class="expand" for="c-37172964">[1 more]</label></div><br/><div class="children"><div class="content">maybe its a reference to Amdahl&#x27;s law?<p>If you can parallelise 90% of your code, you get only 10x improvement even on infinite number of cores</div><br/></div></div></div></div></div></div><div id="37172015" class="c"><input type="checkbox" id="c-37172015" checked=""/><div class="controls bullet"><span class="by">TekMol</span><span>|</span><a href="#37172265">prev</a><span>|</span><label class="collapse" for="c-37172015">[-]</label><label class="expand" for="c-37172015">[6 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    There was a question from Shannon about &quot;&quot;what people think is a
    acceptable slowdown for single-threaded code&quot;&quot; ... he had estimated
    an impact &quot;in the 15-20% range&quot;
</code></pre>
Horror!<p>To me, the acceptable slowdown is exactly zero.<p>I can already use multiple cores by running multiple Python processes.<p>Any slowdown of single-process performance would be a terrible step backwards.<p>Python is already slow. It should look at PHP and see what it can take out of their book. PHP 7, which had no JIT, was already about 6x faster than Python. PHP 8 is even faster for some workloads. I&#x27;m not sure if the overhead of a JIT makes sense though. But PHP 7 is a good place to look for a performance benchmark.</div><br/><div id="37172999" class="c"><input type="checkbox" id="c-37172999" checked=""/><div class="controls bullet"><span class="by">pkolaczk</span><span>|</span><a href="#37172015">parent</a><span>|</span><a href="#37172185">next</a><span>|</span><label class="collapse" for="c-37172999">[-]</label><label class="expand" for="c-37172999">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Horror!
&gt; To me, the acceptable slowdown is exactly zero.<p>If you care about single % of performance, why are you using Python in the first place? You&#x27;re likely losing 100x to more performant languages already (especially when taking about CPU-bound stuff).</div><br/></div></div><div id="37172185" class="c"><input type="checkbox" id="c-37172185" checked=""/><div class="controls bullet"><span class="by">dagw</span><span>|</span><a href="#37172015">parent</a><span>|</span><a href="#37172999">prev</a><span>|</span><a href="#37172291">next</a><span>|</span><label class="collapse" for="c-37172185">[-]</label><label class="expand" for="c-37172185">[2 more]</label></div><br/><div class="children"><div class="content"><i>To me, the acceptable slowdown is exactly zero.</i><p>This has been the reason most of the previous GIL removal projects failed. GvR had a strict zero slowdown in single-thread performance rule for any nogil patches and none of them managed that. I wonder if the core team has decided to abandon that rule now that GvR is gone?</div><br/><div id="37172997" class="c"><input type="checkbox" id="c-37172997" checked=""/><div class="controls bullet"><span class="by">alecmg</span><span>|</span><a href="#37172015">root</a><span>|</span><a href="#37172185">parent</a><span>|</span><a href="#37172291">next</a><span>|</span><label class="collapse" for="c-37172997">[-]</label><label class="expand" for="c-37172997">[1 more]</label></div><br/><div class="children"><div class="content">I saw comments that if introduced together with Faster CPython single thread performance optimisations, nogil would end up no slower than previous version. So it is best time to implement</div><br/></div></div></div></div><div id="37172291" class="c"><input type="checkbox" id="c-37172291" checked=""/><div class="controls bullet"><span class="by">ohgodplsno</span><span>|</span><a href="#37172015">parent</a><span>|</span><a href="#37172185">prev</a><span>|</span><a href="#37172042">next</a><span>|</span><label class="collapse" for="c-37172291">[-]</label><label class="expand" for="c-37172291">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I can already use multiple cores by running multiple Python processes.<p>Because you have terribly simple processes that don&#x27;t need to do synchronization. Even a simple parallel map would have Python scream and kick, because the GIL prevents it from having any kind of reasonable performance, and your multiprocess can&#x27;t handle that unless they all write to files.</div><br/></div></div></div></div></div></div></div></div></div></body></html>