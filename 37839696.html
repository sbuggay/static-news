<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1697014864179" as="style"/><link rel="stylesheet" href="styles.css?v=1697014864179"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://blog.replit.com/replit-code-v1_5">Replit&#x27;s new AI Model now available on Hugging Face</a> <span class="domain">(<a href="https://blog.replit.com">blog.replit.com</a>)</span></div><div class="subtext"><span>todsacerdoti</span> | <span>18 comments</span></div><br/><div><div id="37840864" class="c"><input type="checkbox" id="c-37840864" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#37842143">next</a><span>|</span><label class="collapse" for="c-37840864">[-]</label><label class="expand" for="c-37840864">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Intended use&quot; from their readme:<p>&gt; Replit intends this model be used by anyone as foundational model for application-specific fine-tuning without strict limitations on commercial use.<p>&gt; The model is trained specifically for code completion tasks.<p>Nice, I expected that I would need to give my E-Mail address to them and that it would be &quot;&quot;free&quot;&quot;.</div><br/></div></div><div id="37842143" class="c"><input type="checkbox" id="c-37842143" checked=""/><div class="controls bullet"><span class="by">infruset</span><span>|</span><a href="#37840864">prev</a><span>|</span><a href="#37841373">next</a><span>|</span><label class="collapse" for="c-37842143">[-]</label><label class="expand" for="c-37842143">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Encompasses Replit&#x27;s top 30 programming languages with a custom trained 32K vocabulary for high performance and coverage<p>Any idea where the list can be found?</div><br/></div></div><div id="37841373" class="c"><input type="checkbox" id="c-37841373" checked=""/><div class="controls bullet"><span class="by">IceHegel</span><span>|</span><a href="#37842143">prev</a><span>|</span><a href="#37842091">next</a><span>|</span><label class="collapse" for="c-37841373">[-]</label><label class="expand" for="c-37841373">[3 more]</label></div><br/><div class="children"><div class="content">I know a lot depends on architecture and number representation, but do people have a sense for how big a compute cluster is needed to train these classes of models from 1.5B, 3B, 7B, 13B, 70B?<p>Didn’t Meta say they trained on 2k A100s for LLama 2?</div><br/><div id="37841392" class="c"><input type="checkbox" id="c-37841392" checked=""/><div class="controls bullet"><span class="by">amasad</span><span>|</span><a href="#37841373">parent</a><span>|</span><a href="#37841408">next</a><span>|</span><label class="collapse" for="c-37841392">[-]</label><label class="expand" for="c-37841392">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re on a budget :) trained on 128 H100-80GB GPUs for a week (200B tokens over 5 epochs, ie 1T tokens).<p>Tech talk here with timestamp: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;veShHxQYPzo?si=UlcU9j2kC-C4oWvj&amp;t=2531">https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;veShHxQYPzo?si=UlcU9j2kC-C4oWvj...</a></div><br/></div></div><div id="37841408" class="c"><input type="checkbox" id="c-37841408" checked=""/><div class="controls bullet"><span class="by">grey8</span><span>|</span><a href="#37841373">parent</a><span>|</span><a href="#37841392">prev</a><span>|</span><a href="#37842091">next</a><span>|</span><label class="collapse" for="c-37841408">[-]</label><label class="expand" for="c-37841408">[1 more]</label></div><br/><div class="children"><div class="content">The Huggingface page of Replit 3Bs says &quot;The model has been trained on the MosaicML platform on 128 H100-80GB GPUs.&quot;<p>Source: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;replit&#x2F;replit-code-v1_5-3b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;replit&#x2F;replit-code-v1_5-3b</a><p>I&#x27;m not an ML engineer, just interested in the space - but as a general ballpark, training these models from scratch needs hundreds to thousands of GPUs.</div><br/></div></div></div></div><div id="37842091" class="c"><input type="checkbox" id="c-37842091" checked=""/><div class="controls bullet"><span class="by">iamsaitam</span><span>|</span><a href="#37841373">prev</a><span>|</span><a href="#37841200">next</a><span>|</span><label class="collapse" for="c-37842091">[-]</label><label class="expand" for="c-37842091">[2 more]</label></div><br/><div class="children"><div class="content">Anyone knows how to get this working locally with vscode?</div><br/><div id="37842167" class="c"><input type="checkbox" id="c-37842167" checked=""/><div class="controls bullet"><span class="by">infruset</span><span>|</span><a href="#37842091">parent</a><span>|</span><a href="#37841200">next</a><span>|</span><label class="collapse" for="c-37842167">[-]</label><label class="expand" for="c-37842167">[1 more]</label></div><br/><div class="children"><div class="content">I have the same question, and more generally: Any generic way of doing this for any of the open source or semi open source models, especially Mistral[0]?<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37675496">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37675496</a></div><br/></div></div></div></div><div id="37841200" class="c"><input type="checkbox" id="c-37841200" checked=""/><div class="controls bullet"><span class="by">krasin</span><span>|</span><a href="#37842091">prev</a><span>|</span><a href="#37841581">next</a><span>|</span><label class="collapse" for="c-37841200">[-]</label><label class="expand" for="c-37841200">[1 more]</label></div><br/><div class="children"><div class="content">Nice, Apache 2.0 license. Thank you, Replit!<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;replit&#x2F;replit-code-v1_5-3b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;replit&#x2F;replit-code-v1_5-3b</a></div><br/></div></div><div id="37841581" class="c"><input type="checkbox" id="c-37841581" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#37841200">prev</a><span>|</span><a href="#37841845">next</a><span>|</span><label class="collapse" for="c-37841581">[-]</label><label class="expand" for="c-37841581">[5 more]</label></div><br/><div class="children"><div class="content">Any vibe checks on this model? How does it compare to gpt4 for coding?</div><br/><div id="37841632" class="c"><input type="checkbox" id="c-37841632" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37841581">parent</a><span>|</span><a href="#37841845">next</a><span>|</span><label class="collapse" for="c-37841632">[-]</label><label class="expand" for="c-37841632">[4 more]</label></div><br/><div class="children"><div class="content">This being a 3B model isn&#x27;t remotely comparable to GPT4.<p>WizardCoder 34B and Phind 34B are the only models remotely comparable, and they are still slightly worse than GPT 3.5 (let alone GPT4).</div><br/><div id="37841737" class="c"><input type="checkbox" id="c-37841737" checked=""/><div class="controls bullet"><span class="by">aorth</span><span>|</span><a href="#37841581">root</a><span>|</span><a href="#37841632">parent</a><span>|</span><a href="#37842140">next</a><span>|</span><label class="collapse" for="c-37841737">[-]</label><label class="expand" for="c-37841737">[2 more]</label></div><br/><div class="children"><div class="content">How about Mistral 7B? I saw this article recently:<p><a href="https:&#x2F;&#x2F;wandb.ai&#x2F;byyoung3&#x2F;ml-news&#x2F;reports&#x2F;Fine-Tuning-Mistral7B-on-Python-Code-With-A-Single-GPU---Vmlldzo1NTg0NzY5" rel="nofollow noreferrer">https:&#x2F;&#x2F;wandb.ai&#x2F;byyoung3&#x2F;ml-news&#x2F;reports&#x2F;Fine-Tuning-Mistra...</a></div><br/><div id="37842110" class="c"><input type="checkbox" id="c-37842110" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#37841581">root</a><span>|</span><a href="#37841737">parent</a><span>|</span><a href="#37842140">next</a><span>|</span><label class="collapse" for="c-37842110">[-]</label><label class="expand" for="c-37842110">[1 more]</label></div><br/><div class="children"><div class="content">Mistral 7B is very cool for its size. But unfortunately no open model is close to GPT4 as of right now.</div><br/></div></div></div></div><div id="37842140" class="c"><input type="checkbox" id="c-37842140" checked=""/><div class="controls bullet"><span class="by">raverbashing</span><span>|</span><a href="#37841581">root</a><span>|</span><a href="#37841632">parent</a><span>|</span><a href="#37841737">prev</a><span>|</span><a href="#37841845">next</a><span>|</span><label class="collapse" for="c-37842140">[-]</label><label class="expand" for="c-37842140">[1 more]</label></div><br/><div class="children"><div class="content">But the thing is, it doesn&#x27;t need to know much about &quot;other stuff&quot;, just about code (and basic English instructions)<p>So comparing it with big models I&#x27;d say it&#x27;s good but might have limited usefulness<p>(you can probably go further with 3B with only code)</div><br/></div></div></div></div></div></div><div id="37841845" class="c"><input type="checkbox" id="c-37841845" checked=""/><div class="controls bullet"><span class="by">evertedsphere</span><span>|</span><a href="#37841581">prev</a><span>|</span><label class="collapse" for="c-37841845">[-]</label><label class="expand" for="c-37841845">[4 more]</label></div><br/><div class="children"><div class="content">&gt; When fine-tuned on public Replit user code, the model outperforms models of much larger size such as CodeLlama7B:<p>The table just below this shows the other models doing better on half of the benchmarks; the Replit column being in boldface is misleading.</div><br/><div id="37841896" class="c"><input type="checkbox" id="c-37841896" checked=""/><div class="controls bullet"><span class="by">itake</span><span>|</span><a href="#37841845">parent</a><span>|</span><a href="#37842083">next</a><span>|</span><label class="collapse" for="c-37841896">[-]</label><label class="expand" for="c-37841896">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the boldface is meant to mean &quot;better&quot;.<p>I just thought it was meant to draw attention to their numbers.</div><br/><div id="37841964" class="c"><input type="checkbox" id="c-37841964" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#37841845">root</a><span>|</span><a href="#37841896">parent</a><span>|</span><a href="#37842083">next</a><span>|</span><label class="collapse" for="c-37841964">[-]</label><label class="expand" for="c-37841964">[1 more]</label></div><br/><div class="children"><div class="content">The standard is to bold the best figure per column. If none are significantly different you don&#x27;t bold any generally but it&#x27;s standard practice to use this to highlight which approach is best in each task.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>