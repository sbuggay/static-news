<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684832454008" as="style"/><link rel="stylesheet" href="styles.css?v=1684832454008"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2305.10947">In Defense of Pure 16-Bit Floating-Point Neural Networks</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>belter</span> | <span>25 comments</span></div><br/><div><div id="36040173" class="c"><input type="checkbox" id="c-36040173" checked=""/><div class="controls bullet"><span class="by">ramity</span><span>|</span><a href="#36040370">next</a><span>|</span><label class="collapse" for="c-36040173">[-]</label><label class="expand" for="c-36040173">[2 more]</label></div><br/><div class="children"><div class="content">The precision used should match the requirements of the dataset, the training process, and the available compute. There are practical uses to 16-Bit FP training.<p>&quot;Our findings demonstrate that pure 16-bit floating-point neural networks can achieve similar or even better performance than their mixed-precision and 32-bit counterparts.&quot; This is a very deceptive statement. Take 100 initialization states and train a FP16 vs a FP32 network, and you&#x27;ll find FP32 will have an accuracy advantage. It&#x27;s certainly possible to conclude this if a small sample of networks are trained. This paper goes on to state, &quot;Lowering the precision of real numbers used for the neural network’s weights to fixed-point, as shown in [11], leads to a significant decrease in accuracy.&quot;, while later concluding, &quot;we have shown that pure 16-bit networks can perform on par with, if not better than, mixed-precision and 32-bit networks in various image classification tasks.&quot; The results certainly do, but that doesn&#x27;t really give an accurate evaluation of what&#x27;s really going on here. A FP64 network can fall into a local minima and be outperformed by a PF16 network, but is it correct to say the FP16 network is better. I&#x27;m getting a lot of mixed signals.<p>I feel like, &quot;significant implications&quot; is quite a stretch.<p>A few concerns: Besides figure 3, other results do not provide side-by-side test vs validation accuracy to attempt demonstrate the network is not overfit, and the only mention of normalization was the custom batch normalization operation.<p>This may more be a rant about the current state of ML, but in a perfect world, we wouldn&#x27;t use GPUs&#x2F;would enforce deterministic calculations, results would be replicable, we&#x27;d train hundreds if not thousands of networks to draw conclusions from, we&#x27;d better understand how to visualize network accuracies and overfitting, and all datasets would be free of bias and accurately generalize the problem attempting to be modelled. We can dream.</div><br/><div id="36040977" class="c"><input type="checkbox" id="c-36040977" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#36040173">parent</a><span>|</span><a href="#36040370">next</a><span>|</span><label class="collapse" for="c-36040977">[-]</label><label class="expand" for="c-36040977">[1 more]</label></div><br/><div class="children"><div class="content">This is generally incorrect. FP16 matches FP32 via bfloat usually with almost no sweat, and generally any additional noise tends to have a positive regularization effect.<p>I train directly in _pure_ fp16&#x2F;bf16 with no issues and the benefits greatly outweigh the tradeoffs. On smaller networks, I use 0 gradient clipping whatsoever.<p>FP32 has almost no uses outside of bizzarely intricate simulation-kinds of things, in which case FP64 is still generally important.</div><br/></div></div></div></div><div id="36040370" class="c"><input type="checkbox" id="c-36040370" checked=""/><div class="controls bullet"><span class="by">gumby</span><span>|</span><a href="#36040173">prev</a><span>|</span><a href="#36038724">next</a><span>|</span><label class="collapse" for="c-36040370">[-]</label><label class="expand" for="c-36040370">[2 more]</label></div><br/><div class="children"><div class="content">The authors should have referred to this Allen Institute paper in Nature back in 2021 which got good results with just one bit neurons!  They also published earlier work in this area going back to 2016 or 2017 but this more recent, refereed paper was what I quickly found in a web search:  <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-021-03705-x" rel="nofollow">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-021-03705-x</a><p>Also if you wanted you could get more resolution by just using the mantissa, not that any hardware supports that these days.  I love the 1-bit work but I suspect the future is four or 8 bit mantissa between 0 and 1.  Not sure you’d even need a GPU at that point, just a vector machine with a small lookup table in L1 cache.</div><br/><div id="36040863" class="c"><input type="checkbox" id="c-36040863" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#36040370">parent</a><span>|</span><a href="#36038724">next</a><span>|</span><label class="collapse" for="c-36040863">[-]</label><label class="expand" for="c-36040863">[1 more]</label></div><br/><div class="children"><div class="content">If you just want the mantissa, why not use integers?</div><br/></div></div></div></div><div id="36038724" class="c"><input type="checkbox" id="c-36038724" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#36040370">prev</a><span>|</span><a href="#36038358">next</a><span>|</span><label class="collapse" for="c-36038724">[-]</label><label class="expand" for="c-36038724">[2 more]</label></div><br/><div class="children"><div class="content">&gt; This theoretical exploration offers perspective that is distinct from the literature which attributes the success of low-precision neural networks to its regularization effect<p>They only tease in the abstract what their &quot;perspective&quot; is, defining only by what it is not. I can&#x27;t see it in the conclusion either. Unfortunately not up for reading through their formalisation to try and understand the point.<p>I have a denoising autoencoder that manages to ridiculously overfit a complex data set despite bottlenecking on a tiny set of neurons, which I attribute to it managing to exploit all the bits of precision within the bottleneck to effectively make it hold far more information than you would naively think. So I&#x27;m sceptical if they are saying this is not a real effect.</div><br/><div id="36039512" class="c"><input type="checkbox" id="c-36039512" checked=""/><div class="controls bullet"><span class="by">bckr</span><span>|</span><a href="#36038724">parent</a><span>|</span><a href="#36038358">next</a><span>|</span><label class="collapse" for="c-36039512">[-]</label><label class="expand" for="c-36039512">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Remark. It is important to acknowledge that previous research has highlighted the ability of neural
networks to tolerate noise and even act as a regularization method. However, our proposed analysis
differs in two ways. Firstly, it is the first comparison of its kind that focuses on pure 16-bit neural
networks. Secondly, our analysis offers a unique quantification approach that distinguishes it from
previous explanations related to regularization. Our approach examines floating-point errors and
formalizes the concept of tolerance, allowing for its quantification as a measure of comparison with
floating-point errors.<p>&gt; Explanation of the Lemma.... in the worst case.... But as long as... the two classifiers M16 and M32 must have the same classification result on x.<p>So my rough understanding is that they&#x27;re saying &quot;Hey, it&#x27;s not&#x2F;not only because of regularization; it&#x27;s because fp16 is really as good as fp32 for what we need it to do (but more efficient?&quot;</div><br/></div></div></div></div><div id="36038358" class="c"><input type="checkbox" id="c-36038358" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#36038724">prev</a><span>|</span><a href="#36039003">next</a><span>|</span><label class="collapse" for="c-36038358">[-]</label><label class="expand" for="c-36038358">[3 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it known for quite a few years now that these CNN networks (ResNet, VGG etc) trains well with FP16? The problem is that attention layer with softmax can have dynamic range higher than FP16 can handle hence you have to go to BF16? I am lost in what&#x27;s the novelty here in this paper.</div><br/><div id="36041122" class="c"><input type="checkbox" id="c-36041122" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#36038358">parent</a><span>|</span><a href="#36038473">next</a><span>|</span><label class="collapse" for="c-36041122">[-]</label><label class="expand" for="c-36041122">[1 more]</label></div><br/><div class="children"><div class="content">You can control range with a temperature, works pretty well! <a href="https:&#x2F;&#x2F;github.com&#x2F;tysam-code&#x2F;hlb-CIFAR10&#x2F;blob&#x2F;3bb104ce16d169e14783c406a3ac9b5a1e5f6678&#x2F;main.py#L324">https:&#x2F;&#x2F;github.com&#x2F;tysam-code&#x2F;hlb-CIFAR10&#x2F;blob&#x2F;3bb104ce16d16...</a></div><br/></div></div><div id="36038473" class="c"><input type="checkbox" id="c-36038473" checked=""/><div class="controls bullet"><span class="by">hedgehog</span><span>|</span><a href="#36038358">parent</a><span>|</span><a href="#36041122">prev</a><span>|</span><a href="#36039003">next</a><span>|</span><label class="collapse" for="c-36038473">[-]</label><label class="expand" for="c-36038473">[1 more]</label></div><br/><div class="children"><div class="content">It looks like they&#x27;re formalizing the behavior of pure 16-bit training which is different from the mixed precision pipelines I&#x27;m aware of.</div><br/></div></div></div></div><div id="36039003" class="c"><input type="checkbox" id="c-36039003" checked=""/><div class="controls bullet"><span class="by">em3rgent0rdr</span><span>|</span><a href="#36038358">prev</a><span>|</span><a href="#36041022">next</a><span>|</span><label class="collapse" for="c-36039003">[-]</label><label class="expand" for="c-36039003">[6 more]</label></div><br/><div class="children"><div class="content">Why stop at 16-bit?  I&#x27;d be curious to see a study that tries every number of bits from 32-bit down.  I see <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minifloat" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minifloat</a> says there is an 8-bit float which uses 4-bits for exponent and 3-bits for significand.  Maybe there is a sweet spot between 8-to-16 bits for a good-enough accuracy tradeoff.  Of course the hardware for that isn&#x27;t standard, but maybe low-bit float hardware would be useful.</div><br/><div id="36040550" class="c"><input type="checkbox" id="c-36040550" checked=""/><div class="controls bullet"><span class="by">_nalply</span><span>|</span><a href="#36039003">parent</a><span>|</span><a href="#36039083">next</a><span>|</span><label class="collapse" for="c-36040550">[-]</label><label class="expand" for="c-36040550">[1 more]</label></div><br/><div class="children"><div class="content">Posits seems to be better for 8-bit or even 6-bit. There is only one not-a-number place, the NaR (not a real). This means for 6-bit posits you have 63 points in the number space.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Unum_(number_format)#Unum_III" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Unum_(number_format)#Unum_III</a></div><br/></div></div><div id="36039083" class="c"><input type="checkbox" id="c-36039083" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#36039003">parent</a><span>|</span><a href="#36040550">prev</a><span>|</span><a href="#36041022">next</a><span>|</span><label class="collapse" for="c-36039083">[-]</label><label class="expand" for="c-36039083">[4 more]</label></div><br/><div class="children"><div class="content">What would it mean though? If the information is embedded in the network, and we find some performance figure of merit, wouldn&#x27;t it probably be about the same performance when normalized to power utilization?<p>Maybe it&#x27;s about optimizing every clock cycle?</div><br/><div id="36039116" class="c"><input type="checkbox" id="c-36039116" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#36039003">root</a><span>|</span><a href="#36039083">parent</a><span>|</span><a href="#36041022">next</a><span>|</span><label class="collapse" for="c-36039116">[-]</label><label class="expand" for="c-36039116">[3 more]</label></div><br/><div class="children"><div class="content">Why would you expect performance to be constant when normalized to power utilisation?<p>If your 16 bit floats perform about the same as 32 bit floats in an absolute sense, then they will probably perform even better when normalised for power utilisation.<p>And if 16 bit work, 15 bit floats might perform well, too, for all we know.  That&#x27;s what the original commenter was getting at, I think.</div><br/><div id="36039204" class="c"><input type="checkbox" id="c-36039204" checked=""/><div class="controls bullet"><span class="by">em3rgent0rdr</span><span>|</span><a href="#36039003">root</a><span>|</span><a href="#36039116">parent</a><span>|</span><a href="#36041022">next</a><span>|</span><label class="collapse" for="c-36039204">[-]</label><label class="expand" for="c-36039204">[2 more]</label></div><br/><div class="children"><div class="content">Yes, that is what I was getting at.  Floating point hardware with fewer bits have less complexity and so have smaller transistor die area and power consumption.</div><br/><div id="36041084" class="c"><input type="checkbox" id="c-36041084" checked=""/><div class="controls bullet"><span class="by">WithinReason</span><span>|</span><a href="#36039003">root</a><span>|</span><a href="#36039204">parent</a><span>|</span><a href="#36041022">next</a><span>|</span><label class="collapse" for="c-36041084">[-]</label><label class="expand" for="c-36041084">[1 more]</label></div><br/><div class="children"><div class="content">If I recall correctly, area&#x2F;power is proportional to the square of the length of the mantissa.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36041022" class="c"><input type="checkbox" id="c-36041022" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#36039003">prev</a><span>|</span><a href="#36021653">next</a><span>|</span><label class="collapse" for="c-36041022">[-]</label><label class="expand" for="c-36041022">[1 more]</label></div><br/><div class="children"><div class="content">As a practitioner specializing in extremely fast-training neural networks, seeing a paper in 2023 considering fp32 as a gold standard over pure non-mixed fp16&#x2F;bp16 is a bit shocking to me and feels dated&#x2F;distracting from the discussion. They make good points but unless I am hopelessly misinformed, it&#x27;s been pretty well established at this point in a number of circles that fp32 is overkill for the majority of uses for many modern-day practitioners. Loads of networks train directly in bfloat16 as the standard -- a lot of the modern LLMs among them. Mixed precision is very much no longer needed, not even with fp16 if you&#x27;re willing to tolerate some range hacks. If you don&#x27;t want the range hacks, just use bfloat16 directly. The complexity is not worth it, adds not much at all, and the dynamic loss scaler a lot of people use is just begging for more issues.<p>Both of the main repos that I&#x27;ve published in terms of speed benchmarks train directly in pure fp16 and bf16 respectively without any fp32 frippery, if you want to see an example of both paradigms successfully feel free to take a look (I&#x27;ll note that bf16 is simpler on the whole for a few reasons, generally seamless): <a href="https:&#x2F;&#x2F;github.com&#x2F;tysam-code&#x2F;hlb-CIFAR10">https:&#x2F;&#x2F;github.com&#x2F;tysam-code&#x2F;hlb-CIFAR10</a> [for fp16] and <a href="https:&#x2F;&#x2F;github.com&#x2F;tysam-code&#x2F;hlb-gpt">https:&#x2F;&#x2F;github.com&#x2F;tysam-code&#x2F;hlb-gpt</a> [for bf16]<p>Personally from my experience, I think fp16&#x2F;bf16 is honestly a bit too expressive for what we need, fp8 seems to do just fine and I think will be quite alright with some accommodations, just as with pure fp16. The what and the how of that is a story for a different day (and at this point, the max pooling operation is basically one of the slowest now).<p>You&#x27;ll have to excuse my frustration a bit, it just is a bit jarring to see a streetsign from way in the past fly forward in the wind to hit you in the face before tumbling on its merry way. And additionally in the comment section the general discussion doesn&#x27;t seem to talk about what seems to be a pretty clearly-established consensus in certain research circles. It&#x27;s not really too much of a debate anymore, it works and we&#x27;re off to bigger and better problems that I think we should talk about. I guess in one sense it does justify the paper&#x27;s utility, but also a bit frustrating because it normalizes the conversation as a few notches back from where I personally feel that it actually is at the moment.<p>We&#x27;ve got to move out of the past, this fp32 business to me personally is like writing a Relu-activated VGG network in Keras on Tensorflow. Phew.<p>And while we&#x27;re at it, if I shall throw my frumpy-grumpy hat right back into the ring, this is an information-theoretic problem! Not enough discussion of Shannon and co. Let&#x27;s please fix that too. See my other rants for x-references to that, should you be so-inclined to punish yourself in that manner.</div><br/></div></div><div id="36021653" class="c"><input type="checkbox" id="c-36021653" checked=""/><div class="controls bullet"><span class="by">belter</span><span>|</span><a href="#36041022">prev</a><span>|</span><a href="#36038671">next</a><span>|</span><label class="collapse" for="c-36021653">[-]</label><label class="expand" for="c-36021653">[1 more]</label></div><br/><div class="children"><div class="content">&quot;...Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption...Our findings demonstrate that pure 16-bit floating-point neural networks can achieve similar or even better performance than their mixed-precision and 32-bit counterparts. We believe the results presented in this paper will have significant implications for machine learning practitioners, offering an opportunity to reconsider using pure 16-bit networks in various applications...&quot;</div><br/></div></div><div id="36038671" class="c"><input type="checkbox" id="c-36038671" checked=""/><div class="controls bullet"><span class="by">pmarreck</span><span>|</span><a href="#36021653">prev</a><span>|</span><label class="collapse" for="c-36038671">[-]</label><label class="expand" for="c-36038671">[7 more]</label></div><br/><div class="children"><div class="content">Would posits accomplish this even better, if hardware support wasn’t an issue?</div><br/><div id="36039086" class="c"><input type="checkbox" id="c-36039086" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#36038671">parent</a><span>|</span><a href="#36041137">next</a><span>|</span><label class="collapse" for="c-36039086">[-]</label><label class="expand" for="c-36039086">[2 more]</label></div><br/><div class="children"><div class="content">A (16, 1) posit gives a maximum of 2 extra fractional bits of precision over float16, and (16, 2) gives you 1 extra bit. But also while the numerical distribution a posit assumes is slightly better but still not well geared to the distribution of values as seen in NNs in any case, it&#x27;s not going to get you that much more (you hardly need anything above 10.0 because people like hyperparameters that look like 0.01 or whatever, even though with typical floating point you can multiply all the values by 10^-3 or 10^3 (or in float32, even 10^10 or 10^-10) and things would work just as well.<p>A posit is also more expensive in hardware, because the adders, multipliers and shifters are bigger (provisioned for the maximum fractional precision).<p>The trick is still in the scaling still. I&#x27;ve done full training in (16, 1) posit (like, everything in (16, 1), no float32 or shadow floating point values, like this paper) for some of these convnets. It doesn&#x27;t work well out of the box without doing scaling tricks, and then it&#x27;s ~the same as float16 with the tricks I find. It simply doesn&#x27;t add that much more precision in such reduced space (that 1 or 2 extra bits at best).<p>What they benchmarked on here in this paper is ancient history too, not sure how these models are that relevant to modern practice these days.</div><br/><div id="36039489" class="c"><input type="checkbox" id="c-36039489" checked=""/><div class="controls bullet"><span class="by">adgjlsfhk1</span><span>|</span><a href="#36038671">root</a><span>|</span><a href="#36039086">parent</a><span>|</span><a href="#36041137">next</a><span>|</span><label class="collapse" for="c-36039489">[-]</label><label class="expand" for="c-36039489">[1 more]</label></div><br/><div class="children"><div class="content">The big advantage of Posit16 over Float16 is way less overflow potential. Float16s have 1 less bit than Posit16, but they also max out at 65000 which can cause a lot of overflow&#x2F;NaNs. BFloats have a really big range (up to 10^38), but they only have a 7 bit mantissa. Posit16 gives you higher accuracy than BFloat16 for the entire range of Float16, while being almost as resistant to overflow as BFloat16. Yes, you lose a lot of accuracy for the big posits, but in a NN context, often for the giant values all you care about is that it&#x27;s large and infinite. The hardware is a little more expensive, but it has a lot fewer special cases and you can share a lot more operations with Int hardware (although none of the expensive stuff).</div><br/></div></div></div></div><div id="36041137" class="c"><input type="checkbox" id="c-36041137" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#36038671">parent</a><span>|</span><a href="#36039086">prev</a><span>|</span><a href="#36039138">next</a><span>|</span><label class="collapse" for="c-36041137">[-]</label><label class="expand" for="c-36041137">[1 more]</label></div><br/><div class="children"><div class="content">This paper feels behind by a few years, I think. But bfloat16 and fp16 are both natively supported in hardware.<p>We&#x27;re down to fp8 now with NVIDIA&#x27;s latest hardware. This conversation is wayyyyy back from where it is in a few other places. FP8 even shouldn&#x27;t be a huge issue (at least for mixed at first), it&#x27;s things like the 4-bit datatypes and such where things really and truly get spicy IMO.</div><br/></div></div><div id="36039138" class="c"><input type="checkbox" id="c-36039138" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#36038671">parent</a><span>|</span><a href="#36041137">prev</a><span>|</span><a href="#36038916">next</a><span>|</span><label class="collapse" for="c-36039138">[-]</label><label class="expand" for="c-36039138">[1 more]</label></div><br/><div class="children"><div class="content">For people who don&#x27;t know posits (like me), <a href="https:&#x2F;&#x2F;www.cs.cornell.edu&#x2F;courses&#x2F;cs6120&#x2F;2019fa&#x2F;blog&#x2F;posits&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.cs.cornell.edu&#x2F;courses&#x2F;cs6120&#x2F;2019fa&#x2F;blog&#x2F;posits...</a> provides a good introduction, if somewhat tongue-in-cheek.<p>See discussion at <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30856623" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30856623</a></div><br/></div></div><div id="36038916" class="c"><input type="checkbox" id="c-36038916" checked=""/><div class="controls bullet"><span class="by">nestorD</span><span>|</span><a href="#36038671">parent</a><span>|</span><a href="#36039138">prev</a><span>|</span><label class="collapse" for="c-36038916">[-]</label><label class="expand" for="c-36038916">[2 more]</label></div><br/><div class="children"><div class="content">Somewhat because the few bits they save on things like Nan give them a sensible boost on those low precisions.
But the best way to go is to explucitly fit the distribution of values you are expecting: precisely what Meta did when they introduced their own 16bits format.</div><br/><div id="36039962" class="c"><input type="checkbox" id="c-36039962" checked=""/><div class="controls bullet"><span class="by">pmarreck</span><span>|</span><a href="#36038671">root</a><span>|</span><a href="#36038916">parent</a><span>|</span><label class="collapse" for="c-36039962">[-]</label><label class="expand" for="c-36039962">[1 more]</label></div><br/><div class="children"><div class="content">What is the name of the Meta custom 16 bit format?</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>