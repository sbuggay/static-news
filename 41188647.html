<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723107666744" as="style"/><link rel="stylesheet" href="styles.css?v=1723107666744"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/karpathy/status/1821277264996352246">RLHF is just barely RL</a> <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>tosh</span> | <span>28 comments</span></div><br/><div><div id="41189328" class="c"><input type="checkbox" id="c-41189328" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#41189437">next</a><span>|</span><label class="collapse" for="c-41189328">[-]</label><label class="expand" for="c-41189328">[2 more]</label></div><br/><div class="children"><div class="content">A cheap DIY way of achieving the same thing as RLHF is to fine tune the model to append a score to its output every time.<p>Remember: The reason we need RLHF at all is that we cannot write a loss function for what makes a good answer. There are just many ways a good answer could look like, which cannot be calculated on the basis of next-token-probability.<p>So you start by having your vanilla model generate n completions for your prompt. You the. manually score them. And then those prompt =&gt; (completion,score) pairs become your training set.<p>Once the model is trained, you may find that you can cheat:<p>Because if you include the desired score in your prompt, the model will now strive to produce an answer that is consistent with that score.</div><br/><div id="41189459" class="c"><input type="checkbox" id="c-41189459" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41189328">parent</a><span>|</span><a href="#41189437">next</a><span>|</span><label class="collapse" for="c-41189459">[-]</label><label class="expand" for="c-41189459">[1 more]</label></div><br/><div class="children"><div class="content">That works in the same way as actor-critic pair, right? Just all wrapped in the same network&#x2F;output?</div><br/></div></div></div></div><div id="41189437" class="c"><input type="checkbox" id="c-41189437" checked=""/><div class="controls bullet"><span class="by">daly</span><span>|</span><a href="#41189328">prev</a><span>|</span><a href="#41189425">next</a><span>|</span><label class="collapse" for="c-41189437">[-]</label><label class="expand" for="c-41189437">[1 more]</label></div><br/><div class="children"><div class="content">I think that the field of proofs, such as LEAN, which have states (the current
subgoal), actions (the applicable theorems, especially effective in LEAN due
to strong Typing of arguments), a progress measure (simplified subgoals), 
a final goal state (the proof completes), and a hierarchy in the theorems
so there is a &quot;path metric&quot; from simple theorems to complex theorems.<p>If Karpathy were to focus on automating LEAN proofs it could change mathematics forever.</div><br/></div></div><div id="41189425" class="c"><input type="checkbox" id="c-41189425" checked=""/><div class="controls bullet"><span class="by">toxik</span><span>|</span><a href="#41189437">prev</a><span>|</span><a href="#41189108">next</a><span>|</span><label class="collapse" for="c-41189425">[-]</label><label class="expand" for="c-41189425">[1 more]</label></div><br/><div class="children"><div class="content">Another little tidbit about RLHF and InstructGPT is that the training scheme is by far dominated by supervised learning. There is a bit of RL sprinkled on top, but the term is scaled down by a lot and 8x more compute time is spent on the supervised loss terms.</div><br/></div></div><div id="41189108" class="c"><input type="checkbox" id="c-41189108" checked=""/><div class="controls bullet"><span class="by">normie3000</span><span>|</span><a href="#41189425">prev</a><span>|</span><a href="#41189347">next</a><span>|</span><label class="collapse" for="c-41189108">[-]</label><label class="expand" for="c-41189108">[1 more]</label></div><br/><div class="children"><div class="content">&gt; In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent to human preferences.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Reinforcement_learning_from_human_feedback" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Reinforcement_learning_from_...</a></div><br/></div></div><div id="41189347" class="c"><input type="checkbox" id="c-41189347" checked=""/><div class="controls bullet"><span class="by">gizmo</span><span>|</span><a href="#41189108">prev</a><span>|</span><a href="#41189232">next</a><span>|</span><label class="collapse" for="c-41189347">[-]</label><label class="expand" for="c-41189347">[1 more]</label></div><br/><div class="children"><div class="content">This is why AI coding assistance will leap ahead in the coming years. Chat AI has no clear reward function (basically impossible to judge the quality of responses to open-ended questions like historical causes for a war). Coding AI can write tests, write code, compile, examine failed test cases, search for different coding solutions that satisfy more test cases or rewrite the tests, all in an unsupervised loop. And then whole process can turn into training data for future AI coding models.<p>I expect language models to also get crazy good at mathematical theorem proving. The search space is huge but theorem verification software will provide 100% accurate feedback that makes real reinforcement learning possible. It&#x27;s the combination of vibes (how to approach the proof) and formal verification that works.<p>Formal verification of program correctness never got traction because it&#x27;s so tedious and most of the time approximately correct is good enough. But with LLMs in the mix the equation changes. Having LLMs generate annotations that an engine can use to prove correctness might be the missing puzzle piece.</div><br/></div></div><div id="41189232" class="c"><input type="checkbox" id="c-41189232" checked=""/><div class="controls bullet"><span class="by">rocqua</span><span>|</span><a href="#41189347">prev</a><span>|</span><a href="#41189336">next</a><span>|</span><label class="collapse" for="c-41189232">[-]</label><label class="expand" for="c-41189232">[2 more]</label></div><br/><div class="children"><div class="content">Alphago didn&#x27;t have human feedback, but it did learn from humans before surpassing them. Specifically, it had a network to &#x27;suggest good moves&#x27; that was trained on predicting moves from pro level human games.<p>The entire point of alpha zero was to eliminate this human influence, and go with pure reinforcement learning (i.e. zero human influence).</div><br/><div id="41189355" class="c"><input type="checkbox" id="c-41189355" checked=""/><div class="controls bullet"><span class="by">cherryteastain</span><span>|</span><a href="#41189232">parent</a><span>|</span><a href="#41189336">next</a><span>|</span><label class="collapse" for="c-41189355">[-]</label><label class="expand" for="c-41189355">[1 more]</label></div><br/><div class="children"><div class="content">A game like Go has a clearly defined objective (win the game or not). A network like you described can therefore be trained to give a score to each move. Point here is that assessing whether a given sentence sounds good to humans or not does not have a clearly defined objective, the only way we came up with so far is to ask real humans.</div><br/></div></div></div></div><div id="41189336" class="c"><input type="checkbox" id="c-41189336" checked=""/><div class="controls bullet"><span class="by">jrflowers</span><span>|</span><a href="#41189232">prev</a><span>|</span><a href="#41189412">next</a><span>|</span><label class="collapse" for="c-41189336">[-]</label><label class="expand" for="c-41189336">[1 more]</label></div><br/><div class="children"><div class="content">I enjoyed this Karpathy post about how there is absolutely no extant solution to training language models to reliably solve open ended problems.<p>I preferred Zitron’s point* that we would need to invent several branches of science to solve this problem, but it’s good to see the point made tweet-sized.<p>*<a href="https:&#x2F;&#x2F;www.wheresyoured.at&#x2F;to-serve-altman&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.wheresyoured.at&#x2F;to-serve-altman&#x2F;</a></div><br/></div></div><div id="41189412" class="c"><input type="checkbox" id="c-41189412" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#41189336">prev</a><span>|</span><a href="#41189278">next</a><span>|</span><label class="collapse" for="c-41189412">[-]</label><label class="expand" for="c-41189412">[1 more]</label></div><br/><div class="children"><div class="content">It always annoys and amazes me that people in this field have no basic understanding that closed-world finite-information abstract games are a unique and trivial problem. So much of the so-called &quot;world model&quot; ideological mumbojumbo comes from these setups.<p>Sampling board state from an abstract board space isn&#x27;t a statistical inference problem. There&#x27;s no missing information.<p>The whole edifice of science is a set of experimental and inferential practices to overcome the massive information gap between the state of a measuring device and the state of what, we believe, it measures.<p>In the case of natural language the gap between a sequence of symbols, &quot;the war in ukraine&quot; and those aspects of the world these symbols refer to is <i>enormous</i>.<p>The idea that there is even a RL-style &quot;reward&quot; function to describe this gap is pseudoscience. As is the false equivocation between sampling of abstracta such as games, and <i>measuring the world</i>.</div><br/></div></div><div id="41189278" class="c"><input type="checkbox" id="c-41189278" checked=""/><div class="controls bullet"><span class="by">timthelion</span><span>|</span><a href="#41189412">prev</a><span>|</span><a href="#41189317">next</a><span>|</span><label class="collapse" for="c-41189278">[-]</label><label class="expand" for="c-41189278">[5 more]</label></div><br/><div class="children"><div class="content">Karpathy writes that there is no cheeply computed objective check for &quot;Or re-writing some Java code to Python? &quot; Among other things. But it seems to me that Reinforced Learning should be possible for code translation using automated integration testing. Run it, see if it does,the same thing!</div><br/><div id="41189384" class="c"><input type="checkbox" id="c-41189384" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#41189278">parent</a><span>|</span><a href="#41189398">next</a><span>|</span><label class="collapse" for="c-41189384">[-]</label><label class="expand" for="c-41189384">[2 more]</label></div><br/><div class="children"><div class="content">Even there how you score it is hard.<p>&quot;Is it the same for this s y of inputs?&quot; May be fine for a subset of things, but then that&#x27;s  a binary thing. If it&#x27;s slightly wrong do you score by number of outputs that match? A purely binary thing gives little useful help for nudging a model in the right direction. How do you compare two that both work, which is more &quot;idiomatic&quot;?</div><br/><div id="41189450" class="c"><input type="checkbox" id="c-41189450" checked=""/><div class="controls bullet"><span class="by">theqwxas</span><span>|</span><a href="#41189278">root</a><span>|</span><a href="#41189384">parent</a><span>|</span><a href="#41189398">next</a><span>|</span><label class="collapse" for="c-41189450">[-]</label><label class="expand" for="c-41189450">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it&#x27;s a very difficult problem. I&#x27;d like to mention AlphaDev [0], an RL algorithm that builds other algorithms, there they combined the measure of correctness and a measure of algorithm speed (latency) to get the reward. But the algorithms they built were super small (e.g., sorting just three numbers), therefore they could measure correctness using all input combinations. It is still unclear how to scale this to larger problems.<p>[0] <a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;alphadev-discovers-faster-sorting-algorithms&#x2F;" rel="nofollow">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;alphadev-discovers-fas...</a></div><br/></div></div></div></div><div id="41189398" class="c"><input type="checkbox" id="c-41189398" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#41189278">parent</a><span>|</span><a href="#41189384">prev</a><span>|</span><a href="#41189325">next</a><span>|</span><label class="collapse" for="c-41189398">[-]</label><label class="expand" for="c-41189398">[1 more]</label></div><br/><div class="children"><div class="content">“Programs must be written for people to read, and only incidentally for machines to execute.” — Harold Abelson</div><br/></div></div><div id="41189325" class="c"><input type="checkbox" id="c-41189325" checked=""/><div class="controls bullet"><span class="by">jeroenvlek</span><span>|</span><a href="#41189278">parent</a><span>|</span><a href="#41189398">prev</a><span>|</span><a href="#41189317">next</a><span>|</span><label class="collapse" for="c-41189325">[-]</label><label class="expand" for="c-41189325">[1 more]</label></div><br/><div class="children"><div class="content">My takeaway is that it&#x27;s difficult to make a &quot;generic enough&quot; evaluation that encompasses all things we use an LLM for, e.g. code, summaries, jokes. Something with free lunches.</div><br/></div></div></div></div><div id="41189219" class="c"><input type="checkbox" id="c-41189219" checked=""/><div class="controls bullet"><span class="by">codeflo</span><span>|</span><a href="#41189317">prev</a><span>|</span><a href="#41189291">next</a><span>|</span><label class="collapse" for="c-41189219">[-]</label><label class="expand" for="c-41189219">[3 more]</label></div><br/><div class="children"><div class="content">Reminder that AlphaGo and its successors have <i>not</i> solved Go and that reinforcement learning still sucks when encountering out-of-distribution strategies:<p><a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;02&#x2F;man-beats-machine-at-go-in-human-victory-over-ai&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;information-technology&#x2F;2023&#x2F;02&#x2F;man-b...</a></div><br/><div id="41189415" class="c"><input type="checkbox" id="c-41189415" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#41189219">parent</a><span>|</span><a href="#41189339">next</a><span>|</span><label class="collapse" for="c-41189415">[-]</label><label class="expand" for="c-41189415">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t say it sucks. You just need to keep training it for as long as needed. You can do adversarial techniques to generate new paths. You can also use the winning human strategies to further improve. Hopefully we&#x27;ll find better approaches, but this is extremely successful and far from sucking.<p>Sure, Go is not solved yet. But RL is just fine continuing to that asymptote for as long as we want.<p>The funny part is that this applies to people too. Masters don&#x27;t like to play low ranked people because they&#x27;re unpredictable and the ELO loss for them is not worth the risk.</div><br/></div></div><div id="41189339" class="c"><input type="checkbox" id="c-41189339" checked=""/><div class="controls bullet"><span class="by">thanhdotr</span><span>|</span><a href="#41189219">parent</a><span>|</span><a href="#41189415">prev</a><span>|</span><a href="#41189291">next</a><span>|</span><label class="collapse" for="c-41189339">[-]</label><label class="expand" for="c-41189339">[1 more]</label></div><br/><div class="children"><div class="content">well, as yann lecun said :<p>&quot;Adversarial training, RLHF, and input-space contrastive methods have limited performance.
Why?
Because input spaces are <i>BIG</i>.
There are just too many ways to be wrong&quot; [1]<p>A way to solve the problem is projecting onto latent space and then try and discriminate&#x2F;predict the best action down there. There&#x27;s much less feature correlation down in latent space than in your observation space. [2]<p>[1]:<a href="https:&#x2F;&#x2F;x.com&#x2F;ylecun&#x2F;status&#x2F;1803696298068971992" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;ylecun&#x2F;status&#x2F;1803696298068971992</a>
[2]: <a href="https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=BZ5a1r-kVsf" rel="nofollow">https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=BZ5a1r-kVsf</a></div><br/></div></div></div></div><div id="41189258" class="c"><input type="checkbox" id="c-41189258" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#41189291">prev</a><span>|</span><label class="collapse" for="c-41189258">[-]</label><label class="expand" for="c-41189258">[7 more]</label></div><br/><div class="children"><div class="content">Been shouting this for over a year now. We&#x27;re training AI to be convincing, not to be actually helpful. We&#x27;re sampling the wrong distributions.</div><br/><div id="41189304" class="c"><input type="checkbox" id="c-41189304" checked=""/><div class="controls bullet"><span class="by">dgb23</span><span>|</span><a href="#41189258">parent</a><span>|</span><a href="#41189321">next</a><span>|</span><label class="collapse" for="c-41189304">[-]</label><label class="expand" for="c-41189304">[2 more]</label></div><br/><div class="children"><div class="content">Depends on who you ask.<p>Advertisement and propaganda is not necessarily helpful for consumers, but just needs to be convincing in order to be helpful for producers.</div><br/><div id="41189452" class="c"><input type="checkbox" id="c-41189452" checked=""/><div class="controls bullet"><span class="by">khafra</span><span>|</span><a href="#41189258">root</a><span>|</span><a href="#41189304">parent</a><span>|</span><a href="#41189321">next</a><span>|</span><label class="collapse" for="c-41189452">[-]</label><label class="expand" for="c-41189452">[1 more]</label></div><br/><div class="children"><div class="content">It would be interesting to see RL on a chatbot that&#x27;s the last stage of a sales funnel for some high-volume item--it&#x27;d have fast, real-world feedback on how convincing it is, in the form of a purchase decision.</div><br/></div></div></div></div><div id="41189321" class="c"><input type="checkbox" id="c-41189321" checked=""/><div class="controls bullet"><span class="by">iamatoool</span><span>|</span><a href="#41189258">parent</a><span>|</span><a href="#41189304">prev</a><span>|</span><a href="#41189285">next</a><span>|</span><label class="collapse" for="c-41189321">[-]</label><label class="expand" for="c-41189321">[1 more]</label></div><br/><div class="children"><div class="content">Sideways eye look at leetcode culture</div><br/></div></div><div id="41189285" class="c"><input type="checkbox" id="c-41189285" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#41189258">parent</a><span>|</span><a href="#41189321">prev</a><span>|</span><label class="collapse" for="c-41189285">[-]</label><label class="expand" for="c-41189285">[3 more]</label></div><br/><div class="children"><div class="content">I find them very helpful, personally.</div><br/><div id="41189392" class="c"><input type="checkbox" id="c-41189392" checked=""/><div class="controls bullet"><span class="by">sussmannbaka</span><span>|</span><a href="#41189258">root</a><span>|</span><a href="#41189285">parent</a><span>|</span><label class="collapse" for="c-41189392">[-]</label><label class="expand" for="c-41189392">[2 more]</label></div><br/><div class="children"><div class="content">Understandable, they have been trained to convince you of their helpfulness.</div><br/><div id="41189420" class="c"><input type="checkbox" id="c-41189420" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#41189258">root</a><span>|</span><a href="#41189392">parent</a><span>|</span><label class="collapse" for="c-41189420">[-]</label><label class="expand" for="c-41189420">[1 more]</label></div><br/><div class="children"><div class="content">If they convinced me of their helpfulness, and their output is actually helpful in solving my problems.. well, if it walks like a duck and quacks like a duck, and all that.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>