<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705136466852" as="style"/><link rel="stylesheet" href="styles.css?v=1705136466852"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://tge-data-web.nyc3.digitaloceanspaces.com/docs/Vector%20Databases%20-%20A%20Technical%20Primer.pdf">Vector Databases: A Technical Primer [pdf]</a> <span class="domain">(<a href="https://tge-data-web.nyc3.digitaloceanspaces.com">tge-data-web.nyc3.digitaloceanspaces.com</a>)</span></div><div class="subtext"><span>jide_tracc</span> | <span>93 comments</span></div><br/><div><div id="38971877" class="c"><input type="checkbox" id="c-38971877" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#38974158">next</a><span>|</span><label class="collapse" for="c-38971877">[-]</label><label class="expand" for="c-38971877">[11 more]</label></div><br/><div class="children"><div class="content">Some more resources:<p>- <i>A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge</i>, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.11703" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.11703</a><p>- <i>Survey of Vector Database Management Systems</i>, <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.14021" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.14021</a><p>- <i>What are Embeddings</i>, <a href="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;veekaybee&#x2F;what_are_embeddings&#x2F;main&#x2F;embeddings.pdf" rel="nofollow">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;veekaybee&#x2F;what_are_embeddi...</a><p>---<p>h&#x2F;t: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;eatonphil&#x2F;status&#x2F;1745524630624862314" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;eatonphil&#x2F;status&#x2F;1745524630624862314</a> and <a href="https:&#x2F;&#x2F;twitter.com&#x2F;ChristophMolnar&#x2F;status&#x2F;1745731602682982675" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ChristophMolnar&#x2F;status&#x2F;17457316026829826...</a></div><br/><div id="38972761" class="c"><input type="checkbox" id="c-38972761" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38971877">parent</a><span>|</span><a href="#38972056">next</a><span>|</span><label class="collapse" for="c-38972761">[-]</label><label class="expand" for="c-38972761">[8 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s my attempt at this: Embeddings: What they are and why they matter <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Oct&#x2F;23&#x2F;embeddings&#x2F;</a><p>Available as an annotated presentation + an optional 38 minute video.</div><br/><div id="38973551" class="c"><input type="checkbox" id="c-38973551" checked=""/><div class="controls bullet"><span class="by">Scorpiion</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38972761">parent</a><span>|</span><a href="#38975780">next</a><span>|</span><label class="collapse" for="c-38973551">[-]</label><label class="expand" for="c-38973551">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for writing this one Simon, I read it some time ago and I just wanted to say thanks and recommend it to folks browsing the comments, it&#x27;s really good!</div><br/></div></div><div id="38975780" class="c"><input type="checkbox" id="c-38975780" checked=""/><div class="controls bullet"><span class="by">sva_</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38972761">parent</a><span>|</span><a href="#38973551">prev</a><span>|</span><a href="#38974711">next</a><span>|</span><label class="collapse" for="c-38975780">[-]</label><label class="expand" for="c-38975780">[5 more]</label></div><br/><div class="children"><div class="content">Nicely written and very approachable. Might add a paragraph on why to use cosine similarity, as it gives a chance to illustrate how the n-dimensional vector embedding is used.</div><br/><div id="38976295" class="c"><input type="checkbox" id="c-38976295" checked=""/><div class="controls bullet"><span class="by">soVeryTired</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38975780">parent</a><span>|</span><a href="#38974711">next</a><span>|</span><label class="collapse" for="c-38976295">[-]</label><label class="expand" for="c-38976295">[4 more]</label></div><br/><div class="children"><div class="content">I still don&#x27;t understand why cosine similarity is used (other than maybe being cheap to compute). I&#x27;ve read a bunch of hand-wavey articles but none of them add up for me.<p>If all your vectors have the same 2-norm, the cosine similarity of two vectors is a monotonic function of their euclidean distance (i.e. the minimum in one measure will be the minimum in the other).<p>If all your vectors don&#x27;t have the same norm, where&#x27;s the sense in projecting them onto the unit sphere and measuring the angle between them? You&#x27;re throwing away important information about the embedding.<p>Could anyone explain?</div><br/><div id="38976711" class="c"><input type="checkbox" id="c-38976711" checked=""/><div class="controls bullet"><span class="by">estebarb</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38976295">parent</a><span>|</span><a href="#38976506">next</a><span>|</span><label class="collapse" for="c-38976711">[-]</label><label class="expand" for="c-38976711">[1 more]</label></div><br/><div class="children"><div class="content">I asked it some time ago: <a href="https:&#x2F;&#x2F;cs.stackexchange.com&#x2F;questions&#x2F;147713&#x2F;why-word-embeddings-are-compared-with-cosine-distance-and-not-euclidean" rel="nofollow">https:&#x2F;&#x2F;cs.stackexchange.com&#x2F;questions&#x2F;147713&#x2F;why-word-embed...</a><p>There is other aspect: if all vectors are normalized then cosine distance is just dot product. It is faster to calculate than euclidean distance.</div><br/></div></div><div id="38976506" class="c"><input type="checkbox" id="c-38976506" checked=""/><div class="controls bullet"><span class="by">pjot</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38976295">parent</a><span>|</span><a href="#38976711">prev</a><span>|</span><a href="#38976473">next</a><span>|</span><label class="collapse" for="c-38976506">[-]</label><label class="expand" for="c-38976506">[1 more]</label></div><br/><div class="children"><div class="content">Cosine similarity is not affected by the size of the vector but only by the angle between them. This means that vectors with large or small values will have the same cosine similarity as long as they point in the same direction.<p>In semantic search the magnitude of vectors isn’t relevant - if you needed a measure that took magnitude into account than Euclidean would make sense. E.g. image embeddings based on pixel intensity.</div><br/></div></div><div id="38976473" class="c"><input type="checkbox" id="c-38976473" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38976295">parent</a><span>|</span><a href="#38976506">prev</a><span>|</span><a href="#38974711">next</a><span>|</span><label class="collapse" for="c-38976473">[-]</label><label class="expand" for="c-38976473">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, I use cosine similarity because that&#x27;s what everyone else does and I haven&#x27;t spent even a moment considering any of the alternatives.</div><br/></div></div></div></div></div></div><div id="38974711" class="c"><input type="checkbox" id="c-38974711" checked=""/><div class="controls bullet"><span class="by">AndyNemmity</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38972761">parent</a><span>|</span><a href="#38975780">prev</a><span>|</span><a href="#38972056">next</a><span>|</span><label class="collapse" for="c-38974711">[-]</label><label class="expand" for="c-38974711">[1 more]</label></div><br/><div class="children"><div class="content">Very helpful to make it clear, in concrete terms</div><br/></div></div></div></div><div id="38972056" class="c"><input type="checkbox" id="c-38972056" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#38971877">parent</a><span>|</span><a href="#38972761">prev</a><span>|</span><a href="#38974158">next</a><span>|</span><label class="collapse" for="c-38972056">[-]</label><label class="expand" for="c-38972056">[2 more]</label></div><br/><div class="children"><div class="content">Throwing a few more on here (mix of beginner and advanced):<p>- Wikipedia article: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector_database" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vector_database</a><p>- Vector Database 101: <a href="https:&#x2F;&#x2F;zilliz.com&#x2F;learn&#x2F;introduction-to-unstructured-data" rel="nofollow">https:&#x2F;&#x2F;zilliz.com&#x2F;learn&#x2F;introduction-to-unstructured-data</a><p>- ANN &amp; Similarity search: <a href="https:&#x2F;&#x2F;vinija.ai&#x2F;concepts&#x2F;ann-similarity-search&#x2F;" rel="nofollow">https:&#x2F;&#x2F;vinija.ai&#x2F;concepts&#x2F;ann-similarity-search&#x2F;</a><p>- Distributed database: <a href="https:&#x2F;&#x2F;15445.courses.cs.cmu.edu&#x2F;fall2021&#x2F;notes&#x2F;21-distributed.pdf" rel="nofollow">https:&#x2F;&#x2F;15445.courses.cs.cmu.edu&#x2F;fall2021&#x2F;notes&#x2F;21-distribut...</a></div><br/><div id="38972151" class="c"><input type="checkbox" id="c-38972151" checked=""/><div class="controls bullet"><span class="by">rammy1234</span><span>|</span><a href="#38971877">root</a><span>|</span><a href="#38972056">parent</a><span>|</span><a href="#38974158">next</a><span>|</span><label class="collapse" for="c-38972151">[-]</label><label class="expand" for="c-38972151">[1 more]</label></div><br/><div class="children"><div class="content">Throwing one more - <a href="https:&#x2F;&#x2F;www.pinecone.io&#x2F;learn&#x2F;vector-database&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pinecone.io&#x2F;learn&#x2F;vector-database&#x2F;</a></div><br/></div></div></div></div></div></div><div id="38974158" class="c"><input type="checkbox" id="c-38974158" checked=""/><div class="controls bullet"><span class="by">ripley12</span><span>|</span><a href="#38971877">prev</a><span>|</span><a href="#38975630">next</a><span>|</span><label class="collapse" for="c-38974158">[-]</label><label class="expand" for="c-38974158">[6 more]</label></div><br/><div class="children"><div class="content">The &quot;Do you need a Dedicated Vector Database?&quot; slide is quite interesting, but doesn&#x27;t really answer its own question! This is something I&#x27;ve been wondering myself, if anyone has any guidelines or rules of thumb I would appreciate it.<p>I&#x27;ve recently been using Simon Willison&#x27;s (hi simonw) excellent `llm` tool that can help with embeddings, and it takes the simplest approach possible: just store embeddings in SQLite with a few UDFs for calculating distance etc.<p>The simplicity of that approach is very appealing, but presumably at some level of traffic+data an application will outgrow it and need a more specialized database. Does anyone have a good intuition for where that cutoff might be?</div><br/><div id="38976678" class="c"><input type="checkbox" id="c-38976678" checked=""/><div class="controls bullet"><span class="by">estebarb</span><span>|</span><a href="#38974158">parent</a><span>|</span><a href="#38976070">next</a><span>|</span><label class="collapse" for="c-38976678">[-]</label><label class="expand" for="c-38976678">[1 more]</label></div><br/><div class="children"><div class="content">Some years ago I wrote a vector index that used LSH. Search was implemented in the most trivial way: scanning everything and comparing using hamming distance (xor and popcount). I was able to scan 200k hashes and find the most similar in less than 10ms, in a single core of a 2011 MBP.</div><br/></div></div><div id="38976070" class="c"><input type="checkbox" id="c-38976070" checked=""/><div class="controls bullet"><span class="by">tristanho</span><span>|</span><a href="#38974158">parent</a><span>|</span><a href="#38976678">prev</a><span>|</span><a href="#38975404">next</a><span>|</span><label class="collapse" for="c-38976070">[-]</label><label class="expand" for="c-38976070">[1 more]</label></div><br/><div class="children"><div class="content">Honestly just loading all vectors in-memory (and stuff like sqlite, pgvector) is totally fine when you&#x27;re dealing with O(100k) vectors, but beyond that all the workable options like pinecone get gnarly, slow, and ridiculously expensive.<p>The best option by far I know of is turbopuffer.com , which is like 100x cheaper than pinecone and seems to actually scale.<p>Since it&#x27;s not listed in the suggested vector dbs section of the slides, wanted to lob it in as a solid suggestion :)</div><br/></div></div><div id="38975404" class="c"><input type="checkbox" id="c-38975404" checked=""/><div class="controls bullet"><span class="by">darkteflon</span><span>|</span><a href="#38974158">parent</a><span>|</span><a href="#38976070">prev</a><span>|</span><a href="#38975032">next</a><span>|</span><label class="collapse" for="c-38975404">[-]</label><label class="expand" for="c-38975404">[1 more]</label></div><br/><div class="children"><div class="content">YMMV, of course, but this[1] article - posted here a few weeks back - really helped us to focus on how to think about what we actually needed: in short, a search engine.<p>[1] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38703943">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38703943</a></div><br/></div></div><div id="38975032" class="c"><input type="checkbox" id="c-38975032" checked=""/><div class="controls bullet"><span class="by">summarity</span><span>|</span><a href="#38974158">parent</a><span>|</span><a href="#38975404">prev</a><span>|</span><a href="#38975341">next</a><span>|</span><label class="collapse" for="c-38975032">[-]</label><label class="expand" for="c-38975032">[1 more]</label></div><br/><div class="children"><div class="content">I’ve had some success scaling a quick and dirty engine (that powers findsight.ai) to tens of millions of vectors, details in the talk here: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;elNrRU12xRc?t=1556" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;elNrRU12xRc?t=1556</a><p>That’s maybe 1kLOC so I didn’t need an external one after all.</div><br/></div></div><div id="38975341" class="c"><input type="checkbox" id="c-38975341" checked=""/><div class="controls bullet"><span class="by">jide_tracc</span><span>|</span><a href="#38974158">parent</a><span>|</span><a href="#38975032">prev</a><span>|</span><a href="#38975630">next</a><span>|</span><label class="collapse" for="c-38975341">[-]</label><label class="expand" for="c-38975341">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback! I&#x27;ll make this the topic of my next article and do a deeper dive on this</div><br/></div></div></div></div><div id="38975630" class="c"><input type="checkbox" id="c-38975630" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38974158">prev</a><span>|</span><a href="#38971222">next</a><span>|</span><label class="collapse" for="c-38975630">[-]</label><label class="expand" for="c-38975630">[8 more]</label></div><br/><div class="children"><div class="content">A nice introduction. However, I feel that many such introductions, it skips too quickly over the question of feature selection.<p>This is the step that introduces, rather subtly and sometimes almost ignorably, human judgement into what otherwise feels like a very automated and &quot;it&#x27;s just math&quot; system.<p>Let&#x27;s take audio. What features will you select from it to make your N-dimensional vector? An easy answer might appear to be &quot;as many as possible&quot;. However, your first problem is that you may not have access to feature characterization even for just the features you can trivially name. Your second problem is that without deep knowledge of the domain, you may not even be aware of potential features that you should probably be using. Your third problem is that even with deep knowledge of the domain, you may still be unaware of potential features that you probably should be using.<p>A practical example may help. Perhaps you&#x27;re a fan of Reichian phase-shifting minimalist music. You (indirectly) interrogate a vector database to find music similar to a canonical piece from this genre (say, Piano Phase). The database uses many audio &amp; music related features, including but not limited to: dominant frequency, note onset intervals, volume, frequency-distribution-based timbral characteristics, apparent root notes and scales etc etc etc.<p>Unfortunately for you, the feature set in the database does not include &quot;note-to-note intervals over time is constant&quot;. Your query will manage to find things that are timbrally, harmonically, melodically and rythmically similar, but it will be by sheer good luck that any of them share the key characteristic: steady changes in the relative phase of two different (or the same) melodic lines.<p>This is just one example. It&#x27;s not hard to cook up equivalents for visual, textual, numerical ... any data at all.<p>Now of course, none of this makes vector databases and feature classification useless. It does mean that when you do or do not find match patterns in a given data set, one of the early questions you need to ask yourself is whether the feature set has any strong guarantee of being complete, and if not, how it may need to be expanded.</div><br/><div id="38975770" class="c"><input type="checkbox" id="c-38975770" checked=""/><div class="controls bullet"><span class="by">yunyu</span><span>|</span><a href="#38975630">parent</a><span>|</span><a href="#38975783">next</a><span>|</span><label class="collapse" for="c-38975770">[-]</label><label class="expand" for="c-38975770">[5 more]</label></div><br/><div class="children"><div class="content">Vector databases are oriented around search and retrieval. The usual approach of generating vectors is to fine tune a large pretrained model and extract the inner representations. Because the dataset contains successful queries and retrieval results, all you need to do is optimize a loss function (usually contrastively or approximated versions of common ranking functions) using raw inputs on the similarity objective supported by the vector database. For common modalities like tabular&#x2F;text&#x2F;image&#x2F;audio data, there is basically no human judgement involved in feature selection - just apply attention.<p>Note: current state of the art text to vector models like E5-Mistral (note: very different from original E5) don’t even require human curation in the dataset</div><br/><div id="38976315" class="c"><input type="checkbox" id="c-38976315" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38975630">root</a><span>|</span><a href="#38975770">parent</a><span>|</span><a href="#38975783">next</a><span>|</span><label class="collapse" for="c-38976315">[-]</label><label class="expand" for="c-38976315">[4 more]</label></div><br/><div class="children"><div class="content">This is also just eliding the work that humans still have to do.<p>I give you an audio file (let&#x27;s say just regular old PCM wav format). You cannot do anything with this without making some decisions about what happens next to the data. For audio, you&#x27;re very minimally faced with the question of whether to do a transform into the frequency domain. If you don&#x27;t do that, there&#x27;s a ton of feature classification that can never be done. No audio to vector model can make that sort of decision for itself - humans have to make the possible.<p>Raw inputs are suitable for some things, but essentially E5 is just one that has already had a large number of assumptions built into it that happen to give pretty good results. Nevertheless, were you interested for some weird reason in a very strange metric of text similarity, nothing prebuilt, even E5,  is going to give that to you. Let&#x27;s look at what E5 does:<p>&gt; The primary purpose of embedding models is to convert discrete symbols, such as words, into continuous-valued vectors. These vectors are designed in such a way that similar words or entities have vectors that are close to each other in the vector space, reflecting their semantic similarity.<p>This is great, but useful for only a particular type of textual similarity consideration.<p>And oh, what&#x27;s this:<p>&gt; However, the innovation doesn’t stop there. To further elevate the model’s performance, supervised fine-tuning was introduced. This involved training the E5 embeddings with labeled data, effectively incorporating human knowledge into the learning process. The outcome was a consistent improvement in performance, making E5 a promising approach for advancing the field of embeddings and natural language understanding.<p>Hmmm ....<p>Anyway, my point still stands: choosing how to transform raw data into &quot;features&quot; is a human activity, even if the actual transformation itself is automated.</div><br/><div id="38976472" class="c"><input type="checkbox" id="c-38976472" checked=""/><div class="controls bullet"><span class="by">yunyu</span><span>|</span><a href="#38975630">root</a><span>|</span><a href="#38976315">parent</a><span>|</span><a href="#38975783">next</a><span>|</span><label class="collapse" for="c-38976472">[-]</label><label class="expand" for="c-38976472">[3 more]</label></div><br/><div class="children"><div class="content">I agree with your point at the highest (pretrained model architect) level, but tokenization&#x2F;encoding things into the frequency domain are decisions that typically aren’t made (or thought of) by the model consumer. They’re also not strictly theoretically necessary and are artifacts of current compute limitations.
 Btw E5 != E5 Mistral, the latter achieves SOTA performance without any labeled data - all you need is a prompt to generate synthetic data for your particular similarity metric.<p>&gt; Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets… We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages.<p>It’s true that ultimately there’s a judgement call (what does “distance” mean?), but I think the original post far overcomplicates what’s standard practice today.</div><br/><div id="38976956" class="c"><input type="checkbox" id="c-38976956" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38975630">root</a><span>|</span><a href="#38976472">parent</a><span>|</span><a href="#38975783">next</a><span>|</span><label class="collapse" for="c-38976956">[-]</label><label class="expand" for="c-38976956">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, I just not believe this generalizes in any meaningful sense for arbitrary data.<p>You cannot determine frequencies from audio PCM data. If you want to build a vector database of audio, with frequency&#x2F;frequencies as one of the features, at the very least you will have to arrange for a transform to the frequency domain. Unless you claim that a system is somehow capable of discovering fourier&#x27;s theorem and implementing the transform for itself, this is a necessary precursor to any system being able to embed using a vector that includes frequency considerations.<p>But ... that&#x27;s a human decision because humans think that frequencies are important to their experience of music. A person who totally deaf, however, and thus has extremely limited frequency perception, can (often) still detect rythmic structure due to bone conduction. Such a person who was interested in similarity analysis of audio would have no reason to perform a domain transform, and would be more interested in timing correlations that probably could be fully automated into various models <i>as long as someone remembers to ensure that the system is time-aware</i> which is, again, just another particular human judgement regarding what aspects of the audio have significance.<p>I just read the E5 Mistral paper. I don&#x27;t see anything that contradicts my point, which wasn&#x27;t about the need for human labelling, but about the need for human identification of significant features. In the case of text, because of the way languages evolve, we know that a semantic-free character-based analysis will likely bump into lots of interesting syntactic and semantic features. Doing that for arbitrary data (images, sound, air pressure, temperature) lacks any such pre-existing reason to treat the data in any particular way.<p>Consider, for example, if the &quot;true meaning&quot; of text was encoded in a somewhat Kaballah-esque type scheme, in which far distance words and even phonemes create tangled loops of reference and meaning. Even a system like E5 Mistral isn&#x27;t going to find that, because that&#x27;s absolutely not how we consider language to work, and thus that&#x27;s not part of the fundamentals of how even E5 Mistral operates.</div><br/><div id="38977053" class="c"><input type="checkbox" id="c-38977053" checked=""/><div class="controls bullet"><span class="by">yunyu</span><span>|</span><a href="#38975630">root</a><span>|</span><a href="#38976956">parent</a><span>|</span><a href="#38975783">next</a><span>|</span><label class="collapse" for="c-38977053">[-]</label><label class="expand" for="c-38977053">[1 more]</label></div><br/><div class="children"><div class="content">The above is a common anthropocentric take that has been repeatedly disproven by the last decade of deep learning research: <a href="http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html" rel="nofollow">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a><p>Understanding audio with <i>inputs</i> in the frequency domain isn’t required for <i>understanding frequencies in audio</i>.<p>A large enough system with sufficient training data would definitely be able to come up with a Fourier transform (or something resembling one), if encoding it helped the loss go down.<p>&gt; In computer vision, there has been a similar pattern. Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.<p>Today’s diffusion models learn representations from raw pixels, without even the concept of convolutions.<p>Ditto for language - as long as the architecture is 1) capable of modeling long range dependencies and 2) can be scaled reasonably, whether you pass in tokens, individual characters, or raw ASCII bytes is irrelevant. Character based models perform just as well (or better than) token&#x2F;word level models at a given parameter count&#x2F;training corpus size - the main reason they aren’t common (yet) is due to memory limitations, not anything fundamental.<p>For further reading, I’d recommend literature on transformer circuits for learning arithmetic without axioms: <a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;CJsxd8ofLjGFxkmAP&#x2F;explaining-the-transformer-circuits-framework-by-example" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;CJsxd8ofLjGFxkmAP&#x2F;explaining...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="38975783" class="c"><input type="checkbox" id="c-38975783" checked=""/><div class="controls bullet"><span class="by">ravetcofx</span><span>|</span><a href="#38975630">parent</a><span>|</span><a href="#38975770">prev</a><span>|</span><a href="#38971222">next</a><span>|</span><label class="collapse" for="c-38975783">[-]</label><label class="expand" for="c-38975783">[2 more]</label></div><br/><div class="children"><div class="content">Great comment about you don&#x27;t know what you don&#x27;t know. Also wonderful to see another Steve Reich aficionado. What are some similar pieces to music for 18 Musicians which I adore?</div><br/><div id="38976853" class="c"><input type="checkbox" id="c-38976853" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38975630">root</a><span>|</span><a href="#38975783">parent</a><span>|</span><a href="#38971222">next</a><span>|</span><label class="collapse" for="c-38976853">[-]</label><label class="expand" for="c-38976853">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s nothing quite like Mf18M.<p>However, I do love several of the pieces by Marc Mellits, check out &quot;Real Quiet plays the music of Marc Mellits&quot;. All rather short, but some totally lovely.<p>You may enjoy an 80&#x27;s album by Andrew Poppy, in particular the track &quot;The Object is a Hungry Wolf&quot;.<p>Do be sure to check &quot;Shift&quot; by Chris Hughes, which are re-instrumentations of several early Reich pieces including Piano Phase.<p>It&#x27;s quite different in many ways from Mf18M, and you may have heard it, but I adore &quot;The Chairman Dances&quot; by John Adams. It couples the &quot;chugging along&quot; of Mf18M with much less minimalist melodic and harmonic structures. YMMV.<p>I&#x27;m sure I am going to remember a bunch more after I send this, so check back here in a couple of days, and I&#x27;ll reply again with any extras.<p>Do you have any &quot;similars&quot; ?</div><br/></div></div></div></div></div></div><div id="38971222" class="c"><input type="checkbox" id="c-38971222" checked=""/><div class="controls bullet"><span class="by">jide_tracc</span><span>|</span><a href="#38975630">prev</a><span>|</span><a href="#38972774">next</a><span>|</span><label class="collapse" for="c-38971222">[-]</label><label class="expand" for="c-38971222">[7 more]</label></div><br/><div class="children"><div class="content">A few months ago, I taught a class on Vector Databases for a TGE Data private client and then decided to record it into a short course for a wider audience.<p>The course is a mix of theory and demos discussing some of the underlying concepts of Vectors, Vector Databases, Indexing, Search Similarity and ending with demos specifically for Pinecone and Weaviate databases.</div><br/><div id="38971690" class="c"><input type="checkbox" id="c-38971690" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#38971222">parent</a><span>|</span><a href="#38972774">next</a><span>|</span><label class="collapse" for="c-38971690">[-]</label><label class="expand" for="c-38971690">[6 more]</label></div><br/><div class="children"><div class="content">Cool! Is there a video as well, then?</div><br/><div id="38972017" class="c"><input type="checkbox" id="c-38972017" checked=""/><div class="controls bullet"><span class="by">jide_tracc</span><span>|</span><a href="#38971222">root</a><span>|</span><a href="#38971690">parent</a><span>|</span><a href="#38972774">next</a><span>|</span><label class="collapse" for="c-38972017">[-]</label><label class="expand" for="c-38972017">[5 more]</label></div><br/><div class="children"><div class="content">There is but it&#x27;s a udemy course unfortunately. I don&#x27;t want to spam the group with links but the link is on the lat page of the document</div><br/><div id="38972710" class="c"><input type="checkbox" id="c-38972710" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#38971222">root</a><span>|</span><a href="#38972017">parent</a><span>|</span><a href="#38972774">next</a><span>|</span><label class="collapse" for="c-38972710">[-]</label><label class="expand" for="c-38972710">[4 more]</label></div><br/><div class="children"><div class="content">If you have a discount code you can share, it would provide a neat excuse for posting the link in a comment along side it ;)</div><br/><div id="38975130" class="c"><input type="checkbox" id="c-38975130" checked=""/><div class="controls bullet"><span class="by">jide_tracc</span><span>|</span><a href="#38971222">root</a><span>|</span><a href="#38972710">parent</a><span>|</span><a href="#38975101">next</a><span>|</span><label class="collapse" for="c-38975130">[-]</label><label class="expand" for="c-38975130">[2 more]</label></div><br/><div class="children"><div class="content">Thank you! That&#x27;s a great idea. I created a discount coupon which is active for the next 5 days up until Jan 17th<p><a href="https:&#x2F;&#x2F;www.udemy.com&#x2F;course&#x2F;vector-databases-deep-dive&#x2F;?couponCode=59670CA7401E60FE031A" rel="nofollow">https:&#x2F;&#x2F;www.udemy.com&#x2F;course&#x2F;vector-databases-deep-dive&#x2F;?cou...</a></div><br/><div id="38975338" class="c"><input type="checkbox" id="c-38975338" checked=""/><div class="controls bullet"><span class="by">codetrotter</span><span>|</span><a href="#38971222">root</a><span>|</span><a href="#38975130">parent</a><span>|</span><a href="#38975101">next</a><span>|</span><label class="collapse" for="c-38975338">[-]</label><label class="expand" for="c-38975338">[1 more]</label></div><br/><div class="children"><div class="content">Thanks :) I bought a copy now.<p>I already have a Udemy account so when I went to checkout and logged in it said the promo was for new users only. But I went back to the course page again and reloaded the page so that I was logged in and then I was able to buy it with the coupon applied.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38972774" class="c"><input type="checkbox" id="c-38972774" checked=""/><div class="controls bullet"><span class="by">danjl</span><span>|</span><a href="#38971222">prev</a><span>|</span><a href="#38972748">next</a><span>|</span><label class="collapse" for="c-38972774">[-]</label><label class="expand" for="c-38972774">[3 more]</label></div><br/><div class="children"><div class="content">Great overview, but the final section doesn&#x27;t address the obvious question about how to decide between using a &quot;vector store&quot;, like Postgres+pgvector vs a &quot;vector database&quot;, like Pinecone. I&#x27;d love to see another presentation which discusses the various tradeoffs, like query speed, insertion&#x2F;index-building speed, ease-of-use, and others to help guide people trying to decide which of the options is best for their application.</div><br/><div id="38975349" class="c"><input type="checkbox" id="c-38975349" checked=""/><div class="controls bullet"><span class="by">jide_tracc</span><span>|</span><a href="#38972774">parent</a><span>|</span><a href="#38974193">next</a><span>|</span><label class="collapse" for="c-38975349">[-]</label><label class="expand" for="c-38975349">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback! I&#x27;ll explore this topic for upcoming articles for a deeper dive</div><br/></div></div><div id="38974193" class="c"><input type="checkbox" id="c-38974193" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38972774">parent</a><span>|</span><a href="#38975349">prev</a><span>|</span><a href="#38972748">next</a><span>|</span><label class="collapse" for="c-38974193">[-]</label><label class="expand" for="c-38974193">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d call the former a vector <i>extension</i>. A database is a store with bells and whistles.</div><br/></div></div></div></div><div id="38972748" class="c"><input type="checkbox" id="c-38972748" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#38972774">prev</a><span>|</span><a href="#38974808">next</a><span>|</span><label class="collapse" for="c-38972748">[-]</label><label class="expand" for="c-38972748">[2 more]</label></div><br/><div class="children"><div class="content">Since digitaloceanspaces.com is an S3-style hosting provider, it would be neat if Hacker News could special-case it to display something like tge-data-web.nyc3.digitaloceanspaces.com as the domain instead of just digitaloceanspaces.com<p>Though it looks like S3 has the same problem: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38876761">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38876761</a><p>There&#x27;s precedent for this elsewhere though - sites on a x.github.io subdomain have special treatment here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=lfranke.github.io">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=lfranke.github.io</a></div><br/><div id="38975158" class="c"><input type="checkbox" id="c-38975158" checked=""/><div class="controls bullet"><span class="by">A_Duck</span><span>|</span><a href="#38972748">parent</a><span>|</span><a href="#38974808">next</a><span>|</span><label class="collapse" for="c-38975158">[-]</label><label class="expand" for="c-38975158">[1 more]</label></div><br/><div class="children"><div class="content">You may be interested in this project: <a href="https:&#x2F;&#x2F;publicsuffix.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;publicsuffix.org&#x2F;</a></div><br/></div></div></div></div><div id="38974808" class="c"><input type="checkbox" id="c-38974808" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#38972748">prev</a><span>|</span><a href="#38973364">next</a><span>|</span><label class="collapse" for="c-38974808">[-]</label><label class="expand" for="c-38974808">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know why PQ is listed as an &quot;indexing strategy&quot;. It&#x27;s a vector compression&#x2F;quantization technique, not a means of partitioning the search space. You could encode vectors with PQ when using brute-force&#x2F;flat index, an IVF index, with HNSW (all of which are present in Faiss with PQ encoding as IndexPQ, IndexIVFPQ and IndexHNSWPQ respectively), or even k-D trees or ANNOY if someone wanted to do that.<p>&quot;Use HNSW or Annoy for very large datasets where query speed is more important than precision&quot;: Graph-based methods have huge memory overhead and construction cost, and they aren&#x27;t practical for billion-scale datasets. Also, they will usually be more accurate and faster than IVF techniques (as you would need to visit a large number of IVF cells to get comparable accuracy), though IVF can scale to trillion-sized databases without much overhead yet with reasonable speed&#x2F;accuracy tradeoffs unlike other techniques. I&#x27;d say &quot;use for medium-scale datasets where query speed is important, yet high accuracy is still desired and flat&#x2F;brute-force indexing is impractical&quot;.</div><br/><div id="38975131" class="c"><input type="checkbox" id="c-38975131" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#38974808">parent</a><span>|</span><a href="#38973364">next</a><span>|</span><label class="collapse" for="c-38975131">[-]</label><label class="expand" for="c-38975131">[2 more]</label></div><br/><div class="children"><div class="content">It turns a continuous space into a discrete space. You would first do PQ, then do KNN of the new discrete vector. This way you can compress the vocabulary to a fixed size.</div><br/><div id="38975511" class="c"><input type="checkbox" id="c-38975511" checked=""/><div class="controls bullet"><span class="by">jhj</span><span>|</span><a href="#38974808">root</a><span>|</span><a href="#38975131">parent</a><span>|</span><a href="#38973364">next</a><span>|</span><label class="collapse" for="c-38975511">[-]</label><label class="expand" for="c-38975511">[1 more]</label></div><br/><div class="children"><div class="content">The implementations that I am aware of, including the one in Faiss which I wrote (described in detail in <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1702.08734" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1702.08734</a>), do not index the vector based on its PQ encoding (e.g., in IVFPQ). The IVF cell chosen in which to put the vector is based on its pre-quantized (full precision floating point) representation. It would lose too much precision to perform all comparisons in the compressed space.<p>Also, distance comparisons are usually of the &quot;ADC&quot; (asymmetric distance comparison) form: the query vector is in full floating point format, and vectors, if quantized&#x2F;compressed in the database, are effectively decompressed and compared in the full floating point domain. This is true even with PQ, as the distance between the query vector subspaces with each of the 2^n PQ codes for the same subspace are precomputed before comparison (and then the distance computation becomes a lookup-add based on the precomputed distance tables).<p>LSH techniques unlike PQ are more accurately described as an indexing technique, since the buckets into which vectors are placed are based on their encoding (via hashing&#x2F;binarization) and are fully compared in that space.</div><br/></div></div></div></div></div></div><div id="38973364" class="c"><input type="checkbox" id="c-38973364" checked=""/><div class="controls bullet"><span class="by">dimatura</span><span>|</span><a href="#38974808">prev</a><span>|</span><a href="#38976709">next</a><span>|</span><label class="collapse" for="c-38973364">[-]</label><label class="expand" for="c-38973364">[11 more]</label></div><br/><div class="children"><div class="content">Any recommendations for an embedded embedding database (heh)? Embedded, as in sqlite. For smaller-scale problems, but hopefully more convenient than say, LMDB + FAISS.</div><br/><div id="38978157" class="c"><input type="checkbox" id="c-38978157" checked=""/><div class="controls bullet"><span class="by">andre-z</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38978069">next</a><span>|</span><label class="collapse" for="c-38978157">[-]</label><label class="expand" for="c-38978157">[1 more]</label></div><br/><div class="children"><div class="content">Qdrant as a library project
<a href="https:&#x2F;&#x2F;github.com&#x2F;tyrchen&#x2F;qdrant-lib">https:&#x2F;&#x2F;github.com&#x2F;tyrchen&#x2F;qdrant-lib</a></div><br/></div></div><div id="38978069" class="c"><input type="checkbox" id="c-38978069" checked=""/><div class="controls bullet"><span class="by">wdroz</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38978157">prev</a><span>|</span><a href="#38975558">next</a><span>|</span><label class="collapse" for="c-38978069">[-]</label><label class="expand" for="c-38978069">[1 more]</label></div><br/><div class="children"><div class="content">The qdrant clients support a local mode where you point to a file[0].<p>[0] -- <a href="https:&#x2F;&#x2F;github.com&#x2F;qdrant&#x2F;qdrant-client#local-mode">https:&#x2F;&#x2F;github.com&#x2F;qdrant&#x2F;qdrant-client#local-mode</a></div><br/></div></div><div id="38975558" class="c"><input type="checkbox" id="c-38975558" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38978069">prev</a><span>|</span><a href="#38973539">next</a><span>|</span><label class="collapse" for="c-38975558">[-]</label><label class="expand" for="c-38975558">[1 more]</label></div><br/><div class="children"><div class="content">You can take a look at txtai (<a href="https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai">https:&#x2F;&#x2F;github.com&#x2F;neuml&#x2F;txtai</a>). It can run in a Python process. It has support for storing content in SQLite and embeddings vectors in local vector index formats (Faiss, HNSW, Annoy).<p>Disclaimer: I&#x27;m the primary author of txtai.</div><br/></div></div><div id="38973539" class="c"><input type="checkbox" id="c-38973539" checked=""/><div class="controls bullet"><span class="by">ripley12</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38975558">prev</a><span>|</span><a href="#38974340">next</a><span>|</span><label class="collapse" for="c-38973539">[-]</label><label class="expand" for="c-38973539">[2 more]</label></div><br/><div class="children"><div class="content">FWIW, Simon Willison&#x27;s `llm` tool just uses SQLite plus a few UDFs. The simplicity of that approach is appealing to me but I don&#x27;t have a good sense of when+why it becomes insufficient.</div><br/><div id="38973664" class="c"><input type="checkbox" id="c-38973664" checked=""/><div class="controls bullet"><span class="by">dimatura</span><span>|</span><a href="#38973364">root</a><span>|</span><a href="#38973539">parent</a><span>|</span><a href="#38974340">next</a><span>|</span><label class="collapse" for="c-38973664">[-]</label><label class="expand" for="c-38973664">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, I&#x27;ll check it out.</div><br/></div></div></div></div><div id="38974340" class="c"><input type="checkbox" id="c-38974340" checked=""/><div class="controls bullet"><span class="by">PhilippGille</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38973539">prev</a><span>|</span><a href="#38974365">next</a><span>|</span><label class="collapse" for="c-38974340">[-]</label><label class="expand" for="c-38974340">[1 more]</label></div><br/><div class="children"><div class="content">For Python I believe Chroma [1] can be used embedded.<p>For Go I recently started building chromem-go, inspired by the Chroma interface: <a href="https:&#x2F;&#x2F;github.com&#x2F;philippgille&#x2F;chromem-go">https:&#x2F;&#x2F;github.com&#x2F;philippgille&#x2F;chromem-go</a><p>It&#x27;s neither advanced nor for scale yet, but the RAG demo works.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;chroma-core&#x2F;chroma">https:&#x2F;&#x2F;github.com&#x2F;chroma-core&#x2F;chroma</a></div><br/></div></div><div id="38974365" class="c"><input type="checkbox" id="c-38974365" checked=""/><div class="controls bullet"><span class="by">catketch</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38974340">prev</a><span>|</span><a href="#38975887">next</a><span>|</span><label class="collapse" for="c-38974365">[-]</label><label class="expand" for="c-38974365">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;asg017&#x2F;sqlite-vss">https:&#x2F;&#x2F;github.com&#x2F;asg017&#x2F;sqlite-vss</a></div><br/><div id="38974710" class="c"><input type="checkbox" id="c-38974710" checked=""/><div class="controls bullet"><span class="by">dimatura</span><span>|</span><a href="#38973364">root</a><span>|</span><a href="#38974365">parent</a><span>|</span><a href="#38975887">next</a><span>|</span><label class="collapse" for="c-38974710">[-]</label><label class="expand" for="c-38974710">[1 more]</label></div><br/><div class="children"><div class="content">awesome, how did I not find this :D</div><br/></div></div></div></div><div id="38975887" class="c"><input type="checkbox" id="c-38975887" checked=""/><div class="controls bullet"><span class="by">ehsanu1</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38974365">prev</a><span>|</span><a href="#38973849">next</a><span>|</span><label class="collapse" for="c-38975887">[-]</label><label class="expand" for="c-38975887">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve used usearch successfully for a small project: <a href="https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;usearch&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;unum-cloud&#x2F;usearch&#x2F;</a></div><br/></div></div><div id="38973849" class="c"><input type="checkbox" id="c-38973849" checked=""/><div class="controls bullet"><span class="by">pjot</span><span>|</span><a href="#38973364">parent</a><span>|</span><a href="#38975887">prev</a><span>|</span><a href="#38976709">next</a><span>|</span><label class="collapse" for="c-38973849">[-]</label><label class="expand" for="c-38973849">[1 more]</label></div><br/><div class="children"><div class="content">I actually just finished a POC using DuckDB that does similarity search for HN comments.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;patricktrainer&#x2F;hackernews-comment-search">https:&#x2F;&#x2F;github.com&#x2F;patricktrainer&#x2F;hackernews-comment-search</a></div><br/></div></div></div></div><div id="38976709" class="c"><input type="checkbox" id="c-38976709" checked=""/><div class="controls bullet"><span class="by">anotheraccount9</span><span>|</span><a href="#38973364">prev</a><span>|</span><a href="#38974971">next</a><span>|</span><label class="collapse" for="c-38976709">[-]</label><label class="expand" for="c-38976709">[1 more]</label></div><br/><div class="children"><div class="content">If I read organized data from a traditional db, and add new tables based on potential similarities from the existing data&#x2F;tables, with scores, would that be a vector db?</div><br/></div></div><div id="38974971" class="c"><input type="checkbox" id="c-38974971" checked=""/><div class="controls bullet"><span class="by">perone</span><span>|</span><a href="#38976709">prev</a><span>|</span><a href="#38973050">next</a><span>|</span><label class="collapse" for="c-38974971">[-]</label><label class="expand" for="c-38974971">[1 more]</label></div><br/><div class="children"><div class="content">I find it interesting how everyone ignore EuclidesDB (<a href="https:&#x2F;&#x2F;euclidesdb.readthedocs.io" rel="nofollow">https:&#x2F;&#x2F;euclidesdb.readthedocs.io</a>) which came before Milvus and others in 2018, it is free and open-source. Same for all presentations from major DBs.</div><br/></div></div><div id="38973050" class="c"><input type="checkbox" id="c-38973050" checked=""/><div class="controls bullet"><span class="by">stefanha</span><span>|</span><a href="#38974971">prev</a><span>|</span><a href="#38972611">next</a><span>|</span><label class="collapse" for="c-38973050">[-]</label><label class="expand" for="c-38973050">[2 more]</label></div><br/><div class="children"><div class="content">In the table on slide 15 the Indexing &amp; Search Efficiency cells for Traditional Databases and Vector Databases appear to be swapped.</div><br/><div id="38973549" class="c"><input type="checkbox" id="c-38973549" checked=""/><div class="controls bullet"><span class="by">kevindamm</span><span>|</span><a href="#38973050">parent</a><span>|</span><a href="#38972611">next</a><span>|</span><label class="collapse" for="c-38973549">[-]</label><label class="expand" for="c-38973549">[1 more]</label></div><br/><div class="children"><div class="content">Yes that last row looks swapped to me as well.</div><br/></div></div></div></div><div id="38972611" class="c"><input type="checkbox" id="c-38972611" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38973050">prev</a><span>|</span><a href="#38972705">next</a><span>|</span><label class="collapse" for="c-38972611">[-]</label><label class="expand" for="c-38972611">[2 more]</label></div><br/><div class="children"><div class="content">Missing coverage of hybrid search (vector + lexical).</div><br/><div id="38974949" class="c"><input type="checkbox" id="c-38974949" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38972611">parent</a><span>|</span><a href="#38972705">next</a><span>|</span><label class="collapse" for="c-38974949">[-]</label><label class="expand" for="c-38974949">[1 more]</label></div><br/><div class="children"><div class="content">this is quite important. every vector db will provide lexical features, every traditional db will provide vector features. everything trends hybrid.</div><br/></div></div></div></div><div id="38972705" class="c"><input type="checkbox" id="c-38972705" checked=""/><div class="controls bullet"><span class="by">gk1</span><span>|</span><a href="#38972611">prev</a><span>|</span><a href="#38974292">next</a><span>|</span><label class="collapse" for="c-38972705">[-]</label><label class="expand" for="c-38972705">[1 more]</label></div><br/><div class="children"><div class="content">This is really cool, thanks for making this and sharing with the community!</div><br/></div></div><div id="38974292" class="c"><input type="checkbox" id="c-38974292" checked=""/><div class="controls bullet"><span class="by">ok123456</span><span>|</span><a href="#38972705">prev</a><span>|</span><a href="#38972292">next</a><span>|</span><label class="collapse" for="c-38974292">[-]</label><label class="expand" for="c-38974292">[3 more]</label></div><br/><div class="children"><div class="content">Are these dedicated vector databases doing anything more complicated than what can be accomplished using Postgres with the Cube extension?</div><br/><div id="38975182" class="c"><input type="checkbox" id="c-38975182" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#38974292">parent</a><span>|</span><a href="#38975308">next</a><span>|</span><label class="collapse" for="c-38975182">[-]</label><label class="expand" for="c-38975182">[1 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need a dedicated vector db, you can use pgvector.<p>You could maybe use Cube for euclidean space search, but you&#x27;re better off using optimized algorithms for embedding space search.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;pgvector&#x2F;pgvector">https:&#x2F;&#x2F;github.com&#x2F;pgvector&#x2F;pgvector</a></div><br/></div></div><div id="38975308" class="c"><input type="checkbox" id="c-38975308" checked=""/><div class="controls bullet"><span class="by">gk1</span><span>|</span><a href="#38974292">parent</a><span>|</span><a href="#38975182">prev</a><span>|</span><a href="#38972292">next</a><span>|</span><label class="collapse" for="c-38975308">[-]</label><label class="expand" for="c-38975308">[1 more]</label></div><br/><div class="children"><div class="content">Yes, see: <a href="https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;hnsw-not-enough&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.pinecone.io&#x2F;blog&#x2F;hnsw-not-enough&#x2F;</a></div><br/></div></div></div></div><div id="38972292" class="c"><input type="checkbox" id="c-38972292" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#38974292">prev</a><span>|</span><a href="#38973168">next</a><span>|</span><label class="collapse" for="c-38972292">[-]</label><label class="expand" for="c-38972292">[4 more]</label></div><br/><div class="children"><div class="content">When does a simple linear search become an issue? For example, a cos similarity function which runs even over a million of vectors is surprisingly fast. We can further shard vectors by user (i.e. if they can only search in their own data they uploaded), or even by document (for things like &quot;chat with this PDF&quot;) then it&#x27;s likely that most searches will happen over small ranges of vectors.<p>I suspect the main downsides would be increased CPU usage and having to load vectors to RAM all the time. However, for small projects with low RPS you could probably get away without a special database?<p>Does anyone know of any benchmarks where they compare vector databases to a plain linear search? How they scale in regards to increased vector count and RPS, etc.</div><br/><div id="38972331" class="c"><input type="checkbox" id="c-38972331" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#38972292">parent</a><span>|</span><a href="#38973665">next</a><span>|</span><label class="collapse" for="c-38972331">[-]</label><label class="expand" for="c-38972331">[2 more]</label></div><br/><div class="children"><div class="content">Do you mean using brute force (exact search) and not approximate nearest neighbor search? From my unscientific benchmarks, doing an exact search with 100k vectors takes about 1 second. Usually chatbots take much longer to generate the text, so that&#x27;s still an acceptable time for doing exact search, which also means you will always find the best matches.</div><br/><div id="38972387" class="c"><input type="checkbox" id="c-38972387" checked=""/><div class="controls bullet"><span class="by">kgeist</span><span>|</span><a href="#38972292">root</a><span>|</span><a href="#38972331">parent</a><span>|</span><a href="#38973665">next</a><span>|</span><label class="collapse" for="c-38972387">[-]</label><label class="expand" for="c-38972387">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Do you mean using brute force (exact search) and not approximate nearest neighbor search?<p>Yes.<p>&gt;doing an exact search with 100k vectors takes about 1 second.<p>The speed can depend on the language&#x2F;runtime of your choice and the number of dimensions. What language&#x2F;runtime did you use, how many dimensions in a vector? (I heard too many dimensions for vector search is an overkill)</div><br/></div></div></div></div><div id="38973665" class="c"><input type="checkbox" id="c-38973665" checked=""/><div class="controls bullet"><span class="by">Radim</span><span>|</span><a href="#38972292">parent</a><span>|</span><a href="#38972331">prev</a><span>|</span><a href="#38973168">next</a><span>|</span><label class="collapse" for="c-38973665">[-]</label><label class="expand" for="c-38973665">[1 more]</label></div><br/><div class="children"><div class="content">Brute force search is both exact (100% accurate) and pleasantly linear – a predictable algorithm. CPUs and caches like that, so performance is much better than you might otherwise expect.<p>From my <a href="https:&#x2F;&#x2F;rare-technologies.com&#x2F;performance-shootout-of-nearest-neighbours-querying&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rare-technologies.com&#x2F;performance-shootout-of-neares...</a> benchmark of kNN libs:<p>&quot;<i>Brute force doesn’t care about the number of neighbours, so its performance is 679ms&#x2F;query regardless of “k”. When run in batch mode (issuing all 100 queries at once), average query time drops to 354ms&#x2F;query.</i>&quot;<p>This was 500 dimensions over a 3.7M dataset (the English Wikipedia), in 2014. So, ~700ms&#x2F;search, or half that if you can batch several searches together at once. YMMV.</div><br/></div></div></div></div><div id="38973168" class="c"><input type="checkbox" id="c-38973168" checked=""/><div class="controls bullet"><span class="by">dmezzetti</span><span>|</span><a href="#38972292">prev</a><span>|</span><a href="#38973244">next</a><span>|</span><label class="collapse" for="c-38973168">[-]</label><label class="expand" for="c-38973168">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s also important to understand why we&#x27;d even want vectors in the first place.<p>This article covers the advantages of semantic search over keyword search - <a href="https:&#x2F;&#x2F;medium.com&#x2F;neuml&#x2F;getting-started-with-semantic-search-a9fd9d8a48cf" rel="nofollow">https:&#x2F;&#x2F;medium.com&#x2F;neuml&#x2F;getting-started-with-semantic-searc...</a></div><br/></div></div><div id="38973244" class="c"><input type="checkbox" id="c-38973244" checked=""/><div class="controls bullet"><span class="by">yagami_takayuki</span><span>|</span><a href="#38973168">prev</a><span>|</span><a href="#38974515">next</a><span>|</span><label class="collapse" for="c-38973244">[-]</label><label class="expand" for="c-38973244">[9 more]</label></div><br/><div class="children"><div class="content">For recognizing features such as hair and skin color, which would do a better job? Machine learning with image classification? Or a vector database?<p>I&#x27;ve had Weaviate return a dog given a human as input when doing an image similarity search, so I was wondering if there&#x27;s some way to improve the results or whether I&#x27;m barking up the wrong tree.</div><br/><div id="38973327" class="c"><input type="checkbox" id="c-38973327" checked=""/><div class="controls bullet"><span class="by">dimatura</span><span>|</span><a href="#38973244">parent</a><span>|</span><a href="#38975426">next</a><span>|</span><label class="collapse" for="c-38973327">[-]</label><label class="expand" for="c-38973327">[2 more]</label></div><br/><div class="children"><div class="content">For those two in particular? You&#x27;d definitely get a better result with an ML model such as a convolutional neural net. In some sense, using an image similarity query is a kind of ML model - nearest neighbor - which can work in some scenarios. But for this specifically, I&#x27;d recommend a CNN.</div><br/><div id="38975333" class="c"><input type="checkbox" id="c-38975333" checked=""/><div class="controls bullet"><span class="by">yagami_takayuki</span><span>|</span><a href="#38973244">root</a><span>|</span><a href="#38973327">parent</a><span>|</span><a href="#38975426">next</a><span>|</span><label class="collapse" for="c-38975333">[-]</label><label class="expand" for="c-38975333">[1 more]</label></div><br/><div class="children"><div class="content">Thanks will experiment with a CNN.</div><br/></div></div></div></div><div id="38975426" class="c"><input type="checkbox" id="c-38975426" checked=""/><div class="controls bullet"><span class="by">Kubuxu</span><span>|</span><a href="#38973244">parent</a><span>|</span><a href="#38973327">prev</a><span>|</span><a href="#38973319">next</a><span>|</span><label class="collapse" for="c-38975426">[-]</label><label class="expand" for="c-38975426">[2 more]</label></div><br/><div class="children"><div class="content">You could consider something like BLIP2. There are multiple ways you could use it: embed images and match them against embeddings of text descriptors, train custom embedding of descriptors, or a classifier on top of the embedding (linear layer on top of the image embedding network).<p>The approaches increase in complexity. It also allows for dataset bootstrap:<p>Let&#x27;s say you want to classify cats by breed. You could start by embedding images and text descriptors and distance-matching the embedded descriptors to the images. This gives you a dataset that might be 90% correct; you can then clean it up, which would be easier to do than manually labelling it.
Based on that improved dataset, you can train a custom embedding for the labels or a classification layer on top of the image embedding network.</div><br/><div id="38975947" class="c"><input type="checkbox" id="c-38975947" checked=""/><div class="controls bullet"><span class="by">yagami_takayuki</span><span>|</span><a href="#38973244">root</a><span>|</span><a href="#38975426">parent</a><span>|</span><a href="#38973319">next</a><span>|</span><label class="collapse" for="c-38975947">[-]</label><label class="expand" for="c-38975947">[1 more]</label></div><br/><div class="children"><div class="content">thank you, will look into BLIP2.</div><br/></div></div></div></div><div id="38973319" class="c"><input type="checkbox" id="c-38973319" checked=""/><div class="controls bullet"><span class="by">bfeynman</span><span>|</span><a href="#38973244">parent</a><span>|</span><a href="#38975426">prev</a><span>|</span><a href="#38975074">next</a><span>|</span><label class="collapse" for="c-38973319">[-]</label><label class="expand" for="c-38973319">[2 more]</label></div><br/><div class="children"><div class="content">I would think you could improve your embedding space to address that issue, partially.  Similarity search (as a result of some contrastive loss) definitely suffers at the tails and the OOD is pretty bad.  That being said, you&#x27;re more likely to have higher recall than a more classical technique.</div><br/><div id="38975335" class="c"><input type="checkbox" id="c-38975335" checked=""/><div class="controls bullet"><span class="by">yagami_takayuki</span><span>|</span><a href="#38973244">root</a><span>|</span><a href="#38973319">parent</a><span>|</span><a href="#38975074">next</a><span>|</span><label class="collapse" for="c-38975335">[-]</label><label class="expand" for="c-38975335">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for your answer!</div><br/></div></div></div></div><div id="38975074" class="c"><input type="checkbox" id="c-38975074" checked=""/><div class="controls bullet"><span class="by">m00x</span><span>|</span><a href="#38973244">parent</a><span>|</span><a href="#38973319">prev</a><span>|</span><a href="#38974515">next</a><span>|</span><label class="collapse" for="c-38975074">[-]</label><label class="expand" for="c-38975074">[2 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t use vector databases independently, you need to input the embedding from a ML model.<p>For your use-case, it should be pretty simple. You could use a CNN and train it, or use YOLO, Deepface, or other face detection algos, then within the face, find the hair and find the skin.<p>From there you can use a vector database to get the colors that resemble other inputs, or you can use a simple CNN to classify the hair and skin to the closest label.</div><br/><div id="38975330" class="c"><input type="checkbox" id="c-38975330" checked=""/><div class="controls bullet"><span class="by">yagami_takayuki</span><span>|</span><a href="#38973244">root</a><span>|</span><a href="#38975074">parent</a><span>|</span><a href="#38974515">next</a><span>|</span><label class="collapse" for="c-38975330">[-]</label><label class="expand" for="c-38975330">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Will looks into this.</div><br/></div></div></div></div></div></div><div id="38974515" class="c"><input type="checkbox" id="c-38974515" checked=""/><div class="controls bullet"><span class="by">shreyas44</span><span>|</span><a href="#38973244">prev</a><span>|</span><a href="#38975396">next</a><span>|</span><label class="collapse" for="c-38974515">[-]</label><label class="expand" for="c-38974515">[1 more]</label></div><br/><div class="children"><div class="content">From a few months ago - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35814381">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35814381</a></div><br/></div></div><div id="38975396" class="c"><input type="checkbox" id="c-38975396" checked=""/><div class="controls bullet"><span class="by">koreankid227</span><span>|</span><a href="#38974515">prev</a><span>|</span><a href="#38972840">next</a><span>|</span><label class="collapse" for="c-38975396">[-]</label><label class="expand" for="c-38975396">[1 more]</label></div><br/><div class="children"><div class="content">Nice resource! One observation: the last row of the table in slide 15 (comparing traditional DBs with Vector DBs) seems flipped?</div><br/></div></div><div id="38972840" class="c"><input type="checkbox" id="c-38972840" checked=""/><div class="controls bullet"><span class="by">takinola</span><span>|</span><a href="#38975396">prev</a><span>|</span><a href="#38972250">next</a><span>|</span><label class="collapse" for="c-38972840">[-]</label><label class="expand" for="c-38972840">[1 more]</label></div><br/><div class="children"><div class="content">When I started playing around with AI applications and learnt about RAG techniques, it took me a while to grok vector databases and was a bit of pain to learn how to set one up.  So I built my own little pet project, RagTag [1], which abstracts the vector database behind a simple CRUD API.  I simply POST documents to RagTag and they are automatically converted into embeddings and made available to be queried for similarity searches.<p>[1] RagTag - <a href="https:&#x2F;&#x2F;ragtag.weaveapi.com" rel="nofollow">https:&#x2F;&#x2F;ragtag.weaveapi.com</a></div><br/></div></div><div id="38972250" class="c"><input type="checkbox" id="c-38972250" checked=""/><div class="controls bullet"><span class="by">jpasmore</span><span>|</span><a href="#38972840">prev</a><span>|</span><a href="#38972440">next</a><span>|</span><label class="collapse" for="c-38972250">[-]</label><label class="expand" for="c-38972250">[1 more]</label></div><br/><div class="children"><div class="content">helpful - thx - building Latimer.ai using Pinecone</div><br/></div></div><div id="38972440" class="c"><input type="checkbox" id="c-38972440" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38972250">prev</a><span>|</span><a href="#38973068">next</a><span>|</span><label class="collapse" for="c-38972440">[-]</label><label class="expand" for="c-38972440">[3 more]</label></div><br/><div class="children"><div class="content">Is this how most AI services implements RAG?</div><br/><div id="38972754" class="c"><input type="checkbox" id="c-38972754" checked=""/><div class="controls bullet"><span class="by">gk1</span><span>|</span><a href="#38972440">parent</a><span>|</span><a href="#38972880">next</a><span>|</span><label class="collapse" for="c-38972754">[-]</label><label class="expand" for="c-38972754">[1 more]</label></div><br/><div class="children"><div class="content">Yes, most RAG is done with a vector database. Or something that approximates a vector database, like a traditional database with a vector-index add-on.</div><br/></div></div><div id="38972880" class="c"><input type="checkbox" id="c-38972880" checked=""/><div class="controls bullet"><span class="by">charcircuit</span><span>|</span><a href="#38972440">parent</a><span>|</span><a href="#38972754">prev</a><span>|</span><a href="#38973068">next</a><span>|</span><label class="collapse" for="c-38972880">[-]</label><label class="expand" for="c-38972880">[1 more]</label></div><br/><div class="children"><div class="content">Keyword or text matching based search is likely still more popular due to how long its been around, its simplicity, and the tooling built around it. Most companies who have internal search are most likely not using vector &#x2F; semantic search, but are doing something more basic.</div><br/></div></div></div></div><div id="38973068" class="c"><input type="checkbox" id="c-38973068" checked=""/><div class="controls bullet"><span class="by">nostrebored</span><span>|</span><a href="#38972440">prev</a><span>|</span><a href="#38971753">next</a><span>|</span><label class="collapse" for="c-38973068">[-]</label><label class="expand" for="c-38973068">[3 more]</label></div><br/><div class="children"><div class="content">From looking at this, I think it’s a very risky starting point for an engineer to kick off from.<p>Things like mentioning they’re clustered by meaning and optimized for analytics are questionable.<p>The clustering depends on the embedding you calculate. If you think that the embedding is a good semantic approximation of the data then maybe this is a fine way of thinking about it. But it’s not hard to imagine embeddings that may violate this — eg if I use an audio file and a text file that are identical in meaning through the same embed process, unless it is multimodal they will likely be distant in the embedding vector space.<p>I fully expect to see embeddings that put things close together in the vector space based on utilization rather than semantic similarity. If I’m creating a recommender system, I don’t want to group different varieties of one off purchases closely. For instance, the most semantically similar flight is going to be another fight to the same destination at a different time or a flight to a nearby airport. But I would want to group hotels often purchased by people who have previously bought the flight.<p>Vector databases also allow you to provide extra dimensionality into the data, like time awareness. Nothing is forcing you to use a vector that encodes semantic meaning.<p>And from this, you can see that we’re optimized for lookups or searches based on an input vector. This is not analogous to OLAP queries. This is more akin to elasticsearch than snowflake. If you are using a vector database thinking it’s going to give you reporting or large scale analytics on the vector space afaik there isn’t a readily available offering.</div><br/><div id="38973628" class="c"><input type="checkbox" id="c-38973628" checked=""/><div class="controls bullet"><span class="by">chasd00</span><span>|</span><a href="#38973068">parent</a><span>|</span><a href="#38971753">next</a><span>|</span><label class="collapse" for="c-38973628">[-]</label><label class="expand" for="c-38973628">[2 more]</label></div><br/><div class="children"><div class="content">calculating the embeddings is still a mystery to me. I get going from a picture of an Apple to a vector representing &quot;appleness&quot; and then comparing that vector to other vectors using all the usual math. What I don&#x27;t get is, who&#x2F;what takes the image as input and outputs the vector. Same goes for documents, let&#x27;s say i want to add a dimension (another number in the array) what part of the vector database do I modify to include this dimension in the vector calculation? Or is going from doc&#x2F;image&#x2F;whatver to the vector representation done outside the database in some other way?<p>edit: it seems like calculating embeddings would be something an ML algorithm would do but then, again, you have to train that one first. ...it&#x27;s training all the way down.</div><br/><div id="38974212" class="c"><input type="checkbox" id="c-38974212" checked=""/><div class="controls bullet"><span class="by">nostrebored</span><span>|</span><a href="#38973068">root</a><span>|</span><a href="#38973628">parent</a><span>|</span><a href="#38971753">next</a><span>|</span><label class="collapse" for="c-38974212">[-]</label><label class="expand" for="c-38974212">[1 more]</label></div><br/><div class="children"><div class="content">Yup it happens outside of the system — but there are a number of perks to being able to store that data in a db — including easily adding metadata, updating entries, etc.<p>I think in 10y we will see retail systems heavily utilizing vector dbs and many embedding as a service products that take into account things like conversion. In this model you can add metadata about products to the vector db and direct program flow instead of querying back out to one or more databases to retrieve relevant metadata.<p>They’ll also start to enable things like search via image for features like “show us your favorite outfit” pulling up a customized wardrobe based on individual items extracted from the photo and run through the embedder.<p>Just one of many ways these products will exist outside of RAG. I think we’ll actually see a lot of the opposite — GAR.</div><br/></div></div></div></div></div></div><div id="38971753" class="c"><input type="checkbox" id="c-38971753" checked=""/><div class="controls bullet"><span class="by">AtlasBarfed</span><span>|</span><a href="#38973068">prev</a><span>|</span><a href="#38976212">next</a><span>|</span><label class="collapse" for="c-38971753">[-]</label><label class="expand" for="c-38971753">[2 more]</label></div><br/><div class="children"><div class="content">ANNOY is essentially a binary space partition tree for points without having to worry about sets of points aka polygons?</div><br/><div id="38971936" class="c"><input type="checkbox" id="c-38971936" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#38971753">parent</a><span>|</span><a href="#38976212">next</a><span>|</span><label class="collapse" for="c-38971936">[-]</label><label class="expand" for="c-38971936">[1 more]</label></div><br/><div class="children"><div class="content">Like IVF, Annoy partitions the entire embedding space into high-dimensional polygons. The difference is how the two algorithms do it - IVF (<a href="https:&#x2F;&#x2F;zilliz.com&#x2F;learn&#x2F;vector-index" rel="nofollow">https:&#x2F;&#x2F;zilliz.com&#x2F;learn&#x2F;vector-index</a>) uses centroids, while Annoy (<a href="https:&#x2F;&#x2F;zilliz.com&#x2F;learn&#x2F;approximate-nearest-neighbor-oh-yeah-ANNOY" rel="nofollow">https:&#x2F;&#x2F;zilliz.com&#x2F;learn&#x2F;approximate-nearest-neighbor-oh-yea...</a>) is basically just one big binary tree.</div><br/></div></div></div></div><div id="38976212" class="c"><input type="checkbox" id="c-38976212" checked=""/><div class="controls bullet"><span class="by">derrickrburns</span><span>|</span><a href="#38971753">prev</a><span>|</span><a href="#38976492">next</a><span>|</span><label class="collapse" for="c-38976212">[-]</label><label class="expand" for="c-38976212">[1 more]</label></div><br/><div class="children"><div class="content">Well done!</div><br/></div></div><div id="38976492" class="c"><input type="checkbox" id="c-38976492" checked=""/><div class="controls bullet"><span class="by">itsthecourier</span><span>|</span><a href="#38976212">prev</a><span>|</span><a href="#38972640">next</a><span>|</span><label class="collapse" for="c-38976492">[-]</label><label class="expand" for="c-38976492">[1 more]</label></div><br/><div class="children"><div class="content">The table with index types in the slide &quot;comparison&quot; is inverted</div><br/></div></div><div id="38972640" class="c"><input type="checkbox" id="c-38972640" checked=""/><div class="controls bullet"><span class="by">cranberryturkey</span><span>|</span><a href="#38976492">prev</a><span>|</span><label class="collapse" for="c-38972640">[-]</label><label class="expand" for="c-38972640">[3 more]</label></div><br/><div class="children"><div class="content">is surrealdb anygood as a vector database?</div><br/><div id="38974227" class="c"><input type="checkbox" id="c-38974227" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38972640">parent</a><span>|</span><a href="#38973259">next</a><span>|</span><label class="collapse" for="c-38974227">[-]</label><label class="expand" for="c-38974227">[1 more]</label></div><br/><div class="children"><div class="content">That doesn&#x27;t make sense. SurrealDB doesn&#x27;t even tout itself as having good performance. If you don&#x27;t want a custom vector database, you might as well use postgres.</div><br/></div></div><div id="38973259" class="c"><input type="checkbox" id="c-38973259" checked=""/><div class="controls bullet"><span class="by">m1117</span><span>|</span><a href="#38972640">parent</a><span>|</span><a href="#38974227">prev</a><span>|</span><label class="collapse" for="c-38973259">[-]</label><label class="expand" for="c-38973259">[1 more]</label></div><br/><div class="children"><div class="content">Pinecone is surreal!</div><br/></div></div></div></div></div></div></div></div></div></body></html>