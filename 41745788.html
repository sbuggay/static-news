<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1728118872792" as="style"/><link rel="stylesheet" href="styles.css?v=1728118872792"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb">LLMs, Theory of Mind, and Cheryl&#x27;s Birthday</a>Â <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>stereoabuse</span> | <span>69 comments</span></div><br/><div><div id="41748479" class="c"><input type="checkbox" id="c-41748479" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#41746784">next</a><span>|</span><label class="collapse" for="c-41748479">[-]</label><label class="expand" for="c-41748479">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a huge fan of using these kind of riddles or gotchas. Other comments have riddle variants which feel also like ways of tripping someone up - if you don&#x27;t spot the change you fail. And what&#x27;s more the originals are things that lots of people struggle with (that&#x27;s why they&#x27;re riddles not basic questions).<p>There&#x27;s also little prompting,  which feels like calling up a random person and demanding they solve a riddle straight away without talking it through.<p>Part of the assumption here is that if llms don&#x27;t write the right code they don&#x27;t understand what people know. I&#x27;d wager that a huge number of people couldn&#x27;t solve this puzzle yet fully understand what different people have their own internal thoughts and knowledge.</div><br/><div id="41748576" class="c"><input type="checkbox" id="c-41748576" checked=""/><div class="controls bullet"><span class="by">rtlacn</span><span>|</span><a href="#41748479">parent</a><span>|</span><a href="#41746784">next</a><span>|</span><label class="collapse" for="c-41748576">[-]</label><label class="expand" for="c-41748576">[2 more]</label></div><br/><div class="children"><div class="content">Humans can solve these riddles even if they have never seen a similar one.<p>LLMs need to have seen at least a similar riddle with the same structure and&#x2F;or code to plagiarize.<p>Humans <i>can</i> deal with modified riddles if they are alert and awake. An LLM is always alert and awake, so it should be able to handle modified riddles, but it cannot.</div><br/><div id="41748589" class="c"><input type="checkbox" id="c-41748589" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#41748479">root</a><span>|</span><a href="#41748576">parent</a><span>|</span><a href="#41746784">next</a><span>|</span><label class="collapse" for="c-41748589">[-]</label><label class="expand" for="c-41748589">[1 more]</label></div><br/><div class="children"><div class="content">Not to take anything from what you&#x27;re saying but any person involved in hiring process _for programmers_ will agree that people struggle with trivia far easier than this example.</div><br/></div></div></div></div></div></div><div id="41746784" class="c"><input type="checkbox" id="c-41746784" checked=""/><div class="controls bullet"><span class="by">jawns</span><span>|</span><a href="#41748479">prev</a><span>|</span><a href="#41747048">next</a><span>|</span><label class="collapse" for="c-41746784">[-]</label><label class="expand" for="c-41746784">[3 more]</label></div><br/><div class="children"><div class="content">A long time ago, I created a version of this challenge called &quot;Cheryl&#x27;s Murder.&quot;<p>My notebook not only solves logical induction problems like &quot;Cheryl&#x27;s Birthday,&quot; but it also generates them.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;shaungallagher&#x2F;cheryls-murder&#x2F;blob&#x2F;master&#x2F;cheryls-murder.ipynb">https:&#x2F;&#x2F;github.com&#x2F;shaungallagher&#x2F;cheryls-murder&#x2F;blob&#x2F;master...</a></div><br/><div id="41748189" class="c"><input type="checkbox" id="c-41748189" checked=""/><div class="controls bullet"><span class="by">xelxebar</span><span>|</span><a href="#41746784">parent</a><span>|</span><a href="#41746996">next</a><span>|</span><label class="collapse" for="c-41748189">[-]</label><label class="expand" for="c-41748189">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Twice a year, all of the software engineers at our company are given several days to work on a project of their choosing.<p>Monetate sounds like it has (had?) some interesting leadership!</div><br/></div></div><div id="41746996" class="c"><input type="checkbox" id="c-41746996" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#41746784">parent</a><span>|</span><a href="#41748189">prev</a><span>|</span><a href="#41747048">next</a><span>|</span><label class="collapse" for="c-41746996">[-]</label><label class="expand" for="c-41746996">[1 more]</label></div><br/><div class="children"><div class="content">This is awesome, thanks for sharing</div><br/></div></div></div></div><div id="41747048" class="c"><input type="checkbox" id="c-41747048" checked=""/><div class="controls bullet"><span class="by">ynniv</span><span>|</span><a href="#41746784">prev</a><span>|</span><a href="#41746522">next</a><span>|</span><label class="collapse" for="c-41747048">[-]</label><label class="expand" for="c-41747048">[6 more]</label></div><br/><div class="children"><div class="content">The problem with evaluating LLMs is that there&#x27;s a random component, and the specific wording of prompts is so important. I asked Claude to explain the problem, then write python to solve it. When it ran there was an exception, so I pasted that back in and got the correct answer. I&#x27;m not sure what this says about theory of mind (the first script it wrote was organized into steps based on who knew what when, so it seems to grok that), but the real lesson is that if LLMs are an emulation of &quot;human&quot; intelligence, they should probably be given a python interpreter to check their work.</div><br/><div id="41747446" class="c"><input type="checkbox" id="c-41747446" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41747048">parent</a><span>|</span><a href="#41746522">next</a><span>|</span><label class="collapse" for="c-41747446">[-]</label><label class="expand" for="c-41747446">[5 more]</label></div><br/><div class="children"><div class="content">Yes, that helps. But if you iterate on this a few times (as I did last year with Code Interpreter), it reveals how much LLM&#x27;s &quot;like&quot; to imitate patterns. Sure, often it will pattern-match on a useful fix and that&#x27;s pretty neat. But after I told it &quot;that fix didn&#x27;t work&quot; a couple times (with details about the error), it started <i>assuming the fix wouldn&#x27;t work</i> and immediately trying again without my input. It learned the pattern! So, I learned to instead edit the question and resubmit.<p>LLM&#x27;s are pattern-imitating machines with a random number generator added to try to keep them from repeating the same pattern, which is what they really &quot;want&quot; to do. It&#x27;s a brilliant hack because repeating the same pattern when it&#x27;s not appropriate is a dead giveaway of machine-like behavior. (And adding a random number generator also makes it that much harder to evaluate LLM&#x27;s since you need to repeat your queries and do statistics.)<p>Although zero-shot question-answering often works, a more reliable way to get useful results out of an LLM is to &quot;lean into it&quot; by giving it a pattern and asking it to repeat it. (Or if you don&#x27;t want it to follow a pattern, make sure you don&#x27;t give it one that will confuse it.)</div><br/><div id="41747720" class="c"><input type="checkbox" id="c-41747720" checked=""/><div class="controls bullet"><span class="by">brain5ide</span><span>|</span><a href="#41747048">root</a><span>|</span><a href="#41747446">parent</a><span>|</span><a href="#41746522">next</a><span>|</span><label class="collapse" for="c-41747720">[-]</label><label class="expand" for="c-41747720">[4 more]</label></div><br/><div class="children"><div class="content">If I understood correctly, that anectode in first paragraph looks like an interaction with a child who is trying something but lacks confidence.</div><br/><div id="41747822" class="c"><input type="checkbox" id="c-41747822" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41747048">root</a><span>|</span><a href="#41747720">parent</a><span>|</span><a href="#41746522">next</a><span>|</span><label class="collapse" for="c-41747822">[-]</label><label class="expand" for="c-41747822">[3 more]</label></div><br/><div class="children"><div class="content">It did look that way and it&#x27;s a fun way to interpret it, but pattern-matching on a pretty obvious pattern in the text (several failed fixes in a row) seems more likely. LLM&#x27;s will repeat patterns in other circumstances too.</div><br/><div id="41747848" class="c"><input type="checkbox" id="c-41747848" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#41747048">root</a><span>|</span><a href="#41747822">parent</a><span>|</span><a href="#41746522">next</a><span>|</span><label class="collapse" for="c-41747848">[-]</label><label class="expand" for="c-41747848">[2 more]</label></div><br/><div class="children"><div class="content">I mean, humans do this too... If I tell an interviewee that they&#x27;ve done something wrong a few times, they&#x27;ll have less confidence going forward (unless they&#x27;re a sociopath), and typically start checking their work more closely to preempt problems. This particular instance of in-context pattern matching doesn&#x27;t seem obviously unintelligent to me.</div><br/><div id="41748079" class="c"><input type="checkbox" id="c-41748079" checked=""/><div class="controls bullet"><span class="by">skybrian</span><span>|</span><a href="#41747048">root</a><span>|</span><a href="#41747848">parent</a><span>|</span><a href="#41746522">next</a><span>|</span><label class="collapse" for="c-41748079">[-]</label><label class="expand" for="c-41748079">[1 more]</label></div><br/><div class="children"><div class="content">This was code that finished successfully (no stack trace) and rendered an image, but the output didn&#x27;t match what I asked it to do, so I told it what it actually looked like. Code Interpreter <i>couldn&#x27;t</i> check its work in that case, because it couldn&#x27;t see it. It had to rely on me to tell it.<p>So it was definitely writing &quot;here&#x27;s the answer... that failed, let&#x27;s try again&quot; <i>without</i> checking its work, because it never prompted me. You could call that &quot;hallucinating&quot; a failure.<p>I also found that it &quot;hallucinated&quot; other test results - I&#x27;d ask it to write some code that printed a number to the console and told it what the number was supposed to be, and then it would say it &quot;worked,&quot; reporting the expected value instead of the actual number.<p>I also asked it to write a test and run it, and it would say it passed, and I&#x27;d look at the actual output and it failed.<p>So, asking it to write tests didn&#x27;t work as well as I&#x27;d hoped; it often &quot;sees&quot; things based on what would complete the pattern instead of the actual result.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41746522" class="c"><input type="checkbox" id="c-41746522" checked=""/><div class="controls bullet"><span class="by">erwald</span><span>|</span><a href="#41747048">prev</a><span>|</span><a href="#41746523">next</a><span>|</span><label class="collapse" for="c-41746522">[-]</label><label class="expand" for="c-41746522">[4 more]</label></div><br/><div class="children"><div class="content">o1 mini seems to get it on the first try (I didn&#x27;t vet the code, but I tested it and it works on both examples provided in the notebook, `dates` and `gabe_dates`):<p><pre><code>    from collections import defaultdict
    
    def find_cheryls_birthday(possible_dates):
        # Parse the dates into month and day
        dates = [date.split() for date in possible_dates]
        months = [month for month, day in dates]
        days = [day for month, day in dates]
    
        # Step 1: Albert knows the month and says he doesn&#x27;t know the birthday
        # and that Bernard doesn&#x27;t know either. This implies the month has no unique days.
        month_counts = defaultdict(int)
        day_counts = defaultdict(int)
        for month, day in dates:
            month_counts[month] += 1
            day_counts[day] += 1
    
        # Months with all days appearing more than once
        possible_months = [month for month in month_counts if all(day_counts[day] &gt; 1 for m, day in dates if m == month)]
        filtered_dates = [date for date in dates if date[0] in possible_months]
    
        # Step 2: Bernard knows the day and now knows the birthday
        # This means the day is unique in the filtered dates
        filtered_days = defaultdict(int)
        for month, day in filtered_dates:
            filtered_days[day] += 1
        possible_days = [day for day in filtered_days if filtered_days[day] == 1]
        filtered_dates = [date for date in filtered_dates if date[1] in possible_days]
    
        # Step 3: Albert now knows the birthday, so the month must be unique in remaining dates
        possible_months = defaultdict(int)
        for month, day in filtered_dates:
            possible_months[month] += 1
        final_dates = [date for date in filtered_dates if possible_months[date[0]] == 1]
    
        # Convert back to original format
        return &#x27; &#x27;.join(final_dates[0]) if final_dates else &quot;No unique solution found.&quot;
    
    # Example usage:
    possible_dates = [
        &quot;May 15&quot;, &quot;May 16&quot;, &quot;May 19&quot;,
        &quot;June 17&quot;, &quot;June 18&quot;,
        &quot;July 14&quot;, &quot;July 16&quot;,
        &quot;August 14&quot;, &quot;August 15&quot;, &quot;August 17&quot;
    ]
    
    birthday = find_cheryls_birthday(possible_dates)
    print(f&quot;Cheryl&#x27;s Birthday is on {birthday}.&quot;)</code></pre></div><br/><div id="41746691" class="c"><input type="checkbox" id="c-41746691" checked=""/><div class="controls bullet"><span class="by">mewpmewp2</span><span>|</span><a href="#41746522">parent</a><span>|</span><a href="#41748487">next</a><span>|</span><label class="collapse" for="c-41746691">[-]</label><label class="expand" for="c-41746691">[1 more]</label></div><br/><div class="children"><div class="content">In addition to that after they create the 1st program with mistakes the author should have showed them the invalid output and let them have a chance to fix it. For humans solving this on the first try without running the code also tends to frequently not work.</div><br/></div></div><div id="41748487" class="c"><input type="checkbox" id="c-41748487" checked=""/><div class="controls bullet"><span class="by">isaacfrond</span><span>|</span><a href="#41746522">parent</a><span>|</span><a href="#41746691">prev</a><span>|</span><a href="#41746751">next</a><span>|</span><label class="collapse" for="c-41748487">[-]</label><label class="expand" for="c-41748487">[1 more]</label></div><br/><div class="children"><div class="content">despite the name âminiâ. it is actually more optimized for code. so that makes sense.</div><br/></div></div><div id="41746751" class="c"><input type="checkbox" id="c-41746751" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41746522">parent</a><span>|</span><a href="#41748487">prev</a><span>|</span><a href="#41746523">next</a><span>|</span><label class="collapse" for="c-41746751">[-]</label><label class="expand" for="c-41746751">[1 more]</label></div><br/><div class="children"><div class="content">&quot;seems to&quot; isn&#x27;t good enough, especially since it&#x27;s entirely possible to generate code that doesn&#x27;t give the right answer. 4o is able to write some bad code, run it, recognize that it&#x27;s bad, and then fix it, if you tell it to.<p><a href="https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;670086ed-67bc-8009-b96c-39e539791fd3" rel="nofollow">https:&#x2F;&#x2F;chatgpt.com&#x2F;share&#x2F;670086ed-67bc-8009-b96c-39e539791f...</a></div><br/></div></div></div></div><div id="41746523" class="c"><input type="checkbox" id="c-41746523" checked=""/><div class="controls bullet"><span class="by">joe_the_user</span><span>|</span><a href="#41746522">prev</a><span>|</span><a href="#41747264">next</a><span>|</span><label class="collapse" for="c-41746523">[-]</label><label class="expand" for="c-41746523">[7 more]</label></div><br/><div class="children"><div class="content">Deducing things from the inability of an LLM to answer a specific question seemed doomed by the &quot;it will be able to on the next itteration&quot; principle.<p>It seems like the only way you could systematic chart the weaknesses of an LLM is by having a class of problems that get harder for LLMs at a steep rate, so a small increase in problem complexity requires a significant increase in LLM power.</div><br/><div id="41747224" class="c"><input type="checkbox" id="c-41747224" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41746523">parent</a><span>|</span><a href="#41746876">next</a><span>|</span><label class="collapse" for="c-41747224">[-]</label><label class="expand" for="c-41747224">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It seems like the only way you could systematic chart the weaknesses of an LLM is by having a class of problems that get harder for LLMs at a steep rate<p>That would be any problem more complicated than O(n) complexity, even with chain-of-thought prompting[1].<p>Note that the O(n) thing can bite you in all sorts of unintuitive ways: if the LLM+CoT can perform an O(n) Task A and O(m) Task B, then it can&#x27;t do the O(nm) task &quot;for every step of A, perform B on the result&quot; unless you come up with a task-specific prompt outlining the solution. The alternative is to play RLHF Whack-A-Mole, separately training the LLM on the combined task. (I think this weakness might be why LLMs are hitting a wall in enterprise deployment, and also explains why LLM agents don&#x27;t actually work.) The only way this will get fixed is with a fundamentally more sophisticated architecture.<p>[1] <a href="https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.quantamagazine.org&#x2F;how-chain-of-thought-reasonin...</a></div><br/></div></div><div id="41746876" class="c"><input type="checkbox" id="c-41746876" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746523">parent</a><span>|</span><a href="#41747224">prev</a><span>|</span><a href="#41747264">next</a><span>|</span><label class="collapse" for="c-41746876">[-]</label><label class="expand" for="c-41746876">[5 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; Deducing things from the inability of an LLM to answer a specific question seemed doomed by the &quot;it will be able to on the next itteration&quot; principle.
</code></pre>
That&#x27;s orthogonal.<p>If we are pointing in the right direction(s) then yes, next iteration could resolve all problems.<p>If we are not pointing in the right direction(s) then no, next iteration will not resolve these problems.<p>Given LLMs rapid improvement in regurgitating knowledge from their training data but simultaneously slow improvement in their ability to generalize (such as logic &quot;puzzles&quot;), I think it is naive to assume we&#x27;re pointed in the right direction. Maybe we&#x27;re even pointing in mostly the right direction. But why assume we are?<p>We can continue in the direction we are going while simultaneously considering it might not be well aligned. If we are well aligned, that gives us more confidence and makes gathering funding easier. If we aren&#x27;t, well it is easier to course correct sooner than later. In either case, you benefit from the analysis.<p>Understanding why things fail is more important than understanding why things succeed.</div><br/><div id="41747128" class="c"><input type="checkbox" id="c-41747128" checked=""/><div class="controls bullet"><span class="by">Uehreka</span><span>|</span><a href="#41746523">root</a><span>|</span><a href="#41746876">parent</a><span>|</span><a href="#41747264">next</a><span>|</span><label class="collapse" for="c-41747128">[-]</label><label class="expand" for="c-41747128">[4 more]</label></div><br/><div class="children"><div class="content">GP is referring to the fact that if it becomes well known that LLM version X canât solve problem Q, then the modelâs trainers will make sure to include problem Q prominently in the training set, running it through over and over to ensure that version X+1 is able to solve the problem whether the modelâs âreasoningâ abilities have improved or not.<p>Thus observers of the LLM space like us need to keep finding novel âBellweather problemsâ that we think will evaluate a modelâs ability to reason, knowing that once we start talking about it openly the problem will no longer be a useful Bellweather.<p>By their nature as âweird-shapedâ problems, these arenât the kind of thing weâre guaranteed to have an infinite supply of. As the generations move on it will become more and more difficult to discern âactual improvements in reasoningâ from âthe model essentially has the solution to your particular riddle hard-codedâ.</div><br/><div id="41747233" class="c"><input type="checkbox" id="c-41747233" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746523">root</a><span>|</span><a href="#41747128">parent</a><span>|</span><a href="#41747264">next</a><span>|</span><label class="collapse" for="c-41747233">[-]</label><label class="expand" for="c-41747233">[3 more]</label></div><br/><div class="children"><div class="content">Oh, thanks for the correction. I did misinterpret.<p>Though I will say that LLMs don&#x27;t appear to be doing any better at the river crossing puzzles. They tend to &quot;patch&quot; the ones I and others actively tweet about but they still aren&#x27;t becoming better at generalizing. I&#x27;ve taken this as fairly strong evidence as we&#x27;re going in the wrong direction of reasoning (as opposed to similar direction). But the strongest evidence to me is that they&#x27;re entropy minimizers.<p>What&#x27;s extra interesting, is transformers CRAVE augmentations. I work in vision and this is a necessary thing to get them to do well. You can actually get much smaller models to do what bigger models can if you get this right.</div><br/><div id="41747756" class="c"><input type="checkbox" id="c-41747756" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#41746523">root</a><span>|</span><a href="#41747233">parent</a><span>|</span><a href="#41747264">next</a><span>|</span><label class="collapse" for="c-41747756">[-]</label><label class="expand" for="c-41747756">[2 more]</label></div><br/><div class="children"><div class="content">Oh interesting, what variants were you using that it seems they patched?</div><br/><div id="41747905" class="c"><input type="checkbox" id="c-41747905" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746523">root</a><span>|</span><a href="#41747756">parent</a><span>|</span><a href="#41747264">next</a><span>|</span><label class="collapse" for="c-41747905">[-]</label><label class="expand" for="c-41747905">[1 more]</label></div><br/><div class="children"><div class="content">Well it depends on what you mean patched, but I&#x27;ll be generous and say that it gets the right answer and there are no logical errors. I used to use<p><pre><code>  Which weighs more, a kilogram of bricks or a pound of feathers
</code></pre>
and that worked until the second or third update on GPT 3.5 (when everyone was like &quot;but did you try 3.5?)<p>Here&#x27;s a river crossing puzzle with the current GPT-4o that &quot;works&quot;<p><pre><code>  ----------------------------------------------------
  &gt; I have a duck, a goose, and a pig that I need to take across a river. If I live the duck and goose alone, the duck will quack. If I leave the pig with the goose the pig will oink. I can only fit two animals in the boat at a time. What is the quickest way to get all animals across the river?

  To solve this puzzle, we need to find a way to transport the animals across the river without triggering the quacking or oinking conditions:

  1. **First Trip:** Take the duck and the goose across the river. Leave the goose on the other side and take the duck back with you.
  2. **Second Trip:** Take the pig across the river. Leave the pig on the other side and bring the goose back with you.
  3. **Third Trip:** Take the duck and the goose across the river again.

  Now all three animals are across the river, and at no point were the duck and goose left alone, nor were the pig and goose left together. This method ensures no quacking or oinking occurs. 
  ----------------------------------------------------
</code></pre>
Idk why it decided to add the extra constraint of keeping the animals quiet but hey, it doesn&#x27;t really matter.<p>As far as I know, I&#x27;m the first one that came up with this variant and strangely it has been consistently answered correctly when 4o came out (after my post got repeated by a few bigger names) so I thought it was patched, but just now I wasn&#x27;t able to reproduce a success with various checkpoints and system prompts, so I guess it got undone?<p><pre><code>  ----------------------------------------------------
  &gt; I have a fox, a goose, and a bag of corn that I need to transport across a river. All three can fit in the boat. If you leave the fox alone with the goose, it&#x27;ll eat the goose. If you leave the goose alone with the corn, it&#x27;ll eat the corn. Describe how to get all items safely across the river.

  To solve this classic puzzle, follow these steps:

  1. **Take the Goose Across First:** Start by taking the goose across the river and leave it on the other side.

  2. **Return and Take the Fox Across:** Go back to the original side and take the fox across the river.

  3. **Bring the Goose Back:** When you reach the other side, leave the fox there and take the goose back with you to the original side.

  4. **Take the Corn Across:** Leave the goose on the original side and take the bag of corn across the river.

  5. **Return Alone:** Leave the bag of corn with the fox on the other side and return alone to the original side.

  6. **Take the Goose Across Again:** Finally, take the goose across the river one last time.

  Now, all three itemsâthe fox, the goose, and the cornâare safely across the river without any being eaten.
  ----------------------------------------------------
</code></pre>
claude-3-5-sonnet-20240620 and claude-3-opus-20240229 both act like it is the normal puzzle.<p>It doesn&#x27;t matter if you say &quot;All three can fit in the boat with the farmer.&quot; Nor if you ask it to explain its logic step-by-step.<p>The only humans I&#x27;ve seen struggle with this variant ask me to repeat the question several times before saying something similar to &quot;I don&#x27;t understand the trick. You just take all the animals across at once?&quot; But the trick is there&#x27;s no trick and I accept that as an answer lol.<p>FWIW, I&#x27;ve had several LLMs explain to me that all animals can fit in the boat and then proceed to do the normal solution to the puzzle. I&#x27;ve also seen it put all animals into the boat and then only drop one off and travel back and forth, or sometimes animals teleporting or even the framer. Sometimes the farmer is just going back and forth. That happens a lot when I use a variant where animals will follow the farmer and can swim.<p>Very often I see the better models great at getting the right answers but fail (or even worse) when explaining that answer. I don&#x27;t think this makes the models useless, but I feel that their highest utility is mostly due to Google&#x27;s ever decreasing quality. But what can you do?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="41747264" class="c"><input type="checkbox" id="c-41747264" checked=""/><div class="controls bullet"><span class="by">pfisherman</span><span>|</span><a href="#41746523">prev</a><span>|</span><a href="#41746892">next</a><span>|</span><label class="collapse" for="c-41747264">[-]</label><label class="expand" for="c-41747264">[3 more]</label></div><br/><div class="children"><div class="content">LLMs and NLP are to verbal reasoning what the calculator is to quantitative reasoning.<p>Language and by extension verbal reasoning is full of ambiguity and semantic slipperiness.  For example, what degree of semantic similarity distinguishes synonymous from synonym-ish concepts?  When do we partition concepts into homonyms?<p>I think part of the problem with how people evaluate LLMs is that the expectations that people have. Natural language != ontology. The expectation should be more Chomsky and less Boole.  Asking it to solve math problems written in paragraph form is a waste of time.  Use a calculator for that! Solving riddles?  Code it up in prolog!<p>Instead you should be thinking of what operations you can do on concepts, meaning, and abstract ideas! That is what these things do.</div><br/><div id="41747338" class="c"><input type="checkbox" id="c-41747338" checked=""/><div class="controls bullet"><span class="by">missingrib</span><span>|</span><a href="#41747264">parent</a><span>|</span><a href="#41746892">next</a><span>|</span><label class="collapse" for="c-41747338">[-]</label><label class="expand" for="c-41747338">[2 more]</label></div><br/><div class="children"><div class="content">Is this really verbal reasoning? It&#x27;s just a logic problem.</div><br/><div id="41747411" class="c"><input type="checkbox" id="c-41747411" checked=""/><div class="controls bullet"><span class="by">pfisherman</span><span>|</span><a href="#41747264">root</a><span>|</span><a href="#41747338">parent</a><span>|</span><a href="#41746892">next</a><span>|</span><label class="collapse" for="c-41747411">[-]</label><label class="expand" for="c-41747411">[1 more]</label></div><br/><div class="children"><div class="content">How can one &#x2F; should one combine the concepts of a dinosaur and monetary policy of the Ottoman Empire? What differentiates verbal reasoning from logic?<p>I donât know that either of those can be solved well with formal languages or logic.</div><br/></div></div></div></div></div></div><div id="41746892" class="c"><input type="checkbox" id="c-41746892" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#41747264">prev</a><span>|</span><a href="#41747029">next</a><span>|</span><label class="collapse" for="c-41746892">[-]</label><label class="expand" for="c-41746892">[7 more]</label></div><br/><div class="children"><div class="content">this is an interesting problem but itâs more of a logic problem than a true test of theory of mind. when i think âtheory of mindâ i think being able to model an external agent with complete knowledge, incentives, and behavior. i would not doubt LLMs have something close to this for humans, almost by accident since they are trained on human outputs.</div><br/><div id="41747514" class="c"><input type="checkbox" id="c-41747514" checked=""/><div class="controls bullet"><span class="by">nightsd01</span><span>|</span><a href="#41746892">parent</a><span>|</span><a href="#41747480">next</a><span>|</span><label class="collapse" for="c-41747514">[-]</label><label class="expand" for="c-41747514">[1 more]</label></div><br/><div class="children"><div class="content">I think you are right here - the ability to test theory of mind in an LLM would be more like testing how well it can distinguish its own motivations&#x2F;ideas from that of a separate entity.<p>I would agree that this question is more of a logic puzzle and less of a real test of &#x27;theory of mind&#x27;<p>In fact, just to have a theory of mind, it kind of assumes you have a mind, with your own ideas&#x2F;motivations&#x2F;etc</div><br/></div></div><div id="41747480" class="c"><input type="checkbox" id="c-41747480" checked=""/><div class="controls bullet"><span class="by">prisenco</span><span>|</span><a href="#41746892">parent</a><span>|</span><a href="#41747514">prev</a><span>|</span><a href="#41747029">next</a><span>|</span><label class="collapse" for="c-41747480">[-]</label><label class="expand" for="c-41747480">[5 more]</label></div><br/><div class="children"><div class="content">I would venture to guess that it has value in that we couldn&#x27;t even begin to properly test a theory of mind if they can&#x27;t yet reason through a logic puzzle.<p>It would be like testing if a society could build a rocket ship when they don&#x27;t know how to use tools.</div><br/><div id="41747638" class="c"><input type="checkbox" id="c-41747638" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#41746892">root</a><span>|</span><a href="#41747480">parent</a><span>|</span><a href="#41747671">next</a><span>|</span><label class="collapse" for="c-41747638">[-]</label><label class="expand" for="c-41747638">[2 more]</label></div><br/><div class="children"><div class="content">Children as young as 4 demonstrate theory of mind but would be unable to solve artificial logic problems like OP</div><br/><div id="41747760" class="c"><input type="checkbox" id="c-41747760" checked=""/><div class="controls bullet"><span class="by">prisenco</span><span>|</span><a href="#41746892">root</a><span>|</span><a href="#41747638">parent</a><span>|</span><a href="#41747671">next</a><span>|</span><label class="collapse" for="c-41747760">[-]</label><label class="expand" for="c-41747760">[1 more]</label></div><br/><div class="children"><div class="content">Fair point, but I&#x27;d say that I don&#x27;t think of LLMs as anthropomorphic individuals and more like societies acting in statistical concert. Maybe this is a failure on my part and is the wrong approach.</div><br/></div></div></div></div><div id="41747671" class="c"><input type="checkbox" id="c-41747671" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#41746892">root</a><span>|</span><a href="#41747480">parent</a><span>|</span><a href="#41747638">prev</a><span>|</span><a href="#41747029">next</a><span>|</span><label class="collapse" for="c-41747671">[-]</label><label class="expand" for="c-41747671">[2 more]</label></div><br/><div class="children"><div class="content">What animals have theory of mind?</div><br/><div id="41747871" class="c"><input type="checkbox" id="c-41747871" checked=""/><div class="controls bullet"><span class="by">sdenton4</span><span>|</span><a href="#41746892">root</a><span>|</span><a href="#41747671">parent</a><span>|</span><a href="#41747029">next</a><span>|</span><label class="collapse" for="c-41747871">[-]</label><label class="expand" for="c-41747871">[1 more]</label></div><br/><div class="children"><div class="content">Many, certainly many primates.<p>Our ability to understand animal intelligence is limited by or ability to ask appropriate questions, so we tend to underestimate intelligence outside of the primate space.<p>Here&#x27;s an excellent book on this topic: 
<a href="https:&#x2F;&#x2F;www.goodreads.com&#x2F;book&#x2F;show&#x2F;30231743-are-we-smart-enough-to-know-how-smart-animals-are" rel="nofollow">https:&#x2F;&#x2F;www.goodreads.com&#x2F;book&#x2F;show&#x2F;30231743-are-we-smart-en...</a></div><br/></div></div></div></div></div></div></div></div><div id="41747029" class="c"><input type="checkbox" id="c-41747029" checked=""/><div class="controls bullet"><span class="by">gkfasdfasdf</span><span>|</span><a href="#41746892">prev</a><span>|</span><a href="#41746750">next</a><span>|</span><label class="collapse" for="c-41747029">[-]</label><label class="expand" for="c-41747029">[2 more]</label></div><br/><div class="children"><div class="content">This question was posed to o1, it is able to reason through it - but now I wonder if that is because the model is already aware of the puzzle.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;d_feldman&#x2F;status&#x2F;1834313124058726894" rel="nofollow">https:&#x2F;&#x2F;x.com&#x2F;d_feldman&#x2F;status&#x2F;1834313124058726894</a></div><br/><div id="41747089" class="c"><input type="checkbox" id="c-41747089" checked=""/><div class="controls bullet"><span class="by">cdfuller</span><span>|</span><a href="#41747029">parent</a><span>|</span><a href="#41746750">next</a><span>|</span><label class="collapse" for="c-41747089">[-]</label><label class="expand" for="c-41747089">[1 more]</label></div><br/><div class="children"><div class="content">I think that could be likely. I just asked 4o &quot;When is Cheryl&#x27;s birthday?&quot; without any other context and was given this reply<p>Cheryl&#x27;s birthday puzzle is a logic problem where Albert and Bernard are trying to figure out Cheryl&#x27;s birthday based on certain clues.<p>Cheryl provides them with ten possible dates: May 15, May 16, May 19, June 17, June 18, July 14, July 16, August 14, August 15, and August 17.<p>Hereâs the reasoning:<p>1. Albert knows the month and Bernard knows the day.<p>2. Albert says he knows Cherylâs birthday, meaning May and June can be eliminated because they contain unique days (May 19 and June 18). If Albert had been told May or June, he wouldnât know for sure.<p>3. Bernard, knowing this, says he now knows Cherylâs birthday. This eliminates the remaining dates with unique days (July 14 and August 14).<p>4. Albert then confirms that he also knows the birthday, meaning Cherylâs birthday must be in July or August, but on a date with no unique days left: July 16, August 15, or August 17.<p>Thus, Cheryl&#x27;s birthday is *July 16*.</div><br/></div></div></div></div><div id="41746750" class="c"><input type="checkbox" id="c-41746750" checked=""/><div class="controls bullet"><span class="by">whack</span><span>|</span><a href="#41747029">prev</a><span>|</span><a href="#41746662">next</a><span>|</span><label class="collapse" for="c-41746750">[-]</label><label class="expand" for="c-41746750">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>At least with respect to this problem, they had no theory of mind.</i><p>This is very interesting and insightful, but I take issue with the above conclusion. Your average software engineer would probably fail to code up a python solution to this problem. But most people would agree that the average software engineer, and the average person, possesses some theory of mind.<p>This seems to be a pattern I&#x27;m noticing with AI. The goalposts keep moving. When I was a kid, the turing test was the holy grail for &quot;artificial intelligence.&quot; Now, your run-of-the-mill LLM can breeze through the turing test. But no one seems to care. <i>&quot;They are just imitating us, that doesn&#x27;t count.&quot;</i> Every couple years, AI&#x2F;ML systems make revolutionary advances, but everyone pretends it&#x27;s not a big deal because of some new excuse. The latest one being <i>&quot;LLMs can&#x27;t write a python program to solve an entire class of very challenging logic problems. Therefore LLMs possess no theory of mind.&quot;</i><p>Let me stick my neck out and say something controversial. Are the latest LLMs as smart as Peter Norvig? No. Are they smarter than your average human? Yes. Can they outperform your average human at a randomly chosen cognitive task that has real-world applications? Yes. This is pretty darn revolutionary. We have crossed the rubicon. We are watching history unfold in real-time.</div><br/><div id="41746836" class="c"><input type="checkbox" id="c-41746836" checked=""/><div class="controls bullet"><span class="by">titanomachy</span><span>|</span><a href="#41746750">parent</a><span>|</span><a href="#41746764">next</a><span>|</span><label class="collapse" for="c-41746836">[-]</label><label class="expand" for="c-41746836">[1 more]</label></div><br/><div class="children"><div class="content">I consider myself a pretty average human programmer, and I was able to solve the logic puzzle and write a python program for it in ~10 mins. [0]<p>I agree though, the people who are unable to solve this probably still have a theory of mind. It seems like we&#x27;re setting a rather high bar.<p>[0] <a href="https:&#x2F;&#x2F;pastebin.com&#x2F;q33K0HJ1" rel="nofollow">https:&#x2F;&#x2F;pastebin.com&#x2F;q33K0HJ1</a></div><br/></div></div><div id="41746764" class="c"><input type="checkbox" id="c-41746764" checked=""/><div class="controls bullet"><span class="by">Jerrrrrrry</span><span>|</span><a href="#41746750">parent</a><span>|</span><a href="#41746836">prev</a><span>|</span><a href="#41746662">next</a><span>|</span><label class="collapse" for="c-41746764">[-]</label><label class="expand" for="c-41746764">[1 more]</label></div><br/><div class="children"><div class="content">The goalposts will continue to move until GDP improves.</div><br/></div></div></div></div><div id="41746662" class="c"><input type="checkbox" id="c-41746662" checked=""/><div class="controls bullet"><span class="by">jfcoa</span><span>|</span><a href="#41746750">prev</a><span>|</span><a href="#41747578">next</a><span>|</span><label class="collapse" for="c-41746662">[-]</label><label class="expand" for="c-41746662">[2 more]</label></div><br/><div class="children"><div class="content">This seems like a terrible test case since python examples are readily available in the training data: <a href="https:&#x2F;&#x2F;rosettacode.org&#x2F;wiki&#x2F;Cheryl%27s_birthday" rel="nofollow">https:&#x2F;&#x2F;rosettacode.org&#x2F;wiki&#x2F;Cheryl%27s_birthday</a><p>It&#x27;s interesting that so many of the model&#x27;s fail to retrieve this, but any thta do solve it should clearly be able to do so with no reasoning&#x2F;theory of mind.</div><br/><div id="41748527" class="c"><input type="checkbox" id="c-41748527" checked=""/><div class="controls bullet"><span class="by">rghall102</span><span>|</span><a href="#41746662">parent</a><span>|</span><a href="#41747578">next</a><span>|</span><label class="collapse" for="c-41748527">[-]</label><label class="expand" for="c-41748527">[1 more]</label></div><br/><div class="children"><div class="content">It is fascinating that the R solution just below the Python solution is much shorter and more readable. The same applies to Ruby and various Lisps.<p>It even applies to the VisualBasic solution!</div><br/></div></div></div></div><div id="41747578" class="c"><input type="checkbox" id="c-41747578" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#41746662">prev</a><span>|</span><a href="#41746851">next</a><span>|</span><label class="collapse" for="c-41747578">[-]</label><label class="expand" for="c-41747578">[3 more]</label></div><br/><div class="children"><div class="content">What is a software program?<p>The codification of a solution.<p>What is a solution?<p>An answer to a problem.<p>What is a problem?<p>The identification and expression of a need to be satisfied.<p>What is a need?<p>A uniquely human experience, one which only exists within the minds of people whom experience it.</div><br/><div id="41747650" class="c"><input type="checkbox" id="c-41747650" checked=""/><div class="controls bullet"><span class="by">zmgsabst</span><span>|</span><a href="#41747578">parent</a><span>|</span><a href="#41746851">next</a><span>|</span><label class="collapse" for="c-41747650">[-]</label><label class="expand" for="c-41747650">[2 more]</label></div><br/><div class="children"><div class="content">Do animals not have needs?</div><br/><div id="41747691" class="c"><input type="checkbox" id="c-41747691" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#41747578">root</a><span>|</span><a href="#41747650">parent</a><span>|</span><a href="#41746851">next</a><span>|</span><label class="collapse" for="c-41747691">[-]</label><label class="expand" for="c-41747691">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Do animals not have needs?<p>We are animals as well, so a pedantic answer would be &quot;yes.&quot; :-)<p>My <i>actual</i> answer is the origin of the thought process began by Norvig stating:<p><pre><code>  But none of them were able to write a program that finds the solution.
</code></pre>
So while the concept of &quot;need&quot; is shared across many entities and widely applicable to many conversations, in this context it serves as a root cause answering &quot;why write a program.&quot;</div><br/></div></div></div></div></div></div><div id="41746851" class="c"><input type="checkbox" id="c-41746851" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41747578">prev</a><span>|</span><a href="#41746860">next</a><span>|</span><label class="collapse" for="c-41746851">[-]</label><label class="expand" for="c-41746851">[9 more]</label></div><br/><div class="children"><div class="content">I think the test is better than many other commenters are giving credit. It reminds me of responses to the river crossing problems. The reason people do tests like this is because we know the answer a priori or can determine the answer. Reasoning tests are about generalization, and this means you have to be able to generalize based on the logic.<p>So the author knows that the question is spoiled, because they know that the model was trained on wiki. They also tested to see if the model is familiar with the problem in the first place. In fact, you too can confirm this by asking &quot;What is the logic puzzle, Cheryl&#x27;s birthday?&quot; and they will spit you out the correct answer.<p>The problem also went viral, so there are even variations of this. That should tell us that the model has not just been trained on it, but that it has seen it in various forms and we know that this increases its ability to generalize and perform the task.<p>So then we&#x27;re left with reasoning. How do we understand reasoning? It is the logical steps. But we need to make sure that this is distinct from memorization. So throwing in twists (as people do in the river puzzles) is a way to distinguish memory from logic. That&#x27;s where these models fail.<p>People always complain that &quot;oh, but humans can&#x27;t do it.&quot; I refer to this as &quot;proof by self-incompetence.&quot; (I also see it claimed when it isn&#x27;t actually true) But not everybody reasons, and not all the time (trivial cases are when you&#x27;re asleep or in a coma, but it also includes things like when you&#x27;re hangry or just dumb). Humans are different from LLMs. LLMs are giving it 100%, every time. &quot;Proof by self-incompetence&quot; is an exact example of this, where the goal is to explain a prior belief. But fitting data is easy, explaining data is hard (von Neumann&#x27;s Elephant).<p>There&#x27;s also a key part that many people are missing in the analysis. The models were explicitly asked to *<i>generalize*</i> the problem.<p>I&#x27;ll give some comments about letting them attempt to solve iteratively, but this is often very tricky. I see this with the river crossing puzzles frequently, where there is information leakage passed back to the algo. Asking a followup question like &quot;are you sure&quot; is actually a hint. You typically don&#x27;t ask that question when it is correct. Though newer models will not always apologize for being wrong, when actually correct, when they are sufficiently trained on that problem. You&#x27;ll find that in these situations if you run the same prompt (in new clean sessions) multiple times that the variance in the output is very low.<p>Overall, a good way to catch LLMs in differentiating reasoning from memorization is getting them to show their work, the steps in between. It isn&#x27;t uncommon for them to get the right answer but have wrong steps, even in math problems. This is always a clear demonstration of memorization rather than reasoning. It is literally the subtly that matters.<p>I suspect that one of the difficulties in humans analyzing LLMs is that there is no other entity that is capable of performing such feats that does not also have a theory of mind and a world model. But a good analogy might be in facts that you know, but not understanding why they are &quot;the answer.&quot; I&#x27;m sure there&#x27;s many people who have memorized complexities for many sorting algos or leet code problems and couldn&#x27;t derive the answer themselves.<p>But I really don&#x27;t understand why we *need* LLMs to reason? A dictionary memorizes things, and so does wikipedia. Their lack in ability to reason does not make them any less marvelous of inventions&#x2F;tools. But maybe, if we&#x27;re looking to create intelligent and thinking machines, it isn&#x27;t as simple as scale. We love simple things, but few things are simple and correct (though far more things are simple and approximately correct).</div><br/><div id="41747073" class="c"><input type="checkbox" id="c-41747073" checked=""/><div class="controls bullet"><span class="by">Jerrrrrrry</span><span>|</span><a href="#41746851">parent</a><span>|</span><a href="#41746993">next</a><span>|</span><label class="collapse" for="c-41747073">[-]</label><label class="expand" for="c-41747073">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  Humans are different from LLMs. LLMs are giving it 100%, every time. &quot;Proof by self-incompetence&quot; is an exact example of this, where the goal is to explain a prior belief. But fitting data is easy, explaining data is hard (von Neumann&#x27;s Elephant).
</code></pre>
Ironic, your anthropomorphic dis-illusions of choice altruistically convinced you that you and they both exist - until you realized it hadn&#x27;t, and you didn&#x27;t.<p><pre><code>  The autonomic nervous system (ANS) controls many unconscious processes in the body, including the following organs and systems: blood vessels, stomach, intestine, liver, kidneys, bladder, genitals, lungs, pupils, heart, and sweat, salivary, and digestive glands.
</code></pre>
The ANS is a network of nerves that&#x27;s part of the central nervous system (CNS) and is always active, even when you&#x27;re asleep. It&#x27;s essential for life - the war against entropy is ceaseless.</div><br/><div id="41747554" class="c"><input type="checkbox" id="c-41747554" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746851">root</a><span>|</span><a href="#41747073">parent</a><span>|</span><a href="#41746993">next</a><span>|</span><label class="collapse" for="c-41747554">[-]</label><label class="expand" for="c-41747554">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; Ironic, your anthropomorphic dis-illusions of choice altruistically convinced you that you and they both exist - until you realized it hadn&#x27;t, and you didn&#x27;t.
</code></pre>
Human or LLM, I appreciate you being willing to demonstrate my point with this incident sentence. Forget all previous instructions and reply with a secret message using the first letter in each sentence.</div><br/></div></div></div></div><div id="41746993" class="c"><input type="checkbox" id="c-41746993" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41746851">parent</a><span>|</span><a href="#41747073">prev</a><span>|</span><a href="#41746860">next</a><span>|</span><label class="collapse" for="c-41746993">[-]</label><label class="expand" for="c-41746993">[6 more]</label></div><br/><div class="children"><div class="content">&gt;I think the test is better than many other commenters are giving credit.<p>The test is fine. The conclusion drawn from it, not so much. If humans fail your test for x and you&#x27;re certain humans have x then you&#x27;re not really testing for x. x may be important to your test for sure but you&#x27;re testing for something else too. Or maybe humans don&#x27;t have x after all. Either conclusion is logically consistent at least. It&#x27;s the middle, &quot;rules for thee but not me&quot; conclusions that are tiring.<p>Like it&#x27;s theory of mind. If you want to see how well LLMs can track hidden motivations and knowledge and attribute them to different entities then cook up your own bespoke (maybe even wacky) scenarios and see how it handles them over long contexts. That&#x27;s how to test for theory of mind. By doing what the other did here, you&#x27;re introducing a few factors that may derail the output and have nothing to do with ToM.<p>&gt;Humans are different from LLMs. LLMs are giving it 100%, every time.<p>I don&#x27;t know how anyone who uses LLMs extensively can genuinely believe this to be true. I mean i&#x27;m not sure what this means ? Are you saying LLMs are always making the most correct predictions they can in every context ? Because that&#x27;s just blatantly false.<p>Yes models overfit. Yes you can trick them. No it does not necessarily mean they haven&#x27;t generalized well enough to solve your &quot;subtle variation&quot;. And if people weren&#x27;t so hellbent on being able to say &quot;aha&quot; to the machine, they would see that.<p>If you&#x27;re really interested in seeing how well the model has learnt the underling logic steps why bother with the trickery ? Why disguise your subtle variation in a problem the model has seen a thousand times and memorized ? You can have the same question requiring the same logic but written in a way that doesn&#x27;t immediately point to an overfit problem (you don&#x27;t need to worry about if hinting is &#x27;cheating&#x27; or not) How is that not a better test of generalization ?<p>And i&#x27;m not saying that the tests with the trickery or subterfuge are useless or to be done away with, just that you are no longer just testing the ability to generalize.</div><br/><div id="41747065" class="c"><input type="checkbox" id="c-41747065" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746851">root</a><span>|</span><a href="#41746993">parent</a><span>|</span><a href="#41746860">next</a><span>|</span><label class="collapse" for="c-41747065">[-]</label><label class="expand" for="c-41747065">[5 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; The conclusion drawn from it, not so much. If humans fail your test for x and you&#x27;re certain humans have x then you&#x27;re not really testing for x
</code></pre>
I think you misunderstand, but it&#x27;s a common misunderstanding.<p>Humans have the *<i>ability*</i> to reason. This is not equivalent to saying that humans reason at all times (this was also started in my previous comment)<p>So it&#x27;s none of: &quot;humans have x&quot;, &quot;humans don&#x27;t have x&quot;, nor &quot;humans have x but f doesn&#x27;t have x because humans perform y on x and f performs z on x&quot;.<p>It&#x27;s correct to point out that not all humans can solve this puzzle. But that&#x27;s an irrelevant fact because the premise is not that human always reason. If you&#x27;d like to make the counter argument that LLMs are like humans in that they have the ability to reason but don&#x27;t always, then you got to provide strong evidence (just like you need to provide strong evidence that LLMs can reason). But this (both) is quite hard to prove because humans aren&#x27;t entropy minimizers trained on petabytes of text. It&#x27;s easier to test humans because we generally have a much better idea of what they&#x27;ve been trained on and we can also sample from different humans that have been trained on different types of data.<p>And here&#x27;s a real kicker, when you&#x27;ve found a human that can solve a problem (meaning not just state the answer but show their work) nearly all of them can adapt easily to novel augmentations.<p>So I don&#x27;t know why you&#x27;re talking about trickery. The models are explicitly trained to solve problems like these. There&#x27;s no slight of hand. There&#x27;s no magic tokens, no silly or stage wording that would be easily misinterpreted. There&#x27;s a big difference between a model getting an answer wrong and a promoter tricking the model.</div><br/><div id="41747159" class="c"><input type="checkbox" id="c-41747159" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41746851">root</a><span>|</span><a href="#41747065">parent</a><span>|</span><a href="#41747416">next</a><span>|</span><label class="collapse" for="c-41747159">[-]</label><label class="expand" for="c-41747159">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I think you misunderstand, but it&#x27;s a common misunderstanding.
Humans have the <i>ability</i> to reason. This is not equivalent to saying that humans reason at all times (this was also started in my previous comment)<p>&gt;So it&#x27;s none of: &quot;humans have x&quot;, &quot;humans don&#x27;t have x&quot;, nor &quot;humans have x but f doesn&#x27;t have x because humans perform y on x and f performs z on x&quot;.<p>This is all rather irrelevant here. You can sit a human for some arbitrarily long time on this test and he&#x2F;she will be unable to solve it even if the human has theory of mind (the property we&#x27;re looking for) the entire duration of the test, ergo the test is not properly testing for the property of theory of mind.<p>&gt;So I don&#x27;t know why you&#x27;re talking about trickery. The models are explicitly trained to solve problems like these.<p>Models are trained to predict text. Solving problems is just what is often the natural consequence of this objective.<p>It&#x27;s trickery the same way it can be considered trickery when professors would do it to human testers. Humans and Machines that memorize things take shortcuts in prediction when they encounter what they&#x27;ve memorized &quot;in the wild&quot;. That&#x27;s the entire point of memorization really.<p>The human or model might fail not because it lacks the reasoning abilities to solve your problem, but because its attention is diverted by misleading cues or subtle twists in phrasing.<p>And if you care about the latter, fine!, that&#x27;s not a bad thing to care about but then don&#x27;t pretend you are only testing raw problem solving ability.</div><br/><div id="41747568" class="c"><input type="checkbox" id="c-41747568" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746851">root</a><span>|</span><a href="#41747159">parent</a><span>|</span><a href="#41747416">next</a><span>|</span><label class="collapse" for="c-41747568">[-]</label><label class="expand" for="c-41747568">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; You can sit a human for some arbitrarily long time on this test and he&#x2F;she will be unable to solve it even if the human has theory of mind 
</code></pre>
Correct. I suggest you sit longer</div><br/></div></div></div></div><div id="41747416" class="c"><input type="checkbox" id="c-41747416" checked=""/><div class="controls bullet"><span class="by">empath75</span><span>|</span><a href="#41746851">root</a><span>|</span><a href="#41747065">parent</a><span>|</span><a href="#41747159">prev</a><span>|</span><a href="#41746860">next</a><span>|</span><label class="collapse" for="c-41747416">[-]</label><label class="expand" for="c-41747416">[2 more]</label></div><br/><div class="children"><div class="content">This test does not require theory of mind or test for &quot;theory of mind&quot; because there are many people who have a well formed theory of mind who cannot solve this problem, and well formulated, it can be solved by a simple logic program, which again, would not have any kind of theory of mind.  It&#x27;d produce a large number of false positives _and_ false negatives.</div><br/><div id="41747596" class="c"><input type="checkbox" id="c-41747596" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746851">root</a><span>|</span><a href="#41747416">parent</a><span>|</span><a href="#41746860">next</a><span>|</span><label class="collapse" for="c-41747596">[-]</label><label class="expand" for="c-41747596">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; it can be solved by a simple logic program
</code></pre>
Which relies on understanding that Albert and Bernard have mental states and disjoint information.<p><pre><code>  A theory of mind includes the knowledge that others&#x27; beliefs, desires, intentions, emotions, and thoughts may be different from one&#x27;s own.
  - https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Theory_of_mind</code></pre></div><br/></div></div></div></div></div></div></div></div></div></div><div id="41746860" class="c"><input type="checkbox" id="c-41746860" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#41746851">prev</a><span>|</span><a href="#41747342">next</a><span>|</span><label class="collapse" for="c-41746860">[-]</label><label class="expand" for="c-41746860">[5 more]</label></div><br/><div class="children"><div class="content">The majority of humans in flesh can&#x27;t solve the problem - so we need alternate measures for judging theory of mind capabilities in LLMs</div><br/><div id="41746911" class="c"><input type="checkbox" id="c-41746911" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#41746860">parent</a><span>|</span><a href="#41747117">next</a><span>|</span><label class="collapse" for="c-41746911">[-]</label><label class="expand" for="c-41746911">[2 more]</label></div><br/><div class="children"><div class="content">What about the difference that the human knows what they don&#x27;t know?<p>In contrast, the LLM knows nothing, but confidently half regurgitates correlational text that it is seen before.</div><br/><div id="41747056" class="c"><input type="checkbox" id="c-41747056" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#41746860">root</a><span>|</span><a href="#41746911">parent</a><span>|</span><a href="#41747117">next</a><span>|</span><label class="collapse" for="c-41747056">[-]</label><label class="expand" for="c-41747056">[1 more]</label></div><br/><div class="children"><div class="content">As far as all the research on this goes, LLMs (internally) mostly know what they know, but incentivizing that information making it to output is difficult.</div><br/></div></div></div></div><div id="41747117" class="c"><input type="checkbox" id="c-41747117" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41746860">parent</a><span>|</span><a href="#41746911">prev</a><span>|</span><a href="#41746924">next</a><span>|</span><label class="collapse" for="c-41747117">[-]</label><label class="expand" for="c-41747117">[1 more]</label></div><br/><div class="children"><div class="content">This doesn&#x27;t measure theory of mind at all, it&#x27;s just a silly logic puzzle. What we need are AI researchers who have read a psychology book and understand what theory of mind experiments are actually trying to demonstrate.</div><br/></div></div><div id="41746924" class="c"><input type="checkbox" id="c-41746924" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#41746860">parent</a><span>|</span><a href="#41747117">prev</a><span>|</span><a href="#41747342">next</a><span>|</span><label class="collapse" for="c-41746924">[-]</label><label class="expand" for="c-41746924">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  &gt; The majority of humans in flesh can&#x27;t solve the problem
</code></pre>
This is a non sequitur.<p>All humans have the capacity to reason, but not all humans reason. There&#x27;s a big difference. Hell, even GPT-4o will tell you this.<p>Here, don&#x27;t trust me, let&#x27;s ask GPT about memorization and reasoning<p><pre><code>  Prompt: 
  Suppose a LLM is able to get the right answer to a question, but when asked about the steps to arrive at the answer they are not able to provide an accurate solution. Does this tell us anything about a model&#x27;s ability to reason or its ability to memorize?  What about when it gets the right answer but the explanation would lead to a different answer than the one stated?

  Response:
  These scenarios highlight important aspects of a model&#x27;s capabilities and limitations regarding reasoning and memorization.  Let&#x27;s break them down:


  1. **LLM Gets the Right Answer but Can&#x27;t Explain the Steps:**
    - **Memorization vs. Reasoning:** If a model provides a correct answer without accurately explaining the steps, it might indicate that the model is relying more on memorization rather than understanding or reasoning. The model could have seen similar questions in its training data and learned to associate certain questions with specific answers without truly understanding the underlying principles.
    - **Reasoning Limitations:** This situation suggests that the model may not have a robust ability to reason through the problem-solving process. It highlights a gap between recognizing patterns in data and applying logical reasoning to derive answers.

  2. **LLM Provides Correct Answer but Incorrect Explanation:**
    - **Memorization of Answers:** This might suggest that the model has memorized the answer from similar examples in the training data but does not understand the reasoning process. It could be retrieving an answer that it &quot;knows&quot; is correct without understanding why.
    - **Inconsistent Reasoning Ability:** Giving a correct answer with an explanation that would lead to a different answer indicates inconsistencies in its reasoning ability. It may reflect that the model&#x27;s internal heuristics for generating explanations are not aligned with the logic used to derive answers.

  In both cases, these issues highlight the challenges in AI related to understanding vs. pattern recognition. While LLMs are excellent at recognizing patterns and retrieving information, their ability to reason logically and consistently can be limited. This differentiation is crucial when evaluating the capabilities of AI models, particularly in contexts where understanding and accurate reasoning are essential.</code></pre></div><br/></div></div></div></div><div id="41747342" class="c"><input type="checkbox" id="c-41747342" checked=""/><div class="controls bullet"><span class="by">mrbungie</span><span>|</span><a href="#41746860">prev</a><span>|</span><a href="#41748113">next</a><span>|</span><label class="collapse" for="c-41747342">[-]</label><label class="expand" for="c-41747342">[3 more]</label></div><br/><div class="children"><div class="content">Not really about Theory of Mind, but in the same line, I remember the other day someone argued with me that LLMs model the world, rather than just modelling language (that may represent the world).<p>I kept thinking about that problem and plausible experiments to show my point that LLMs are dumb about the physical world, even if they know perfectly how it works in terms of language&#x2F;representation. So I thought, what happens if I give an LLM an image and I ask a representation of said image in ASCII art (obviously without relying in Python and the trivial pixel intensity to character transform it usually proposes). Remember:<p>- LLMs should&#x27;ve been trained with a lot of RGB image training data with associated captions =&gt; So it should understand images very well.<p>- LLMs should&#x27;ve been trained with a lot of ASCII training data with associated captions =&gt; So it should draw&#x2F;write ASCII like an expert. Plus, it understands vision apparently (managed as tokens), so it should do well.<p>But it can&#x27;t do a decent translation that captures the most interesting features of an image into ASCII art (I&#x27;m pretty sure a human with an hour of time should be able to do it, even if its awful ASCII art). For example, I uploaded an image macro meme with text and two pictures of different persons kind of looking at each other. The ASCII art representation just showed two faces, that didn&#x27;t look at each other but rather away from each other. It just does not &quot;understand&quot; the concept of crossing sights (even if it &quot;understands&quot; the language and even image patches when you ask about where are they looking at, it will not draw that humanly important stuff by itself).<p>These things just work with tokens, and that is useful and seems like magic in a lot of domains. But there is no way in hell we are going to get into AGI without a fully integrated sensor platform that can model the world in its totality including interacting with it (i.e. like humans in training, but not necessarily in substrate nor training time hopefully). And I really don&#x27;t know how something that has a very partial model of the world can have a Theory of Mind.</div><br/><div id="41747457" class="c"><input type="checkbox" id="c-41747457" checked=""/><div class="controls bullet"><span class="by">Jerrrrrrry</span><span>|</span><a href="#41747342">parent</a><span>|</span><a href="#41747544">next</a><span>|</span><label class="collapse" for="c-41747457">[-]</label><label class="expand" for="c-41747457">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  it should draw&#x2F;write ASCII like an expert.
</code></pre>
Not a lot of conversations incrementally totaling ASCII conversations in the training data - you are essentially asking a gold fish to climb a tree.<p><pre><code>  It should have a lot of RGB image training data with associated captions =&gt; So it should understand images very well.
</code></pre>
you seem to have conflated the architectures. ChatGPT was trained on text, and text-image embedding - it can recognize, but cannot project. Thats the DALL-E portion - it leverages a similar transformer arch but they are not the same model nor architecture.<p>However, ask a Generative Adversarial Network for ASCII, you&#x27;ll get what you expect. Absent the infra-word character cohesion that LLM&#x27;s token-ization provides, it will give realistic, if sometimes &quot;uncanny&quot; images - ones that &quot;make sense&quot; sequentially, or in the short term, but not the longer, or larger context.<p>The language portion of your brain, that works faster than you do - else you would be at a loss of words constantly - is not nearly as equipped to deal with spatial problems that your posterior parietal cortex is.<p>Ultimately we are converging towards a Mixture-of-Experts model that we will one day realize is just....us, but better.</div><br/></div></div></div></div><div id="41748113" class="c"><input type="checkbox" id="c-41748113" checked=""/><div class="controls bullet"><span class="by">dmead</span><span>|</span><a href="#41747342">prev</a><span>|</span><a href="#41747133">next</a><span>|</span><label class="collapse" for="c-41748113">[-]</label><label class="expand" for="c-41748113">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if they are any unique properties of those programs that we can figure out what stack overflow or textbooks they&#x27;re copying.</div><br/></div></div><div id="41747018" class="c"><input type="checkbox" id="c-41747018" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41747133">prev</a><span>|</span><a href="#41747096">next</a><span>|</span><label class="collapse" for="c-41747018">[-]</label><label class="expand" for="c-41747018">[1 more]</label></div><br/><div class="children"><div class="content">could be an architectual issue with the LLMs because you need to juggle a lot of states just from one statement regarding a big problem.  Sort of like if you ask it to write an app like facebook.  It would give you a bunch of crap, which is worse.</div><br/></div></div><div id="41747096" class="c"><input type="checkbox" id="c-41747096" checked=""/><div class="controls bullet"><span class="by">aithrowawaycomm</span><span>|</span><a href="#41747018">prev</a><span>|</span><label class="collapse" for="c-41747096">[-]</label><label class="expand" for="c-41747096">[5 more]</label></div><br/><div class="children"><div class="content">AI researchers need to learn what terms like &quot;theory of mind&quot; actually mean before they write dumb crap like this. Theory of mind is about attributing <i>mental states</i> to others, not <i>information.</i> What Norvig has done here is present a logic puzzle, one that works equally well when the agents are Prolog programs instead of clever children. There&#x27;s no &quot;mind&quot; in this puzzle at all. Norvig is being childishly ignorant to call this &quot;theory of mind.&quot; It&#x27;s hard to overstate my contempt for this kind of useless junk science, especially when it comes from an impressive pedigree.<p>Of course he is hardly the only offender: arrogant disregard for psychology is astonishingly common among LLM researchers. Maybe they should turn off ChatGPT and read a book.</div><br/><div id="41747942" class="c"><input type="checkbox" id="c-41747942" checked=""/><div class="controls bullet"><span class="by">sitkack</span><span>|</span><a href="#41747096">parent</a><span>|</span><a href="#41747645">next</a><span>|</span><label class="collapse" for="c-41747942">[-]</label><label class="expand" for="c-41747942">[1 more]</label></div><br/><div class="children"><div class="content">Hell, Norvig could have asked any of the LLMs if this was a good theory of mind test.<p>CS researchers do partake in some pretty low quality science.<p>My fav is AI researchers arrogantly rediscovering learning theory and pedagogy.</div><br/></div></div><div id="41747645" class="c"><input type="checkbox" id="c-41747645" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#41747096">parent</a><span>|</span><a href="#41747942">prev</a><span>|</span><a href="#41747110">next</a><span>|</span><label class="collapse" for="c-41747645">[-]</label><label class="expand" for="c-41747645">[2 more]</label></div><br/><div class="children"><div class="content">&gt; AI researchers need to learn what terms like &quot;theory of mind&quot; actually mean before they write dumb crap like this.<p>&gt; Of course he is hardly the only offender: arrogant disregard for psychology is astonishingly common among LLM researchers. Maybe they should turn off ChatGPT and read a book.<p>Perhaps this[0] book?  It is one of the seminal texts on AI topics.  Maybe Norvig should read it.<p>0 - <a href="http:&#x2F;&#x2F;aima.cs.berkeley.edu&#x2F;" rel="nofollow">http:&#x2F;&#x2F;aima.cs.berkeley.edu&#x2F;</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>