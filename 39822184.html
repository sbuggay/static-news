<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1711443656754" as="style"/><link rel="stylesheet" href="styles.css?v=1711443656754"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://thechipletter.substack.com/p/googles-first-tpu-architecture">Google&#x27;s First Tensor Processing Unit: Architecture</a> <span class="domain">(<a href="https://thechipletter.substack.com">thechipletter.substack.com</a>)</span></div><div class="subtext"><span>c_joly</span> | <span>122 comments</span></div><br/><div><div id="39823235" class="c"><input type="checkbox" id="c-39823235" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39823598">next</a><span>|</span><label class="collapse" for="c-39823235">[-]</label><label class="expand" for="c-39823235">[21 more]</label></div><br/><div class="children"><div class="content">On the podcast interview now Groq CEO Jonathon Ross did[1] he talked about the creation of the original TPUs (which he built at Google). Apparently originally it was a FPGA he did in his 20% time because he sat near the team who was having inference speed issues.<p>They got it working, then Jeff Dean did the math and the decided to do an ASIC.<p>Now of course Google should spin off the TPU team as a separate company. It&#x27;s the only credible competition NVidia has, and the software support is second only to NVidia.<p>[1] <a href="https:&#x2F;&#x2F;open.spotify.com&#x2F;episode&#x2F;0V9kRgNS7Ds6zh3GjdXUAQ?si=qE3MLNuFQLCFmSnvhG87jQ&amp;nd=1&amp;dlsi=3752b373bdfd4f2c" rel="nofollow">https:&#x2F;&#x2F;open.spotify.com&#x2F;episode&#x2F;0V9kRgNS7Ds6zh3GjdXUAQ?si=q...</a></div><br/><div id="39824311" class="c"><input type="checkbox" id="c-39824311" checked=""/><div class="controls bullet"><span class="by">Laremere</span><span>|</span><a href="#39823235">parent</a><span>|</span><a href="#39825578">next</a><span>|</span><label class="collapse" for="c-39824311">[-]</label><label class="expand" for="c-39824311">[10 more]</label></div><br/><div class="children"><div class="content">The way I see, NVidia only has a few advantages ordered from most important to least:<p>1. Reserved fab space.<p>2. Highly integrated software.<p>3. Hardware architecture that exists today.<p>4. Customer relationships.<p>but all of these aspects are weak in one way or another:<p>For #1, fab space is tight, and NVidia can strangle its consumer GPU market if it means selling more AI chips at a higher price.  This advantage is gone if a competitor makes big bets years in advance, or another company that has a lot of fab space (intel?) is willing to change priorities.<p>2. Life is good when your proprietary software is the industry standard.  Whether this actually matters will depend on the use case heavily.<p>3. A benefit now, but not for long.  It&#x27;s my estimation that the hardware design for TPUs is fundamentally much simpler than for GPUs.  No need for raytracing, texture samplers, or rasterization.  Mostly just needs lots of matrix multiplication and memory.  Others moving into the space will be able to catch up quickly.<p>4. Useful to stay in the conversation, but in a field hungry for any advantage, the hardware vendor with the highest FLOPS (or equivalent) per dollar is going to win enough customers to saturate their manufacturing ability.<p>So overall, I give them a few years, and then the competition is going to be real quite fast.</div><br/><div id="39824647" class="c"><input type="checkbox" id="c-39824647" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824311">parent</a><span>|</span><a href="#39824830">next</a><span>|</span><label class="collapse" for="c-39824647">[-]</label><label class="expand" for="c-39824647">[1 more]</label></div><br/><div class="children"><div class="content">Actually their real advantage is the large set of highly optimised CUDA kernels.<p>This is the thing that lets them outperform AMD chips even on inferior hardware. And the fact that anything new gets written for CUDA first.<p>There is OpenAI&#x27;s Triton language for this too and people are beginning to use it (shout out to Unsloth here!).<p>&gt; Reserved fab space.<p>While this is true, it&#x27;s worth noting that the inference only Groq chip which gets 2x-5x better LLM inference performance is on a 12nm process.</div><br/></div></div><div id="39824830" class="c"><input type="checkbox" id="c-39824830" checked=""/><div class="controls bullet"><span class="by">dagmx</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824311">parent</a><span>|</span><a href="#39824647">prev</a><span>|</span><a href="#39825252">next</a><span>|</span><label class="collapse" for="c-39824830">[-]</label><label class="expand" for="c-39824830">[1 more]</label></div><br/><div class="children"><div class="content">Don’t underestimate CUDA as the moat. It’s been a decade of sheer dominance with multiple attempts to loosen its grip that haven’t been super fruitful.<p>I’ll also add that their second moat is Mellanox. They have state of the art interconnect and networking that puts them ahead of the competition that are currently focusing just on the single unit.</div><br/></div></div><div id="39825252" class="c"><input type="checkbox" id="c-39825252" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824311">parent</a><span>|</span><a href="#39824830">prev</a><span>|</span><a href="#39824620">next</a><span>|</span><label class="collapse" for="c-39825252">[-]</label><label class="expand" for="c-39825252">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia&#x27;s datacenter AI chips don&#x27;t have raytracing or rasterization. Heck, for all we know the new blackwell chip is almost exclusively tensor cores. They gave no numbers for regular CUDA perf.</div><br/></div></div><div id="39824620" class="c"><input type="checkbox" id="c-39824620" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824311">parent</a><span>|</span><a href="#39825252">prev</a><span>|</span><a href="#39824619">next</a><span>|</span><label class="collapse" for="c-39824620">[-]</label><label class="expand" for="c-39824620">[1 more]</label></div><br/><div class="children"><div class="content">&gt;2. Highly integrated software.<p>NVidia&#x27;s biggest advantage is that AMD is unwilling to pay for top notch software engineers (and unwilling to pay the corresponding increase in hardware engineer salaries this would entail). If you check online you&#x27;ll see NVidia pays both hardware and software engineers significantly more than AMD does. This is a cultural&#x2F;management problem, which AMD&#x27;s unlikely to overcome in the near-term future. Apple so far seems like the only other hardware company that doesn&#x27;t underpay its engineers, but Apple&#x27;s unlikely to release a discrete&#x2F;stand-alone GPU any time soon.</div><br/></div></div><div id="39824619" class="c"><input type="checkbox" id="c-39824619" checked=""/><div class="controls bullet"><span class="by">7e</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824311">parent</a><span>|</span><a href="#39824620">prev</a><span>|</span><a href="#39825060">next</a><span>|</span><label class="collapse" for="c-39824619">[-]</label><label class="expand" for="c-39824619">[2 more]</label></div><br/><div class="children"><div class="content">These have always been NVIDIA&#x27;s &quot;few&quot; advantages and yet they&#x27;ve still dominated for years. It&#x27;s their relentless pace of innovation that is their advantage. They resemble Intel of old, and despite Intel&#x27;s same &quot;few&quot; advantages, Intel is still dominant in the PC space (even with recent missteps).</div><br/><div id="39824721" class="c"><input type="checkbox" id="c-39824721" checked=""/><div class="controls bullet"><span class="by">weweersdfsd</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824619">parent</a><span>|</span><a href="#39825060">next</a><span>|</span><label class="collapse" for="c-39824721">[-]</label><label class="expand" for="c-39824721">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;ve dominated for years, but now all big tech companies are using their products in scale not seen before, and all have vested interest in cutting their margins by introducing some real competition.<p>Nvidia will do good in the future, but perhaps not good enough to justify their stock price.</div><br/></div></div></div></div><div id="39825060" class="c"><input type="checkbox" id="c-39825060" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824311">parent</a><span>|</span><a href="#39824619">prev</a><span>|</span><a href="#39825578">next</a><span>|</span><label class="collapse" for="c-39825060">[-]</label><label class="expand" for="c-39825060">[3 more]</label></div><br/><div class="children"><div class="content">CUDA is absolute shit, segfaults or compiler errors if you look at it wrong.<p>NVidia&#x27;s software is the only reason I&#x27;m not using GPU&#x27;s for ML tasks and likely never will.</div><br/><div id="39825245" class="c"><input type="checkbox" id="c-39825245" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39825060">parent</a><span>|</span><a href="#39825548">next</a><span>|</span><label class="collapse" for="c-39825245">[-]</label><label class="expand" for="c-39825245">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just C. If you&#x27;re accessing your arrays out of bounds it&#x27;s going to segfault. hopefully.<p>Can&#x27;t blame CUDA for that one.</div><br/></div></div><div id="39825548" class="c"><input type="checkbox" id="c-39825548" checked=""/><div class="controls bullet"><span class="by">Culonavirus</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39825060">parent</a><span>|</span><a href="#39825245">prev</a><span>|</span><a href="#39825578">next</a><span>|</span><label class="collapse" for="c-39825548">[-]</label><label class="expand" for="c-39825548">[1 more]</label></div><br/><div class="children"><div class="content">Skill issue.</div><br/></div></div></div></div></div></div><div id="39825578" class="c"><input type="checkbox" id="c-39825578" checked=""/><div class="controls bullet"><span class="by">pyb</span><span>|</span><a href="#39823235">parent</a><span>|</span><a href="#39824311">prev</a><span>|</span><a href="#39823350">next</a><span>|</span><label class="collapse" for="c-39825578">[-]</label><label class="expand" for="c-39825578">[1 more]</label></div><br/><div class="children"><div class="content">There seem to be conflicting reports as to who came up with the TPU <a href="https:&#x2F;&#x2F;mastodon.social&#x2F;@danluu&#x2F;109641269333636407" rel="nofollow">https:&#x2F;&#x2F;mastodon.social&#x2F;@danluu&#x2F;109641269333636407</a></div><br/></div></div><div id="39823350" class="c"><input type="checkbox" id="c-39823350" checked=""/><div class="controls bullet"><span class="by">summerlight</span><span>|</span><a href="#39823235">parent</a><span>|</span><a href="#39825578">prev</a><span>|</span><a href="#39824499">next</a><span>|</span><label class="collapse" for="c-39823350">[-]</label><label class="expand" for="c-39823350">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Now of course Google should spin off the TPU team as a separate company.<p>Given the size of the market and its near-monopoly situation, I strongly think this has the potential to (almost immediately) surpass the Pixel hardware business. But the problem here is that TPU is a relatively scarce computing resource even inside Google and it&#x27;s very likely that Google has a hard time to meet its internal demands...</div><br/><div id="39824669" class="c"><input type="checkbox" id="c-39824669" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39823350">parent</a><span>|</span><a href="#39823488">next</a><span>|</span><label class="collapse" for="c-39824669">[-]</label><label class="expand" for="c-39824669">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I strongly think this has the potential to (almost immediately) surpass the Pixel hardware business. But the problem here is that TPU is a relatively scarce computing resource even inside Google and it&#x27;s very likely that Google has a hard time to meet its internal demands...<p>Yes.<p>But imagine how the company would do: they have a guaranteed market at Google say for 3 years, and while yes maybe Google takes 100% of the production in the first 12 months it&#x27;s not a bad position to start from.<p>Plus there are other products which they could ship that might not always need to be built on the latest process. I imagine there would be demand for inference only earlier generation TPUs that can run LLMs fast if the power usage is low enough.</div><br/></div></div><div id="39823488" class="c"><input type="checkbox" id="c-39823488" checked=""/><div class="controls bullet"><span class="by">fnbr</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39823350">parent</a><span>|</span><a href="#39824669">prev</a><span>|</span><a href="#39824499">next</a><span>|</span><label class="collapse" for="c-39823488">[-]</label><label class="expand" for="c-39823488">[2 more]</label></div><br/><div class="children"><div class="content">I’m surprised they sell any to external customers, to be honest.</div><br/><div id="39825260" class="c"><input type="checkbox" id="c-39825260" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39823488">parent</a><span>|</span><a href="#39824499">next</a><span>|</span><label class="collapse" for="c-39825260">[-]</label><label class="expand" for="c-39825260">[1 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t sell any TPUs, do they? Besides the, now ancient, coral toy-TPUs.</div><br/></div></div></div></div></div></div><div id="39824499" class="c"><input type="checkbox" id="c-39824499" checked=""/><div class="controls bullet"><span class="by">ipsum2</span><span>|</span><a href="#39823235">parent</a><span>|</span><a href="#39823350">prev</a><span>|</span><a href="#39823493">next</a><span>|</span><label class="collapse" for="c-39824499">[-]</label><label class="expand" for="c-39824499">[4 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s the only credible competition NVidia has<p>This is wrong, both AMD and Intel (through Habana) have GPUs comparable to H100s in performance.</div><br/><div id="39824650" class="c"><input type="checkbox" id="c-39824650" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824499">parent</a><span>|</span><a href="#39824788">next</a><span>|</span><label class="collapse" for="c-39824650">[-]</label><label class="expand" for="c-39824650">[2 more]</label></div><br/><div class="children"><div class="content">Yes, but they don&#x27;t have the custom kernels that CUDA has. TPUs do have some!</div><br/><div id="39825217" class="c"><input type="checkbox" id="c-39825217" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824650">parent</a><span>|</span><a href="#39824788">next</a><span>|</span><label class="collapse" for="c-39825217">[-]</label><label class="expand" for="c-39825217">[1 more]</label></div><br/><div class="children"><div class="content">They have Vulcan, which is cross-compatible.<p>And AMD has ROCm.
pytorch is standard and pytorch has ROCm support. And the Google TPU v5 also has pytorch support.<p>We do have a couple of H100&#x27;s, but I&#x27;d love to replace them with AMD&#x27;s</div><br/></div></div></div></div><div id="39824788" class="c"><input type="checkbox" id="c-39824788" checked=""/><div class="controls bullet"><span class="by">kettleballroll</span><span>|</span><a href="#39823235">root</a><span>|</span><a href="#39824499">parent</a><span>|</span><a href="#39824650">prev</a><span>|</span><a href="#39823493">next</a><span>|</span><label class="collapse" for="c-39824788">[-]</label><label class="expand" for="c-39824788">[1 more]</label></div><br/><div class="children"><div class="content">But they&#x27;re far behind in adoption in the AI space, while TPUs have both adoption (inside Google and on top) and a very strong software offering (Jax and TF)</div><br/></div></div></div></div><div id="39823493" class="c"><input type="checkbox" id="c-39823493" checked=""/><div class="controls bullet"><span class="by">bfeynman</span><span>|</span><a href="#39823235">parent</a><span>|</span><a href="#39824499">prev</a><span>|</span><a href="#39823598">next</a><span>|</span><label class="collapse" for="c-39823493">[-]</label><label class="expand" for="c-39823493">[1 more]</label></div><br/><div class="children"><div class="content">Amazon acquired Annapurna labs doing the same thing and have their own train,&#x2F;inferentia silicon, and they definitely have more support than Google.</div><br/></div></div></div></div><div id="39823598" class="c"><input type="checkbox" id="c-39823598" checked=""/><div class="controls bullet"><span class="by">hipadev23</span><span>|</span><a href="#39823235">prev</a><span>|</span><a href="#39823575">next</a><span>|</span><label class="collapse" for="c-39823598">[-]</label><label class="expand" for="c-39823598">[27 more]</label></div><br/><div class="children"><div class="content">How is it that Google invented the TPU and Google Research came up with <i>the</i> paper on LLM and NVDA and AI startup companies have captured ~100% of the value</div><br/><div id="39823751" class="c"><input type="checkbox" id="c-39823751" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39825078">next</a><span>|</span><label class="collapse" for="c-39823751">[-]</label><label class="expand" for="c-39823751">[13 more]</label></div><br/><div class="children"><div class="content">There&#x27;s an old joke explanation about Xerox and PARC, about the difficulty of &quot;pitching a &#x27;paperless office&#x27; to a photocopier company&quot;.<p>In Google&#x27;s case, an example analogy would be pitching making something like ChatGPT widely available, when that would disrupt revenue from search engine paid placements, and from ads on sites that people wouldn&#x27;t need to visit.  (So maybe someone says, better to phase it in subtly, as needed for competitiveness, but in non-disruptive ways.)<p>I doubt it&#x27;s as simple as that, but would be funny if that was it.</div><br/><div id="39824102" class="c"><input type="checkbox" id="c-39824102" checked=""/><div class="controls bullet"><span class="by">halflings</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823751">parent</a><span>|</span><a href="#39824058">next</a><span>|</span><label class="collapse" for="c-39824102">[-]</label><label class="expand" for="c-39824102">[9 more]</label></div><br/><div class="children"><div class="content">This (innovator&#x27;s dilemma &#x2F; too afraid of disrupting your own ads business model) is the most common explanation folks are giving for this, but seems to be some sort of post-rationalization of why such a large company full of competent researchers&#x2F;engineers would drop the ball this hard.<p>My read (having seen some of this on the inside), is that it was a mix of being too worried about safety issues (OMG, the chatbot occasionally says something offensive!) and being too complacent (too comfortable with incremental changes in Search, no appetite for launching an entirely new type of product &#x2F; doing something really out there). There are many ways to monetize a chatbot, OpenAI for example is raking billions in subscription fees.</div><br/><div id="39824623" class="c"><input type="checkbox" id="c-39824623" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824102">parent</a><span>|</span><a href="#39824186">next</a><span>|</span><label class="collapse" for="c-39824623">[-]</label><label class="expand" for="c-39824623">[2 more]</label></div><br/><div class="children"><div class="content">Google gets much more scrutiny then smaller companies so it&#x27;s understandable to be worried. Pretty much any small mistake of theirs turns into clickbait on here and the other tech news sites and you get hundreds of comments about how evil Big Tech is. Of course it&#x27;s their own fault that their PR hews negative so frequently but still it&#x27;s understandable why they were so shy.</div><br/><div id="39824639" class="c"><input type="checkbox" id="c-39824639" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824623">parent</a><span>|</span><a href="#39824186">next</a><span>|</span><label class="collapse" for="c-39824639">[-]</label><label class="expand" for="c-39824639">[1 more]</label></div><br/><div class="children"><div class="content">Sydney when initially released was much less censored and the vast majority of responses online were positive, &quot;this is hilarious&#x2F;cool&quot;, not &quot;OMG Sydney should be banned!&quot;.</div><br/></div></div></div></div><div id="39824186" class="c"><input type="checkbox" id="c-39824186" checked=""/><div class="controls bullet"><span class="by">nemothekid</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824102">parent</a><span>|</span><a href="#39824623">prev</a><span>|</span><a href="#39824876">next</a><span>|</span><label class="collapse" for="c-39824186">[-]</label><label class="expand" for="c-39824186">[5 more]</label></div><br/><div class="children"><div class="content">&gt;<i>There are many ways to monetize a chatbot, OpenAI for example is raking billions in subscription fees.</i><p>Compared to Google, OpenAI&#x27;s billions is peanuts, while costing a fortune to generate. GPT-4 doesn&#x27;t seem profitable (if it was, would they need to throttle it?)</div><br/><div id="39824294" class="c"><input type="checkbox" id="c-39824294" checked=""/><div class="controls bullet"><span class="by">ro_sharp</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824186">parent</a><span>|</span><a href="#39824740">next</a><span>|</span><label class="collapse" for="c-39824294">[-]</label><label class="expand" for="c-39824294">[1 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 doesn&#x27;t seem profitable (if it was, would they need to throttle it?)<p>Maybe? Hardware supply isn’t perfectly elastic</div><br/></div></div><div id="39824740" class="c"><input type="checkbox" id="c-39824740" checked=""/><div class="controls bullet"><span class="by">nequo</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824186">parent</a><span>|</span><a href="#39824294">prev</a><span>|</span><a href="#39824876">next</a><span>|</span><label class="collapse" for="c-39824740">[-]</label><label class="expand" for="c-39824740">[3 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t Google be better able to integrate ads into a &quot;ChatGoogle&quot; service than OpenAI is into ChatGPT?</div><br/><div id="39824974" class="c"><input type="checkbox" id="c-39824974" checked=""/><div class="controls bullet"><span class="by">exitheone</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824740">parent</a><span>|</span><a href="#39824876">next</a><span>|</span><label class="collapse" for="c-39824974">[-]</label><label class="expand" for="c-39824974">[2 more]</label></div><br/><div class="children"><div class="content">The cost per ad is still astronomically different between search ads and LLMs</div><br/><div id="39825455" class="c"><input type="checkbox" id="c-39825455" checked=""/><div class="controls bullet"><span class="by">varjag</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824974">parent</a><span>|</span><a href="#39824876">next</a><span>|</span><label class="collapse" for="c-39825455">[-]</label><label class="expand" for="c-39825455">[1 more]</label></div><br/><div class="children"><div class="content">There could be an opposite avenue: ad-free Google Premium subscription with AI chat as a crown jewel. An ultimate opportunity to diversify from ad revenue.</div><br/></div></div></div></div></div></div></div></div><div id="39824876" class="c"><input type="checkbox" id="c-39824876" checked=""/><div class="controls bullet"><span class="by">rs11</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824102">parent</a><span>|</span><a href="#39824186">prev</a><span>|</span><a href="#39824058">next</a><span>|</span><label class="collapse" for="c-39824876">[-]</label><label class="expand" for="c-39824876">[1 more]</label></div><br/><div class="children"><div class="content">Monetizing a chatbot is one thing. Beating revenues every year when you are already making 300b a year is a whole different ball game
There must be tens of execs who understand this but their payout depends on keeping status quo</div><br/></div></div></div></div><div id="39824058" class="c"><input type="checkbox" id="c-39824058" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823751">parent</a><span>|</span><a href="#39824102">prev</a><span>|</span><a href="#39825078">next</a><span>|</span><label class="collapse" for="c-39824058">[-]</label><label class="expand" for="c-39824058">[3 more]</label></div><br/><div class="children"><div class="content">The answer is far weirder - they had a chat bot, and no one even discussed it in the context of search replacements. They didn’t want to release it because they just didn’t think it should be a product. Only after OpenAI actually disrupted search did they start releasing Gemini&#x2F;Bard which takes advantage of search.</div><br/><div id="39824095" class="c"><input type="checkbox" id="c-39824095" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824058">parent</a><span>|</span><a href="#39825078">next</a><span>|</span><label class="collapse" for="c-39824095">[-]</label><label class="expand" for="c-39824095">[2 more]</label></div><br/><div class="children"><div class="content">They were afraid to release it because of unaligned output and hallucinations.<p>ChatGPT showed that people could still get value out of something that wasn’t perfect.<p>E.g. they had this in their labs: <a href="https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2022&#x2F;jun&#x2F;12&#x2F;google-engineer-ai-bot-sentient-blake-lemoine" rel="nofollow">https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2022&#x2F;jun&#x2F;12&#x2F;google-en...</a> from July, 2022z</div><br/><div id="39824113" class="c"><input type="checkbox" id="c-39824113" checked=""/><div class="controls bullet"><span class="by">halflings</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824095">parent</a><span>|</span><a href="#39825078">next</a><span>|</span><label class="collapse" for="c-39824113">[-]</label><label class="expand" for="c-39824113">[1 more]</label></div><br/><div class="children"><div class="content">Agree re:hallucinations&#x2F;safety issues, that was likely one of the main blockers.<p>And here&#x27;s the sad part: they had this back in 2019... see this paper released in Jan 2020:
<a href="https:&#x2F;&#x2F;blog.research.google&#x2F;2020&#x2F;01&#x2F;towards-conversational-agent-that-can.html" rel="nofollow">https:&#x2F;&#x2F;blog.research.google&#x2F;2020&#x2F;01&#x2F;towards-conversational-...</a></div><br/></div></div></div></div></div></div></div></div><div id="39825078" class="c"><input type="checkbox" id="c-39825078" checked=""/><div class="controls bullet"><span class="by">willvarfar</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39823751">prev</a><span>|</span><a href="#39823744">next</a><span>|</span><label class="collapse" for="c-39825078">[-]</label><label class="expand" for="c-39825078">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI lured everyone away from Google with way higher pay.<p><a href="https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;eolver_googles-defense-against-openai-talent-grab-activity-7155229581942095872-Gqgs" rel="nofollow">https:&#x2F;&#x2F;www.linkedin.com&#x2F;posts&#x2F;eolver_googles-defense-agains...</a></div><br/></div></div><div id="39823744" class="c"><input type="checkbox" id="c-39823744" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39825078">prev</a><span>|</span><a href="#39823707">next</a><span>|</span><label class="collapse" for="c-39823744">[-]</label><label class="expand" for="c-39823744">[1 more]</label></div><br/><div class="children"><div class="content">Because Google can’t focus on a product for more than 18 months if it isn’t generating several billion in PROFIT.  They are punch drunk on advertising.</div><br/></div></div><div id="39823707" class="c"><input type="checkbox" id="c-39823707" checked=""/><div class="controls bullet"><span class="by">wstrange</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39823744">prev</a><span>|</span><a href="#39823611">next</a><span>|</span><label class="collapse" for="c-39823707">[-]</label><label class="expand" for="c-39823707">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s far too early to suggest Google will not capture value from AI. They have plenty of opportunity to integrate AI into their products.</div><br/><div id="39825221" class="c"><input type="checkbox" id="c-39825221" checked=""/><div class="controls bullet"><span class="by">earthnail</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823707">parent</a><span>|</span><a href="#39823611">next</a><span>|</span><label class="collapse" for="c-39825221">[-]</label><label class="expand" for="c-39825221">[1 more]</label></div><br/><div class="children"><div class="content">Microsoft is sooooo far ahead in this game, it’s borderline ridiculous. Google really missed an opportunity to grab major market share from MS Office.</div><br/></div></div></div></div><div id="39823611" class="c"><input type="checkbox" id="c-39823611" checked=""/><div class="controls bullet"><span class="by">abraae</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39823707">prev</a><span>|</span><a href="#39824042">next</a><span>|</span><label class="collapse" for="c-39823611">[-]</label><label class="expand" for="c-39823611">[6 more]</label></div><br/><div class="children"><div class="content">For historical precedent see Xerox Parc.</div><br/><div id="39823639" class="c"><input type="checkbox" id="c-39823639" checked=""/><div class="controls bullet"><span class="by">readyplayernull</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823611">parent</a><span>|</span><a href="#39824042">next</a><span>|</span><label class="collapse" for="c-39823639">[-]</label><label class="expand" for="c-39823639">[5 more]</label></div><br/><div class="children"><div class="content">IBM, Intel, Apple&#x27;s Newton.</div><br/><div id="39824206" class="c"><input type="checkbox" id="c-39824206" checked=""/><div class="controls bullet"><span class="by">klodolph</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823639">parent</a><span>|</span><a href="#39823746">next</a><span>|</span><label class="collapse" for="c-39824206">[-]</label><label class="expand" for="c-39824206">[2 more]</label></div><br/><div class="children"><div class="content">The story I like to tell for the Newton is that it was launched before the technology was ready yet. Like the Sega Game Gear. Old video phones. All those tablets that launched before the iPad.<p>They’re good ideas, but they shipped a few years too early, and the technology to make them work well at a good price point wasn’t available until later. Like, the Sega Game Gear had a cool active matrix LCD screen, but it took six AA batteries and the batteries only lasted like four hours.</div><br/><div id="39824246" class="c"><input type="checkbox" id="c-39824246" checked=""/><div class="controls bullet"><span class="by">technofiend</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39824206">parent</a><span>|</span><a href="#39823746">next</a><span>|</span><label class="collapse" for="c-39824246">[-]</label><label class="expand" for="c-39824246">[1 more]</label></div><br/><div class="children"><div class="content">The Palm Pilot V had a dockable cell phone modem, but the connectivity wasn&#x27;t integrated into the OS.  It worked but only as a demonstration.  Then Palm released a model with integrated data, but the BlackBerry came out the same year. You can be first and still if someone comes along with a much more compelling product, that&#x27;s the end of you.<p>Google has a few years left as a search company, but their enshittification of results has doomed them to replacement by LLMs. They seem to have forgotten Google pushed out their predecessors by having the best search results.  Targeted advertisements don&#x27;t qualify.</div><br/></div></div></div></div><div id="39823746" class="c"><input type="checkbox" id="c-39823746" checked=""/><div class="controls bullet"><span class="by">chillfox</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823639">parent</a><span>|</span><a href="#39824206">prev</a><span>|</span><a href="#39824042">next</a><span>|</span><label class="collapse" for="c-39823746">[-]</label><label class="expand" for="c-39823746">[2 more]</label></div><br/><div class="children"><div class="content">Kodak</div><br/><div id="39823884" class="c"><input type="checkbox" id="c-39823884" checked=""/><div class="controls bullet"><span class="by">BolexNOLA</span><span>|</span><a href="#39823598">root</a><span>|</span><a href="#39823746">parent</a><span>|</span><a href="#39824042">next</a><span>|</span><label class="collapse" for="c-39823884">[-]</label><label class="expand" for="c-39823884">[1 more]</label></div><br/><div class="children"><div class="content">Man I remember my last semester of college taking a history of photography course that was only offered every 3-4 years by a pretty legendary professor. The the day before the first day of class (or super close), Eastman Kodak declared bankruptcy after what? 110 years?<p>He scrapped his day 1 lecture and threw together a talk - with photos of course - about Kodak and how an intrepid engineer developed then the company foolishly hid the first digital camera because it would compete with their film line.<p>Incredible lecturer for sure haha</div><br/></div></div></div></div></div></div></div></div><div id="39824042" class="c"><input type="checkbox" id="c-39824042" checked=""/><div class="controls bullet"><span class="by">vineyardmike</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39823611">prev</a><span>|</span><a href="#39823958">next</a><span>|</span><label class="collapse" for="c-39824042">[-]</label><label class="expand" for="c-39824042">[1 more]</label></div><br/><div class="children"><div class="content">I think the TPU is simple. They do sell it (via cloud), but they focus on themselves first. When there was no shortage of compute, it was an also-ran in the ML hardware market. Now it’s trendy.<p>ChatGPT v Google is a far crazier history. Not only did Google invent Transformers, not only did Google open-source PaLM and Bert, but they even built <i>chat tuned</i> LLM chat bots and let employees talk with it. This isn’t a case where they were avoiding for disruption or protecting search - they genuinely didn’t see its potential. Worse, they got so much negative publicity over it that they considered it an AI safety issue to release. If that guy hadn’t gone to the press and claimed LaMDA was sentient than they may have entirely open sourced it like PaLM. This would likely mean that GPT-3 was open sourced and maybe never chat tuned either.<p>GPT-2 was freely available and OpenAI showed off GPT-3 freely as a parlor trick before ChatGPT came out. ChatGPT was originally the same - fun text generation as chat not a full product.<p>TLDR - Tensors probably didn’t have a lot of value until NVidia because scarce and they actively invented the original ChatGPT and “AI Safety” concerns caused them to lock it down.</div><br/></div></div><div id="39823958" class="c"><input type="checkbox" id="c-39823958" checked=""/><div class="controls bullet"><span class="by">snats</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39824042">prev</a><span>|</span><a href="#39823876">next</a><span>|</span><label class="collapse" for="c-39823958">[-]</label><label class="expand" for="c-39823958">[1 more]</label></div><br/><div class="children"><div class="content">to this day i am impressed that they have not figured out how to embed advertisments to bard outputs so that they can go free.</div><br/></div></div><div id="39823876" class="c"><input type="checkbox" id="c-39823876" checked=""/><div class="controls bullet"><span class="by">romanovcode</span><span>|</span><a href="#39823598">parent</a><span>|</span><a href="#39823958">prev</a><span>|</span><a href="#39823575">next</a><span>|</span><label class="collapse" for="c-39823876">[-]</label><label class="expand" for="c-39823876">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure it is because if ChatGPT likes would update as frequently as google website index it would render search engines like google obsolete and thus make their revenue nonexistent.</div><br/></div></div></div></div><div id="39823575" class="c"><input type="checkbox" id="c-39823575" checked=""/><div class="controls bullet"><span class="by">formercoder</span><span>|</span><a href="#39823598">prev</a><span>|</span><a href="#39823295">next</a><span>|</span><label class="collapse" for="c-39823575">[-]</label><label class="expand" for="c-39823575">[9 more]</label></div><br/><div class="children"><div class="content">Googler here, if you haven’t looked at TPUs in a while check out the v5. They support PyTorch&#x2F;JAX now, makes them much easier to use than TF only.</div><br/><div id="39823749" class="c"><input type="checkbox" id="c-39823749" checked=""/><div class="controls bullet"><span class="by">tw04</span><span>|</span><a href="#39823575">parent</a><span>|</span><a href="#39823295">next</a><span>|</span><label class="collapse" for="c-39823749">[-]</label><label class="expand" for="c-39823749">[8 more]</label></div><br/><div class="children"><div class="content">Where can I buy a TPU v5 to install in my server? If the answer is “cloud”: that’s why NVidia is wiping the floor.</div><br/><div id="39823848" class="c"><input type="checkbox" id="c-39823848" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#39823575">root</a><span>|</span><a href="#39823749">parent</a><span>|</span><a href="#39823819">next</a><span>|</span><label class="collapse" for="c-39823848">[-]</label><label class="expand" for="c-39823848">[3 more]</label></div><br/><div class="children"><div class="content">How many people are out there buying H100s for their personal use?</div><br/><div id="39823901" class="c"><input type="checkbox" id="c-39823901" checked=""/><div class="controls bullet"><span class="by">Workaccount2</span><span>|</span><a href="#39823575">root</a><span>|</span><a href="#39823848">parent</a><span>|</span><a href="#39823819">next</a><span>|</span><label class="collapse" for="c-39823901">[-]</label><label class="expand" for="c-39823901">[2 more]</label></div><br/><div class="children"><div class="content">Probably many orders of magnitude greater than those buying TPU&#x27;s for personal use...</div><br/><div id="39824324" class="c"><input type="checkbox" id="c-39824324" checked=""/><div class="controls bullet"><span class="by">foota</span><span>|</span><a href="#39823575">root</a><span>|</span><a href="#39823901">parent</a><span>|</span><a href="#39823819">next</a><span>|</span><label class="collapse" for="c-39824324">[-]</label><label class="expand" for="c-39824324">[1 more]</label></div><br/><div class="children"><div class="content">Technically correct, but only because TPUs aren&#x27;t for sale. H100s cost like 30,000 USD, if you can even get one.</div><br/></div></div></div></div></div></div><div id="39823819" class="c"><input type="checkbox" id="c-39823819" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#39823575">root</a><span>|</span><a href="#39823749">parent</a><span>|</span><a href="#39823848">prev</a><span>|</span><a href="#39823778">next</a><span>|</span><label class="collapse" for="c-39823819">[-]</label><label class="expand" for="c-39823819">[3 more]</label></div><br/><div class="children"><div class="content">You probably can&#x27;t even rent them from Google if you wanted to, in my experience.</div><br/><div id="39824013" class="c"><input type="checkbox" id="c-39824013" checked=""/><div class="controls bullet"><span class="by">inhumantsar</span><span>|</span><a href="#39823575">root</a><span>|</span><a href="#39823819">parent</a><span>|</span><a href="#39823778">next</a><span>|</span><label class="collapse" for="c-39824013">[-]</label><label class="expand" for="c-39824013">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu</a></div><br/><div id="39824046" class="c"><input type="checkbox" id="c-39824046" checked=""/><div class="controls bullet"><span class="by">jedberg</span><span>|</span><a href="#39823575">root</a><span>|</span><a href="#39824013">parent</a><span>|</span><a href="#39823778">next</a><span>|</span><label class="collapse" for="c-39824046">[-]</label><label class="expand" for="c-39824046">[1 more]</label></div><br/><div class="children"><div class="content">I think OPs point was Google claims to have TPUs in their cloud but in reality they are rarely available.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39823295" class="c"><input type="checkbox" id="c-39823295" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#39823575">prev</a><span>|</span><a href="#39824361">next</a><span>|</span><label class="collapse" for="c-39823295">[-]</label><label class="expand" for="c-39823295">[1 more]</label></div><br/><div class="children"><div class="content">This article really connected a lot of abstract pieces together into how they flow through silicon. I really enjoyed seeing the simple CISC instructions and how they basically map on to LLM inference steps.</div><br/></div></div><div id="39824361" class="c"><input type="checkbox" id="c-39824361" checked=""/><div class="controls bullet"><span class="by">uptownfunk</span><span>|</span><a href="#39823295">prev</a><span>|</span><a href="#39823570">next</a><span>|</span><label class="collapse" for="c-39824361">[-]</label><label class="expand" for="c-39824361">[2 more]</label></div><br/><div class="children"><div class="content">What Google really needs to do is get into the 2nm EUV space and go sub 2nm. When they have the electro lithography (or whatever tech ASML has that prints on the chips) then you have something really dangerous. Probably a hardcore Google X moonshot type project. Or maybe they have 500mm sitting around to just buy one of the machines. If their tpu are really that good - maybe it is a good business - especially if they can integrate all the way to having their own fab with their own tech</div><br/><div id="39824627" class="c"><input type="checkbox" id="c-39824627" checked=""/><div class="controls bullet"><span class="by">ejiblabahaba</span><span>|</span><a href="#39824361">parent</a><span>|</span><a href="#39823570">next</a><span>|</span><label class="collapse" for="c-39824627">[-]</label><label class="expand" for="c-39824627">[1 more]</label></div><br/><div class="children"><div class="content">This is frankly infeasible. Between the decades of trade secrets they would first need to discover, the tens- or maybe hundreds- of billions in capital needed to build their very first leading edge fab, the decade or two it would take for any such business to mature to the extent it would be functional, and the completely inconsequential volumes of devices they&#x27;d produce, they would probably be lighting half a trillion dollars on fire just to get a few years behind where the leading edge sits today, ten or more years from now. The only reason leading edge fabs are profitable today is because of decades of talent and engineering focused on producing general purpose computing devices for a wide variety of applications and customers, often with those very same customers driving innovation independently in critical focus areas (e.g. Micron with chip-on-chip HDI yield improvements, Xilinx with interdie communication fabric and multi chip substrate design). TPUs will never generate the required volumes, or attract the necessary customers, to achieve remotely profitable economies of scale, particularly when Google also has to set an attractive price against their competitors.<p>If Google has a compelling-enough business case, existing fabs will happily allocate space for their hardware. TPU is not remotely compelling enough.</div><br/></div></div></div></div><div id="39823570" class="c"><input type="checkbox" id="c-39823570" checked=""/><div class="controls bullet"><span class="by">kleton</span><span>|</span><a href="#39824361">prev</a><span>|</span><a href="#39823413">next</a><span>|</span><label class="collapse" for="c-39823570">[-]</label><label class="expand" for="c-39823570">[1 more]</label></div><br/><div class="children"><div class="content">Which ocean creature name is the current TPU?</div><br/></div></div><div id="39823413" class="c"><input type="checkbox" id="c-39823413" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#39823570">prev</a><span>|</span><a href="#39822957">next</a><span>|</span><label class="collapse" for="c-39823413">[-]</label><label class="expand" for="c-39823413">[8 more]</label></div><br/><div class="children"><div class="content">Quote from the OP:  &quot;The TPU v1 uses a CISC (Complex Instruction Set Computer) design with around only about 20 instructions.&quot;<p><i>chuckle</i>  CISC&#x2F;RISC has gone from astute observation, to research program, to revolutionary technology, to marketing buzzwords....and finally to being just completely meaningless sounds.<p>I suppose it&#x27;s the terminological circle of life.</div><br/><div id="39823527" class="c"><input type="checkbox" id="c-39823527" checked=""/><div class="controls bullet"><span class="by">dmoy</span><span>|</span><a href="#39823413">parent</a><span>|</span><a href="#39823551">next</a><span>|</span><label class="collapse" for="c-39823527">[-]</label><label class="expand" for="c-39823527">[1 more]</label></div><br/><div class="children"><div class="content">Idk maybe it&#x27;s just me, but what I was taught in comp architecture was that cisc vs risc has more to do with the complexity of the instructions, not the raw count.  So TPU having a smaller number of instructions can still be a cisc if the instructions are fairly complex.<p>Granted the last time I took any comp architecture was a grad course like 15 years ago, so my memory is pretty fuzzy (also we spent most of that semester dicking around with Itanium stuff that is beyond useless now)</div><br/></div></div><div id="39823551" class="c"><input type="checkbox" id="c-39823551" checked=""/><div class="controls bullet"><span class="by">cowsandmilk</span><span>|</span><a href="#39823413">parent</a><span>|</span><a href="#39823527">prev</a><span>|</span><a href="#39822957">next</a><span>|</span><label class="collapse" for="c-39823551">[-]</label><label class="expand" for="c-39823551">[6 more]</label></div><br/><div class="children"><div class="content">You’re seeming to imply the number of instructions available is what distinguishes CISC, but it never has been.</div><br/><div id="39824021" class="c"><input type="checkbox" id="c-39824021" checked=""/><div class="controls bullet"><span class="by">xarope</span><span>|</span><a href="#39823413">root</a><span>|</span><a href="#39823551">parent</a><span>|</span><a href="#39823596">next</a><span>|</span><label class="collapse" for="c-39824021">[-]</label><label class="expand" for="c-39824021">[1 more]</label></div><br/><div class="children"><div class="content">Right.  CISC vs RISC has always been about simplifying the underlying micro-instructions and register set usage.  It&#x27;s definitely CISC if you have a large complex operation on multiple memory direct locations (albeit the lines between RISC and CISC being blurred, as all such polar philosophies do, when real-life performance optimizations come into play)</div><br/></div></div><div id="39823596" class="c"><input type="checkbox" id="c-39823596" checked=""/><div class="controls bullet"><span class="by">LelouBil</span><span>|</span><a href="#39823413">root</a><span>|</span><a href="#39823551">parent</a><span>|</span><a href="#39824021">prev</a><span>|</span><a href="#39823610">next</a><span>|</span><label class="collapse" for="c-39823596">[-]</label><label class="expand" for="c-39823596">[1 more]</label></div><br/><div class="children"><div class="content">The fact that it&#x27;s opposed to RISC (Reduced Instruction Set) adds to the confusion.</div><br/></div></div><div id="39823610" class="c"><input type="checkbox" id="c-39823610" checked=""/><div class="controls bullet"><span class="by">rhelz</span><span>|</span><a href="#39823413">root</a><span>|</span><a href="#39823551">parent</a><span>|</span><a href="#39823596">prev</a><span>|</span><a href="#39822957">next</a><span>|</span><label class="collapse" for="c-39823610">[-]</label><label class="expand" for="c-39823610">[3 more]</label></div><br/><div class="children"><div class="content">Guys....what are the instructions?  The on-chip memory they are talking about is essentially...a big register set.  So we have load from main memory into registers, store from registers into main memory, multiply matrices--source and dest are stored in registers....<p>We have a 20 instruction, load-store cpu....how is this not RISC?  At least RISC how we used the term in 1995?</div><br/><div id="39823855" class="c"><input type="checkbox" id="c-39823855" checked=""/><div class="controls bullet"><span class="by">brigade</span><span>|</span><a href="#39823413">root</a><span>|</span><a href="#39823610">parent</a><span>|</span><a href="#39823635">next</a><span>|</span><label class="collapse" for="c-39823855">[-]</label><label class="expand" for="c-39823855">[1 more]</label></div><br/><div class="children"><div class="content">Its design follows the old idea that an ISA should be designed for assembly programmers; that instructions should implement complex or higher-level functions intended for a programmer to use directly.<p>RISC rejected that notion (among other things) and focused on designing ISAs for a compiler to target when compiling high level languages, without wasting silicon on instructions a compiler cannot easily use. For the TPU, a compiler cannot easily take a 256x256 matrix multiply written in a high-level language like C and emit a Matrix_Multiply instruction.</div><br/></div></div><div id="39823635" class="c"><input type="checkbox" id="c-39823635" checked=""/><div class="controls bullet"><span class="by">dmoy</span><span>|</span><a href="#39823413">root</a><span>|</span><a href="#39823610">parent</a><span>|</span><a href="#39823855">prev</a><span>|</span><a href="#39822957">next</a><span>|</span><label class="collapse" for="c-39823635">[-]</label><label class="expand" for="c-39823635">[1 more]</label></div><br/><div class="children"><div class="content">I think the &quot;multiply matrices&quot; instruction is the one that makes it a cisc</div><br/></div></div></div></div></div></div></div></div><div id="39822957" class="c"><input type="checkbox" id="c-39822957" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39823413">prev</a><span>|</span><a href="#39824313">next</a><span>|</span><label class="collapse" for="c-39822957">[-]</label><label class="expand" for="c-39822957">[39 more]</label></div><br/><div class="children"><div class="content">&gt; However, although tensors describe the relationship between arbitrary higher-dimensional arrays, in practice the TPU hardware that we will consider is designed to perform calculations associated with one and two-dimensional arrays. Or, more specifically, vector and matrix operations.<p>I still don’t understand why the term “tensor” is used if it’s only vectors and matrices.</div><br/><div id="39825482" class="c"><input type="checkbox" id="c-39825482" checked=""/><div class="controls bullet"><span class="by">adrian_b</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39824249">next</a><span>|</span><label class="collapse" for="c-39825482">[-]</label><label class="expand" for="c-39825482">[1 more]</label></div><br/><div class="children"><div class="content">I do not know which is the real origin of the fashion to use the word tensor in the context of AI&#x2F;ML.<p>Nevertheless, I have always interpreted it as a reference to the fact that the optimal method of multiplying matrices is to decompose the matrix multiplication into tensor products of vectors.<p>The other 2 alternative methods, i.e. decomposing the matrix multiplication into scalar products of vectors or into AXPY operations on pairs of vectors, have a much worse ratio between computation operations and transfer operations.<p>Unfortunately, most people learn in school the much less useful definition of the matrix multiplication based on scalar products of vectors, instead of its definition based on tensor products of vectors, which is the one needed in practice.<p>The 3 possible methods for multiplying matrices correspond to the 6 possible orders for the 3 indices of the 3 nested loops that compute a matrix product.</div><br/></div></div><div id="39824249" class="c"><input type="checkbox" id="c-39824249" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39825482">prev</a><span>|</span><a href="#39823748">next</a><span>|</span><label class="collapse" for="c-39824249">[-]</label><label class="expand" for="c-39824249">[1 more]</label></div><br/><div class="children"><div class="content">It says:
tensors describe the relationship between high-d arrays<p>It does not say:
tensors “only” describe the relationship between high-d arrays<p>The term “tensor” is used because it covers all cases: scalars, vectors, matrices, and higher-dimensional arrays.<p>Tensors are still a generalization of vectors and matrices.<p>Note the context: In ML and computer science, they are considered a generalization.  From a strict pure math standpoint they can be considered different.<p>As frustrating as it seems one is not really more right and context is the decider. There are lots of definitions across STEM fields that change based on the context or field they’re applied to.</div><br/></div></div><div id="39823748" class="c"><input type="checkbox" id="c-39823748" checked=""/><div class="controls bullet"><span class="by">jeffhwang</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39824249">prev</a><span>|</span><a href="#39823037">next</a><span>|</span><label class="collapse" for="c-39823748">[-]</label><label class="expand" for="c-39823748">[1 more]</label></div><br/><div class="children"><div class="content">(I think) technically, all of these mathematical objects are tensors of different ranks:<p>0. Scalar numbers are tensors of rank 0.<p>1. Vectors (eg velocity, acceleration in intro high school physics) are tensors of rank 1.<p>2. Matrices that you learn in intro linear anlgebra are tensors of rank 2. Nested arrays 1 level deep, aka a 2d array.<p>0. Tensors numbers are tensors of rank 3 or higher. I explain this as ‘nested arrays’ to people with programming backgrounds as nested arrays of arrays with 3dimensions of arrays or higher.<p>But I’m mostly self-taught in math so ymmv.</div><br/></div></div><div id="39823037" class="c"><input type="checkbox" id="c-39823037" checked=""/><div class="controls bullet"><span class="by">necroforest</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39823748">prev</a><span>|</span><a href="#39823188">next</a><span>|</span><label class="collapse" for="c-39823037">[-]</label><label class="expand" for="c-39823037">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s branding (see: TensorFlow); also, pretty much anything (linear) you would do with an arbitrarily ranked tensor can be expressed in terms of vector ops and matmuls</div><br/><div id="39823103" class="c"><input type="checkbox" id="c-39823103" checked=""/><div class="controls bullet"><span class="by">nxobject</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823037">parent</a><span>|</span><a href="#39823188">next</a><span>|</span><label class="collapse" for="c-39823103">[-]</label><label class="expand" for="c-39823103">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Fixed-Function Matrix Accelerator&quot; just doesn&#x27;t have the same buzzy ring to it.</div><br/><div id="39823196" class="c"><input type="checkbox" id="c-39823196" checked=""/><div class="controls bullet"><span class="by">bryzaguy</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823103">parent</a><span>|</span><a href="#39823326">next</a><span>|</span><label class="collapse" for="c-39823196">[-]</label><label class="expand" for="c-39823196">[1 more]</label></div><br/><div class="children"><div class="content">It’s the perfect name for my next EDM SoundCloud mix, though.</div><br/></div></div><div id="39823326" class="c"><input type="checkbox" id="c-39823326" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823103">parent</a><span>|</span><a href="#39823196">prev</a><span>|</span><a href="#39823188">next</a><span>|</span><label class="collapse" for="c-39823326">[-]</label><label class="expand" for="c-39823326">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Fixed-Function Matrix Accelerator&quot; just doesn&#x27;t have the same buzzy ring to it.<p>FixMax™ or Maxxelerator™ would be good brands.</div><br/></div></div></div></div></div></div><div id="39823188" class="c"><input type="checkbox" id="c-39823188" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39823037">prev</a><span>|</span><a href="#39823095">next</a><span>|</span><label class="collapse" for="c-39823188">[-]</label><label class="expand" for="c-39823188">[3 more]</label></div><br/><div class="children"><div class="content">Well, in the transformer forward pass there are a bunch of 4-dimensional arrays being used.</div><br/><div id="39823632" class="c"><input type="checkbox" id="c-39823632" checked=""/><div class="controls bullet"><span class="by">smilekzs</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823188">parent</a><span>|</span><a href="#39823245">next</a><span>|</span><label class="collapse" for="c-39823632">[-]</label><label class="expand" for="c-39823632">[1 more]</label></div><br/><div class="children"><div class="content">Came in to say this.<p>The Einsum notation makes it desirable to formulate your model&#x2F;layer as multi-dimensional arrays connected by (loosely) named axes, without worrying too much about breaking it down to primitives yourself. Once you get used to it, the terseness is liberating.</div><br/></div></div></div></div><div id="39823095" class="c"><input type="checkbox" id="c-39823095" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39823188">prev</a><span>|</span><a href="#39822972">next</a><span>|</span><label class="collapse" for="c-39823095">[-]</label><label class="expand" for="c-39823095">[15 more]</label></div><br/><div class="children"><div class="content">I was confused as hell for a long time when I first got into ML, until I figured out how to think about tensors in a visual way.<p>You&#x27;re right: fundamentally ML is about vector and matrix operations (1D and 2D). So then why are most ML programs 3D, 4D, and in a transformer sometimes up to 6D (?!)<p>One reasonable guess is that the third dimension is time. Actually not. It turns out that time is pretty rare in ML, and it&#x27;s only (relatively) recently that it&#x27;s been introduced into e.g. video models.<p>Another guess is that it&#x27;s to represent &quot;time&quot; as in, think of how transformers work: they generate a token, then another given the previous, then a third given the first two, etc. That&#x27;s a certain way of describing &quot;time&quot;. But it turns out that transformers don&#x27;t do this as a 3D or 4D dimension. It only needs to be 2D, because tokens are 1D -- if you&#x27;re representing tokens over time, you get a 2D output. So even with a cutting edge model like transformers, you still only need plain old 2D matrix operations. The attention layer creates a mask, which ends up being 2D.<p>So then why do models get to 3D and above? Usually batching. You get a certain efficiency boost when you pack a bunch of operations together. And if you pack a bunch of 2D operations together, that third dimension is the batch dimension.<p>For images, you typically end up with 4D, with the convension N,C,H,W, which stands for &quot;Batch, Channel, Height, Width&quot;. It can also be N,H,W,C, which is the same thing but it&#x27;s packed in memory as red green blue, red green blue, etc instead of all the red pixels first, then all the green pixels, then all the blue pixels. This matters in various subtle ways.<p>I have no idea why the batch dimension is called N, but it&#x27;s probably &quot;number of images&quot;.<p>&quot;Vector&quot; wouldn&#x27;t quite cover all of this, and although &quot;tensor&quot; is confusing, it&#x27;s fine. It&#x27;s the ham sandwich of naming conventions: flexible, satisfying to some, and you can make them in a bunch of different varieties.<p>Under the hood, TPUs actually flatten 3D tensors down into 2D matrix multiplications. I was surprised by this, but it makes total sense. The native size for a TPU is 8x128 -- you can think of it a bit like the native width of a CPU, except it&#x27;s 2D. So if you have a 3x4x256 tensor, it actually gets flattened out to 12x256, then the XLA black box magic figures out how to split that across a certain number of 8x128 vector registers. Note they&#x27;re called &quot;vector registers&quot; rather than &quot;tensor registers&quot;, which is interesting. See <a href="https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;performance-guide" rel="nofollow">https:&#x2F;&#x2F;cloud.google.com&#x2F;tpu&#x2F;docs&#x2F;performance-guide</a></div><br/><div id="39823127" class="c"><input type="checkbox" id="c-39823127" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823095">parent</a><span>|</span><a href="#39823156">next</a><span>|</span><label class="collapse" for="c-39823127">[-]</label><label class="expand" for="c-39823127">[9 more]</label></div><br/><div class="children"><div class="content">Thanks for the background! I still don’t think it’s appropriate to call a batch of matrices a tensor.</div><br/><div id="39823143" class="c"><input type="checkbox" id="c-39823143" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823127">parent</a><span>|</span><a href="#39823242">next</a><span>|</span><label class="collapse" for="c-39823143">[-]</label><label class="expand" for="c-39823143">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;d hate particle physics then. &quot;Spin&quot; and &quot;action&quot; and so on are terrible names, but scientists live with them, because convention.<p>Convention dominates most of what we do. I&#x27;m not sure there&#x27;s a good way around this. Most conventions suck, but they were established back before there was a clear idea of what the best long-term convention should be.</div><br/><div id="39823199" class="c"><input type="checkbox" id="c-39823199" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823143">parent</a><span>|</span><a href="#39823242">next</a><span>|</span><label class="collapse" for="c-39823199">[-]</label><label class="expand" for="c-39823199">[4 more]</label></div><br/><div class="children"><div class="content">At least in physics you can understand how the terms came about historically, where at some point they made sense. But “tensor” here, as note in sibling comments, seems to have been chosen primarily for marketing reasons.</div><br/><div id="39823364" class="c"><input type="checkbox" id="c-39823364" checked=""/><div class="controls bullet"><span class="by">FridgeSeal</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823199">parent</a><span>|</span><a href="#39823242">next</a><span>|</span><label class="collapse" for="c-39823364">[-]</label><label class="expand" for="c-39823364">[3 more]</label></div><br/><div class="children"><div class="content">It comes from the maths, where tensors are generalisations of matrices&#x2F;vectors. They got cribbed, because the ML stuff directly used a bunch of the underlying maths. It’s a novel term, it sounds cool, not surprised it also then got promoted up into a marketing term.</div><br/><div id="39823685" class="c"><input type="checkbox" id="c-39823685" checked=""/><div class="controls bullet"><span class="by">cowsandmilk</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823364">parent</a><span>|</span><a href="#39823242">next</a><span>|</span><label class="collapse" for="c-39823685">[-]</label><label class="expand" for="c-39823685">[2 more]</label></div><br/><div class="children"><div class="content">&gt; tensors are generalisations of matrices&#x2F;vectors.<p>Is that what they are though? Because that really is not my understanding. Tensors are mappings which not all matrices and vectors are. Maybe the matrices in ML layers are all mappings, but a matrix in general is not, not is a vector always a mapping. So tensors aren’t generalizations of matrices and vectors.</div><br/><div id="39824678" class="c"><input type="checkbox" id="c-39824678" checked=""/><div class="controls bullet"><span class="by">kergonath</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823685">parent</a><span>|</span><a href="#39823242">next</a><span>|</span><label class="collapse" for="c-39824678">[-]</label><label class="expand" for="c-39824678">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Tensors are mappings which not all matrices and vectors are.<p>A tensor in Physics is an object that follows some rules when changing reference frame. Their matrix representation is just one way of writing them. It’s the same with vectors: a list with their components is a representation of a vector, not the vector itself. We can think about it that way: the velocity of an object does not depend on the reference frame. Changing the axes does not make the object change its trajectory, but it does change the numerical values of the components of the velocity vector.<p>&gt; So tensors aren’t generalizations of matrices and vectors.<p>Indeed. Tensors in ML have pretty much nothing to do with tensors in Maths or Physics. It is very unfortunate that they settled on the same name just because it sounds cool and sciency.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39823242" class="c"><input type="checkbox" id="c-39823242" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823127">parent</a><span>|</span><a href="#39823143">prev</a><span>|</span><a href="#39824320">next</a><span>|</span><label class="collapse" for="c-39823242">[-]</label><label class="expand" for="c-39823242">[1 more]</label></div><br/><div class="children"><div class="content">why not? multilinear mappings can be represented by “batches of matrices” and that’s all that a tensor is</div><br/></div></div><div id="39824320" class="c"><input type="checkbox" id="c-39824320" checked=""/><div class="controls bullet"><span class="by">WhitneyLand</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823127">parent</a><span>|</span><a href="#39823242">prev</a><span>|</span><a href="#39823415">next</a><span>|</span><label class="collapse" for="c-39824320">[-]</label><label class="expand" for="c-39824320">[1 more]</label></div><br/><div class="children"><div class="content">It is appropriate in ML and computer science. It’s not in pure math.<p>There are many terms in math and science where the definition changes based on the context.</div><br/></div></div><div id="39823415" class="c"><input type="checkbox" id="c-39823415" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823127">parent</a><span>|</span><a href="#39824320">prev</a><span>|</span><a href="#39823156">next</a><span>|</span><label class="collapse" for="c-39823415">[-]</label><label class="expand" for="c-39823415">[1 more]</label></div><br/><div class="children"><div class="content">I think to be a tensor, all the bases should be independent.  The way I think of it is you use a tensor to describe the rotation of an asteroid around all its major axes (inertia tensor?)</div><br/></div></div></div></div><div id="39823156" class="c"><input type="checkbox" id="c-39823156" checked=""/><div class="controls bullet"><span class="by">parpfish</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823095">parent</a><span>|</span><a href="#39823127">prev</a><span>|</span><a href="#39823642">next</a><span>|</span><label class="collapse" for="c-39823156">[-]</label><label class="expand" for="c-39823156">[1 more]</label></div><br/><div class="children"><div class="content">just because an image is 2-D doesn’t mean that the model can’t use higher dimensional representations in subsequent layers.<p>For an image, you could imagine a network learning to push the image through a filter bank that does oriented local frequency decomposition and turns it into 4D {height}x{width}x{spatial freq}X{orientation} before dealing with color channels or image batches</div><br/></div></div><div id="39823642" class="c"><input type="checkbox" id="c-39823642" checked=""/><div class="controls bullet"><span class="by">bsdpufferfish</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823095">parent</a><span>|</span><a href="#39823156">prev</a><span>|</span><a href="#39823161">next</a><span>|</span><label class="collapse" for="c-39823642">[-]</label><label class="expand" for="c-39823642">[1 more]</label></div><br/><div class="children"><div class="content">higher dimensional vectors or matrices are still not tensors.</div><br/></div></div><div id="39823161" class="c"><input type="checkbox" id="c-39823161" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823095">parent</a><span>|</span><a href="#39823642">prev</a><span>|</span><a href="#39823177">next</a><span>|</span><label class="collapse" for="c-39823161">[-]</label><label class="expand" for="c-39823161">[2 more]</label></div><br/><div class="children"><div class="content">For whatever reason, I have held a mental image of a Tensor as a Tesseract&#x2F;HyperCube where the connections are like the Elastic workout bands where they have differing tensile resistances, and they pull on one another to create their encapsulated info-cluster - but I have no clue if thats truly an accurate depiction, but it works in my head....<p><a href="https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;thumb&#x2F;1&#x2F;1a&#x2F;Orthogonal_Tesseract_Gif.gif&#x2F;220px-Orthogonal_Tesseract_Gif.gif" rel="nofollow">https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;thumb&#x2F;1&#x2F;1a&#x2F;Or...</a></div><br/><div id="39823477" class="c"><input type="checkbox" id="c-39823477" checked=""/><div class="controls bullet"><span class="by">sillysaurusx</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823161">parent</a><span>|</span><a href="#39823177">next</a><span>|</span><label class="collapse" for="c-39823477">[-]</label><label class="expand" for="c-39823477">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m reluctant to tell people &quot;no, don&#x27;t think of it that way,&quot; <i>especially</i> if it works for you, because I don&#x27;t know the best way to think of things. I only know what works well for me. But for me, it&#x27;d be ~impossible to use your mental model to do anything useful. That doesn&#x27;t mean it&#x27;s bad, just that I don&#x27;t understand what you mean.<p>The most straightforward mental model I&#x27;ve ever found for ML is, think of it as 2D matrix operations, like high school linear algebra. Matrix-matrix, matrix-vector, vector-matrix, and vector-vector will get you through 95% of what comes up in practice. In fact I&#x27;m having trouble thinking of something that doesn&#x27;t work that way, because even if you have an RGB image that you multiply against a 2D matrix (i.e. HxWxC multiplied by a mask) the matrix is still only going to apply to 2 of the channels (height and width), since that&#x27;s the only thing that makes sense. That&#x27;s why there&#x27;s all kinds of flattening and rearranging everywhere in practice -- everyone is trying to get a format like N,C,H,W down to a 2D matrix representation.<p>People like to talk up the higher level maths in ML, but highschool linear algebra (or for the gamedevs in the audience, the stuff you&#x27;d normally do in a rendering engine) really will carry you most of the way through your ML journey without loss of generality. The higher level maths usually happens when you start understanding how differentiation works, which you don&#x27;t even need to understand until way later after you&#x27;re doing useful things already.</div><br/></div></div></div></div><div id="39823177" class="c"><input type="checkbox" id="c-39823177" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823095">parent</a><span>|</span><a href="#39823161">prev</a><span>|</span><a href="#39822972">next</a><span>|</span><label class="collapse" for="c-39823177">[-]</label><label class="expand" for="c-39823177">[1 more]</label></div><br/><div class="children"><div class="content">&gt;<i>One reasonable guess is that the third dimension is time. Actually not. It turns out that time is pretty rare in ML, and it&#x27;s only (relatively) recently that it&#x27;s been introduced into e.g. video models.</i><p>WRT to ML - may time be better thought of where a thing lives in relation to other things that occurred within the same temporal window?<p>so <i>&quot;all the shit that happened in 1999 also has an expression within this cluster of events from 1999&quot;</i> - but the same information appears in any location  where it is relationally contextual to the other neighbors, such as the SUBJECT of the information? Is this accurate to say why its &#x27;quantum&#x27; because the information will show up depending on where the Observation (query) for it is occurring?<p>(sorry for my kindergarten understanding of this)</div><br/></div></div></div></div><div id="39822972" class="c"><input type="checkbox" id="c-39822972" checked=""/><div class="controls bullet"><span class="by">ralusek</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39823095">prev</a><span>|</span><a href="#39824194">next</a><span>|</span><label class="collapse" for="c-39822972">[-]</label><label class="expand" for="c-39822972">[5 more]</label></div><br/><div class="children"><div class="content">If nothing else, the term &quot;tensor&quot; is shorter than &quot;vectors and matrices,&quot; and then has the added benefit of representing n-dimensional arrays.</div><br/><div id="39823052" class="c"><input type="checkbox" id="c-39823052" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39822972">parent</a><span>|</span><a href="#39824194">next</a><span>|</span><label class="collapse" for="c-39823052">[-]</label><label class="expand" for="c-39823052">[4 more]</label></div><br/><div class="children"><div class="content">How is that an added benefit if the hardware doesn’t actually support n-dimensional arrays (other the n = 1 and 2)?<p>And, strictly speaking, a vector can be considered a 1x<i>n</i> (or <i>n</i>x1) matrix, so Matrix Processing Unit would have been fine.</div><br/><div id="39823202" class="c"><input type="checkbox" id="c-39823202" checked=""/><div class="controls bullet"><span class="by">thatguysaguy</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823052">parent</a><span>|</span><a href="#39823231">next</a><span>|</span><label class="collapse" for="c-39823202">[-]</label><label class="expand" for="c-39823202">[1 more]</label></div><br/><div class="children"><div class="content">At the end of the day all the arrays are 1 dimensional and thinking of them as 2 dimensional is just an indexing convenience. A matrix multiply is a bunch of vector dot products in a row. Higher tensor contractions can be built out of lower-dimensional ones, so I don&#x27;t think it&#x27;s really fair to say the hardware doesn&#x27;t support it.</div><br/></div></div><div id="39823231" class="c"><input type="checkbox" id="c-39823231" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823052">parent</a><span>|</span><a href="#39823202">prev</a><span>|</span><a href="#39824194">next</a><span>|</span><label class="collapse" for="c-39823231">[-]</label><label class="expand" for="c-39823231">[2 more]</label></div><br/><div class="children"><div class="content">it’s an abstraction, just like 2d arrays</div><br/><div id="39823262" class="c"><input type="checkbox" id="c-39823262" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823231">parent</a><span>|</span><a href="#39824194">next</a><span>|</span><label class="collapse" for="c-39823262">[-]</label><label class="expand" for="c-39823262">[1 more]</label></div><br/><div class="children"><div class="content">I’d say it’s more like calling an ALU that can perform unary and binary operations (so 1 or 2 inputs) an “array processing unit” because it’s like it can process 1- and 2-element arrays. ;)</div><br/></div></div></div></div></div></div></div></div><div id="39824194" class="c"><input type="checkbox" id="c-39824194" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39822972">prev</a><span>|</span><a href="#39823117">next</a><span>|</span><label class="collapse" for="c-39824194">[-]</label><label class="expand" for="c-39824194">[1 more]</label></div><br/><div class="children"><div class="content">Every tensor is just a stack of vectors wearing a trench coat.</div><br/></div></div><div id="39823117" class="c"><input type="checkbox" id="c-39823117" checked=""/><div class="controls bullet"><span class="by">shrubble</span><span>|</span><a href="#39822957">parent</a><span>|</span><a href="#39824194">prev</a><span>|</span><a href="#39822985">next</a><span>|</span><label class="collapse" for="c-39823117">[-]</label><label class="expand" for="c-39823117">[6 more]</label></div><br/><div class="children"><div class="content">Tensor is from mathematics and was popularized over a century ago.</div><br/><div id="39823142" class="c"><input type="checkbox" id="c-39823142" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823117">parent</a><span>|</span><a href="#39822985">next</a><span>|</span><label class="collapse" for="c-39823142">[-]</label><label class="expand" for="c-39823142">[5 more]</label></div><br/><div class="children"><div class="content">I know what a tensor is mathematically. However, as far as I can see, ML isn’t based on tensor calculus as such.</div><br/><div id="39823181" class="c"><input type="checkbox" id="c-39823181" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823142">parent</a><span>|</span><a href="#39823227">next</a><span>|</span><label class="collapse" for="c-39823181">[-]</label><label class="expand" for="c-39823181">[3 more]</label></div><br/><div class="children"><div class="content">Something similar happens on Wikipedia, where topics that use math inevitably get explained in the highest level math possible. It makes topics harder to understand than they need to be.</div><br/><div id="39823216" class="c"><input type="checkbox" id="c-39823216" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823181">parent</a><span>|</span><a href="#39823227">next</a><span>|</span><label class="collapse" for="c-39823216">[-]</label><label class="expand" for="c-39823216">[2 more]</label></div><br/><div class="children"><div class="content">As a helpful Wiki editor just trying to make sure that we don&#x27;t lead people astray, I&#x27;ve made some small changes to clarify your statement:<p>In the virtual compendium of Wikipedia, an extensive repository of human knowledge, there is a discernible proclivity for the hermeneutics of mathematically-infused topics to be articulated through the prism of esoteric and sophisticated mathematical constructs, often employing a panoply of arcane lexemes and syntactic structures of Greek and Latin etymology. This phenomenon, redolent of an academic periphrasis, tends to transmute the exegesis of such subjects into a crucible of abstruse and high-order mathematical discourse. Consequently, this modus operandi obfuscates the intrinsic didactic intent, thereby precipitating an epistemological chasm that challenges the layperson&#x27;s erudition and obviates the pedagogical utility of the exposition.</div><br/><div id="39823985" class="c"><input type="checkbox" id="c-39823985" checked=""/><div class="controls bullet"><span class="by">xarope</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823216">parent</a><span>|</span><a href="#39823227">next</a><span>|</span><label class="collapse" for="c-39823985">[-]</label><label class="expand" for="c-39823985">[1 more]</label></div><br/><div class="children"><div class="content">scarily, I actually understood this.</div><br/></div></div></div></div></div></div><div id="39823227" class="c"><input type="checkbox" id="c-39823227" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39822957">root</a><span>|</span><a href="#39823142">parent</a><span>|</span><a href="#39823181">prev</a><span>|</span><a href="#39822985">next</a><span>|</span><label class="collapse" for="c-39823227">[-]</label><label class="expand" for="c-39823227">[1 more]</label></div><br/><div class="children"><div class="content">multidimensional arrays are multilinear mappings, and that is how they are used in ml usually. it seems fine to me</div><br/></div></div></div></div></div></div></div></div><div id="39823133" class="c"><input type="checkbox" id="c-39823133" checked=""/><div class="controls bullet"><span class="by">IncreasePosts</span><span>|</span><a href="#39823411">prev</a><span>|</span><a href="#39823615">next</a><span>|</span><label class="collapse" for="c-39823133">[-]</label><label class="expand" for="c-39823133">[10 more]</label></div><br/><div class="children"><div class="content">Sigh...learning about TPUs a decade ago made me invest heavily in $GOOG  for the coming AI revolution...got that one 100% wrong. +400% over 10 years isn&#x27;t bad but I can&#x27;t help but feel shortchanged seeing nvidia&#x2F;etc</div><br/><div id="39823623" class="c"><input type="checkbox" id="c-39823623" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#39823133">parent</a><span>|</span><a href="#39823173">next</a><span>|</span><label class="collapse" for="c-39823623">[-]</label><label class="expand" for="c-39823623">[1 more]</label></div><br/><div class="children"><div class="content">An out and out cherrypicker complaining about missing out on gains.. couldnt write a better script</div><br/></div></div><div id="39823173" class="c"><input type="checkbox" id="c-39823173" checked=""/><div class="controls bullet"><span class="by">smallmancontrov</span><span>|</span><a href="#39823133">parent</a><span>|</span><a href="#39823623">prev</a><span>|</span><a href="#39824325">next</a><span>|</span><label class="collapse" for="c-39823173">[-]</label><label class="expand" for="c-39823173">[4 more]</label></div><br/><div class="children"><div class="content">+400% over 10 years isn&#x27;t bad.</div><br/><div id="39823232" class="c"><input type="checkbox" id="c-39823232" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#39823133">root</a><span>|</span><a href="#39823173">parent</a><span>|</span><a href="#39824325">next</a><span>|</span><label class="collapse" for="c-39823232">[-]</label><label class="expand" for="c-39823232">[3 more]</label></div><br/><div class="children"><div class="content">It’s almost 15% per year, quite a lot.</div><br/><div id="39823439" class="c"><input type="checkbox" id="c-39823439" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39823133">root</a><span>|</span><a href="#39823232">parent</a><span>|</span><a href="#39824325">next</a><span>|</span><label class="collapse" for="c-39823439">[-]</label><label class="expand" for="c-39823439">[2 more]</label></div><br/><div class="children"><div class="content">yeah, but nvda is up like  500% in 2 years, so if you’re naive enough to think you can time the market, you’d have fomo over having invested in the “wrong” thing.</div><br/><div id="39824449" class="c"><input type="checkbox" id="c-39824449" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39823133">root</a><span>|</span><a href="#39823439">parent</a><span>|</span><a href="#39824325">next</a><span>|</span><label class="collapse" for="c-39824449">[-]</label><label class="expand" for="c-39824449">[1 more]</label></div><br/><div class="children"><div class="content">Seeing the difference between GPT2 and GPT3 made me run to NVDA immediately. One of the few bets in my life I&#x27;ve ever been confident about. I think NVDA was a pretty reasonable bet on AI like 5+, maybe 10 years ago when deep learning was ramping up.</div><br/></div></div></div></div></div></div></div></div><div id="39824325" class="c"><input type="checkbox" id="c-39824325" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#39823133">parent</a><span>|</span><a href="#39823173">prev</a><span>|</span><a href="#39823190">next</a><span>|</span><label class="collapse" for="c-39824325">[-]</label><label class="expand" for="c-39824325">[1 more]</label></div><br/><div class="children"><div class="content">Could&#x27;ve bought $QQQ unless you expected one of the components to do especially badly.</div><br/></div></div><div id="39823190" class="c"><input type="checkbox" id="c-39823190" checked=""/><div class="controls bullet"><span class="by">genidoi</span><span>|</span><a href="#39823133">parent</a><span>|</span><a href="#39824325">prev</a><span>|</span><a href="#39823183">next</a><span>|</span><label class="collapse" for="c-39823190">[-]</label><label class="expand" for="c-39823190">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think anybody in 2014 believed that the performance of GPT-4&#x2F;Claude Opus&#x2F;... was 10 years away. 25 years maybe, 50 years probably, but not 10.</div><br/><div id="39824467" class="c"><input type="checkbox" id="c-39824467" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39823133">root</a><span>|</span><a href="#39823190">parent</a><span>|</span><a href="#39823183">next</a><span>|</span><label class="collapse" for="c-39824467">[-]</label><label class="expand" for="c-39824467">[1 more]</label></div><br/><div class="children"><div class="content">It wasn&#x27;t just that, it was also all the deep learning stuff. Atari games playing themselves, deep style and variants. There was some interesting image generation happening. AlphaGo was 2015, etc. that was really when things started accelerating imo.</div><br/></div></div></div></div></div></div><div id="39823615" class="c"><input type="checkbox" id="c-39823615" checked=""/><div class="controls bullet"><span class="by">brcmthrowaway</span><span>|</span><a href="#39823133">prev</a><span>|</span><label class="collapse" for="c-39823615">[-]</label><label class="expand" for="c-39823615">[1 more]</label></div><br/><div class="children"><div class="content">Broadcom did the TPU</div><br/></div></div></div></div></div></div></div></body></html>