<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1723021276192" as="style"/><link rel="stylesheet" href="styles.css?v=1723021276192"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">Structured Outputs in the API</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>davidbarker</span> | <span>197 comments</span></div><br/><div><div id="41175068" class="c"><input type="checkbox" id="c-41175068" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41173964">next</a><span>|</span><label class="collapse" for="c-41175068">[-]</label><label class="expand" for="c-41175068">[23 more]</label></div><br/><div class="children"><div class="content">By using JSON mode, GPT-4{o} has been able to do this reliably for months (100k+ calls).<p>We use GPT-4o to build dynamic UI+code[0], and almost all of our calls are using JSON mode. Previously it mostly worked, but we had to do some massaging on our end (backtick removal, etc.).<p>With that said, this will be great for GPT-4o-mini, as it often struggles&#x2F;forgets to format things as we ask.<p>Note: we haven&#x27;t had the same success rate with function calling compared to pure JSON mode, as the function calling seems to add a level of indirection that can reduce the quality of the LLMs output YMMV.<p>Anyhow, excited for this!<p>[0]<a href="https:&#x2F;&#x2F;magicloops.dev">https:&#x2F;&#x2F;magicloops.dev</a></div><br/><div id="41177141" class="c"><input type="checkbox" id="c-41177141" checked=""/><div class="controls bullet"><span class="by">geepytee</span><span>|</span><a href="#41175068">parent</a><span>|</span><a href="#41175439">next</a><span>|</span><label class="collapse" for="c-41177141">[-]</label><label class="expand" for="c-41177141">[3 more]</label></div><br/><div class="children"><div class="content">This model appears to be full of surprises.<p>The 50% drop in price for inputs and 33% for outputs vs. the previous 4o model is huge.<p>It also appears to be topping various benchmarks, ZeroEval&#x27;s Leaderboard on hugging face [0] actually shows that it beats even Claude 3.5 Sonnet on CRUX [1] which is a code reasoning benchmark.<p>Shameless plug, I&#x27;m the co-founder of Double.bot (YC W23). After seeing the leaderboard above we actually added it to our copilot for anyone to try for free [2]. We try to add all new models the same day they are released<p>[0]<a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;allenai&#x2F;ZeroEval" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;allenai&#x2F;ZeroEval</a><p>[1]<a href="https:&#x2F;&#x2F;crux-eval.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;crux-eval.github.io&#x2F;</a><p>[2]<a href="https:&#x2F;&#x2F;double.bot&#x2F;">https:&#x2F;&#x2F;double.bot&#x2F;</a></div><br/><div id="41178228" class="c"><input type="checkbox" id="c-41178228" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41177141">parent</a><span>|</span><a href="#41175439">next</a><span>|</span><label class="collapse" for="c-41178228">[-]</label><label class="expand" for="c-41178228">[2 more]</label></div><br/><div class="children"><div class="content">&gt; ZeroEval&#x27;s Leaderboard on hugging face [0] actually shows that it beats even Claude 3.5 Sonnet on CRUX [1] which is a code reasoning benchmark.<p>The previous version of 4o also beat 3.5 Sonnet on Crux.</div><br/><div id="41179378" class="c"><input type="checkbox" id="c-41179378" checked=""/><div class="controls bullet"><span class="by">nunodonato</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41178228">parent</a><span>|</span><a href="#41175439">next</a><span>|</span><label class="collapse" for="c-41179378">[-]</label><label class="expand" for="c-41179378">[1 more]</label></div><br/><div class="children"><div class="content">which is a good hint that that benchmark sucks. No way 4o beats sonnet 3.5</div><br/></div></div></div></div></div></div><div id="41175439" class="c"><input type="checkbox" id="c-41175439" checked=""/><div class="controls bullet"><span class="by">qwertox</span><span>|</span><a href="#41175068">parent</a><span>|</span><a href="#41177141">prev</a><span>|</span><a href="#41177389">next</a><span>|</span><label class="collapse" for="c-41175439">[-]</label><label class="expand" for="c-41175439">[8 more]</label></div><br/><div class="children"><div class="content">What a cool product! I was about to recommend you to submit it as a &quot;Show HN&quot;, but it turns out that it already got submitted one year ago.<p>Would you mind sharing a bit on how things have evolved?</div><br/><div id="41175565" class="c"><input type="checkbox" id="c-41175565" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175439">parent</a><span>|</span><a href="#41177389">next</a><span>|</span><label class="collapse" for="c-41175565">[-]</label><label class="expand" for="c-41175565">[7 more]</label></div><br/><div class="children"><div class="content">Thanks and great question :)<p>When we first launched, the tool was very manual; you had to generate each step via the UI. We then added a &quot;Loop Creator agent&quot; that now builds Loops for you without intervention. Over the past few months we&#x27;ve mostly been fixing feature gaps and improving the Loop Creator.<p>Based on recent user feedback, we&#x27;ve put a few things in motion:<p>- Form generator (for manual loops)<p>- Chrome extension (for local automations)<p>- In-house Google Sheets integration<p>- Custom outputs (charts, tables, etc.)<p>- Custom Blocks (shareable with other users)<p>With these improvements, you&#x27;ll be able to create &quot;single page apps&quot; like this one I made for my wife&#x27;s annual mango tasting party[0].<p>In addition to those features, we&#x27;re also launching a new section for Loop templates + educational content&#x2F;how-tos, in an effort to help people get started.<p>To be super candid, the Loop Creator has been a pain. We started at an 8% success rate and we&#x27;re only just now at 25%. Theoretically we should be able to hit 80%+ based on existing loop requests, but we&#x27;re running into limits with the current state of LLMs.<p>[0]<a href="https:&#x2F;&#x2F;mangota.ngo" rel="nofollow">https:&#x2F;&#x2F;mangota.ngo</a></div><br/><div id="41175650" class="c"><input type="checkbox" id="c-41175650" checked=""/><div class="controls bullet"><span class="by">gleb</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175565">parent</a><span>|</span><a href="#41177145">next</a><span>|</span><label class="collapse" for="c-41175650">[-]</label><label class="expand" for="c-41175650">[3 more]</label></div><br/><div class="children"><div class="content">Where do you get such a large variety of mangoes?</div><br/><div id="41175885" class="c"><input type="checkbox" id="c-41175885" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175650">parent</a><span>|</span><a href="#41175869">next</a><span>|</span><label class="collapse" for="c-41175885">[-]</label><label class="expand" for="c-41175885">[1 more]</label></div><br/><div class="children"><div class="content">My mother-in-law is the President of the Central Florida Fruit Society, and is in charge of sourcing mangoes for their annual party. She sends us all the excess mangoes :)<p>As I understand it, this year&#x27;s mangoes mostly came from Merritt Island, as there was some not-so-great weather in southern Florida.</div><br/></div></div><div id="41175869" class="c"><input type="checkbox" id="c-41175869" checked=""/><div class="controls bullet"><span class="by">tomcam</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175650">parent</a><span>|</span><a href="#41175885">prev</a><span>|</span><a href="#41177145">next</a><span>|</span><label class="collapse" for="c-41175869">[-]</label><label class="expand" for="c-41175869">[1 more]</label></div><br/><div class="children"><div class="content">Asking the important questions</div><br/></div></div></div></div><div id="41177145" class="c"><input type="checkbox" id="c-41177145" checked=""/><div class="controls bullet"><span class="by">frabjoused</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175565">parent</a><span>|</span><a href="#41175650">prev</a><span>|</span><a href="#41176771">next</a><span>|</span><label class="collapse" for="c-41177145">[-]</label><label class="expand" for="c-41177145">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for all the detail and I’m fascinated by this.<p>We’re working on fairly similar problems, would love to have a chat and share ideas and experiences.<p>In this something you’d be interested in?</div><br/><div id="41177598" class="c"><input type="checkbox" id="c-41177598" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41177145">parent</a><span>|</span><a href="#41176771">next</a><span>|</span><label class="collapse" for="c-41177598">[-]</label><label class="expand" for="c-41177598">[1 more]</label></div><br/><div class="children"><div class="content">Always happy to chat!<p>You can ping me at {username}@gmail.com</div><br/></div></div></div></div><div id="41176771" class="c"><input type="checkbox" id="c-41176771" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175565">parent</a><span>|</span><a href="#41177145">prev</a><span>|</span><a href="#41177389">next</a><span>|</span><label class="collapse" for="c-41176771">[-]</label><label class="expand" for="c-41176771">[1 more]</label></div><br/><div class="children"><div class="content">This would be great in the extension format:<p><i>highlights-text</i> --&gt; Right-Click --&gt; New ML --&gt; (smart dropdown for watch [price|name|date|{typed-in-prompt-instructions}] --&gt; TAB --&gt; (smart frequency - tabbing through {watch blah (and its auto-filling every N ) --&gt; NAME_ML=ML01.<p>THEN:<p><i>highlights-text</i> --&gt; Right-Click .... WHEN {ML01} == N DO {this|ML0X} --&gt; ML00<p>ML00 == EMAIL|CSV|GDrive results.<p>ML11 == Graph all the above outputs.<p>:-)</div><br/></div></div></div></div></div></div><div id="41177389" class="c"><input type="checkbox" id="c-41177389" checked=""/><div class="controls bullet"><span class="by">__jl__</span><span>|</span><a href="#41175068">parent</a><span>|</span><a href="#41175439">prev</a><span>|</span><a href="#41176965">next</a><span>|</span><label class="collapse" for="c-41177389">[-]</label><label class="expand" for="c-41177389">[3 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing your experience.<p>We also get pretty reliable JSON output (on a smaller scale though) even without  JSON mode. We usually don&#x27;t use JSON mode because we often include a chain of thought part in &lt;brainstorming&gt; and then ask for JSON in &lt;json&gt; tags. With some prompt engineering, we get over 98% valid JSON in complex prompts (with long context and modestly complex JSON format). We catch the rest with json5.loads, which is only used as a fallback if json.loads fails.<p>4o-mini has been less reliable for us particularly with large context. The new structured output might make it possible to use mini in more situations.</div><br/><div id="41178279" class="c"><input type="checkbox" id="c-41178279" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41177389">parent</a><span>|</span><a href="#41177551">next</a><span>|</span><label class="collapse" for="c-41178279">[-]</label><label class="expand" for="c-41178279">[1 more]</label></div><br/><div class="children"><div class="content">The linked article includes a section on this, under <i>“Separating a final answer from supporting reasoning or additional commentary”</i>. They suggest defining a JSON schema with a reasoning field and an answer field.</div><br/></div></div><div id="41177551" class="c"><input type="checkbox" id="c-41177551" checked=""/><div class="controls bullet"><span class="by">borsch</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41177389">parent</a><span>|</span><a href="#41178279">prev</a><span>|</span><a href="#41176965">next</a><span>|</span><label class="collapse" for="c-41177551">[-]</label><label class="expand" for="c-41177551">[1 more]</label></div><br/><div class="children"><div class="content">my recommendation?  use chain of thought, then feed that into a second prompt asking for json</div><br/></div></div></div></div><div id="41176965" class="c"><input type="checkbox" id="c-41176965" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#41175068">parent</a><span>|</span><a href="#41177389">prev</a><span>|</span><a href="#41175864">next</a><span>|</span><label class="collapse" for="c-41176965">[-]</label><label class="expand" for="c-41176965">[1 more]</label></div><br/><div class="children"><div class="content">Had the same experience with function calling—we get much better results simply asking for JSON. With simple schemas (basically dictionaries), gpt-4 and 4o are basically bulletproof.</div><br/></div></div><div id="41175864" class="c"><input type="checkbox" id="c-41175864" checked=""/><div class="controls bullet"><span class="by">tomcam</span><span>|</span><a href="#41175068">parent</a><span>|</span><a href="#41176965">prev</a><span>|</span><a href="#41176214">next</a><span>|</span><label class="collapse" for="c-41175864">[-]</label><label class="expand" for="c-41175864">[2 more]</label></div><br/><div class="children"><div class="content">Very interesting. Did you build magicloops using this tech?</div><br/><div id="41176074" class="c"><input type="checkbox" id="c-41176074" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41175864">parent</a><span>|</span><a href="#41176214">next</a><span>|</span><label class="collapse" for="c-41176074">[-]</label><label class="expand" for="c-41176074">[1 more]</label></div><br/><div class="children"><div class="content">We first built Magic Loops with GPT-4, about a year ago, well before JSON mode was a thing.<p>We had to a do a bunch of extra prompting to make it work, as GPT would often include backticks or broken JSON (most commonly extra commas). At the time, YAML was a much better approach.<p>Thankfully we&#x27;ve been able to remove most of these hacks, but we still use a best effort JSON parser[0] to help stream partial UI back to the client.<p>[0]<a href="https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;best-effort-json-parser" rel="nofollow">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;best-effort-json-parser</a></div><br/></div></div></div></div><div id="41176214" class="c"><input type="checkbox" id="c-41176214" checked=""/><div class="controls bullet"><span class="by">diego_sandoval</span><span>|</span><a href="#41175068">parent</a><span>|</span><a href="#41175864">prev</a><span>|</span><a href="#41175244">next</a><span>|</span><label class="collapse" for="c-41176214">[-]</label><label class="expand" for="c-41176214">[4 more]</label></div><br/><div class="children"><div class="content">Can I use Magic Loops to generate Magic Loops for me?</div><br/><div id="41176387" class="c"><input type="checkbox" id="c-41176387" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41176214">parent</a><span>|</span><a href="#41175244">next</a><span>|</span><label class="collapse" for="c-41176387">[-]</label><label class="expand" for="c-41176387">[3 more]</label></div><br/><div class="children"><div class="content">Technically yes, but it would require reverse-engineering some of our APIs.<p>Practically speaking, we have quite a few use-cases where users call Loops from other Loops, so we&#x27;re investigating a first-class API to generate Loops in one go.<p>Similar to regular software engineering, what you put in is what you get out, so we&#x27;ve been hesitant to launch this with the current state of LLMs&#x2F;the Loop Creator as it will fail more often than not.</div><br/><div id="41176823" class="c"><input type="checkbox" id="c-41176823" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41176387">parent</a><span>|</span><a href="#41175244">next</a><span>|</span><label class="collapse" for="c-41176823">[-]</label><label class="expand" for="c-41176823">[2 more]</label></div><br/><div class="children"><div class="content">This would be great in the extension format:<p><i>highlights-text</i> --&gt; Right-Click --&gt; New ML --&gt; (smart dropdown for watch [price|name|date|{typed-in-prompt-instructions}] --&gt; TAB --&gt; (smart frequency - tabbing through {watch blah (and its auto-filling every N ) --&gt; NAME_ML=ML01.<p>THEN:<p><i>highlights-text</i> --&gt; Right-Click .... WHEN {ML01} == N DO {this|ML0X} --&gt; ML00<p>ML00 == EMAIL|CSV|GDrive results.<p>ML11 == Graph all the above outputs.<p>:-)<p>--<p>A MasterLoop would be good - where you have all [public or private] loops register - and then you can route logic based on loops that exist - and since its promptable - it can summarize and suggest logic when weaving loops into cohesive lattices of behavior. And if loops can subscribe to the output of other loops -- when youre looking for certain output strands -- you can say:<p>Find all the MLB loops daily and summarize what they say about only the Dodgers and the Giants - and keep a running table for the pitchers, and catchers stats only.<p>EDIT: Aside from just subscribing, maybe a loop can #include# the loop in its behavior to accomplish its goal&#x2F;assimilate its function &#x2F; mate&#x2F;spawn. :-)<p>These loops are pretty Magic!</div><br/><div id="41177070" class="c"><input type="checkbox" id="c-41177070" checked=""/><div class="controls bullet"><span class="by">jumploops</span><span>|</span><a href="#41175068">root</a><span>|</span><a href="#41176823">parent</a><span>|</span><a href="#41175244">next</a><span>|</span><label class="collapse" for="c-41177070">[-]</label><label class="expand" for="c-41177070">[1 more]</label></div><br/><div class="children"><div class="content">&gt; where you have all [public or private] loops register - and then you can route logic based on loops that exist - and since its promptable<p>This is actually where we started :)<p>We had trouble keeping all the context in via RAG and the original 8k token window, but we&#x27;re aiming to bring this back in the (hopefully near) future.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41173964" class="c"><input type="checkbox" id="c-41173964" checked=""/><div class="controls bullet"><span class="by">__jl__</span><span>|</span><a href="#41175068">prev</a><span>|</span><a href="#41179453">next</a><span>|</span><label class="collapse" for="c-41173964">[-]</label><label class="expand" for="c-41173964">[11 more]</label></div><br/><div class="children"><div class="content">There is another big change in gpt-4o-2024-08-06: It supports 16k output tokens compared to 4k before. I think it was only available in beta before. So gpt-4o-2024-08-06 actually brings three changes. Pretty significant for API users<p>1. Reliable structured outputs
2. Reduced costs by 50% for input, 33% for output
3. Up to 16k output tokens compared to 4k<p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;gpt-4o" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;gpt-4o</a></div><br/><div id="41175362" class="c"><input type="checkbox" id="c-41175362" checked=""/><div class="controls bullet"><span class="by">santiagobasulto</span><span>|</span><a href="#41173964">parent</a><span>|</span><a href="#41174856">next</a><span>|</span><label class="collapse" for="c-41175362">[-]</label><label class="expand" for="c-41175362">[9 more]</label></div><br/><div class="children"><div class="content">I’ve noticed that lately GPT has gotten more and more verbose. I’m wondering if it’s a subtle way to “raise prices”, as the average response is going to incur I more tokens, which makes any API conversation to keep growing in tokens of course (each IN message concatenates the previous OUT messages).</div><br/><div id="41175988" class="c"><input type="checkbox" id="c-41175988" checked=""/><div class="controls bullet"><span class="by">tedsanders</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175362">parent</a><span>|</span><a href="#41176133">next</a><span>|</span><label class="collapse" for="c-41175988">[-]</label><label class="expand" for="c-41175988">[4 more]</label></div><br/><div class="children"><div class="content">GPT has indeed been getting more verbose, but revenue has zero bearing on that decision. There&#x27;s always a tradeoff here, and we do our imperfect best to pick a default that makes the most people happy.<p>I suspect the reason why most big LLMs have ended up in a pretty verbose spot is that it&#x27;s easier for users to scroll &amp; skim than to ask follow-up questions (which requires formulation + typing + waiting for a response).<p>With regard to this new gpt-4o model: you&#x27;ll find it actually bucks the recent trend and is less verbose than its predecessor.</div><br/><div id="41177428" class="c"><input type="checkbox" id="c-41177428" checked=""/><div class="controls bullet"><span class="by">OJFord</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175988">parent</a><span>|</span><a href="#41177091">next</a><span>|</span><label class="collapse" for="c-41177428">[-]</label><label class="expand" for="c-41177428">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I suspect the reason why most big LLMs have ended up in a pretty verbose spot is that it&#x27;s easier for users to scroll &amp; skim than to ask follow-up questions<p>Maybe it&#x27;s a &#x27;technical&#x27; user divide, but that seems wrong to me. I would much rather a succinct answer that I can probe further or clarify if necessary.<p>Lately it&#x27;s going against my custom prompt&#x2F;profile whatever it&#x27;s called - to tell it to assume some level of competence, a bit about my background etc., to keep it brief - and it&#x27;s worse than it was when I created that out of annoyance with it.<p>Like earlier I asked something about some detail of AWS networking and using reachability analyser with VPC endpoints&#x2F;peering connections&#x2F;Lambda or something, and it starts waffling on like &#x27;first, establish the ID of your Virtual Private Cloud Endpoint. Step 1. To locate the ID, go to ...&#x27;</div><br/><div id="41178461" class="c"><input type="checkbox" id="c-41178461" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41177428">parent</a><span>|</span><a href="#41177091">next</a><span>|</span><label class="collapse" for="c-41178461">[-]</label><label class="expand" for="c-41178461">[1 more]</label></div><br/><div class="children"><div class="content">There’s an interesting discrepancy here.<p>Human users are charged by the number of messages, so longer responses are preferable because follow up questions use up your message allowance.<p>APIs are charged by token so shorter messages are preferable as you don’t pay for unnecessary tokens.</div><br/></div></div></div></div><div id="41177091" class="c"><input type="checkbox" id="c-41177091" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175988">parent</a><span>|</span><a href="#41177428">prev</a><span>|</span><a href="#41176133">next</a><span>|</span><label class="collapse" for="c-41177091">[-]</label><label class="expand" for="c-41177091">[1 more]</label></div><br/><div class="children"><div class="content">Do changes in verbosity tuning have a meaningful impact on the average &quot;correctness&quot; of the responses?<p>Also your about page is very suspicious for someone at an AI company ;).</div><br/></div></div></div></div><div id="41176133" class="c"><input type="checkbox" id="c-41176133" checked=""/><div class="controls bullet"><span class="by">sophiabits</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175362">parent</a><span>|</span><a href="#41175988">prev</a><span>|</span><a href="#41175810">next</a><span>|</span><label class="collapse" for="c-41176133">[-]</label><label class="expand" for="c-41176133">[1 more]</label></div><br/><div class="children"><div class="content">I’ve especially noticed this with gpt-4o-mini [1], and it’s a big problem. My particular use case involves keeping a running summary of a conversation between a user and the LLM, and 4o-mini has a really bad tendency of inventing details in order to hit the desired summary word limit. I didn’t see this with 4o or earlier models<p>Fwiw my subjective experience has been that non-technical stakeholders tend to be more impressed with &#x2F; agreeable to longer AI outputs, regardless of underlying quality. I have lost count of the number of times I’ve been asked to make outputs longer. Maybe this is just OpenAI responding to what users want?<p>[1] <a href="https:&#x2F;&#x2F;sophiabits.com&#x2F;blog&#x2F;new-llms-arent-always-better#examining-gpt-4o-mini" rel="nofollow">https:&#x2F;&#x2F;sophiabits.com&#x2F;blog&#x2F;new-llms-arent-always-better#exa...</a></div><br/></div></div><div id="41175810" class="c"><input type="checkbox" id="c-41175810" checked=""/><div class="controls bullet"><span class="by">throwaway48540</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175362">parent</a><span>|</span><a href="#41176133">prev</a><span>|</span><a href="#41175751">next</a><span>|</span><label class="collapse" for="c-41175810">[-]</label><label class="expand" for="c-41175810">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a subtle way to make it smarter. Making it write out the &quot;thinking process&quot; and decisions has always helped with reliability and quality.</div><br/></div></div><div id="41175751" class="c"><input type="checkbox" id="c-41175751" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175362">parent</a><span>|</span><a href="#41175810">prev</a><span>|</span><a href="#41174856">next</a><span>|</span><label class="collapse" for="c-41175751">[-]</label><label class="expand" for="c-41175751">[2 more]</label></div><br/><div class="children"><div class="content">they also spend more to generate more tokens. The more obvious reason is it seems like people rate responses better the longer they are. Lmsys demonstrated that GPT tops the leaderboard because it tends to give much longer and more detailed answers, and it seems like OpenAI is optimizing or trying to maximize lmsys.</div><br/><div id="41178248" class="c"><input type="checkbox" id="c-41178248" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#41173964">root</a><span>|</span><a href="#41175751">parent</a><span>|</span><a href="#41174856">next</a><span>|</span><label class="collapse" for="c-41178248">[-]</label><label class="expand" for="c-41178248">[1 more]</label></div><br/><div class="children"><div class="content">Agree with this take, though in an even broader way; they&#x27;re optimizing for the leaderboards and benchmarks in general. Longer outputs lead to better scores on those. Even in this thread I see a lot of comments bring them up, so it works for marketing.<p>My take is that the leaderboards and benchmarks are still very flawed if you&#x27;re using LLMs for any non-chat purpose. In the product I&#x27;m building, I <i>have</i> to use all of the big 4 models (GPT, Claude, Llama, Gemini), because for each of them there is at least one tasks that it performs much better than the other 3.</div><br/></div></div></div></div></div></div><div id="41174856" class="c"><input type="checkbox" id="c-41174856" checked=""/><div class="controls bullet"><span class="by">Culonavirus</span><span>|</span><a href="#41173964">parent</a><span>|</span><a href="#41175362">prev</a><span>|</span><a href="#41179453">next</a><span>|</span><label class="collapse" for="c-41174856">[-]</label><label class="expand" for="c-41174856">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s actually pretty impressive... if they didn&#x27;t dumb it down that is, which only time will tell.</div><br/></div></div></div></div><div id="41179453" class="c"><input type="checkbox" id="c-41179453" checked=""/><div class="controls bullet"><span class="by">armcat</span><span>|</span><a href="#41173964">prev</a><span>|</span><a href="#41176718">next</a><span>|</span><label class="collapse" for="c-41179453">[-]</label><label class="expand" for="c-41179453">[1 more]</label></div><br/><div class="children"><div class="content">This is awesome, and simplifies lot of my workflows when using their APIs directly. I also want to give a shout out to Outlines team, <a href="https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines">https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines</a>, they&#x27;ve been doing structured outputs for the last 12 months, and their open source lib can be applied across all open weight and closed&#x2F;API-based models. Most likely, Outlines heavily inspired the OpenAI team, maybe even using some of their codebase.</div><br/></div></div><div id="41176718" class="c"><input type="checkbox" id="c-41176718" checked=""/><div class="controls bullet"><span class="by">roseway4</span><span>|</span><a href="#41179453">prev</a><span>|</span><a href="#41173406">next</a><span>|</span><label class="collapse" for="c-41176718">[-]</label><label class="expand" for="c-41176718">[1 more]</label></div><br/><div class="children"><div class="content">We extensively use vLLM&#x27;s support for Outlines Structured Output with small language models (llama3 8B, for example) in Zep[0][1]. OpenAI&#x27;s Structured Output is a great improvement on JSON mode, but it is rather primitive compared to vLLM and Outlines.<p># Very Limited Field Typing<p>OpenAI offers a very limited set of types[2] (String, Number, Boolean, Object, Array, Enum, anyOf) without the ability to define patterns and max&#x2F;min lengths. Outlines supports defining arbitrary RegEx patterns, making extracting currencies, phone numbers, zip codes, comma-separated lists, and more a trivial exercise.<p># High Schema Setup Cost &#x2F; Latency<p>vLLM and Outlines offer near-zero cost schema setup: RegEx finite state machine construction is extremely cheap on the first inference call. While OpenAI&#x27;s context-free grammar generation has a significant latency penalty of &quot;under ten seconds to a minute&quot;. This may not impact &quot;warmed-up&quot; inference but could present issues if schemas are more dynamic in nature.<p>Right now, this feels like a good first step, focusing on ensuring the right fields are present in schema-ed output. However, it doesn&#x27;t yet offer the functionality to ensure the format of field contents beyond a primitive set of types. It will be interesting to watch where OpenAI takes this.<p>[0] <a href="https:&#x2F;&#x2F;help.getzep.com&#x2F;structured-data-extraction">https:&#x2F;&#x2F;help.getzep.com&#x2F;structured-data-extraction</a><p>[1] <a href="https:&#x2F;&#x2F;help.getzep.com&#x2F;dialog-classification">https:&#x2F;&#x2F;help.getzep.com&#x2F;dialog-classification</a><p>[2] <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;structured-outputs&#x2F;supported-schemas" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;structured-outputs&#x2F;s...</a></div><br/></div></div><div id="41173406" class="c"><input type="checkbox" id="c-41173406" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#41176718">prev</a><span>|</span><a href="#41173616">next</a><span>|</span><label class="collapse" for="c-41173406">[-]</label><label class="expand" for="c-41173406">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad they gave up on their &quot;fine-tuning is all you need&quot; approach to structured output. It&#x27;s possible fine-tuning will work in the long term, but in the short-term, people are trying to build things, and fine-tuning wasn&#x27;t cutting it.<p>Surprised it took them so long — llama.cpp got this feature 1.5 years ago (actually an even more general version of it that allows the user to provide any context free grammar, not just JSON schema)</div><br/><div id="41173492" class="c"><input type="checkbox" id="c-41173492" checked=""/><div class="controls bullet"><span class="by">tcdent</span><span>|</span><a href="#41173406">parent</a><span>|</span><a href="#41173464">next</a><span>|</span><label class="collapse" for="c-41173492">[-]</label><label class="expand" for="c-41173492">[3 more]</label></div><br/><div class="children"><div class="content">GPT is still a language model, so at some point it&#x27;s still just tokens.<p>Is this just a schema validation layer on their end to avoid the round trip (and cost) of repeating the call?</div><br/><div id="41173533" class="c"><input type="checkbox" id="c-41173533" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#41173406">root</a><span>|</span><a href="#41173492">parent</a><span>|</span><a href="#41173464">next</a><span>|</span><label class="collapse" for="c-41173533">[-]</label><label class="expand" for="c-41173533">[2 more]</label></div><br/><div class="children"><div class="content">Language models like GPT output a large vector of probabilities for the next token. Then a sampler decides which of those tokens to pick.<p>The simplest algorithm for getting good quality output is to just always pick the highest probability token.<p>If you want more creativity, maybe you pick randomly among the top 5 highest probability tokens or something. There are a lot of methods.<p>All that grammar-constrained decoding does is zero out the probability of any token that would violate the grammar.</div><br/><div id="41174834" class="c"><input type="checkbox" id="c-41174834" checked=""/><div class="controls bullet"><span class="by">nickreese</span><span>|</span><a href="#41173406">root</a><span>|</span><a href="#41173533">parent</a><span>|</span><a href="#41173464">next</a><span>|</span><label class="collapse" for="c-41174834">[-]</label><label class="expand" for="c-41174834">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for this explanation. A few things just clicked for me.</div><br/></div></div></div></div></div></div><div id="41173464" class="c"><input type="checkbox" id="c-41173464" checked=""/><div class="controls bullet"><span class="by">chhabraamit</span><span>|</span><a href="#41173406">parent</a><span>|</span><a href="#41173492">prev</a><span>|</span><a href="#41173549">next</a><span>|</span><label class="collapse" for="c-41173464">[-]</label><label class="expand" for="c-41173464">[3 more]</label></div><br/><div class="children"><div class="content">How does llama.cpp’s grammar adherence work?<p>Does it keep validating the predicted tokens and backtrack when it’s not valid?</div><br/><div id="41173519" class="c"><input type="checkbox" id="c-41173519" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#41173406">root</a><span>|</span><a href="#41173464">parent</a><span>|</span><a href="#41173549">next</a><span>|</span><label class="collapse" for="c-41173519">[-]</label><label class="expand" for="c-41173519">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s essentially an Earley Parser[0]. It maintains a set of all possible currently valid parses, and zeroes out the probability of any token that isn&#x27;t valid in at least 1 of the current potential parse trees.<p>There are contrived grammars you can give it that will make it use exponential memory, but in practice most real-world grammars aren&#x27;t like this.<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Earley_parser" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Earley_parser</a></div><br/><div id="41178690" class="c"><input type="checkbox" id="c-41178690" checked=""/><div class="controls bullet"><span class="by">jules</span><span>|</span><a href="#41173406">root</a><span>|</span><a href="#41173519">parent</a><span>|</span><a href="#41173549">next</a><span>|</span><label class="collapse" for="c-41178690">[-]</label><label class="expand" for="c-41178690">[1 more]</label></div><br/><div class="children"><div class="content">Earley parsers should not need more than O(n^2) memory.</div><br/></div></div></div></div></div></div><div id="41173549" class="c"><input type="checkbox" id="c-41173549" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#41173406">parent</a><span>|</span><a href="#41173464">prev</a><span>|</span><a href="#41174738">next</a><span>|</span><label class="collapse" for="c-41173549">[-]</label><label class="expand" for="c-41173549">[1 more]</label></div><br/><div class="children"><div class="content">I was surprised it took so long until I reached this line:<p>&gt; The model can fail to follow the schema if the model chooses to refuse an unsafe request. If it chooses to refuse, the return message will have the refusal boolean set to true to indicate this.<p>I&#x27;m not sure how they implemented that, maybe they&#x27;ve figured out a way to give the grammar a token or set of tokens that are always valid mid generation and indicate the model would rather not continue generating.<p>Right now JSON generation is one of the most reliable ways to get around refusals, and they managed not to introduce that weakness into their model</div><br/></div></div><div id="41174738" class="c"><input type="checkbox" id="c-41174738" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41173406">parent</a><span>|</span><a href="#41173549">prev</a><span>|</span><a href="#41173616">next</a><span>|</span><label class="collapse" for="c-41174738">[-]</label><label class="expand" for="c-41174738">[1 more]</label></div><br/><div class="children"><div class="content">For many things, fine-tuning as we know of it will NEVER fully solve it, there&#x27;s no hope. Even fine-tuning a model to not use the letter &quot;e&quot; to an overwhelming degree doesn&#x27;t entirely prevent it, only reduces its chances to increasingly small amounts. Shamesless self plug, and from before the ChatGPT era too! <a href="https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be-poets-too-an-ai-1" rel="nofollow">https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;most-language-models-can-be...</a></div><br/></div></div></div></div><div id="41173616" class="c"><input type="checkbox" id="c-41173616" checked=""/><div class="controls bullet"><span class="by">titzer</span><span>|</span><a href="#41173406">prev</a><span>|</span><a href="#41173804">next</a><span>|</span><label class="collapse" for="c-41173616">[-]</label><label class="expand" for="c-41173616">[36 more]</label></div><br/><div class="children"><div class="content">It&#x27;s so wild that the bar for AI performance is both absurdly high and absurdly low at the same time. To specify an output format (language or grammar) for solving a computational problem is one of the oldest exercises around. On the one hand, it&#x27;s breathtakingly mundane that the model can now do the most basic of tasks: conform to an output specification. It&#x27;s weird reading the kind of self-congratulating blogpost about this, like OpenAI has just discovered flint knives. On the other hand, a computer system can process natural language with extremely ambiguous, open-ended problems, compute solutions to said problems, even correct its own mistakes--<i>and then</i> it can format the output correctly. And then on yet another hand, it only took about 10^25 floating point operations (yeah, just ten million trillion trillion, right!?) to get this outcome.</div><br/><div id="41174385" class="c"><input type="checkbox" id="c-41174385" checked=""/><div class="controls bullet"><span class="by">thruway516</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41174452">next</a><span>|</span><label class="collapse" for="c-41174385">[-]</label><label class="expand" for="c-41174385">[10 more]</label></div><br/><div class="children"><div class="content">I dont understand your complaint at all. If you develop a new revolutionary technology called an automobile, developing steering, brakes, starter, mufflers for it is a pretty big deal even if reins, clamps, mufflers and keys are mundane and have existed for decades. Structured outputs are a pretty big step in making this magic actually usable by developers as opposed to generating impressive cat pictures or whatever has captured the public imagination.</div><br/><div id="41174421" class="c"><input type="checkbox" id="c-41174421" checked=""/><div class="controls bullet"><span class="by">Bjartr</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174385">parent</a><span>|</span><a href="#41175720">next</a><span>|</span><label class="collapse" for="c-41174421">[-]</label><label class="expand" for="c-41174421">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it was an complaint, just a observation.</div><br/><div id="41174912" class="c"><input type="checkbox" id="c-41174912" checked=""/><div class="controls bullet"><span class="by">thruway516</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174421">parent</a><span>|</span><a href="#41175720">next</a><span>|</span><label class="collapse" for="c-41174912">[-]</label><label class="expand" for="c-41174912">[6 more]</label></div><br/><div class="children"><div class="content">Yes probably. But considering non-deterministic outputs is the nature of the beast with Llms and we&#x27;re (mostly) engineers here, calling any part of this mundane sounds almost more like fighting words than just observation</div><br/><div id="41175218" class="c"><input type="checkbox" id="c-41175218" checked=""/><div class="controls bullet"><span class="by">the8thbit</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174912">parent</a><span>|</span><a href="#41175720">next</a><span>|</span><label class="collapse" for="c-41175218">[-]</label><label class="expand" for="c-41175218">[5 more]</label></div><br/><div class="children"><div class="content">Extremely pedantic, but is &quot;non-deterministic&quot; really the right language? The same input will always produce the same output, provided you haven&#x27;t intentionally configured the system to use the model non-deterministically. It seems like the right way to describe it is as a chaotic deterministic system. The same input will always produce the same output, but small shifts in the input or weights can result in dramatic and difficult to predict changes in outputs.</div><br/><div id="41175331" class="c"><input type="checkbox" id="c-41175331" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175218">parent</a><span>|</span><a href="#41175321">next</a><span>|</span><label class="collapse" for="c-41175331">[-]</label><label class="expand" for="c-41175331">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The same input will always produce the same output<p>Not guaranteed even with the same seed. If you don&#x27;t perform all operations in exactly the same order, even a simple float32 sum, if batched differently, will result in different final value. This depends on the load factor and how resources are allocated.</div><br/><div id="41175360" class="c"><input type="checkbox" id="c-41175360" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175331">parent</a><span>|</span><a href="#41178528">next</a><span>|</span><label class="collapse" for="c-41175360">[-]</label><label class="expand" for="c-41175360">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, the fact that floating point multiplication isn&#x27;t associative is a real pain for producing deterministic outputs - especially when you&#x27;re running massively parallel computations on GPUs (or multiple GPUs) making the order of operations even less predictable.</div><br/></div></div><div id="41178528" class="c"><input type="checkbox" id="c-41178528" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175331">parent</a><span>|</span><a href="#41175360">prev</a><span>|</span><a href="#41175321">next</a><span>|</span><label class="collapse" for="c-41178528">[-]</label><label class="expand" for="c-41178528">[1 more]</label></div><br/><div class="children"><div class="content">This doesn’t mean LLMs are inherently non-deterministic, just that current common implementations are non-deterministic.</div><br/></div></div></div></div><div id="41175321" class="c"><input type="checkbox" id="c-41175321" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175218">parent</a><span>|</span><a href="#41175331">prev</a><span>|</span><a href="#41175720">next</a><span>|</span><label class="collapse" for="c-41175321">[-]</label><label class="expand" for="c-41175321">[1 more]</label></div><br/><div class="children"><div class="content">Llms are indeed non deterministic</div><br/></div></div></div></div></div></div></div></div><div id="41175720" class="c"><input type="checkbox" id="c-41175720" checked=""/><div class="controls bullet"><span class="by">jappgar</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174385">parent</a><span>|</span><a href="#41174421">prev</a><span>|</span><a href="#41174452">next</a><span>|</span><label class="collapse" for="c-41175720">[-]</label><label class="expand" for="c-41175720">[2 more]</label></div><br/><div class="children"><div class="content">Structured outputs are hard... but they claimed to have solved this a year ago.<p>They were lying, of course,  and meanwhile charged output tokens for malformed JSON.</div><br/><div id="41177694" class="c"><input type="checkbox" id="c-41177694" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175720">parent</a><span>|</span><a href="#41174452">next</a><span>|</span><label class="collapse" for="c-41177694">[-]</label><label class="expand" for="c-41177694">[1 more]</label></div><br/><div class="children"><div class="content">Structured output is <i>trivial:</i> just select the output tokens from the given list of probability values filtered for the allowed next tokens in the schema.<p>Other LLM vendors figured this out many months ago.</div><br/></div></div></div></div></div></div><div id="41174452" class="c"><input type="checkbox" id="c-41174452" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41174385">prev</a><span>|</span><a href="#41175019">next</a><span>|</span><label class="collapse" for="c-41174452">[-]</label><label class="expand" for="c-41174452">[13 more]</label></div><br/><div class="children"><div class="content">&gt; On the one hand, it&#x27;s breathtakingly mundane that the model can now do the most basic of tasks: conform to an output specification.<p>I highly doubt it&#x27;s the model that does this... It&#x27;s very likely code injected into the token picker.  You could put this into any model all the way down to gpt-2.</div><br/><div id="41174739" class="c"><input type="checkbox" id="c-41174739" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174452">parent</a><span>|</span><a href="#41175019">next</a><span>|</span><label class="collapse" for="c-41174739">[-]</label><label class="expand" for="c-41174739">[12 more]</label></div><br/><div class="children"><div class="content">I wonder if you get 90% of the way with prompt engineering, and then the last 10% is just brute force, validate output, if it fails, rerun the prompt.<p>My assumption is if that&#x27;s all this is they would have done it a long time ago though.</div><br/><div id="41174883" class="c"><input type="checkbox" id="c-41174883" checked=""/><div class="controls bullet"><span class="by">dilap</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174739">parent</a><span>|</span><a href="#41174870">next</a><span>|</span><label class="collapse" for="c-41174883">[-]</label><label class="expand" for="c-41174883">[5 more]</label></div><br/><div class="children"><div class="content">You just sample from a grammar and you automatically get 100%; who knows but it seems the most likely thing they are doing. llama.cpp has supported this for a while ( using a BNF-style grammar -- <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;grammars&#x2F;README.md">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;grammars&#x2F;...</a> )<p>edit: oh actually, we do sort of know -- they call out jsonformer as an inspiration in the acknowledgements<p><a href="https:&#x2F;&#x2F;github.com&#x2F;1rgs&#x2F;jsonformer">https:&#x2F;&#x2F;github.com&#x2F;1rgs&#x2F;jsonformer</a></div><br/><div id="41175644" class="c"><input type="checkbox" id="c-41175644" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174883">parent</a><span>|</span><a href="#41174993">next</a><span>|</span><label class="collapse" for="c-41175644">[-]</label><label class="expand" for="c-41175644">[3 more]</label></div><br/><div class="children"><div class="content">Using this in a naive way can easily degenerate into the LLM outputting syntactically&#x2F;gramatically valid tokens that make no sense, like in this example: <a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;json-format-causes-infinite-n-n-n-n-in-response&#x2F;519333" rel="nofollow">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;json-format-causes-infinite-n...</a><p>This might be even more pronounced when the output is restricted more using the JSON schema.<p>So the heavy lifting here was most likely to align the model to avoid&#x2F;minimize such outcomes, not in tweaking the token sampler.</div><br/><div id="41175883" class="c"><input type="checkbox" id="c-41175883" checked=""/><div class="controls bullet"><span class="by">dilap</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175644">parent</a><span>|</span><a href="#41174993">next</a><span>|</span><label class="collapse" for="c-41175883">[-]</label><label class="expand" for="c-41175883">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t your example showing an issue w&#x2F; the opposite approach, where someone is getting bad output w&#x2F; an earlier openAI json mode that worked via training rather than mechanical output restriction to conform to a schema?<p>FWIW (not too much!) I have used llama.cpp grammars to restrict to specific formats (not particular json, but an expected format), fine-tuned phi2 models, and I didn&#x27;t hit any issues like this.<p>I am not intuitively seeing why restricting sampling to tokens matching a schema would cause the LLM to converge on valid tokens that make no sense...<p>Are there examples of this happening w&#x2F; people using e.g. jsonformer?</div><br/><div id="41176750" class="c"><input type="checkbox" id="c-41176750" checked=""/><div class="controls bullet"><span class="by">TheEzEzz</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175883">parent</a><span>|</span><a href="#41174993">next</a><span>|</span><label class="collapse" for="c-41176750">[-]</label><label class="expand" for="c-41176750">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re basically taking the model &quot;off policy&quot; when you bias the decoder, which can definitely make weird things happen.</div><br/></div></div></div></div></div></div><div id="41174993" class="c"><input type="checkbox" id="c-41174993" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174883">parent</a><span>|</span><a href="#41175644">prev</a><span>|</span><a href="#41174870">next</a><span>|</span><label class="collapse" for="c-41174993">[-]</label><label class="expand" for="c-41174993">[1 more]</label></div><br/><div class="children"><div class="content">Oh, thanks for the links. Super interesting!</div><br/></div></div></div></div><div id="41174870" class="c"><input type="checkbox" id="c-41174870" checked=""/><div class="controls bullet"><span class="by">jeeceebees</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174739">parent</a><span>|</span><a href="#41174883">prev</a><span>|</span><a href="#41175943">next</a><span>|</span><label class="collapse" for="c-41174870">[-]</label><label class="expand" for="c-41174870">[5 more]</label></div><br/><div class="children"><div class="content">You can just mask the output probabilities for each token based on which options are valid according to a grammar.<p>There are quite a few open source implementations of this e.g. <a href="https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines">https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines</a></div><br/><div id="41175582" class="c"><input type="checkbox" id="c-41175582" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174870">parent</a><span>|</span><a href="#41175943">next</a><span>|</span><label class="collapse" for="c-41175582">[-]</label><label class="expand" for="c-41175582">[4 more]</label></div><br/><div class="children"><div class="content">You could simply censor invalid tokens, but that does rely on 2 assumptions.<p>1. There is always a valid next token.<p>2. This greedy algorithm doesn&#x27;t result in a qualitatively different distribution from a rejection sampling algorithm.<p>The latter isn&#x27;t too obvious, and may in fact be (very) false. Look up maze generation algorithms if you want some feeling for the effects this could have.<p>If you just want a quick argument, consider what happens if picking the most likely token would increase the chance of an invalid token further down the line to nearly 100%. By the time your token-picking algorithm has any effect it would be too late to fix it.</div><br/><div id="41176082" class="c"><input type="checkbox" id="c-41176082" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41175582">parent</a><span>|</span><a href="#41175943">next</a><span>|</span><label class="collapse" for="c-41176082">[-]</label><label class="expand" for="c-41176082">[3 more]</label></div><br/><div class="children"><div class="content">Sorry, how could there not be a valid next token?  Presumably your interface would generate a state machine with appropriate masking arrays, and iirc generally speaking all 256 byte choices are in the token list.  There&#x27;s no way to get stuck in a place where the JSON is invalid?  Can you give an example?<p>If you want to be really clever about your picker, a deterministic result would blat out the all the known possible strings.<p>For example, if you had an object with defined a defined set of properties, you could just go ahead and not bother generating tokens for all the properties and just tokenize, E.G. `{&quot;foo&quot;:&quot;` (6-ish tokens) without even passing through the LLM.  As soon as an unescaped `&quot;` arrives, you know the continuation must be `,&quot;bar&quot;:&quot;`, for example<p>&gt; This greedy algorithm doesn&#x27;t result in a qualitatively different distribution from a rejection sampling algorithm.<p>It absolutely will.  But so will adding an extra newline in your prompt, for example.  That sort of thing is part and parcel of how llms work</div><br/><div id="41176345" class="c"><input type="checkbox" id="c-41176345" checked=""/><div class="controls bullet"><span class="by">contravariant</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41176082">parent</a><span>|</span><a href="#41175943">next</a><span>|</span><label class="collapse" for="c-41176345">[-]</label><label class="expand" for="c-41176345">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, I think any example where it can get stuck is going to be a bit contrived since really it&#x27;s a question of how easy it is to recognize a valid prefix. Say for example you want the LLM to generate a valid chess match and it ends up in a situation with just 2 kings left. If you&#x27;re not careful with your definitions you could end up in an endless loop that never ends.<p>That said if you <i>know</i> all valid prefixes in your language in advance then you can always realise when a token leaves no valid continuations.<p>&gt; It absolutely will. But so will adding an extra newline<p>A newline is less likely to dramatically drop the quality, a greedy method could easily end driving itself into a dead end (if not grammatically then semantically).<p>Say you want it to give a weather prediction consisting of a description followed by a tag &#x27;sunny&#x27; or &#x27;cloudy&#x27; and your model is on its way to generate<p><pre><code>    { 
      desc: &quot;Strong winds followed by heavy rainfall.&quot;, 
      tag: &quot;stormy&quot; 
    }
</code></pre>
If it ever gets to the &#x27;s&#x27; in stormy it will be forced to pick &#x27;sunny&#x27;, even if that makes no sense in context.</div><br/><div id="41176437" class="c"><input type="checkbox" id="c-41176437" checked=""/><div class="controls bullet"><span class="by">arjvik</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41176345">parent</a><span>|</span><a href="#41175943">next</a><span>|</span><label class="collapse" for="c-41176437">[-]</label><label class="expand" for="c-41176437">[1 more]</label></div><br/><div class="children"><div class="content">Schema needs to be a part of the prompt as well so it can associatively recall the options</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41175943" class="c"><input type="checkbox" id="c-41175943" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174739">parent</a><span>|</span><a href="#41174870">prev</a><span>|</span><a href="#41175019">next</a><span>|</span><label class="collapse" for="c-41175943">[-]</label><label class="expand" for="c-41175943">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but that&#x27;s hugely wasteful of tokens.</div><br/></div></div></div></div></div></div><div id="41175019" class="c"><input type="checkbox" id="c-41175019" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41174452">prev</a><span>|</span><a href="#41174395">next</a><span>|</span><label class="collapse" for="c-41175019">[-]</label><label class="expand" for="c-41175019">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know, it doesn&#x27;t sound wild at all to me. Human languages are very imprecise, vague and error-tolerant, which is the opposite of an output format like JSON. So the a model can&#x27;t do these two things well at the same time is quite an intuitive conclusion.<p>The wild part is that a model trained with so much human language text can still outputs mostly compilable code.</div><br/></div></div><div id="41174395" class="c"><input type="checkbox" id="c-41174395" checked=""/><div class="controls bullet"><span class="by">ramraj07</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41175019">prev</a><span>|</span><a href="#41174693">next</a><span>|</span><label class="collapse" for="c-41174395">[-]</label><label class="expand" for="c-41174395">[2 more]</label></div><br/><div class="children"><div class="content">This is like saying “we shouldn’t be celebrating a computer that can talk, my parrot can do that!”</div><br/></div></div><div id="41174693" class="c"><input type="checkbox" id="c-41174693" checked=""/><div class="controls bullet"><span class="by">scarmig</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41174395">prev</a><span>|</span><a href="#41173776">next</a><span>|</span><label class="collapse" for="c-41174693">[-]</label><label class="expand" for="c-41174693">[2 more]</label></div><br/><div class="children"><div class="content">I have struggled writing valid YAML before (my tokenizer doesn&#x27;t handle whitespace very well). And it probably takes me a quadrillion operations on the reals to get a minimal YAML file (I think your 10^25 fp ops is an overestimate--I think it&#x27;s more like 10^18-10^19).<p>It&#x27;s kind of like an inverse Moravec&#x27;s paradox.</div><br/><div id="41175928" class="c"><input type="checkbox" id="c-41175928" checked=""/><div class="controls bullet"><span class="by">theturtle32</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41174693">parent</a><span>|</span><a href="#41173776">next</a><span>|</span><label class="collapse" for="c-41175928">[-]</label><label class="expand" for="c-41175928">[1 more]</label></div><br/><div class="children"><div class="content">Relatable!!</div><br/></div></div></div></div><div id="41173776" class="c"><input type="checkbox" id="c-41173776" checked=""/><div class="controls bullet"><span class="by">codingwagie</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41174693">prev</a><span>|</span><a href="#41174380">next</a><span>|</span><label class="collapse" for="c-41173776">[-]</label><label class="expand" for="c-41173776">[2 more]</label></div><br/><div class="children"><div class="content">I think it will take a long time for the world at large to realize and then operationalize the potential of this &quot;mundane&quot; technology. It is revolutionary, and also sitting in plain sight. Such a huge technological shift that was considered decades out only a few years ago</div><br/><div id="41174383" class="c"><input type="checkbox" id="c-41174383" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41173776">parent</a><span>|</span><a href="#41174380">next</a><span>|</span><label class="collapse" for="c-41174383">[-]</label><label class="expand" for="c-41174383">[1 more]</label></div><br/><div class="children"><div class="content">Although I am an optimist* about what this can do, I am very much aware — from personal experience — how easy it is to see more than is really there.<p>The realisation of the tech might be fantastic new things… or it might be that people like me are Clever Hans-ing the models.<p>* that may be the wrong word; &quot;strong capabilities&quot; is what I think is present, those can be used for ill effects which is pessimistic.</div><br/></div></div></div></div><div id="41174380" class="c"><input type="checkbox" id="c-41174380" checked=""/><div class="controls bullet"><span class="by">tommica</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41173776">prev</a><span>|</span><a href="#41173861">next</a><span>|</span><label class="collapse" for="c-41174380">[-]</label><label class="expand" for="c-41174380">[1 more]</label></div><br/><div class="children"><div class="content">For some reason it reminds me of my civilization runs - rush to certain high level tech and then after that discovery writing :D</div><br/></div></div><div id="41173861" class="c"><input type="checkbox" id="c-41173861" checked=""/><div class="controls bullet"><span class="by">srcreigh</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41174380">prev</a><span>|</span><a href="#41174849">next</a><span>|</span><label class="collapse" for="c-41173861">[-]</label><label class="expand" for="c-41173861">[3 more]</label></div><br/><div class="children"><div class="content">If I wanted to be a silly pedant, I’d say that Turing machines are language specifications and thus it’s theoretically impossible for an LLM or any program to validate output formats in general.</div><br/><div id="41174285" class="c"><input type="checkbox" id="c-41174285" checked=""/><div class="controls bullet"><span class="by">jes5199</span><span>|</span><a href="#41173616">root</a><span>|</span><a href="#41173861">parent</a><span>|</span><a href="#41174116">prev</a><span>|</span><a href="#41174849">next</a><span>|</span><label class="collapse" for="c-41174285">[-]</label><label class="expand" for="c-41174285">[1 more]</label></div><br/><div class="children"><div class="content">in _general_ sure, but if you restricted each token to conform to a Kleene-star grammar you should be able to guarantee that you get something that parses according to a context-free grammar</div><br/></div></div></div></div><div id="41174849" class="c"><input type="checkbox" id="c-41174849" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#41173616">parent</a><span>|</span><a href="#41173861">prev</a><span>|</span><a href="#41173804">next</a><span>|</span><label class="collapse" for="c-41174849">[-]</label><label class="expand" for="c-41174849">[1 more]</label></div><br/><div class="children"><div class="content">It’s doing more, it is allowing user to input using natural language and the output is the json format the API that is defined</div><br/></div></div></div></div><div id="41173804" class="c"><input type="checkbox" id="c-41173804" checked=""/><div class="controls bullet"><span class="by">nichochar</span><span>|</span><a href="#41173616">prev</a><span>|</span><a href="#41173900">next</a><span>|</span><label class="collapse" for="c-41173804">[-]</label><label class="expand" for="c-41173804">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a little confused why you have to specify &quot;strict: true&quot; to get this behavior. It is obviously always desired, I would be surprised for people to ever specify &quot;strict: false&quot;. That API design leaves to be desired.<p>I also learned about constrainted decoding[1], that they give a brief explanation about. This is a really clever technique! It will increase reliability as well as reduce latency (less tokens to pick from) once the initial artifacts are loaded.<p>[1] <a href="https:&#x2F;&#x2F;www.aidancooper.co.uk&#x2F;constrained-decoding&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.aidancooper.co.uk&#x2F;constrained-decoding&#x2F;</a></div><br/><div id="41174381" class="c"><input type="checkbox" id="c-41174381" checked=""/><div class="controls bullet"><span class="by">athyuttamre</span><span>|</span><a href="#41173804">parent</a><span>|</span><a href="#41174169">next</a><span>|</span><label class="collapse" for="c-41174381">[-]</label><label class="expand" for="c-41174381">[4 more]</label></div><br/><div class="children"><div class="content">Hi, I work on the OpenAI API — structured outputs schemas have limitations (e.g. all fields must be required, no additional properties allowed): <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;structured-outputs&#x2F;supported-schemas" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;guides&#x2F;structured-outputs&#x2F;s...</a>.<p>If your schema is not supported, but you still want to use the model to generate output, you would use `strict: false`. Unfortunately we cannot make `strict: true` the default because it would break existing users. We hope to make it the default in a future API version.</div><br/><div id="41174763" class="c"><input type="checkbox" id="c-41174763" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41173804">root</a><span>|</span><a href="#41174381">parent</a><span>|</span><a href="#41174169">next</a><span>|</span><label class="collapse" for="c-41174763">[-]</label><label class="expand" for="c-41174763">[3 more]</label></div><br/><div class="children"><div class="content">You should also mention that before you had done custom alignment accounting for this feature, that it was an excellent alignment breaker (therefor a big no-no to release too early)<p>For example, if I ask an LLM to generate social security numbers, it will give the whole &quot;I&#x27;m sorry Hal, I can&#x27;t do that&quot;. If I ban all tokens except numbers and hyphens, prior to your &quot;refusal = True&quot; approach, it was guaranteed that even &quot;aligned&quot; models would generate what appeared to be social security numbers.</div><br/><div id="41175645" class="c"><input type="checkbox" id="c-41175645" checked=""/><div class="controls bullet"><span class="by">ethical_source</span><span>|</span><a href="#41173804">root</a><span>|</span><a href="#41174763">parent</a><span>|</span><a href="#41174169">next</a><span>|</span><label class="collapse" for="c-41175645">[-]</label><label class="expand" for="c-41175645">[2 more]</label></div><br/><div class="children"><div class="content">And if LLMs can generate plausible social security numbers, our civilization will fall &#x2F;s<p>Christ, I hate the AI safety people who brain-damage models so that they refuse to do things trivial to do by other means. Is LLM censorship preventing bad actors from generating social security numbers? Obviously not. THEN WHY DOES DAMAGING AN LLM TO MAKE IT REFUSE THIS TASK MAKE CIVILIZATION BETTER OFF?<p>History will not be kind to safetyist luddites.</div><br/><div id="41176062" class="c"><input type="checkbox" id="c-41176062" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#41173804">root</a><span>|</span><a href="#41175645">parent</a><span>|</span><a href="#41174169">next</a><span>|</span><label class="collapse" for="c-41176062">[-]</label><label class="expand" for="c-41176062">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m less concerned with the AI teams lobotomizing utility, more concerned with damage to language, particularly redefining the term &quot;safe&quot; to mean something like &quot;what we deem suitable&quot;.<p>That said, when zero &quot;safety&quot; is at stake might be the best time to experiment with how to build and where to put safety latches, for when we get to a point we mean actual safety.  I&#x27;m even OK with models that default to parental control for practice provided it can be switched off.</div><br/></div></div></div></div></div></div></div></div><div id="41174169" class="c"><input type="checkbox" id="c-41174169" checked=""/><div class="controls bullet"><span class="by">dgellow</span><span>|</span><a href="#41173804">parent</a><span>|</span><a href="#41174381">prev</a><span>|</span><a href="#41173900">next</a><span>|</span><label class="collapse" for="c-41174169">[-]</label><label class="expand" for="c-41174169">[1 more]</label></div><br/><div class="children"><div class="content">Could you develop a bit re: the API? What do you dislike other than the “strict: true”?</div><br/></div></div></div></div><div id="41173900" class="c"><input type="checkbox" id="c-41173900" checked=""/><div class="controls bullet"><span class="by">cvhc</span><span>|</span><a href="#41173804">prev</a><span>|</span><a href="#41174456">next</a><span>|</span><label class="collapse" for="c-41173900">[-]</label><label class="expand" for="c-41173900">[7 more]</label></div><br/><div class="children"><div class="content">I wonder why the top level has to be an object instead of an array... I have some pretty normal use cases where I expect the model to extract a list of objects from the text.<p>```
openai.BadRequestError: Error code: 400 - {&#x27;error&#x27;: {&#x27;message&#x27;: &#x27;Invalid schema for response_format \&#x27;PolicyStatements\&#x27;: schema must be a JSON Schema of \&#x27;type: &quot;object&quot;\&#x27;, got \&#x27;type: &quot;array&quot;\&#x27;.&#x27;, &#x27;type&#x27;: &#x27;invalid_request_error&#x27;, &#x27;param&#x27;: &#x27;response_format&#x27;, &#x27;code&#x27;: None}}
```<p>I know I can always put the array into a single-key object but it&#x27;s just so annoying I also have to modify the prompts accordingly to accomodate this.</div><br/><div id="41174209" class="c"><input type="checkbox" id="c-41174209" checked=""/><div class="controls bullet"><span class="by">moritzwarhier</span><span>|</span><a href="#41173900">parent</a><span>|</span><a href="#41174100">next</a><span>|</span><label class="collapse" for="c-41174209">[-]</label><label class="expand" for="c-41174209">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a relatively common convention for JSON APIs.<p>Possible reasons:<p>- Extensibility without breaking changes<p>- Forcing an object simplifies parsing of API responses, ideally the key should describe the contents, like additional metadata. It also simplifies validation, if considered separate from parsing<p>- Forcing the root of the API response to be an object makes sure that there is a single entry point into consuming it. There is no way to place non-descript heterogenous data items next to each other<p>- Imagine that you want to declare types (often generated from JSON schemas) for your API responses. That means you should refrain from placing different types, or a single too broad type in an array. Arrays should be used in a similar way to stricter languages, and not contain unexpected types. A top-level array invites dumping unspecified data to the client that is expensive and hard to process<p>- The blurry line between arrays and objects in JS does not cleanly map to other languages, not even very dynamic ones like PHP or Python. I&#x27;m aware that JSON and JS object literals are not the same. But even the JSON subset of JS (apart from number types, where it&#x27;s not a subset AFAIK) already creates interesting edge cases for serialization and deserialization</div><br/></div></div><div id="41174100" class="c"><input type="checkbox" id="c-41174100" checked=""/><div class="controls bullet"><span class="by">manquer</span><span>|</span><a href="#41173900">parent</a><span>|</span><a href="#41174209">prev</a><span>|</span><a href="#41174975">next</a><span>|</span><label class="collapse" for="c-41174100">[-]</label><label class="expand" for="c-41174100">[1 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t say for OpenAI, but in general I have seen and used this design pattern to keep consistency of root object output and remove a lot of unnecessary validations and branching flows<p>Otherwise you will to handle the scenarios in code everywhere if you don&#x27;t know if the root is object or array. If the root has a key that confirms to a known schema then validation becomes easier to write for that scenario,<p>Similar reasons to why so many APIs wrap with a key like &#x27;data&#x27;, &#x27;value&#x27; or &#x27;error&#x27; all responses or in RESTful HTTP endpoints collection say GET &#x2F;v1&#x2F;my-object endpoints do no mix with resource URIs GET &#x2F;v1&#x2F;my-object&#x2F;1 the former is always an array the latter is always an object.</div><br/></div></div><div id="41174975" class="c"><input type="checkbox" id="c-41174975" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41173900">parent</a><span>|</span><a href="#41174100">prev</a><span>|</span><a href="#41174177">next</a><span>|</span><label class="collapse" for="c-41174975">[-]</label><label class="expand" for="c-41174975">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve regretted designing APIs that return an array rather than an object in the past.<p>It&#x27;s all about the extensibility. If you return an object you can add extra keys, for things like &quot;an error occurred, here are the details&quot;, or &quot;this is truncated, here&#x27;s how to paginate it&quot;, or a logs key for extra debug messages, or information about the currently authenticated user.<p>None of those are possible if the root is an array.</div><br/></div></div><div id="41174177" class="c"><input type="checkbox" id="c-41174177" checked=""/><div class="controls bullet"><span class="by">heliophobicdude</span><span>|</span><a href="#41173900">parent</a><span>|</span><a href="#41174975">prev</a><span>|</span><a href="#41174160">next</a><span>|</span><label class="collapse" for="c-41174177">[-]</label><label class="expand" for="c-41174177">[1 more]</label></div><br/><div class="children"><div class="content">Back in the old days, top level arrays were a security risk because the array constructor in JS could be redefined and do bad-guy stuff. I cannot think of any json parsing clients that are vulnerable to this.</div><br/></div></div><div id="41174160" class="c"><input type="checkbox" id="c-41174160" checked=""/><div class="controls bullet"><span class="by">tomComb</span><span>|</span><a href="#41173900">parent</a><span>|</span><a href="#41174177">prev</a><span>|</span><a href="#41174456">next</a><span>|</span><label class="collapse" for="c-41174160">[-]</label><label class="expand" for="c-41174160">[2 more]</label></div><br/><div class="children"><div class="content">Well, this wouldn’t be a very satisfying explanation, but these JSON objects are often represented as Python dictionaries and those can’t have top level arrays.</div><br/><div id="41178477" class="c"><input type="checkbox" id="c-41178477" checked=""/><div class="controls bullet"><span class="by">Too</span><span>|</span><a href="#41173900">root</a><span>|</span><a href="#41174160">parent</a><span>|</span><a href="#41174456">next</a><span>|</span><label class="collapse" for="c-41178477">[-]</label><label class="expand" for="c-41178477">[1 more]</label></div><br/><div class="children"><div class="content">Try json.loads(&quot;[1,2,3]&quot;), you&#x27;ll get a list back.<p>The reasons others already posted about extensibility are more correct.</div><br/></div></div></div></div></div></div><div id="41174456" class="c"><input type="checkbox" id="c-41174456" checked=""/><div class="controls bullet"><span class="by">pton_xd</span><span>|</span><a href="#41173900">prev</a><span>|</span><a href="#41173833">next</a><span>|</span><label class="collapse" for="c-41174456">[-]</label><label class="expand" for="c-41174456">[12 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t &quot;we hardcoded JSON into the latest model&quot; kind of the opposite direction, strategically, from &quot;we&#x27;re on the way to AGI and I need 7 trillion to get there?&quot;</div><br/><div id="41174530" class="c"><input type="checkbox" id="c-41174530" checked=""/><div class="controls bullet"><span class="by">isoprophlex</span><span>|</span><a href="#41174456">parent</a><span>|</span><a href="#41178071">next</a><span>|</span><label class="collapse" for="c-41174530">[-]</label><label class="expand" for="c-41174530">[7 more]</label></div><br/><div class="children"><div class="content">You are witnessing the final stages in the evolution of OpenAI from a messianic hype machine to Yet Another Product Company.<p>Hence all the people leaving, too.</div><br/><div id="41174652" class="c"><input type="checkbox" id="c-41174652" checked=""/><div class="controls bullet"><span class="by">gardenhedge</span><span>|</span><a href="#41174456">root</a><span>|</span><a href="#41174530">parent</a><span>|</span><a href="#41178071">next</a><span>|</span><label class="collapse" for="c-41174652">[-]</label><label class="expand" for="c-41174652">[6 more]</label></div><br/><div class="children"><div class="content">I am ootl, employees are leaving openai?</div><br/><div id="41174695" class="c"><input type="checkbox" id="c-41174695" checked=""/><div class="controls bullet"><span class="by">dangrossman</span><span>|</span><a href="#41174456">root</a><span>|</span><a href="#41174652">parent</a><span>|</span><a href="#41178071">next</a><span>|</span><label class="collapse" for="c-41174695">[-]</label><label class="expand" for="c-41174695">[5 more]</label></div><br/><div class="children"><div class="content">&gt; John Schulman, one of the co-founders of artificial intelligence company OpenAI, has left the ChatGPT maker for rival Anthropic, he said in a post on social media platform X late Monday.<p>&gt; OpenAI&#x27;s President and co-founder Greg Brockman is also taking a sabbatical through the end of the year, he said in a X post late Monday.<p>&gt; Peter Deng, a vice-president of product, also left in recent months, a spokesperson said. And earlier this year, several members of the company’s safety teams exited.<p>That&#x27;s after co-founder and Chief Scientist Ilya Sutskever left in May.</div><br/><div id="41176835" class="c"><input type="checkbox" id="c-41176835" checked=""/><div class="controls bullet"><span class="by">g15jv2dp</span><span>|</span><a href="#41174456">root</a><span>|</span><a href="#41174695">parent</a><span>|</span><a href="#41175171">next</a><span>|</span><label class="collapse" for="c-41176835">[-]</label><label class="expand" for="c-41176835">[2 more]</label></div><br/><div class="children"><div class="content">Some employees quitting in a 1500 people company? Impossible. Any departure must be interpreted as the doom of openai, there&#x27;s no other possibility.</div><br/></div></div><div id="41175171" class="c"><input type="checkbox" id="c-41175171" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#41174456">root</a><span>|</span><a href="#41174695">parent</a><span>|</span><a href="#41176835">prev</a><span>|</span><a href="#41178071">next</a><span>|</span><label class="collapse" for="c-41175171">[-]</label><label class="expand" for="c-41175171">[2 more]</label></div><br/><div class="children"><div class="content">Are there any co-founders left?</div><br/><div id="41175772" class="c"><input type="checkbox" id="c-41175772" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#41174456">root</a><span>|</span><a href="#41175171">parent</a><span>|</span><a href="#41178071">next</a><span>|</span><label class="collapse" for="c-41175772">[-]</label><label class="expand" for="c-41175772">[1 more]</label></div><br/><div class="children"><div class="content">sam Altman for one.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41178071" class="c"><input type="checkbox" id="c-41178071" checked=""/><div class="controls bullet"><span class="by">chamomeal</span><span>|</span><a href="#41174456">parent</a><span>|</span><a href="#41174530">prev</a><span>|</span><a href="#41175078">next</a><span>|</span><label class="collapse" for="c-41178071">[-]</label><label class="expand" for="c-41178071">[1 more]</label></div><br/><div class="children"><div class="content">I mean it’s not groundbreaking, but it makes it <i>much</i> easier to make simple AI tools that aren’t chat-based. It definitely has me interested.<p>GPT-4 has been so mind-blowingly cool, but most of the interesting applications I can think of involve 10 steps of “ok now make sure GPT has actually formatted the question as a list of strings… ok now make sure GPT hasn’t responded with a refusal to answer the question…”<p>Idk what the deal is with their weird hype persona thing, but I’m stoked about this release</div><br/></div></div><div id="41178099" class="c"><input type="checkbox" id="c-41178099" checked=""/><div class="controls bullet"><span class="by">nsonha</span><span>|</span><a href="#41174456">parent</a><span>|</span><a href="#41175078">prev</a><span>|</span><a href="#41174607">next</a><span>|</span><label class="collapse" for="c-41178099">[-]</label><label class="expand" for="c-41178099">[1 more]</label></div><br/><div class="children"><div class="content">AGI is useless if you can&#x27;t figure out how to employ it as part of a system, instead of just chit chat</div><br/></div></div><div id="41174607" class="c"><input type="checkbox" id="c-41174607" checked=""/><div class="controls bullet"><span class="by">KaiMagnus</span><span>|</span><a href="#41174456">parent</a><span>|</span><a href="#41178099">prev</a><span>|</span><a href="#41173833">next</a><span>|</span><label class="collapse" for="c-41174607">[-]</label><label class="expand" for="c-41174607">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, definitely a way to end up with a Siri like mess if you do this long enough. The use case is there and it’s going to be very useful, but the magic is wearing off.</div><br/></div></div></div></div><div id="41173833" class="c"><input type="checkbox" id="c-41173833" checked=""/><div class="controls bullet"><span class="by">srcreigh</span><span>|</span><a href="#41174456">prev</a><span>|</span><a href="#41177237">next</a><span>|</span><label class="collapse" for="c-41173833">[-]</label><label class="expand" for="c-41173833">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The tokens that are valid at the beginning of the output include things like {, {“, {\n, etc. However, once the model has already sampled {“val, then { is no longer a valid token<p>Oops, this is incorrect. {“val{“:2} is valid json.<p>(modulo iOS quotes lol)</div><br/><div id="41174305" class="c"><input type="checkbox" id="c-41174305" checked=""/><div class="controls bullet"><span class="by">jhgg</span><span>|</span><a href="#41173833">parent</a><span>|</span><a href="#41177237">next</a><span>|</span><label class="collapse" for="c-41174305">[-]</label><label class="expand" for="c-41174305">[1 more]</label></div><br/><div class="children"><div class="content">Valid JSON, sure, but that key does not conform to the schema provided in the example. The LLM must generate valid JSON that <i>also</i> conforms to the provided schema.</div><br/></div></div></div></div><div id="41177237" class="c"><input type="checkbox" id="c-41177237" checked=""/><div class="controls bullet"><span class="by">blackcat201</span><span>|</span><a href="#41173833">prev</a><span>|</span><a href="#41179299">next</a><span>|</span><label class="collapse" for="c-41177237">[-]</label><label class="expand" for="c-41177237">[2 more]</label></div><br/><div class="children"><div class="content">Do beware on some reasoning task, our recent work[0] actually found it may cause some performance degradation as well as possible reasoning weakening in JSON. I really hope they fix this in the latest GPT-4o version.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.02442" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.02442</a></div><br/><div id="41177629" class="c"><input type="checkbox" id="c-41177629" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#41177237">parent</a><span>|</span><a href="#41179299">next</a><span>|</span><label class="collapse" for="c-41177629">[-]</label><label class="expand" for="c-41177629">[1 more]</label></div><br/><div class="children"><div class="content">Thank you! This confirms my intuition!<p>Structured generation seems counter to every other signal we have that chain of thought etc improves performance.</div><br/></div></div></div></div><div id="41179299" class="c"><input type="checkbox" id="c-41179299" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#41177237">prev</a><span>|</span><a href="#41175335">next</a><span>|</span><label class="collapse" for="c-41179299">[-]</label><label class="expand" for="c-41179299">[1 more]</label></div><br/><div class="children"><div class="content">Surprised to see OpenAI staff in comments. I don’t recall that being the case on previous threads. (Hello!)</div><br/></div></div><div id="41175335" class="c"><input type="checkbox" id="c-41175335" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41179299">prev</a><span>|</span><a href="#41173744">next</a><span>|</span><label class="collapse" for="c-41175335">[-]</label><label class="expand" for="c-41175335">[8 more]</label></div><br/><div class="children"><div class="content">Using this feature will obviously &quot;lock you in&quot; to OpenAI, specifically to this model too, at least until other companies catch on. While text prompts can more easily be moved to other LLMs, this feature cannot currently be ported as such. I would use it only if a text prompt is insufficient despite retries.</div><br/><div id="41175604" class="c"><input type="checkbox" id="c-41175604" checked=""/><div class="controls bullet"><span class="by">PufPufPuf</span><span>|</span><a href="#41175335">parent</a><span>|</span><a href="#41175444">next</a><span>|</span><label class="collapse" for="c-41175604">[-]</label><label class="expand" for="c-41175604">[1 more]</label></div><br/><div class="children"><div class="content">This feature has existed for quite some time in several inference libraries, like Outlines, under the names &quot;constrained decoding&quot; or &quot;guided decoding&quot;. Some even include it in their OpenAI-compatible API in a very similar form (allowing to pass in a JSON Schema). All this required doing your own inference, though -- so the announcement really just brings this popular feature &quot;to the masses&quot;.</div><br/></div></div><div id="41175444" class="c"><input type="checkbox" id="c-41175444" checked=""/><div class="controls bullet"><span class="by">dtquad</span><span>|</span><a href="#41175335">parent</a><span>|</span><a href="#41175604">prev</a><span>|</span><a href="#41175743">next</a><span>|</span><label class="collapse" for="c-41175444">[-]</label><label class="expand" for="c-41175444">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI-style JSON mode and function calling rapidly became the industry standard way of doing it. It will probably also happen for this feature.</div><br/><div id="41175486" class="c"><input type="checkbox" id="c-41175486" checked=""/><div class="controls bullet"><span class="by">toomuchtodo</span><span>|</span><a href="#41175335">root</a><span>|</span><a href="#41175444">parent</a><span>|</span><a href="#41175743">next</a><span>|</span><label class="collapse" for="c-41175486">[-]</label><label class="expand" for="c-41175486">[1 more]</label></div><br/><div class="children"><div class="content">“S3 compatible”</div><br/></div></div></div></div><div id="41175743" class="c"><input type="checkbox" id="c-41175743" checked=""/><div class="controls bullet"><span class="by">faizshah</span><span>|</span><a href="#41175335">parent</a><span>|</span><a href="#41175444">prev</a><span>|</span><a href="#41175618">next</a><span>|</span><label class="collapse" for="c-41175743">[-]</label><label class="expand" for="c-41175743">[1 more]</label></div><br/><div class="children"><div class="content">The converse API in AWS bedrock lets you use function calling across a number of different providers (doesn’t support OpenAI): <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;bedrock&#x2F;latest&#x2F;userguide&#x2F;conversation-inference.html" rel="nofollow">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;bedrock&#x2F;latest&#x2F;userguide&#x2F;convers...</a><p>I have been using it so that my agents aren’t specific to a particular model or api.<p>Like others have said many other providers already have function calling and json schema for structure outputs.</div><br/></div></div><div id="41175618" class="c"><input type="checkbox" id="c-41175618" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#41175335">parent</a><span>|</span><a href="#41175743">prev</a><span>|</span><a href="#41175383">next</a><span>|</span><label class="collapse" for="c-41175618">[-]</label><label class="expand" for="c-41175618">[2 more]</label></div><br/><div class="children"><div class="content">OTOH, not using it could &quot;lock you out&quot; of building a cool product for your users, so ...</div><br/><div id="41176623" class="c"><input type="checkbox" id="c-41176623" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41175335">root</a><span>|</span><a href="#41175618">parent</a><span>|</span><a href="#41175383">next</a><span>|</span><label class="collapse" for="c-41176623">[-]</label><label class="expand" for="c-41176623">[1 more]</label></div><br/><div class="children"><div class="content">It depends. Quite often the intended schema is simple enough that parsing of a plaintext lines using a regex is sufficient. I do however have to use a verbose prompt to get it to comply with the expected format. Combine this with retries in case of rare failures, and the job is done.<p>I think this new feature is more relevant for intricate schemas and dynamic schemas when a text prompt cannot do the job.</div><br/></div></div></div></div><div id="41175383" class="c"><input type="checkbox" id="c-41175383" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#41175335">parent</a><span>|</span><a href="#41175618">prev</a><span>|</span><a href="#41173744">next</a><span>|</span><label class="collapse" for="c-41175383">[-]</label><label class="expand" for="c-41175383">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s already supported by multiple other providers. Fireworks, Together, probably more.</div><br/></div></div></div></div><div id="41173744" class="c"><input type="checkbox" id="c-41173744" checked=""/><div class="controls bullet"><span class="by">elpocko</span><span>|</span><a href="#41175335">prev</a><span>|</span><a href="#41173516">next</a><span>|</span><label class="collapse" for="c-41173744">[-]</label><label class="expand" for="c-41173744">[3 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t the BNF grammar approach in llama.cpp solve this issue in a generic way that should work with any model? Why wouldn&#x27;t they use that?</div><br/><div id="41173840" class="c"><input type="checkbox" id="c-41173840" checked=""/><div class="controls bullet"><span class="by">ejones</span><span>|</span><a href="#41173744">parent</a><span>|</span><a href="#41175313">next</a><span>|</span><label class="collapse" for="c-41173840">[-]</label><label class="expand" for="c-41173840">[1 more]</label></div><br/><div class="children"><div class="content">Similar approach to llama.cpp under the hood - they convert the schema to a grammar. Llama.cpp&#x27;s implementation was specific to the ggml stack, but what they&#x27;ve built sounds similar to Outlines, which they acknowledged.</div><br/></div></div><div id="41175313" class="c"><input type="checkbox" id="c-41175313" checked=""/><div class="controls bullet"><span class="by">HanClinto</span><span>|</span><a href="#41173744">parent</a><span>|</span><a href="#41173840">prev</a><span>|</span><a href="#41173516">next</a><span>|</span><label class="collapse" for="c-41175313">[-]</label><label class="expand" for="c-41175313">[1 more]</label></div><br/><div class="children"><div class="content">llama.cpp&#x27;s GBNF grammar is generic, and indeed works with any model.<p>I can&#x27;t speak for other approaches, but -- while llama.cpp&#x27;s implementation is nice in that it always generates valid grammars token-by-token (and doesn&#x27;t require any backtracking), it is tough in that -- in the case of ambiguous grammars (where we&#x27;re not always sure where we&#x27;re at in the grammar until it finishes generating), then it keeps all valid parsing option stacks in memory at the same time. This is good for the no-backtracking case, but it adds a (sometimes significant) cost in terms of being rather &quot;explosive&quot; in the memory usage (especially if one uses a particularly large or poorly-formed grammar). Creating a grammar that is openly hostile and crashes the inference server is not difficult.<p>People have done a lot of work to try and address some of the more egregious cases, but the memory load can be significant.<p>One example of memory optimization: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6616">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;6616</a><p>I&#x27;m not entirely sure what other options there are for approaches to take, but I&#x27;d be curious to learn how other libraries (Outlines, jsonformer) handle syntax validation.</div><br/></div></div></div></div><div id="41173516" class="c"><input type="checkbox" id="c-41173516" checked=""/><div class="controls bullet"><span class="by">jjcm</span><span>|</span><a href="#41173744">prev</a><span>|</span><a href="#41174227">next</a><span>|</span><label class="collapse" for="c-41173516">[-]</label><label class="expand" for="c-41173516">[19 more]</label></div><br/><div class="children"><div class="content">Interesting tidbit at the very end that&#x27;s worth noting for anyone using the API today:<p>&gt; By switching to the new gpt-4o-2024-08-06, developers save 50% on inputs ($2.50&#x2F;1M input tokens) and 33% on outputs ($10.00&#x2F;1M output tokens) compared to gpt-4o-2024-05-13.</div><br/><div id="41173540" class="c"><input type="checkbox" id="c-41173540" checked=""/><div class="controls bullet"><span class="by">scrollop</span><span>|</span><a href="#41173516">parent</a><span>|</span><a href="#41173642">next</a><span>|</span><label class="collapse" for="c-41173540">[-]</label><label class="expand" for="c-41173540">[11 more]</label></div><br/><div class="children"><div class="content">From what I&#x27;ve learned from OpenAI, the &quot;latest&quot; &quot;cheaper&quot; model will perform worse than the previous model on various tasks (esp reasoning).</div><br/><div id="41173613" class="c"><input type="checkbox" id="c-41173613" checked=""/><div class="controls bullet"><span class="by">ralusek</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173540">parent</a><span>|</span><a href="#41173694">next</a><span>|</span><label class="collapse" for="c-41173613">[-]</label><label class="expand" for="c-41173613">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s been well enough acknowledged that all of the shortcuts LLMs have been taking with ways of attempting to compress&#x2F;refine&#x2F;index the attention mechanism seem to result in dumber models.<p>GPT 4 Turbo was more like GPT 3.9, and GPT 4o is more like GPT 3.7.</div><br/><div id="41177796" class="c"><input type="checkbox" id="c-41177796" checked=""/><div class="controls bullet"><span class="by">alach11</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173613">parent</a><span>|</span><a href="#41173732">next</a><span>|</span><label class="collapse" for="c-41177796">[-]</label><label class="expand" for="c-41177796">[2 more]</label></div><br/><div class="children"><div class="content">Do you have benchmarks demonstrating this? In my own personal&#x2F;team benchmarks, I&#x27;ve seen 4o consistently outperform the original gpt-4.</div><br/><div id="41178291" class="c"><input type="checkbox" id="c-41178291" checked=""/><div class="controls bullet"><span class="by">maeil</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41177796">parent</a><span>|</span><a href="#41173732">next</a><span>|</span><label class="collapse" for="c-41178291">[-]</label><label class="expand" for="c-41178291">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m building a product that requires complex LLM flows and  out of OpenAI&#x27;s &quot;cheap&quot; tier models, the old versions of Turbo-3.5 are far better than the last versions of it and 4o-mini. I have a number of tasks that the former consistently succeed at and the latter consistently fail at regardless of prompting.<p>Leaderboards and benchmarks are very misleading as OpenAI is optimizing for them, like in the past when certain CPU manufacturers would optimize for synthetic benchmarks.<p>Fwif these aren&#x27;t chat usecases, for which the newer models may well be better.</div><br/></div></div></div></div><div id="41173732" class="c"><input type="checkbox" id="c-41173732" checked=""/><div class="controls bullet"><span class="by">scrollop</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173613">parent</a><span>|</span><a href="#41177796">prev</a><span>|</span><a href="#41174802">next</a><span>|</span><label class="collapse" for="c-41173732">[-]</label><label class="expand" for="c-41173732">[1 more]</label></div><br/><div class="children"><div class="content">Some commenters acknowledge it - and quantify it:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Tf1nooXtUHE&amp;t=689s" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Tf1nooXtUHE&amp;t=689s</a></div><br/></div></div><div id="41174802" class="c"><input type="checkbox" id="c-41174802" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173613">parent</a><span>|</span><a href="#41173732">prev</a><span>|</span><a href="#41173694">next</a><span>|</span><label class="collapse" for="c-41174802">[-]</label><label class="expand" for="c-41174802">[1 more]</label></div><br/><div class="children"><div class="content">They try to gaslight us and tell us this isn&#x27;t true because of benchmarks, as though anyone has done anything but do the latent space exploration equivalent of throwing darts at the ocean from space.<p>It&#x27;s taken years to get even preliminary reliable decision boundary examples from LLMs because doing so is expensive.</div><br/></div></div></div></div><div id="41173694" class="c"><input type="checkbox" id="c-41173694" checked=""/><div class="controls bullet"><span class="by">scrollop</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173540">parent</a><span>|</span><a href="#41173613">prev</a><span>|</span><a href="#41173791">next</a><span>|</span><label class="collapse" for="c-41173694">[-]</label><label class="expand" for="c-41173694">[1 more]</label></div><br/><div class="children"><div class="content">Also, is it a coincidence that at cheaper (potentially faster?) model has been released (just) before they roll out the &quot;new&quot; voice mode (which boasts very low latency)?</div><br/></div></div><div id="41173791" class="c"><input type="checkbox" id="c-41173791" checked=""/><div class="controls bullet"><span class="by">codingwagie</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173540">parent</a><span>|</span><a href="#41173694">prev</a><span>|</span><a href="#41173838">next</a><span>|</span><label class="collapse" for="c-41173791">[-]</label><label class="expand" for="c-41173791">[1 more]</label></div><br/><div class="children"><div class="content">Its usually a distilled smaller model</div><br/></div></div><div id="41173838" class="c"><input type="checkbox" id="c-41173838" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173540">parent</a><span>|</span><a href="#41173791">prev</a><span>|</span><a href="#41173642">next</a><span>|</span><label class="collapse" for="c-41173838">[-]</label><label class="expand" for="c-41173838">[3 more]</label></div><br/><div class="children"><div class="content">Am I the only one that wants to know 1,000% *<i>WHY*</i> such things?<p>Is it a natural function of how models evolve?<p>Is it engineered as such? Why? Marketing&#x2F;money&#x2F;resources&#x2F;what?<p>WHO makes these decisions and why?<p>---<p>I have been building a thing with Claude 3.5 pro account and its *<i>utter fn garbage*</i> of an experience.<p>It lies, hallucinates, malevolently changes code that was already told was correct, removes features - explicitly ignore project files. Has no search, no line items, so much screen real-estate is consumed with useless empty space. It ignores states style guides. get CAUGHT forgetting about a premise we were actively working on them condescendingly apologies &quot;oh you&#x27;re correct - I should have been using XYZ knowledge&quot;<p>It makes things FN harder to learn.<p>If I had any claude engineers sitting in the room watching what a POS service it is from a project continuity point...<p>Its evil. It actively f&#x27;s up things.<p>One should have the ability to CHARGE the model token credit when it Fs up so bad.<p>NO FN SEARCH??? And when asked for line nums in it output - its in txt...<p>Seriously, I practically want not just a refund, I want claude to pay me for my time correcting its mistakes.<p>ChatGPT does the same thing. It forgets things committed to memory - refactors successful things back out of files. ETc....<p>Its been a really eye opening and frustrating experience and my squinty looks are aiming that its specifically intentional:<p>They dont want people using a $20&#x2F;month AI plan to actually be able to do any meaningful work and build a product.</div><br/><div id="41177872" class="c"><input type="checkbox" id="c-41177872" checked=""/><div class="controls bullet"><span class="by">campers</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173838">parent</a><span>|</span><a href="#41174422">next</a><span>|</span><label class="collapse" for="c-41177872">[-]</label><label class="expand" for="c-41177872">[1 more]</label></div><br/><div class="children"><div class="content">It is difficult to get the AI models to get everything right every time. I noticed too that it would sometimes remove comments etc when re-writing code.<p>The way to get better results is with agentic workflows that breakdown the task into smaller steps that the models can iteratively come to a correct result.
One important step I added to mine is a review step (in the reviewChanges.ts
 file) in my workflow at
<a href="https:&#x2F;&#x2F;github.com&#x2F;TrafficGuard&#x2F;nous&#x2F;blob&#x2F;main&#x2F;src&#x2F;swe&#x2F;codeEditingAgent.ts">https:&#x2F;&#x2F;github.com&#x2F;TrafficGuard&#x2F;nous&#x2F;blob&#x2F;main&#x2F;src&#x2F;swe&#x2F;codeE...</a><p>This gets the diff and asks questions like:<p>- Are there any redundant changes in the diff?
- Was any code removed in the changes which should not have been? 
- Review the style of the code changes in the diff carefully against the original code.<p>Maybe try using that, or the package that I use which does the actual code edits called Aider <a href="https:&#x2F;&#x2F;aider.chat&#x2F;" rel="nofollow">https:&#x2F;&#x2F;aider.chat&#x2F;</a></div><br/></div></div><div id="41174422" class="c"><input type="checkbox" id="c-41174422" checked=""/><div class="controls bullet"><span class="by">scrollop</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173838">parent</a><span>|</span><a href="#41177872">prev</a><span>|</span><a href="#41173642">next</a><span>|</span><label class="collapse" for="c-41174422">[-]</label><label class="expand" for="c-41174422">[1 more]</label></div><br/><div class="children"><div class="content">Use an API from the top models with a good frontend, then, and use precise  instructions.<p>It&#x27;s odd, as many people praise claude&#x27;s coding capabilities.</div><br/></div></div></div></div></div></div><div id="41173642" class="c"><input type="checkbox" id="c-41173642" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41173516">parent</a><span>|</span><a href="#41173540">prev</a><span>|</span><a href="#41173759">next</a><span>|</span><label class="collapse" for="c-41173642">[-]</label><label class="expand" for="c-41173642">[2 more]</label></div><br/><div class="children"><div class="content">The new price is also now reflected on the pricing page: <a href="https:&#x2F;&#x2F;openai.com&#x2F;api&#x2F;pricing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;api&#x2F;pricing&#x2F;</a><p>It&#x27;s weird that&#x27;s only a footnote when it&#x27;s actually a major shift.</div><br/><div id="41173733" class="c"><input type="checkbox" id="c-41173733" checked=""/><div class="controls bullet"><span class="by">sjnair96</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173642">parent</a><span>|</span><a href="#41173759">next</a><span>|</span><label class="collapse" for="c-41173733">[-]</label><label class="expand" for="c-41173733">[1 more]</label></div><br/><div class="children"><div class="content">I also looked up the same. I wonder why. They must have a subsequent announcement regarding this I&#x27;d expect.</div><br/></div></div></div></div><div id="41173759" class="c"><input type="checkbox" id="c-41173759" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#41173516">parent</a><span>|</span><a href="#41173642">prev</a><span>|</span><a href="#41174227">next</a><span>|</span><label class="collapse" for="c-41173759">[-]</label><label class="expand" for="c-41173759">[5 more]</label></div><br/><div class="children"><div class="content">If you use the undecorated gpt-4o do you automatically get the latest?</div><br/><div id="41174544" class="c"><input type="checkbox" id="c-41174544" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173759">parent</a><span>|</span><a href="#41174233">next</a><span>|</span><label class="collapse" for="c-41174544">[-]</label><label class="expand" for="c-41174544">[1 more]</label></div><br/><div class="children"><div class="content">For the record, you should never use that in an application. Always explicitly note the full versioned model name. This will prevent bad surprises because not every new version is an improvement; sometimes they get worse, especially at specific tasks.</div><br/></div></div><div id="41174233" class="c"><input type="checkbox" id="c-41174233" checked=""/><div class="controls bullet"><span class="by">tedsanders</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173759">parent</a><span>|</span><a href="#41174544">prev</a><span>|</span><a href="#41174791">next</a><span>|</span><label class="collapse" for="c-41174233">[-]</label><label class="expand" for="c-41174233">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ll update gpt-4o in 3 weeks. (We&#x27;ve always updated it couple weeks after launch, so no one is immediately surprised by a new model drop.)</div><br/></div></div><div id="41174791" class="c"><input type="checkbox" id="c-41174791" checked=""/><div class="controls bullet"><span class="by">voiper1</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173759">parent</a><span>|</span><a href="#41174233">prev</a><span>|</span><a href="#41174151">next</a><span>|</span><label class="collapse" for="c-41174791">[-]</label><label class="expand" for="c-41174791">[1 more]</label></div><br/><div class="children"><div class="content">&gt;We will give a 3-week notice before updating gpt-4o to point to the new snapshot gpt-4o-2024-08-06.<p>Source: <a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;gpt-4o" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;gpt-4o</a></div><br/></div></div><div id="41174151" class="c"><input type="checkbox" id="c-41174151" checked=""/><div class="controls bullet"><span class="by">daguava</span><span>|</span><a href="#41173516">root</a><span>|</span><a href="#41173759">parent</a><span>|</span><a href="#41174791">prev</a><span>|</span><a href="#41174227">next</a><span>|</span><label class="collapse" for="c-41174151">[-]</label><label class="expand" for="c-41174151">[1 more]</label></div><br/><div class="children"><div class="content">The un-postfixed version will point to the older model for the next 3 weeks their docs say</div><br/></div></div></div></div></div></div><div id="41174227" class="c"><input type="checkbox" id="c-41174227" checked=""/><div class="controls bullet"><span class="by">paradite</span><span>|</span><a href="#41173516">prev</a><span>|</span><a href="#41174431">next</a><span>|</span><label class="collapse" for="c-41174227">[-]</label><label class="expand" for="c-41174227">[4 more]</label></div><br/><div class="children"><div class="content">Really important update that was not mentioned:<p>gpt-4o-2024-08-06 has 16,384 tokens output limit instead of 4,096 tokens.<p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;gpt-4o" rel="nofollow">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;models&#x2F;gpt-4o</a><p>We don&#x27;t need the GPT-4o Long Output anymore.</div><br/><div id="41174419" class="c"><input type="checkbox" id="c-41174419" checked=""/><div class="controls bullet"><span class="by">OutOfHere</span><span>|</span><a href="#41174227">parent</a><span>|</span><a href="#41174554">next</a><span>|</span><label class="collapse" for="c-41174419">[-]</label><label class="expand" for="c-41174419">[2 more]</label></div><br/><div class="children"><div class="content">But is this also the default or just the max? Is the default 4k or 16k?<p>Also, the question of the default value applies both at the server level and at the SDK level.</div><br/><div id="41178334" class="c"><input type="checkbox" id="c-41178334" checked=""/><div class="controls bullet"><span class="by">paradite</span><span>|</span><a href="#41174227">root</a><span>|</span><a href="#41174419">parent</a><span>|</span><a href="#41174554">next</a><span>|</span><label class="collapse" for="c-41178334">[-]</label><label class="expand" for="c-41178334">[1 more]</label></div><br/><div class="children"><div class="content">Unlike Anthropic, OpenAI models don&#x27;t have a `max_tokens` setting for API calls, so I assume the max token output limit is automatically applied to API calls.<p>Otherwise the max token output limit stated on the models page would be meaningless.</div><br/></div></div></div></div><div id="41174554" class="c"><input type="checkbox" id="c-41174554" checked=""/><div class="controls bullet"><span class="by">floam</span><span>|</span><a href="#41174227">parent</a><span>|</span><a href="#41174419">prev</a><span>|</span><a href="#41174431">next</a><span>|</span><label class="collapse" for="c-41174554">[-]</label><label class="expand" for="c-41174554">[1 more]</label></div><br/><div class="children"><div class="content">Long Output is 64K though.</div><br/></div></div></div></div><div id="41174431" class="c"><input type="checkbox" id="c-41174431" checked=""/><div class="controls bullet"><span class="by">sansseriff</span><span>|</span><a href="#41174227">prev</a><span>|</span><a href="#41177264">next</a><span>|</span><label class="collapse" for="c-41174431">[-]</label><label class="expand" for="c-41174431">[1 more]</label></div><br/><div class="children"><div class="content">Preprocessing new schema takes &#x27;under 10 seconds&#x27;. That&#x27;s... a huge range? Unless the preprocessing time is a small fraction of the inference time, I don&#x27;t see the point.<p>I&#x27;m working on an app that dynamically generates schema based on user input (a union of arbitrary types pulled from a library). The resulting schema is often in the 800 token range. Curious how long that would take to preprocess</div><br/></div></div><div id="41177264" class="c"><input type="checkbox" id="c-41177264" checked=""/><div class="controls bullet"><span class="by">myprotegeai</span><span>|</span><a href="#41174431">prev</a><span>|</span><a href="#41173839">next</a><span>|</span><label class="collapse" for="c-41177264">[-]</label><label class="expand" for="c-41177264">[1 more]</label></div><br/><div class="children"><div class="content">I send dynamic jsonschemas to LLMs in manifest[1] to simulate function calls based on function signatures. These structured outputs will be enormously useful.<p>Other posts claim that you can generate jsonschema-conformant output reliably without this, and while I mostly agree, there is an edge case where gpt4o struggles, and that is simple data types. For example, a string, in jsonschema, has a schema of simply {&quot;type&quot;: &quot;string&quot;}, and an example value of &quot;hello world.&quot; However, gpt4o would produce something like {&quot;value&quot;: &quot;hello world&quot;} at a very high probability. I had to include specific few shot examples of what not to do in order to make this simple case reliable. I suspect there are other non-obvious cases.<p>1. <a href="https:&#x2F;&#x2F;github.com&#x2F;amoffat&#x2F;manifest">https:&#x2F;&#x2F;github.com&#x2F;amoffat&#x2F;manifest</a></div><br/></div></div><div id="41173839" class="c"><input type="checkbox" id="c-41173839" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41177264">prev</a><span>|</span><a href="#41173797">next</a><span>|</span><label class="collapse" for="c-41173839">[-]</label><label class="expand" for="c-41173839">[4 more]</label></div><br/><div class="children"><div class="content">The price decrease is particularly notable because it represents a 50% cut in the price to handle image inputs, across any OpenAI model.<p>Previously image inputs on GPT-4o-mini were priced the SAME as GPT-4o, so using mini wouldn&#x27;t actually save you any money on image analysis.<p>This new gpt-4o-2024-08-06 model is 50% cheaper than both GPT-4o AND GPT-4o-mini for image inputs, as far as I can tell.<p>UPDATE: I may be wrong about this. The pricing calculator for image inputs on <a href="https:&#x2F;&#x2F;openai.com&#x2F;api&#x2F;pricing&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;api&#x2F;pricing&#x2F;</a> doesn&#x27;t indicate any change in price for the new model.</div><br/><div id="41174929" class="c"><input type="checkbox" id="c-41174929" checked=""/><div class="controls bullet"><span class="by">jeffharris</span><span>|</span><a href="#41173839">parent</a><span>|</span><a href="#41173876">next</a><span>|</span><label class="collapse" for="c-41174929">[-]</label><label class="expand" for="c-41174929">[2 more]</label></div><br/><div class="children"><div class="content">yep image input on the new model is also 50% cheaper<p>and apologies for the outdated pricing calculator ... we&#x27;ll be updating it later today</div><br/><div id="41177298" class="c"><input type="checkbox" id="c-41177298" checked=""/><div class="controls bullet"><span class="by">brianjking</span><span>|</span><a href="#41173839">root</a><span>|</span><a href="#41174929">parent</a><span>|</span><a href="#41173876">next</a><span>|</span><label class="collapse" for="c-41177298">[-]</label><label class="expand" for="c-41177298">[1 more]</label></div><br/><div class="children"><div class="content">So we can send an image + text to the new structured output model and use the chain of thought schema?<p>I&#x27;m getting an error.<p>openai.BadRequestError: Error code: 400 - {&#x27;error&#x27;: {&#x27;message&#x27;: &#x27;Invalid content type. image_url is only supported by certain models.&#x27;, &#x27;type&#x27;: &#x27;invalid_request_error&#x27;, &#x27;param&#x27;: &#x27;messages.[1].content.[1].type&#x27;, &#x27;code&#x27;: None}}</div><br/></div></div></div></div><div id="41173876" class="c"><input type="checkbox" id="c-41173876" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#41173839">parent</a><span>|</span><a href="#41174929">prev</a><span>|</span><a href="#41173797">next</a><span>|</span><label class="collapse" for="c-41173876">[-]</label><label class="expand" for="c-41173876">[1 more]</label></div><br/><div class="children"><div class="content">The calculator doesn&#x27;t account for the fact that there are now two different prices in a given price matrix.</div><br/></div></div></div></div><div id="41173797" class="c"><input type="checkbox" id="c-41173797" checked=""/><div class="controls bullet"><span class="by">enobrev</span><span>|</span><a href="#41173839">prev</a><span>|</span><a href="#41173461">next</a><span>|</span><label class="collapse" for="c-41173797">[-]</label><label class="expand" for="c-41173797">[1 more]</label></div><br/><div class="children"><div class="content">In a startup I was working on last year, I had a surprisingly good experience with using a json-schema in my prompt.  I had to tweak the json response a bit because it was always invalid, but the issue was generally a missing colon or misplaced bracket.  Data-wise it stuck to the schema very well, and cleaning up the json was simple enough that we got to zero parsing errors.  I believe this was with 3.5.<p>Sadly, that project was a final (relatively successful) attempt at getting traction before the startup was sold and is no longer live.<p>Edit: Ouch, are the down-votes disbelief?  Annoyance?  Not sure what the problem is.</div><br/></div></div><div id="41173461" class="c"><input type="checkbox" id="c-41173461" checked=""/><div class="controls bullet"><span class="by">leetharris</span><span>|</span><a href="#41173797">prev</a><span>|</span><a href="#41175097">next</a><span>|</span><label class="collapse" for="c-41173461">[-]</label><label class="expand" for="c-41173461">[17 more]</label></div><br/><div class="children"><div class="content">At the bottom:<p>&gt;Acknowledgements
Structured Outputs takes inspiration from excellent work from the open source community: namely, the outlines, jsonformer, instructor, guidance, and lark libraries.<p>It is cool to see them acknowledge this, but it&#x27;s also lame for a company named &quot;OpenAI&quot; to acknowledge getting their ideas from open source, then contributing absolutely NOTHING back to open source with their own implementation.</div><br/><div id="41176861" class="c"><input type="checkbox" id="c-41176861" checked=""/><div class="controls bullet"><span class="by">g15jv2dp</span><span>|</span><a href="#41173461">parent</a><span>|</span><a href="#41175962">next</a><span>|</span><label class="collapse" for="c-41176861">[-]</label><label class="expand" for="c-41176861">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re unhappy about people using your work without compensating you or contributing back, don&#x27;t release your work as free software. You can&#x27;t have your cake and eat it too...</div><br/></div></div><div id="41175962" class="c"><input type="checkbox" id="c-41175962" checked=""/><div class="controls bullet"><span class="by">sirspacey</span><span>|</span><a href="#41173461">parent</a><span>|</span><a href="#41176861">prev</a><span>|</span><a href="#41173610">next</a><span>|</span><label class="collapse" for="c-41175962">[-]</label><label class="expand" for="c-41175962">[1 more]</label></div><br/><div class="children"><div class="content">You don’t anyone will use it to contribute to open source projects?<p>Seems like an obvious net gain for the community.</div><br/></div></div><div id="41173610" class="c"><input type="checkbox" id="c-41173610" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41173461">parent</a><span>|</span><a href="#41175962">prev</a><span>|</span><a href="#41173599">next</a><span>|</span><label class="collapse" for="c-41173610">[-]</label><label class="expand" for="c-41173610">[12 more]</label></div><br/><div class="children"><div class="content">Is offering gpt4o for free through chatgpt not enough of a contribution? They didn&#x27;t release source code, but they made a product free to use</div><br/><div id="41173757" class="c"><input type="checkbox" id="c-41173757" checked=""/><div class="controls bullet"><span class="by">notarobot123</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173610">parent</a><span>|</span><a href="#41173676">next</a><span>|</span><label class="collapse" for="c-41173757">[-]</label><label class="expand" for="c-41173757">[4 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t generosity, it&#x27;s a well known and much used strategy for market penetration. Free until-we-decide-otherwise is very much not the same as open source.</div><br/><div id="41174165" class="c"><input type="checkbox" id="c-41174165" checked=""/><div class="controls bullet"><span class="by">rvense</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173757">parent</a><span>|</span><a href="#41174847">next</a><span>|</span><label class="collapse" for="c-41174165">[-]</label><label class="expand" for="c-41174165">[1 more]</label></div><br/><div class="children"><div class="content">In so far as it is a conscious strategy to make it more expensive at a later data, it is actually sort of the opposite of generosity.</div><br/></div></div><div id="41174847" class="c"><input type="checkbox" id="c-41174847" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173757">parent</a><span>|</span><a href="#41174165">prev</a><span>|</span><a href="#41173676">next</a><span>|</span><label class="collapse" for="c-41174847">[-]</label><label class="expand" for="c-41174847">[2 more]</label></div><br/><div class="children"><div class="content">So if something is free but only temporarily, then that cancels out the generosity? Also, you and I have no idea how long the features will remain free. If anything, chatgpt has been making <i>more</i> features and stronger models free over  time.</div><br/><div id="41175017" class="c"><input type="checkbox" id="c-41175017" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41174847">parent</a><span>|</span><a href="#41173676">next</a><span>|</span><label class="collapse" for="c-41175017">[-]</label><label class="expand" for="c-41175017">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes it does, yeah. It&#x27;s not unheard of for companies to deliberately operate at a loss in order to drive out their competition, then raise prices again. This is known as &quot;predatory pricing&quot;.</div><br/></div></div></div></div></div></div><div id="41173676" class="c"><input type="checkbox" id="c-41173676" checked=""/><div class="controls bullet"><span class="by">mplewis</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173610">parent</a><span>|</span><a href="#41173757">prev</a><span>|</span><a href="#41173779">next</a><span>|</span><label class="collapse" for="c-41173676">[-]</label><label class="expand" for="c-41173676">[2 more]</label></div><br/><div class="children"><div class="content">No. If it were free you&#x27;d be able to use it as a programming API. It&#x27;s not free and it&#x27;s not unlimited - it&#x27;s a time-limited marketing tool.</div><br/><div id="41174824" class="c"><input type="checkbox" id="c-41174824" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173676">parent</a><span>|</span><a href="#41173779">next</a><span>|</span><label class="collapse" for="c-41174824">[-]</label><label class="expand" for="c-41174824">[1 more]</label></div><br/><div class="children"><div class="content">How are you defining the word free?</div><br/></div></div></div></div><div id="41173779" class="c"><input type="checkbox" id="c-41173779" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173610">parent</a><span>|</span><a href="#41173676">prev</a><span>|</span><a href="#41173736">next</a><span>|</span><label class="collapse" for="c-41173779">[-]</label><label class="expand" for="c-41173779">[4 more]</label></div><br/><div class="children"><div class="content">That can actually make competition from open source <i>harder</i>. New upstarts that are open source can&#x27;t compete with free service from OpenAI and can&#x27;t make money to grow their development or offerings.<p>OpenAI wants to kill everything that isn&#x27;t OpenAI.</div><br/><div id="41174502" class="c"><input type="checkbox" id="c-41174502" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173779">parent</a><span>|</span><a href="#41174859">next</a><span>|</span><label class="collapse" for="c-41174502">[-]</label><label class="expand" for="c-41174502">[1 more]</label></div><br/><div class="children"><div class="content">New open source models* still wouldn&#x27;t be able to compete even if OpenAI was forcibly shut down.<p>Hardware&#x27;s too expensive, and will be for a while, because <i>all</i> the big players are trying to get in on it.<p>* cue arguments: &quot;&#x27;open weights&#x27; or training data&#x27;?&quot;; &quot;does the Meta offering count or are they being sneaky and evil?&quot;; etc.</div><br/></div></div><div id="41174859" class="c"><input type="checkbox" id="c-41174859" checked=""/><div class="controls bullet"><span class="by">spencerchubb</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173779">parent</a><span>|</span><a href="#41174502">prev</a><span>|</span><a href="#41173736">next</a><span>|</span><label class="collapse" for="c-41174859">[-]</label><label class="expand" for="c-41174859">[2 more]</label></div><br/><div class="children"><div class="content">So should OpenAI make their product less accessible, in order to make it easier for competition? That makes no sense</div><br/><div id="41175207" class="c"><input type="checkbox" id="c-41175207" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41174859">parent</a><span>|</span><a href="#41173736">next</a><span>|</span><label class="collapse" for="c-41175207">[-]</label><label class="expand" for="c-41175207">[1 more]</label></div><br/><div class="children"><div class="content">I call chicken. Let them make all their products paid.<p>Hint: they won&#x27;t, it would kill their company. The hype around OpenAI is based on people using it for free, at least at the start.<p>Heck, even drug dealers know this trick!</div><br/></div></div></div></div></div></div><div id="41173736" class="c"><input type="checkbox" id="c-41173736" checked=""/><div class="controls bullet"><span class="by">talldayo</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173610">parent</a><span>|</span><a href="#41173779">prev</a><span>|</span><a href="#41173599">next</a><span>|</span><label class="collapse" for="c-41173736">[-]</label><label class="expand" for="c-41173736">[1 more]</label></div><br/><div class="children"><div class="content">Free service != Open software</div><br/></div></div></div></div><div id="41173599" class="c"><input type="checkbox" id="c-41173599" checked=""/><div class="controls bullet"><span class="by">warkdarrior</span><span>|</span><a href="#41173461">parent</a><span>|</span><a href="#41173610">prev</a><span>|</span><a href="#41175097">next</a><span>|</span><label class="collapse" for="c-41173599">[-]</label><label class="expand" for="c-41173599">[2 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s also lame for a company named &quot;OpenAI&quot; to acknowledge getting their ideas from open source, then contributing absolutely NOTHING back to open source with their own implementation<p>Maybe those projects were used as-is by OpenAI, so there was nothing new to contribute.</div><br/><div id="41173666" class="c"><input type="checkbox" id="c-41173666" checked=""/><div class="controls bullet"><span class="by">reustle</span><span>|</span><a href="#41173461">root</a><span>|</span><a href="#41173599">parent</a><span>|</span><a href="#41175097">next</a><span>|</span><label class="collapse" for="c-41173666">[-]</label><label class="expand" for="c-41173666">[1 more]</label></div><br/><div class="children"><div class="content">I think they may be alluding to sponsorships as well as code contributions.<p>i.e. <a href="https:&#x2F;&#x2F;github.com&#x2F;sponsors&#x2F;jxnl">https:&#x2F;&#x2F;github.com&#x2F;sponsors&#x2F;jxnl</a></div><br/></div></div></div></div></div></div><div id="41175097" class="c"><input type="checkbox" id="c-41175097" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#41173461">prev</a><span>|</span><a href="#41174877">next</a><span>|</span><label class="collapse" for="c-41175097">[-]</label><label class="expand" for="c-41175097">[1 more]</label></div><br/><div class="children"><div class="content">Is the JSON actually being fed into the LLM&#x27;s context or is it still being converted into typescript?<p>The previous setup didn&#x27;t allow for custom types, only objects&#x2F;string&#x2F;num&#x2F;bool.<p>Are the enums put into context or purely used for constrained sampling?</div><br/></div></div><div id="41174877" class="c"><input type="checkbox" id="c-41174877" checked=""/><div class="controls bullet"><span class="by">tarofchaos</span><span>|</span><a href="#41175097">prev</a><span>|</span><a href="#41174869">next</a><span>|</span><label class="collapse" for="c-41174877">[-]</label><label class="expand" for="c-41174877">[1 more]</label></div><br/><div class="children"><div class="content">Two years too late. I think we are going through a bozo period at OpenAI where small things are being highlighted as achievements.</div><br/></div></div><div id="41174869" class="c"><input type="checkbox" id="c-41174869" checked=""/><div class="controls bullet"><span class="by">agtech_andy</span><span>|</span><a href="#41174877">prev</a><span>|</span><a href="#41174338">next</a><span>|</span><label class="collapse" for="c-41174869">[-]</label><label class="expand" for="c-41174869">[2 more]</label></div><br/><div class="children"><div class="content">I have had a lot of success using BoundaryML (<a href="https:&#x2F;&#x2F;www.boundaryml.com&#x2F;">https:&#x2F;&#x2F;www.boundaryml.com&#x2F;</a>) for this. They have also been super responsive for any of my questions.</div><br/><div id="41175461" class="c"><input type="checkbox" id="c-41175461" checked=""/><div class="controls bullet"><span class="by">aaronvg</span><span>|</span><a href="#41174869">parent</a><span>|</span><a href="#41174338">next</a><span>|</span><label class="collapse" for="c-41175461">[-]</label><label class="expand" for="c-41175461">[1 more]</label></div><br/><div class="children"><div class="content">thanks for the shoutout, we benchmarked our approach against other function-calling techniques and we&#x27;ve been able to beat all other approaches every time (even by 8%!) just by getting better at parsing the data and representing schemas with less tokens using type definitions instead of json schema.<p>You can take a look at our BFCL results on that site or the github: <a href="https:&#x2F;&#x2F;github.com&#x2F;BoundaryML&#x2F;baml">https:&#x2F;&#x2F;github.com&#x2F;BoundaryML&#x2F;baml</a><p>We&#x27;ll be publishing our comparison against OpenAI structured outputs in the next 2 days, and a deeper dive into our results, but we aim to include this kind of constrained generation as a capability in the BAML DSL anyway longterm!</div><br/></div></div></div></div><div id="41174338" class="c"><input type="checkbox" id="c-41174338" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#41174869">prev</a><span>|</span><a href="#41174301">next</a><span>|</span><label class="collapse" for="c-41174338">[-]</label><label class="expand" for="c-41174338">[8 more]</label></div><br/><div class="children"><div class="content">Totally tangential, totally not related to the post (unless you squint your eyes and really blur things) ...<p>I was thinking about the old canard of the sufficiently smart compiler. It made me think about LLM output and how in some way the output of a LLM could be bytecode as much as it could be the English language. You have a tokenized input and the translated output. You have a massive and easily generatable training set. I wonder if, one day, our compilers will be LLMs?</div><br/><div id="41174879" class="c"><input type="checkbox" id="c-41174879" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#41174338">parent</a><span>|</span><a href="#41176713">next</a><span>|</span><label class="collapse" for="c-41174879">[-]</label><label class="expand" for="c-41174879">[1 more]</label></div><br/><div class="children"><div class="content">A function that implements natural language -&gt; bytecode is IMO way more likely to be under the hood an LLM <i>operating a compiler</i> (or maybe a compiler operating LLMs) rather than a &quot;bare&quot; LLM. From an end user&#x27;s perspective maybe it won&#x27;t matter but I think it&#x27;s an important technical point. IMO there&#x27;s no evidence that an LLM will ever be the best way to execute general purpose computations.</div><br/></div></div><div id="41176713" class="c"><input type="checkbox" id="c-41176713" checked=""/><div class="controls bullet"><span class="by">shepherdjerred</span><span>|</span><a href="#41174338">parent</a><span>|</span><a href="#41174879">prev</a><span>|</span><a href="#41174362">next</a><span>|</span><label class="collapse" for="c-41176713">[-]</label><label class="expand" for="c-41176713">[1 more]</label></div><br/><div class="children"><div class="content">Compilers require strict semantics and deterministic output. It’s the exact opposite of AI.<p>I could see AI being used (in a deterministic way) to make decisions about what optimizations to apply, to improve error messages, or make languages easier to use&#x2F;reason about, but not for the frontend&#x2F;backend&#x2F;optimizations themselves.</div><br/></div></div><div id="41174362" class="c"><input type="checkbox" id="c-41174362" checked=""/><div class="controls bullet"><span class="by">pjc50</span><span>|</span><a href="#41174338">parent</a><span>|</span><a href="#41176713">prev</a><span>|</span><a href="#41174357">next</a><span>|</span><label class="collapse" for="c-41174362">[-]</label><label class="expand" for="c-41174362">[3 more]</label></div><br/><div class="children"><div class="content">Why would you tolerate a nonreliable compiler with no assured relationship between its inputs and its outputs? Have people just got too comfortable with the C++ model of &quot;UB means I can insert a security bug for you&quot;?</div><br/><div id="41174573" class="c"><input type="checkbox" id="c-41174573" checked=""/><div class="controls bullet"><span class="by">bigyikes</span><span>|</span><a href="#41174338">root</a><span>|</span><a href="#41174362">parent</a><span>|</span><a href="#41174357">next</a><span>|</span><label class="collapse" for="c-41174573">[-]</label><label class="expand" for="c-41174573">[2 more]</label></div><br/><div class="children"><div class="content">In a hypothetical future where the reliability of LLMs improves, I can imagine the model being able to craft optimizations that a traditional compiler cannot.<p>Like there are already cases where hand-rolling assembly can eke out performance gains, but few do that because it’s so arduous. If the LLM could do it reliably it’d be a huge win.<p>It’s a big if, but not outside the realm of possibility.</div><br/><div id="41174669" class="c"><input type="checkbox" id="c-41174669" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#41174338">root</a><span>|</span><a href="#41174573">parent</a><span>|</span><a href="#41174357">next</a><span>|</span><label class="collapse" for="c-41174669">[-]</label><label class="expand" for="c-41174669">[1 more]</label></div><br/><div class="children"><div class="content">I agree it is currently a pipe dream. But if I was looking for a doctoral research idea, it might be fun to work on something like that.<p>Lots of potential avenues to explore, e.g. going from a high-level language to some IR, from some IR to bytecode, or straight from high-level to machine code.<p>I mean, -O3 is already so much of a black box that I can&#x27;t understand it. And the tedium of hand optimizing massive chunks of code is why we automate it at all. Boredom is something we don&#x27;t expect LLMs to suffer, so having one pore over some kind of representation and apply optimizations seems totally reasonable. And if it had some kinds of &quot;emergent behaviors&quot; based on intelligence that allow it to beat the suite of algorithmic optimization we program into compilers, it could actually be a benefit.</div><br/></div></div></div></div></div></div><div id="41174357" class="c"><input type="checkbox" id="c-41174357" checked=""/><div class="controls bullet"><span class="by">jcims</span><span>|</span><a href="#41174338">parent</a><span>|</span><a href="#41174362">prev</a><span>|</span><a href="#41175413">next</a><span>|</span><label class="collapse" for="c-41174357">[-]</label><label class="expand" for="c-41174357">[1 more]</label></div><br/><div class="children"><div class="content">You definitely could, not far removed from text to image or text to audio generators.</div><br/></div></div><div id="41175413" class="c"><input type="checkbox" id="c-41175413" checked=""/><div class="controls bullet"><span class="by">thih9</span><span>|</span><a href="#41174338">parent</a><span>|</span><a href="#41174357">prev</a><span>|</span><a href="#41174301">next</a><span>|</span><label class="collapse" for="c-41175413">[-]</label><label class="expand" for="c-41175413">[1 more]</label></div><br/><div class="children"><div class="content">I guess an actual compiler would be cheaper and more reliable.<p>In theory we could do the same with mathematical computations, 2+2=4 and the like; but computing the result seems easier.</div><br/></div></div></div></div><div id="41174301" class="c"><input type="checkbox" id="c-41174301" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41174338">prev</a><span>|</span><a href="#41178626">next</a><span>|</span><label class="collapse" for="c-41174301">[-]</label><label class="expand" for="c-41174301">[8 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve had this for over 1 year in Lamini - <a href="https:&#x2F;&#x2F;lamini-ai.github.io&#x2F;inference&#x2F;json_output&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lamini-ai.github.io&#x2F;inference&#x2F;json_output&#x2F;</a><p>Works with any open LLM, including Llama 3.1</div><br/><div id="41174384" class="c"><input type="checkbox" id="c-41174384" checked=""/><div class="controls bullet"><span class="by">AStrangeMorrow</span><span>|</span><a href="#41174301">parent</a><span>|</span><a href="#41174547">next</a><span>|</span><label class="collapse" for="c-41174384">[-]</label><label class="expand" for="c-41174384">[3 more]</label></div><br/><div class="children"><div class="content">Also the outlines library: <a href="https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines">https:&#x2F;&#x2F;github.com&#x2F;outlines-dev&#x2F;outlines</a></div><br/><div id="41175147" class="c"><input type="checkbox" id="c-41175147" checked=""/><div class="controls bullet"><span class="by">HanClinto</span><span>|</span><a href="#41174301">root</a><span>|</span><a href="#41174384">parent</a><span>|</span><a href="#41174420">next</a><span>|</span><label class="collapse" for="c-41175147">[-]</label><label class="expand" for="c-41175147">[1 more]</label></div><br/><div class="children"><div class="content">Also note llama.cpp with grammar support:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;grammars">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;tree&#x2F;master&#x2F;grammars</a><p>Supports an EBNF-like syntax, as well as JSON-Schema.</div><br/></div></div><div id="41174420" class="c"><input type="checkbox" id="c-41174420" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41174301">root</a><span>|</span><a href="#41174384">parent</a><span>|</span><a href="#41175147">prev</a><span>|</span><a href="#41174547">next</a><span>|</span><label class="collapse" for="c-41174420">[-]</label><label class="expand" for="c-41174420">[1 more]</label></div><br/><div class="children"><div class="content">Yeah! - outlines, guidance, jsonformer were inspiring for this line of work</div><br/></div></div></div></div><div id="41174547" class="c"><input type="checkbox" id="c-41174547" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#41174301">parent</a><span>|</span><a href="#41174384">prev</a><span>|</span><a href="#41174322">next</a><span>|</span><label class="collapse" for="c-41174547">[-]</label><label class="expand" for="c-41174547">[2 more]</label></div><br/><div class="children"><div class="content">Why not JSON Schema?</div><br/><div id="41175204" class="c"><input type="checkbox" id="c-41175204" checked=""/><div class="controls bullet"><span class="by">gdiamos</span><span>|</span><a href="#41174301">root</a><span>|</span><a href="#41174547">parent</a><span>|</span><a href="#41174322">next</a><span>|</span><label class="collapse" for="c-41175204">[-]</label><label class="expand" for="c-41175204">[1 more]</label></div><br/><div class="children"><div class="content">We did some user studies and found that people found it less intuitive.</div><br/></div></div></div></div><div id="41174322" class="c"><input type="checkbox" id="c-41174322" checked=""/><div class="controls bullet"><span class="by">radarsat1</span><span>|</span><a href="#41174301">parent</a><span>|</span><a href="#41174547">prev</a><span>|</span><a href="#41174525">next</a><span>|</span><label class="collapse" for="c-41174322">[-]</label><label class="expand" for="c-41174322">[1 more]</label></div><br/><div class="children"><div class="content">Looks useful!</div><br/></div></div></div></div><div id="41178626" class="c"><input type="checkbox" id="c-41178626" checked=""/><div class="controls bullet"><span class="by">franze</span><span>|</span><a href="#41174301">prev</a><span>|</span><a href="#41173636">next</a><span>|</span><label class="collapse" for="c-41178626">[-]</label><label class="expand" for="c-41178626">[1 more]</label></div><br/><div class="children"><div class="content">finally<p>at<p>gpt.franzai.com<p>we use a 3 times check until now<p>if chatgpt does not return valid json then trim anything before and after last {}<p>if this is not valid json 
feedback him full response and ask for valid json<p>if this is not valid json put full response into self made json to get al least something we can work with in an &quot;something did not work out&quot; response</div><br/></div></div><div id="41173636" class="c"><input type="checkbox" id="c-41173636" checked=""/><div class="controls bullet"><span class="by">_vaporwave_</span><span>|</span><a href="#41178626">prev</a><span>|</span><a href="#41174619">next</a><span>|</span><label class="collapse" for="c-41173636">[-]</label><label class="expand" for="c-41173636">[2 more]</label></div><br/><div class="children"><div class="content">Anyone else catch this reference in one of the examples?<p>&gt; 9.11 and 9.9 -- which is bigger<p><a href="https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;why-9-11-is-larger-than-9-9-incredible&#x2F;869824" rel="nofollow">https:&#x2F;&#x2F;community.openai.com&#x2F;t&#x2F;why-9-11-is-larger-than-9-9-i...</a></div><br/><div id="41173712" class="c"><input type="checkbox" id="c-41173712" checked=""/><div class="controls bullet"><span class="by">jodacola</span><span>|</span><a href="#41173636">parent</a><span>|</span><a href="#41174619">next</a><span>|</span><label class="collapse" for="c-41173712">[-]</label><label class="expand" for="c-41173712">[1 more]</label></div><br/><div class="children"><div class="content">Amusingly, I immediately thought 9.11 - but in the context of a newer version of software. Ever have those moments where you&#x27;re so deep in context of some ecosystem that you skip right past the basics, like 9.9 being a larger number than 9.11?</div><br/></div></div></div></div><div id="41174619" class="c"><input type="checkbox" id="c-41174619" checked=""/><div class="controls bullet"><span class="by">ramoz</span><span>|</span><a href="#41173636">prev</a><span>|</span><label class="collapse" for="c-41174619">[-]</label><label class="expand" for="c-41174619">[4 more]</label></div><br/><div class="children"><div class="content">Can someone explain how this is different&#x2F;better than the current state of function calling (which I’ve been using to get a consistent json schema response without issue)?</div><br/><div id="41174709" class="c"><input type="checkbox" id="c-41174709" checked=""/><div class="controls bullet"><span class="by">jacobsimon</span><span>|</span><a href="#41174619">parent</a><span>|</span><a href="#41174691">next</a><span>|</span><label class="collapse" for="c-41174709">[-]</label><label class="expand" for="c-41174709">[1 more]</label></div><br/><div class="children"><div class="content">For starters, the naming is much less confusing. But the behavior also appears to be enforced&#x2F;validated at some layer (hopefully?), which function calling did not seem to be. I was experimenting with it a couple weeks ago and it would work like 75% of the time but would often give me invalid results for schemas with relatively simple nested objects.</div><br/></div></div><div id="41174691" class="c"><input type="checkbox" id="c-41174691" checked=""/><div class="controls bullet"><span class="by">mrshu</span><span>|</span><a href="#41174619">parent</a><span>|</span><a href="#41174709">prev</a><span>|</span><a href="#41174721">next</a><span>|</span><label class="collapse" for="c-41174691">[-]</label><label class="expand" for="c-41174691">[1 more]</label></div><br/><div class="children"><div class="content">This post (from an OpenAI researcher) contains a bit more background: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41174213">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41174213</a></div><br/></div></div><div id="41174721" class="c"><input type="checkbox" id="c-41174721" checked=""/><div class="controls bullet"><span class="by">zbyforgotp</span><span>|</span><a href="#41174619">parent</a><span>|</span><a href="#41174691">prev</a><span>|</span><label class="collapse" for="c-41174721">[-]</label><label class="expand" for="c-41174721">[1 more]</label></div><br/><div class="children"><div class="content">This is guaranteed, function calling without it is not. The old way can work for you, but my experience is different, especially with complex schemas.</div><br/></div></div></div></div></div></div></div></div></div></body></html>