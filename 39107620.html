<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1706086859910" as="style"/><link rel="stylesheet" href="styles.css?v=1706086859910"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://crowsonkb.github.io/hourglass-diffusion-transformers/">Direct pixel-space megapixel image generation with diffusion models</a> <span class="domain">(<a href="https://crowsonkb.github.io">crowsonkb.github.io</a>)</span></div><div class="subtext"><span>stefanbaumann</span> | <span>43 comments</span></div><br/><div><div id="39110492" class="c"><input type="checkbox" id="c-39110492" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39111839">next</a><span>|</span><label class="collapse" for="c-39110492">[-]</label><label class="expand" for="c-39110492">[11 more]</label></div><br/><div class="children"><div class="content">I&#x27;m one of the authors; happy to answer questions.
this arch is of course nice for high-resolution synthesis, but there&#x27;s some other cool stuff worth mentioning..<p>activations are small! so you can enjoy bigger batch sizes. this is due to the 4x patching we do on the ingress to the model, and the effectiveness of neighbourhood attention in joining patches at the seams.<p>the model&#x27;s inductive biases are pretty different than (for example) a convolutional UNet&#x27;s. the innermost levels seem to train easily, so images can have good global coherence early in training.<p>there&#x27;s no convolutions! so you don&#x27;t need to worry about artifacts stemming from convolution padding, or having canvas edge padding artifacts leak an implicit position bias.<p>we can finally see what high-resolution diffusion outputs look like _without_ latents! personally I think current latent VAEs don&#x27;t _really_ achieve the high resolutions they claim (otherwise fine details like text would survive a VAE roundtrip faithfully); it&#x27;s common to see latent diffusion outputs with smudgy skin or blurry fur. what I&#x27;d like to see in the future of latent diffusion is to listen to the Emu paper and use more channels, or a less ambitious upsample.<p>it&#x27;s a transformer! so we can try applying to it everything we know about transformers, like sigma reparameterisation or multimodality. some tricks like masked training will require extra support in [NATTEN](<a href="https:&#x2F;&#x2F;github.com&#x2F;SHI-Labs&#x2F;NATTEN">https:&#x2F;&#x2F;github.com&#x2F;SHI-Labs&#x2F;NATTEN</a>), but we&#x27;re very happy with its featureset and performance so far.<p>but honestly I&#x27;m most excited about the efficiency. there&#x27;s too little work on making pretraining possible at GPU-poor scale. so I was very happy to see HDiT could succeed at small-scale tasks within the resources I had at home (you can get nice oxford flowers samples at 256x256px with half an hour on a 4090). I think with models that are better fits for the problem, perhaps we can get good results with smaller models. and I&#x27;d like to see big tech go that direction too!<p>-Alex Birch</div><br/><div id="39114904" class="c"><input type="checkbox" id="c-39114904" checked=""/><div class="controls bullet"><span class="by">bertdb</span><span>|</span><a href="#39110492">parent</a><span>|</span><a href="#39111363">next</a><span>|</span><label class="collapse" for="c-39114904">[-]</label><label class="expand" for="c-39114904">[1 more]</label></div><br/><div class="children"><div class="content">Did you do any inpainting experiments? I can imagine a pixel-space diffusion model to be better at it than one with a latent auto-encoder.</div><br/></div></div><div id="39111363" class="c"><input type="checkbox" id="c-39111363" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39110492">parent</a><span>|</span><a href="#39114904">prev</a><span>|</span><a href="#39111335">next</a><span>|</span><label class="collapse" for="c-39111363">[-]</label><label class="expand" for="c-39111363">[5 more]</label></div><br/><div class="children"><div class="content">Hi Alex
Amazing work. I scanned the paper and dusted off my aging memories of Jeremy Howard’s course. Will your model live happily alongside the existing SD infrastructure such as ControlNet, IPAdapter, and the like? Obviously we will have to retrain these to fit onto your model, but conceptually, does your model have natural places where adapters of various kinds can be attached?</div><br/><div id="39111637" class="c"><input type="checkbox" id="c-39111637" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39110492">root</a><span>|</span><a href="#39111363">parent</a><span>|</span><a href="#39112206">next</a><span>|</span><label class="collapse" for="c-39111637">[-]</label><label class="expand" for="c-39111637">[1 more]</label></div><br/><div class="children"><div class="content">regarding ControlNet:  
we have a UNet backbone, so the idea of &quot;make trainable copies of the encoder blocks&quot; sounds possible. the other part,  &quot;use a zero-inited dense layer to project the peer-encoder output and add it to the frozen-decoder output&quot; also sounds fine. not quite sure what they do with the mid-block but I doubt there&#x27;d be any problem there.<p>regarding IPAdapter:  
I&#x27;m not familiar with it, but from the code it looks like they just run cross-attention again and sum the two attention outputs. feels a bit weird to me, because the attention probabilities add up to 2 instead of 1. and they scale the bonus attention output only instead of lerping. it&#x27;d make more sense to me to formulate it as a cross-cross attention (Q against cat([key0, key1]) and cat([val0, val1])), but maybe they wanted it to begin as a no-op at the start of training or something.  
anyway.. yes, all of that should work fine with HDiT. the paper doesn&#x27;t implement cross-attention, but it can be added in the standard way (e.g. like stable-diffusion) or as self-cross attention (e.g. DeepFloyd IF or Imagen).<p>I&#x27;d recommend though to make use of HDiT&#x27;s mapping network. in our attention blocks, the input gets AdaNormed against the condition from the mapping network. this is currently used to convey stuff like class conditions, Karras augmentation conditions and timestep embeddings. but it supports conditioning on custom (single-token) conditions of your choosing. so you could use this to condition on an image embed (this would give you the same image-conditioning control as IPAdapter but via a simpler mechanism).</div><br/></div></div><div id="39112206" class="c"><input type="checkbox" id="c-39112206" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#39110492">root</a><span>|</span><a href="#39111363">parent</a><span>|</span><a href="#39111637">prev</a><span>|</span><a href="#39111335">next</a><span>|</span><label class="collapse" for="c-39112206">[-]</label><label class="expand" for="c-39112206">[3 more]</label></div><br/><div class="children"><div class="content">IPAdapter, I am curious if there are useful GUIs for this? Creating image masks through uploading to colab is not so cute.</div><br/><div id="39112327" class="c"><input type="checkbox" id="c-39112327" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#39110492">root</a><span>|</span><a href="#39112206">parent</a><span>|</span><a href="#39111335">next</a><span>|</span><label class="collapse" for="c-39112327">[-]</label><label class="expand" for="c-39112327">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s one example: <a href="https:&#x2F;&#x2F;github.com&#x2F;Acly&#x2F;krita-ai-diffusion&#x2F;">https:&#x2F;&#x2F;github.com&#x2F;Acly&#x2F;krita-ai-diffusion&#x2F;</a><p>But generally, most other UIs support it. It has serious limitations though, for example it center-crops the input to 224x224px. (which is enough for a surprisingly large amount of uses, but not enough for many others)</div><br/><div id="39112666" class="c"><input type="checkbox" id="c-39112666" checked=""/><div class="controls bullet"><span class="by">ttul</span><span>|</span><a href="#39110492">root</a><span>|</span><a href="#39112327">parent</a><span>|</span><a href="#39111335">next</a><span>|</span><label class="collapse" for="c-39112666">[-]</label><label class="expand" for="c-39112666">[1 more]</label></div><br/><div class="children"><div class="content">Yes. I discussed this issue with the author of the ComfyUI IP-Adapter nodes. It would doubtless be handy if someone could end-to-end train a higher resolution IP-Adapter model that integrated its own variant of CLIPVision that is not subject to the 224px constraint. I have no idea what kind of horsepower would be required for that.<p>A latent space CLIPVision model would be cool too. Presumably you could leverage the semantic richness of the latent space to efficiently train a more powerful CLIPVision. I don’t know whether anyone has tried this. Maybe there is a good reason for that.</div><br/></div></div></div></div></div></div></div></div><div id="39111335" class="c"><input type="checkbox" id="c-39111335" checked=""/><div class="controls bullet"><span class="by">sophrocyne</span><span>|</span><a href="#39110492">parent</a><span>|</span><a href="#39111363">prev</a><span>|</span><a href="#39114588">next</a><span>|</span><label class="collapse" for="c-39111335">[-]</label><label class="expand" for="c-39111335">[3 more]</label></div><br/><div class="children"><div class="content">Alex - I run Invoke (one of the popular OSS SD UIs for pros)<p>Thanks for your work - it’s been impactful since the early days of the project.<p>Excited to see where we get to this year.</div><br/><div id="39111721" class="c"><input type="checkbox" id="c-39111721" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39110492">root</a><span>|</span><a href="#39111335">parent</a><span>|</span><a href="#39114588">next</a><span>|</span><label class="collapse" for="c-39111721">[-]</label><label class="expand" for="c-39111721">[2 more]</label></div><br/><div class="children"><div class="content">ah, originally lstein&#x2F;stable-diffusion? yeah that was an important fork for us Mac users in the early days. I have to confess I&#x27;ve still never used a UI. :)<p>this year I&#x27;m hoping for efficiency and small models! even if it&#x27;s proprietary. if our work can reduce some energy usage behind closed doors that&#x27;d still be a good outcome.</div><br/><div id="39113311" class="c"><input type="checkbox" id="c-39113311" checked=""/><div class="controls bullet"><span class="by">sophrocyne</span><span>|</span><a href="#39110492">root</a><span>|</span><a href="#39111721">parent</a><span>|</span><a href="#39114588">next</a><span>|</span><label class="collapse" for="c-39113311">[-]</label><label class="expand" for="c-39113311">[1 more]</label></div><br/><div class="children"><div class="content">Yes, indeed. Lincoln&#x27;s still an active maintainer.<p>Energy efficiency is key - Especially with some of these extremely inefficient (wasteful, even) features like real-time canvas.<p>Good luck - Let us know if&#x2F;how we can help.</div><br/></div></div></div></div></div></div></div></div><div id="39111839" class="c"><input type="checkbox" id="c-39111839" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#39110492">prev</a><span>|</span><a href="#39109434">next</a><span>|</span><label class="collapse" for="c-39111839">[-]</label><label class="expand" for="c-39111839">[2 more]</label></div><br/><div class="children"><div class="content">Seems like a solid paper from a skim through it.  My rough summary:<p>The popular large scale diffusion models like StableDiffusion are CNN based at their heart, with attention layers sprinkled throughout.  This paper builds on recent research exploring whether competitive image diffusion models can be built out of purely transformers, no CNN layers.<p>In this paper they build a similar U-Net like structure, but out of transformer layers, to improve efficiency compared to a straight Transformer.  They also use local attention when the resolution is high to save on computational cost, but regular global attention in the middle to maintain global coherence.<p>Based on ablation studies this allows them to maintain or slightly improve FID score compared to Transformer-only diffusion models that don&#x27;t do U-net like structures, but at 1&#x2F;10th the computation cost.  An incredible feat for sure.<p>There is a variety of details: RoPE positional encoding, GEGELU activations, RMSNorm, learnable skip connections, learnable cosine-sim attention, neighborhood attention for the local attention, etc.<p>The biggest gains in FID occur when the authors use &quot;soft-min-snr&quot; as the loss function; FID drops from 41 to 28!<p>Lots of ablation study was done across all their changes (see Table 1).<p>Training is otherwise completely standard AdamW, 5e-4, 0.01, 256 batch, constant LR, 400k steps for most experiments at 128x128 resolution.<p>So yeah, overall seems like solid work that combines a great mixture of techniques and pushes Transformer based diffusion forward.<p>If scaled up I&#x27;m not sure it would be &quot;revolutionary&quot; in terms of FID compared to SDXL or DALLE3, mostly because SD and DALLE already use attention obviating the scaling issue, and lots of tricks like diffusion based VAEs.  But it&#x27;s likely to provide a nice incremental improvement in FID, since in general Transformers perform better than CNNs unless the CNNs are _heavily_ tuned.<p>And being pixel based rather than latent based has many advantages.</div><br/><div id="39111994" class="c"><input type="checkbox" id="c-39111994" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39111839">parent</a><span>|</span><a href="#39109434">next</a><span>|</span><label class="collapse" for="c-39111994">[-]</label><label class="expand" for="c-39111994">[1 more]</label></div><br/><div class="children"><div class="content">FID doesn&#x27;t reward high-resolution detail. the inception feature size is 299x299! so we are forced to <i>downsample</i> our FFHQ-1024 samples to compute FID.<p>it also doesn&#x27;t punish poor detail either! this advantages latent diffusion, which can claim to achieve a high resolution but without actually needing to have correct textures to get good metrics.</div><br/></div></div></div></div><div id="39109434" class="c"><input type="checkbox" id="c-39109434" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#39111839">prev</a><span>|</span><a href="#39111461">next</a><span>|</span><label class="collapse" for="c-39109434">[-]</label><label class="expand" for="c-39109434">[7 more]</label></div><br/><div class="children"><div class="content">I enjoyed this paper (I share a discord with the author so I read it a bit earlier).<p>It&#x27;s not entirely clear from the comparison numbers at the end, but I think the big argument here is efficiency for the amount of performance achieved. One can get lower FID numbers, but also with a ton of compute.<p>I can&#x27;t really speak technically to it as I&#x27;ve not given it a super in depth look, but this seems like a nice set of motifs for going halfway between a standard attention network and a convnet in terms of compute cost (and maybe performance)?<p>The large-resolution scaling seems to be a strong suit as a result. :)</div><br/><div id="39109732" class="c"><input type="checkbox" id="c-39109732" checked=""/><div class="controls bullet"><span class="by">stefanbaumann</span><span>|</span><a href="#39109434">parent</a><span>|</span><a href="#39109854">next</a><span>|</span><label class="collapse" for="c-39109732">[-]</label><label class="expand" for="c-39109732">[1 more]</label></div><br/><div class="children"><div class="content">Thanks a lot!<p>Yeah, the main motivation was trying to find a way to enable transformers to do high-resolution image synthesis: transformers are known to scale well to extreme, multi-billion parameter scales and typically offer superior coherency &amp; composition in image generation, but current architectures are too expensive to train at scale for high-resolution inputs.<p>By using a hierarchical architecture and local attention at high-resolution scales (but retaining global attention at low-resolution scales), it becomes viable to apply transformers at these scales.
Additionally, this architecture can now directly be trained on megapixel-scale inputs and generate high-quality results without having to progressively grow the resolution over the training or applying other &quot;tricks&quot; typically needed to make models at these resolutions work well.</div><br/></div></div><div id="39109854" class="c"><input type="checkbox" id="c-39109854" checked=""/><div class="controls bullet"><span class="by">nwoli</span><span>|</span><a href="#39109434">parent</a><span>|</span><a href="#39109732">prev</a><span>|</span><a href="#39111461">next</a><span>|</span><label class="collapse" for="c-39109854">[-]</label><label class="expand" for="c-39109854">[5 more]</label></div><br/><div class="children"><div class="content">Which discord if its open to the public? I was on one woth kath in 2021 and loved her insights, would love to again</div><br/><div id="39114392" class="c"><input type="checkbox" id="c-39114392" checked=""/><div class="controls bullet"><span class="by">SEGyges</span><span>|</span><a href="#39109434">root</a><span>|</span><a href="#39109854">parent</a><span>|</span><a href="#39111890">next</a><span>|</span><label class="collapse" for="c-39114392">[-]</label><label class="expand" for="c-39114392">[1 more]</label></div><br/><div class="children"><div class="content">You and the guy below you in this thread should probably tag me on twitter, same tag as here, I can point you. I do not especially want to leave the discord link in a frontpage hn thread.</div><br/></div></div><div id="39111890" class="c"><input type="checkbox" id="c-39111890" checked=""/><div class="controls bullet"><span class="by">fpgaminer</span><span>|</span><a href="#39109434">root</a><span>|</span><a href="#39109854">parent</a><span>|</span><a href="#39114392">prev</a><span>|</span><a href="#39111461">next</a><span>|</span><label class="collapse" for="c-39111890">[-]</label><label class="expand" for="c-39111890">[3 more]</label></div><br/><div class="children"><div class="content">Same; a good ML focused discord would be great.  Training ViTs all day is lonely work.  I&#x27;m mostly locked into skimming the &quot;Research&quot; channels of image generation discords.  LAION used to be decent with a good amount of interesting discussion, but it seems to have devolved into toxicity in the last year.</div><br/><div id="39114396" class="c"><input type="checkbox" id="c-39114396" checked=""/><div class="controls bullet"><span class="by">SEGyges</span><span>|</span><a href="#39109434">root</a><span>|</span><a href="#39111890">parent</a><span>|</span><a href="#39112474">next</a><span>|</span><label class="collapse" for="c-39114396">[-]</label><label class="expand" for="c-39114396">[1 more]</label></div><br/><div class="children"><div class="content">See my other comment replying to that.</div><br/></div></div><div id="39112474" class="c"><input type="checkbox" id="c-39112474" checked=""/><div class="controls bullet"><span class="by">l33tman</span><span>|</span><a href="#39109434">root</a><span>|</span><a href="#39111890">parent</a><span>|</span><a href="#39114396">prev</a><span>|</span><a href="#39111461">next</a><span>|</span><label class="collapse" for="c-39112474">[-]</label><label class="expand" for="c-39112474">[1 more]</label></div><br/><div class="children"><div class="content">LAION is good</div><br/></div></div></div></div></div></div></div></div><div id="39111461" class="c"><input type="checkbox" id="c-39111461" checked=""/><div class="controls bullet"><span class="by">gutianpei</span><span>|</span><a href="#39109434">prev</a><span>|</span><a href="#39111165">next</a><span>|</span><label class="collapse" for="c-39111461">[-]</label><label class="expand" for="c-39111461">[2 more]</label></div><br/><div class="children"><div class="content">I think NATTEN does not support cross attention, wonder if the authors have tried any text-conditioned cases? Does the cross-attention can only add to regular attention? Or added through adanorm?</div><br/><div id="39111856" class="c"><input type="checkbox" id="c-39111856" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39111461">parent</a><span>|</span><a href="#39111165">next</a><span>|</span><label class="collapse" for="c-39111856">[-]</label><label class="expand" for="c-39111856">[1 more]</label></div><br/><div class="children"><div class="content">cross-attention doesn&#x27;t need to involve NATTEN. there&#x27;s no neighbourhood involved because it&#x27;s not self-attention. so you can do it the stable-diffusion way: after self-attention, run torch sdp with Q=image and K=V=text.<p>I tried adding &quot;stable-diffusion-style&quot; cross-attn to HDiT, text-conditioning on small class-conditional datasets (oxford flowers), embedding the class labels as text prompts with Phi-1.5. trained it for a few minutes, and the images were relevant to the prompts, so it seemed to be working fine.<p>but if instead of a text condition you have a single-token condition (class label) then yeah the adanorm would be a simpler way.</div><br/></div></div></div></div><div id="39111165" class="c"><input type="checkbox" id="c-39111165" checked=""/><div class="controls bullet"><span class="by">artninja1988</span><span>|</span><a href="#39111461">prev</a><span>|</span><a href="#39110179">next</a><span>|</span><label class="collapse" for="c-39111165">[-]</label><label class="expand" for="c-39111165">[2 more]</label></div><br/><div class="children"><div class="content">Looking at the output image examples, very nice, although they seem a little blurry. But I guess that&#x27;s a dataset issue?
Have you tried training anything above 1024x1024? Hope someone releases a model based on this since open source pixel space models are a rarity afaik</div><br/><div id="39111900" class="c"><input type="checkbox" id="c-39111900" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39111165">parent</a><span>|</span><a href="#39110179">next</a><span>|</span><label class="collapse" for="c-39111900">[-]</label><label class="expand" for="c-39111900">[1 more]</label></div><br/><div class="children"><div class="content">the FFHQ-1024 examples shouldn&#x27;t be blurry. you can download the originals from the project page[0] — click any image in the teaser, or download our 50k samples.<p>the ImageNet-256 examples also aren&#x27;t typically blurry (but they are 256x256 so your viewer may be bicubic scaling them or something). the ImageNet dataset _can_ have blurry, compressed or low resolution training samples, which can afflict some classes more than others, and we learn to produce samples like the training set.<p>[0] <a href="https:&#x2F;&#x2F;crowsonkb.github.io&#x2F;hourglass-diffusion-transformers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;crowsonkb.github.io&#x2F;hourglass-diffusion-transformers...</a></div><br/></div></div></div></div><div id="39110179" class="c"><input type="checkbox" id="c-39110179" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39111165">prev</a><span>|</span><a href="#39109992">next</a><span>|</span><label class="collapse" for="c-39110179">[-]</label><label class="expand" for="c-39110179">[3 more]</label></div><br/><div class="children"><div class="content">I hope that all these insights about diffusion model training that have been explored in last few years will be used by Stability AI to train their large text-to-image models, because when it comes to that they just use to most basic pipeline you can imagine with plenty of problems that get &quot;solved&quot; by some workarounds, for example to train SDXL they used the scheduler used by the DDPM paper(2020), epsilon-objective and noise-offset, an ugly workaround that was created when people realized that SD v1.5 wasn&#x27;t able to generate images that were too dark or bright, a problem related to the epsilon-objective that cause the model to always generate images with a mean close to 0 (the same as the gaussian noise).<p>A few people have finetuned Stable Diffusion models on v-objective and solved the problem from the root.</div><br/><div id="39111706" class="c"><input type="checkbox" id="c-39111706" checked=""/><div class="controls bullet"><span class="by">SEGyges</span><span>|</span><a href="#39110179">parent</a><span>|</span><a href="#39109992">next</a><span>|</span><label class="collapse" for="c-39111706">[-]</label><label class="expand" for="c-39111706">[2 more]</label></div><br/><div class="children"><div class="content">I have good news about who wrote this paper</div><br/><div id="39112033" class="c"><input type="checkbox" id="c-39112033" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39110179">root</a><span>|</span><a href="#39111706">parent</a><span>|</span><a href="#39109992">next</a><span>|</span><label class="collapse" for="c-39112033">[-]</label><label class="expand" for="c-39112033">[1 more]</label></div><br/><div class="children"><div class="content">Two authors are from Stability AI, that&#x27;s the reason why I wrote the comment.</div><br/></div></div></div></div></div></div><div id="39109992" class="c"><input type="checkbox" id="c-39109992" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39110179">prev</a><span>|</span><a href="#39109742">next</a><span>|</span><label class="collapse" for="c-39109992">[-]</label><label class="expand" for="c-39109992">[7 more]</label></div><br/><div class="children"><div class="content">This is probably a stupid question, but what kind of image generation does this do? The architecture overview shows &quot;input image&quot;, and I don&#x27;t see anything about text to image. Is it super resolution? Does class-conditional mean that it takes a class like &quot;car&quot; or &quot;face&quot; and generate a new random image of that class?</div><br/><div id="39112100" class="c"><input type="checkbox" id="c-39112100" checked=""/><div class="controls bullet"><span class="by">Birch-san</span><span>|</span><a href="#39109992">parent</a><span>|</span><a href="#39110010">next</a><span>|</span><label class="collapse" for="c-39112100">[-]</label><label class="expand" for="c-39112100">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Is it super resolution?<p>nope, we don&#x27;t do Imagen-style super-resolution. we go direct to high resolution with a single-stage model.</div><br/><div id="39112165" class="c"><input type="checkbox" id="c-39112165" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39109992">root</a><span>|</span><a href="#39112100">parent</a><span>|</span><a href="#39110010">next</a><span>|</span><label class="collapse" for="c-39112165">[-]</label><label class="expand" for="c-39112165">[4 more]</label></div><br/><div class="children"><div class="content">I was referring to the input image in the diagram, what is that and how is the output image generated from it? Is it 256x256 noise that gets denoised into an image? I guess what I&#x27;m really asking is what guides the process into the final image if it&#x27;s not text to image?</div><br/><div id="39112245" class="c"><input type="checkbox" id="c-39112245" checked=""/><div class="controls bullet"><span class="by">stefanbaumann</span><span>|</span><a href="#39109992">root</a><span>|</span><a href="#39112165">parent</a><span>|</span><a href="#39110010">next</a><span>|</span><label class="collapse" for="c-39112245">[-]</label><label class="expand" for="c-39112245">[3 more]</label></div><br/><div class="children"><div class="content">The &quot;input image&quot; is just the noisy sample from the previous timestep, yes.<p>The overall architecture diagram does not explicitly show the conditioning mechanism, which is a small separate network. For this paper, we only trained on class-conditional ImageNet and completely unconditional megapixel-scale FFHQ.<p>Training large-scale text-to-image models with this architecture is something we have not yet attempted, although there&#x27;s no indication that this shouldn&#x27;t work with a few tweaks.</div><br/><div id="39112441" class="c"><input type="checkbox" id="c-39112441" checked=""/><div class="controls bullet"><span class="by">sorenjan</span><span>|</span><a href="#39109992">root</a><span>|</span><a href="#39112245">parent</a><span>|</span><a href="#39110010">next</a><span>|</span><label class="collapse" for="c-39112441">[-]</label><label class="expand" for="c-39112441">[2 more]</label></div><br/><div class="children"><div class="content">Thank you, I&#x27;m not used to reading this kind of research papers but I think I got the gist of it now.<p>Can this architecture be used to distill models that need fewer timesteps like LCMs or SDXL turbo?</div><br/><div id="39112574" class="c"><input type="checkbox" id="c-39112574" checked=""/><div class="controls bullet"><span class="by">stefanbaumann</span><span>|</span><a href="#39109992">root</a><span>|</span><a href="#39112441">parent</a><span>|</span><a href="#39110010">next</a><span>|</span><label class="collapse" for="c-39112574">[-]</label><label class="expand" for="c-39112574">[1 more]</label></div><br/><div class="children"><div class="content">Both Latent Consistency Models and Adversarial Diffusion Distillation (the method behind SDXL Turbo) are methods that do not depend on any specific properties of the backbone. So, as Hourglass Diffusion Transformers are just a new kind of backbone that can be used just like the Diffusion U-Nets in Stable Diffusion (XL), these methods should also be applicable to it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39110010" class="c"><input type="checkbox" id="c-39110010" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39109992">parent</a><span>|</span><a href="#39112100">prev</a><span>|</span><a href="#39109742">next</a><span>|</span><label class="collapse" for="c-39110010">[-]</label><label class="expand" for="c-39110010">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s Imagenet class-conditioned, FFHQ unconditioned.<p>&gt;Does class-conditional mean that it takes a class like &quot;car&quot; or &quot;face&quot; and generate a new random image of that class?<p>Yup</div><br/></div></div></div></div></div></div></div></div></div></body></html>