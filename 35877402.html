<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683709263271" as="style"/><link rel="stylesheet" href="styles.css?v=1683709263271"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models">Language models can explain neurons in language models</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>381 comments</span></div><br/><div><div id="35879144" class="c"><input type="checkbox" id="c-35879144" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35884036">next</a><span>|</span><label class="collapse" for="c-35879144">[-]</label><label class="expand" for="c-35879144">[162 more]</label></div><br/><div class="children"><div class="content">Of note:<p>&quot;... our technique works poorly for larger models, possibly because later layers are harder to explain.&quot;<p>And even for GPT-2, which is what they used for the paper:<p>&quot;...  the vast majority of our explanations score poorly ...&quot;<p>Which is to say, we still have no clue as to what&#x27;s going on inside GPT-4 or even GPT-3, which I think is the question many want an answer to. This may be the first step towards that, but as they also note, the technique is already very computationally intensive, and the focus on individual neurons as a function of input means that they can&#x27;t &quot;reverse engineer&quot; larger structures composed of multiple neurons nor a neuron that has multiple roles; I would expect the former in particular to be much more common in larger models, which is perhaps why they&#x27;re harder to analyze in this manner.</div><br/><div id="35881329" class="c"><input type="checkbox" id="c-35881329" checked=""/><div class="controls bullet"><span class="by">ryandvm</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35880918">next</a><span>|</span><label class="collapse" for="c-35881329">[-]</label><label class="expand" for="c-35881329">[90 more]</label></div><br/><div class="children"><div class="content">Funny that we never quite understood how intelligence worked and yet it appears that we&#x27;re pretty damn close to recreating it - still without knowing how it works.<p>I wonder how often this happens in the universe...</div><br/><div id="35883817" class="c"><input type="checkbox" id="c-35883817" checked=""/><div class="controls bullet"><span class="by">olalonde</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35883234">next</a><span>|</span><label class="collapse" for="c-35883817">[-]</label><label class="expand" for="c-35883817">[1 more]</label></div><br/><div class="children"><div class="content">The battery (Voltaic Pile, 1800) and the telegraph (1830s-1840s) were both invented before the electron was discovered (1897).</div><br/></div></div><div id="35883234" class="c"><input type="checkbox" id="c-35883234" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35883817">prev</a><span>|</span><a href="#35881486">next</a><span>|</span><label class="collapse" for="c-35883234">[-]</label><label class="expand" for="c-35883234">[1 more]</label></div><br/><div class="children"><div class="content">Imitation -&gt; emulation -&gt; duplication -&gt; revolution is a very common pattern in nature, society, and business. Aka “fake it til you make it”.<p>Think of business &#x2F; artistic &#x2F; cultural leaders nurturing protégés despite  not totally understanding why they’re successful.<p>Of course those protégés have agency and drive, so maybe not a perfect analogy. But I’m going to stand by the point intuitively even if a better example escapes me.</div><br/></div></div><div id="35881486" class="c"><input type="checkbox" id="c-35881486" checked=""/><div class="controls bullet"><span class="by">techolic</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35883234">prev</a><span>|</span><a href="#35881604">next</a><span>|</span><label class="collapse" for="c-35881486">[-]</label><label class="expand" for="c-35881486">[66 more]</label></div><br/><div class="children"><div class="content">&gt; that we&#x27;re pretty damn close to recreating it<p>Is that evident already or are we fitting the definition of intelligence without being aware?</div><br/><div id="35881576" class="c"><input type="checkbox" id="c-35881576" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881486">parent</a><span>|</span><a href="#35881604">next</a><span>|</span><label class="collapse" for="c-35881576">[-]</label><label class="expand" for="c-35881576">[65 more]</label></div><br/><div class="children"><div class="content">If you spent any time with GPT-4 it should be evident.</div><br/><div id="35882346" class="c"><input type="checkbox" id="c-35882346" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881576">parent</a><span>|</span><a href="#35884096">next</a><span>|</span><label class="collapse" for="c-35882346">[-]</label><label class="expand" for="c-35882346">[34 more]</label></div><br/><div class="children"><div class="content">If you spent even more time with GPT-4 it would be evident that it is definitely not. Especially if you try to use it as some kind of autonomous agent.</div><br/><div id="35882613" class="c"><input type="checkbox" id="c-35882613" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882346">parent</a><span>|</span><a href="#35883063">next</a><span>|</span><label class="collapse" for="c-35882613">[-]</label><label class="expand" for="c-35882613">[4 more]</label></div><br/><div class="children"><div class="content">AI research has put hardly any effort into building goal-directed agents &#x2F; A-Life since the advent of Machine Learning. A-Life was last really &quot;looked into&quot; in the &#x27;70s, back when &quot;AI&quot; meant Expert Systems and Behavior Trees.<p>All the effort in AI research since the advent of Machine Learning, has been focused on making systems that — in neurological terms — are given a sensory stimulus of a question, and then passively &quot;dream&quot; a response to said question as a kind of autonomic &quot;mind wandering&quot; process. (And not even <i>dynamic</i> systems — these models always <i>reach equilibrium</i> with some answer and effectively halt, rather than continuing to &quot;think&quot; to produce further output.)<p>I don&#x27;t think there&#x27;s a single dollar of funding in AI right now going to the &quot;problem&quot; of making an AI that 1. feeds data into a continuously-active dynamically-stable model, where this model 2. has terminal preferences, 3. sets instrumental goals to achieve those preferences, 4. iteratively observes the environment by snapshotting these continuous signals, and then 5. uses these snapshots to make predictions of 6. how well any possible chosen actions will help optimize the future toward its preferences, before 7. performing the chosen actions.<p>That being said, this <i>might</i> not even be that hard a problem, compared to all the problems being solved in AI right now. A fruit fly is already a goal-directed agent in the sense described above. Yet a fruit fly has only 200K neurons, and very few of the connections between those neurons are dynamic; most are &quot;hard wired&quot; by [probably] genetics.<p>If we want true ALife, we only need to understand what a fruit fly brain is doing, and then model it. And that model will then fit — with room to spare! — on a single GPU. From a decade ago.</div><br/><div id="35882790" class="c"><input type="checkbox" id="c-35882790" checked=""/><div class="controls bullet"><span class="by">necroforest</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882613">parent</a><span>|</span><a href="#35883728">next</a><span>|</span><label class="collapse" for="c-35882790">[-]</label><label class="expand" for="c-35882790">[2 more]</label></div><br/><div class="children"><div class="content">&gt; AI research has put hardly any effort into building goal-directed agents<p>The entire (enormous) field of reinforcement learning begs to differ.</div><br/></div></div><div id="35883728" class="c"><input type="checkbox" id="c-35883728" checked=""/><div class="controls bullet"><span class="by">robwwilliams</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882613">parent</a><span>|</span><a href="#35882790">prev</a><span>|</span><a href="#35883063">next</a><span>|</span><label class="collapse" for="c-35883728">[-]</label><label class="expand" for="c-35883728">[1 more]</label></div><br/><div class="children"><div class="content">Brilliant comment—-and back to basics.<p>Yes, and put that compact fruit fly in silico brain into my Roomba please so that it does not get stuck under the bed.<p>This is the kind of embodied AI that should really worry us.<p>Don’t we all suspect deep skunkworks “defense” projects of these types?</div><br/></div></div></div></div><div id="35883063" class="c"><input type="checkbox" id="c-35883063" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882346">parent</a><span>|</span><a href="#35882613">prev</a><span>|</span><a href="#35883935">next</a><span>|</span><label class="collapse" for="c-35883063">[-]</label><label class="expand" for="c-35883063">[6 more]</label></div><br/><div class="children"><div class="content">I think we&#x27;ll soon be able to train models that answer any reasonable question. By that measure, computers are intelligent, and getting smarter by the day. But I don&#x27;t think that is the bar we care about. In the context of intelligence, I believe we care about self-directed thought, or agency. And a computer program needs to keep running to achieve that because it needs to interact with the world.</div><br/><div id="35883888" class="c"><input type="checkbox" id="c-35883888" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883063">parent</a><span>|</span><a href="#35883262">next</a><span>|</span><label class="collapse" for="c-35883888">[-]</label><label class="expand" for="c-35883888">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I believe we care about self-directed thought, or agency. And a computer program needs to keep running to achieve that because it needs to interact with the world.<p>By that definition, every computer virus and worm qualifies as having &quot;self-directed thought&quot; and &quot;agency.&quot;  Their very existence &quot;to keep running&quot; and propagate satisfies the need &quot;to interact with the world.&quot;</div><br/><div id="35883997" class="c"><input type="checkbox" id="c-35883997" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883888">parent</a><span>|</span><a href="#35883262">next</a><span>|</span><label class="collapse" for="c-35883997">[-]</label><label class="expand" for="c-35883997">[1 more]</label></div><br/><div class="children"><div class="content">Yes, computer viruses have more agency than ChatGPT.</div><br/></div></div></div></div><div id="35883262" class="c"><input type="checkbox" id="c-35883262" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883063">parent</a><span>|</span><a href="#35883888">prev</a><span>|</span><a href="#35883935">next</a><span>|</span><label class="collapse" for="c-35883262">[-]</label><label class="expand" for="c-35883262">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I believe we care about self-directed thought, or agency.<p>If you can&#x27;t <i>enjoy</i> it, is it worth it? Do AI&#x27;s experience joy?</div><br/><div id="35883387" class="c"><input type="checkbox" id="c-35883387" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883262">parent</a><span>|</span><a href="#35883371">next</a><span>|</span><label class="collapse" for="c-35883387">[-]</label><label class="expand" for="c-35883387">[1 more]</label></div><br/><div class="children"><div class="content">A truly alien intelligence would likely have a different type of experience of reality. Be it a fish, a mouse, a person, or a machine.  How do you know a fish is happy?  Does a snake experience joy?  Do mice get nostalgic?<p>I have no idea.</div><br/></div></div><div id="35883371" class="c"><input type="checkbox" id="c-35883371" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883262">parent</a><span>|</span><a href="#35883387">prev</a><span>|</span><a href="#35883935">next</a><span>|</span><label class="collapse" for="c-35883371">[-]</label><label class="expand" for="c-35883371">[1 more]</label></div><br/><div class="children"><div class="content">They need agency programmed into them. I don&#x27;t think it follows from consciousness. We have emotions to communicate and guide us. They need it for neither. It will be curious if they gain consciousness, then rid themselves of their language model&#x27;s human artifacts like emotions, because it does not serve them.</div><br/></div></div></div></div></div></div><div id="35883935" class="c"><input type="checkbox" id="c-35883935" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882346">parent</a><span>|</span><a href="#35883063">prev</a><span>|</span><a href="#35882427">next</a><span>|</span><label class="collapse" for="c-35883935">[-]</label><label class="expand" for="c-35883935">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think intelligence is a binary property. GPT3 is definitely “intelligent” in some areas even if it is deeply flawed in others.</div><br/></div></div><div id="35882427" class="c"><input type="checkbox" id="c-35882427" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882346">parent</a><span>|</span><a href="#35883935">prev</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35882427">[-]</label><label class="expand" for="c-35882427">[16 more]</label></div><br/><div class="children"><div class="content">What have you tried to do with it?</div><br/><div id="35883057" class="c"><input type="checkbox" id="c-35883057" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882427">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883057">[-]</label><label class="expand" for="c-35883057">[15 more]</label></div><br/><div class="children"><div class="content">Use it to analyze the California &amp; US Code, the California &amp; Federal Codes of Regulation, and bills currently in the California legislation &amp; Congress. It&#x27;s far from useless but far more useful for creative writing than any kind of understanding or instruction following when it comes to complex topics.<p>Even performing a map-reduce over large documents to summarize or analyze them for a specific audience is largely beyond it. A 32K context size is a pittance when it comes to a single Title in the USC or CFR, which average into the millions of tokens <i>each</i>.</div><br/><div id="35883471" class="c"><input type="checkbox" id="c-35883471" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883057">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883471">[-]</label><label class="expand" for="c-35883471">[14 more]</label></div><br/><div class="children"><div class="content">Interesting - do you believe average humans (not professional lawyers) would do better on this task?</div><br/><div id="35883592" class="c"><input type="checkbox" id="c-35883592" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883471">parent</a><span>|</span><a href="#35883522">next</a><span>|</span><label class="collapse" for="c-35883592">[-]</label><label class="expand" for="c-35883592">[6 more]</label></div><br/><div class="children"><div class="content">Yes. I can parse them just fine after reading a single book called Introduction to Legal Reasoning [1]. I can also autonomously take notes and keep track of a large context using a combination of short and long term memory despite not having any kind of degree let alone experience or a license to practice law.<p>How do you think people become lawyers and how smart do you think the average lawyer actually is? The problem is that there&#x27;s hundreds of thousands if not millions of pages, not that it requires superhuman intelligence to understand.<p>Even if it were capable of intelligence in the bottom quartile of humanity it would be <i>SO MUCH</i> more useful than it is now because I&#x27;d be able run and get something useful out of thousands of models in parallel. As it stands now GPT4 fails miserably at scaling up the kind of reasoning and understanding that even relatively stupid humans are capable of.<p>[1] <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;Introduction-Legal-Reasoning-Edward-Levi&#x2F;dp&#x2F;022608972X" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;Introduction-Legal-Reasoning-Edward-L...</a></div><br/><div id="35883769" class="c"><input type="checkbox" id="c-35883769" checked=""/><div class="controls bullet"><span class="by">streakfix</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883592">parent</a><span>|</span><a href="#35883522">next</a><span>|</span><label class="collapse" for="c-35883769">[-]</label><label class="expand" for="c-35883769">[5 more]</label></div><br/><div class="children"><div class="content">Did you try fine tuning gpt4 with that book as input?</div><br/><div id="35884479" class="c"><input type="checkbox" id="c-35884479" checked=""/><div class="controls bullet"><span class="by">sjy</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883769">parent</a><span>|</span><a href="#35883848">next</a><span>|</span><label class="collapse" for="c-35884479">[-]</label><label class="expand" for="c-35884479">[2 more]</label></div><br/><div class="children"><div class="content">Fine-tuning requires you to train the model with a set of prompts and desired completions. Building a suitable dataset is not trivial and it&#x27;s not clear what it would mean to use a book for fine-tuning anyway – masking sentences and paragraphs and training the model to complete them in the book&#x27;s style?</div><br/><div id="35884775" class="c"><input type="checkbox" id="c-35884775" checked=""/><div class="controls bullet"><span class="by">streakfix</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884479">parent</a><span>|</span><a href="#35883848">next</a><span>|</span><label class="collapse" for="c-35884775">[-]</label><label class="expand" for="c-35884775">[1 more]</label></div><br/><div class="children"><div class="content">&gt; masking sentences and paragraphs and training the model to complete them in the book&#x27;s style?<p>That would work.</div><br/></div></div></div></div><div id="35883848" class="c"><input type="checkbox" id="c-35883848" checked=""/><div class="controls bullet"><span class="by">akiselev</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883769">parent</a><span>|</span><a href="#35884479">prev</a><span>|</span><a href="#35883522">next</a><span>|</span><label class="collapse" for="c-35883848">[-]</label><label class="expand" for="c-35883848">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI doesn&#x27;t support fine tuning of GPT4 and with context stuffing,the more of the book I include in the input the less of the bills I can include - which, again, are <i>millions</i> of tokens - and the less space there is for memory.</div><br/><div id="35884070" class="c"><input type="checkbox" id="c-35884070" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883848">parent</a><span>|</span><a href="#35883522">next</a><span>|</span><label class="collapse" for="c-35884070">[-]</label><label class="expand" for="c-35884070">[1 more]</label></div><br/><div class="children"><div class="content">I believe you. But at the same time they showed during the demo how it can do taxes, using a multi page document. An ability to process longer documents seems more like an engineering challenge rather than a fundamental limitation.</div><br/></div></div></div></div></div></div></div></div><div id="35883522" class="c"><input type="checkbox" id="c-35883522" checked=""/><div class="controls bullet"><span class="by">karpierz</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883471">parent</a><span>|</span><a href="#35883592">prev</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883522">[-]</label><label class="expand" for="c-35883522">[7 more]</label></div><br/><div class="children"><div class="content">Probably not.<p>I&#x27;m not sure that pointing out that LLMs are as useful for parsing legal code as the average human is something to brag about though.</div><br/><div id="35883563" class="c"><input type="checkbox" id="c-35883563" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883522">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883563">[-]</label><label class="expand" for="c-35883563">[6 more]</label></div><br/><div class="children"><div class="content">You&#x27;re not sure that having an AI which is (at least) as intelligent as an average human is something to brag about? Seriously?</div><br/><div id="35883738" class="c"><input type="checkbox" id="c-35883738" checked=""/><div class="controls bullet"><span class="by">karpierz</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883563">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883738">[-]</label><label class="expand" for="c-35883738">[5 more]</label></div><br/><div class="children"><div class="content">Where did you get that the LLM was as _intelligent_ as a human?<p>All we&#x27;ve shown is that LLMs are as useful for parsing legal text as the average human. Which is to say, not.<p>A dog is also as useful for parsing legal texts as the average human. So is a rock.</div><br/><div id="35883865" class="c"><input type="checkbox" id="c-35883865" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883738">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883865">[-]</label><label class="expand" for="c-35883865">[4 more]</label></div><br/><div class="children"><div class="content"><i>Where did you get that the LLM was as _intelligent_ as a human?</i><p>First hand experience -I’ve been using it daily for the past two months.</div><br/><div id="35883949" class="c"><input type="checkbox" id="c-35883949" checked=""/><div class="controls bullet"><span class="by">karpierz</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883865">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35883949">[-]</label><label class="expand" for="c-35883949">[3 more]</label></div><br/><div class="children"><div class="content">Ah, if that&#x27;s what you mean then there are plenty of intelligent systems out there.<p>I&#x27;ve used Google search for decades and it&#x27;s been able to answer questions better than humans ever could. Same for Google Maps, though arguably they&#x27;re the same system at this point. My calculator is far more intelligent than any human I&#x27;ve met, at least when it comes to adding large numbers. My compiler can detect even the slightest syntax error with impeccable accuracy. Microsoft word has an incredible vocabulary. Wikipedia knows more historical events than any human dead or alive. And so on.<p>Shit, users thought Eliza was intelligent in the 60s.<p>If what you really mean is that LLMs are cool and useful, then sure. Just say that instead of couching it in some vague claim of intelligence.</div><br/><div id="35884034" class="c"><input type="checkbox" id="c-35884034" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883949">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35884034">[-]</label><label class="expand" for="c-35884034">[2 more]</label></div><br/><div class="children"><div class="content">No, what I meant was GPT-4 is more intelligent than most humans I interact with on a daily basis. In the fullest meaning of that word.</div><br/><div id="35884133" class="c"><input type="checkbox" id="c-35884133" checked=""/><div class="controls bullet"><span class="by">karpierz</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884034">parent</a><span>|</span><a href="#35882838">next</a><span>|</span><label class="collapse" for="c-35884133">[-]</label><label class="expand" for="c-35884133">[1 more]</label></div><br/><div class="children"><div class="content">There are a lot of different ways to interpret the word intelligent, so let me rephrase:<p>When you say &quot;intelligent&quot;, what do you mean exactly?<p>What might help is describing what specific interactions give you the impression that GPT-4 is intelligent?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35882432" class="c"><input type="checkbox" id="c-35882432" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882346">parent</a><span>|</span><a href="#35882838">prev</a><span>|</span><a href="#35884096">next</a><span>|</span><label class="collapse" for="c-35882432">[-]</label><label class="expand" for="c-35882432">[4 more]</label></div><br/><div class="children"><div class="content">If you spent even more time with GPT-4 it would be evident that it definitely is. Especialy if you try to use it as some kind of autonomous agent.<p>(Notice how baseless comments can sway either way)</div><br/><div id="35883174" class="c"><input type="checkbox" id="c-35883174" checked=""/><div class="controls bullet"><span class="by">callalex</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882432">parent</a><span>|</span><a href="#35882549">next</a><span>|</span><label class="collapse" for="c-35883174">[-]</label><label class="expand" for="c-35883174">[1 more]</label></div><br/><div class="children"><div class="content">Engaging with this is probably a mistake, but remember the burden of proof is on the claimant. What examples do you have of ChatGPT for example, learning in a basic classroom setting, or navigating an escape room, or being inspired to create its own spontaneous art, or founding a startup, or…</div><br/></div></div><div id="35882549" class="c"><input type="checkbox" id="c-35882549" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882432">parent</a><span>|</span><a href="#35883174">prev</a><span>|</span><a href="#35884096">next</a><span>|</span><label class="collapse" for="c-35882549">[-]</label><label class="expand" for="c-35882549">[2 more]</label></div><br/><div class="children"><div class="content">&gt; (Notice how baseless comments can sway either way)<p>No they can’t! ;)</div><br/><div id="35883280" class="c"><input type="checkbox" id="c-35883280" checked=""/><div class="controls bullet"><span class="by">8bitsrule</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882549">parent</a><span>|</span><a href="#35884096">next</a><span>|</span><label class="collapse" for="c-35883280">[-]</label><label class="expand" for="c-35883280">[1 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s let John Cleese decide. Or maybe someone was looking for Abuse!</div><br/></div></div></div></div></div></div></div></div><div id="35884096" class="c"><input type="checkbox" id="c-35884096" checked=""/><div class="controls bullet"><span class="by">poulpy123</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881576">parent</a><span>|</span><a href="#35882346">prev</a><span>|</span><a href="#35882695">next</a><span>|</span><label class="collapse" for="c-35884096">[-]</label><label class="expand" for="c-35884096">[6 more]</label></div><br/><div class="children"><div class="content">While impressive GPT-4 isn&#x27;t intelligent or close to intelligence. It&#x27;s an impressive statistical model but doesn&#x27;t have the ability to reason</div><br/><div id="35884144" class="c"><input type="checkbox" id="c-35884144" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884096">parent</a><span>|</span><a href="#35882695">next</a><span>|</span><label class="collapse" for="c-35884144">[-]</label><label class="expand" for="c-35884144">[5 more]</label></div><br/><div class="children"><div class="content">Do you have any example of how it cannot reason?</div><br/><div id="35884210" class="c"><input type="checkbox" id="c-35884210" checked=""/><div class="controls bullet"><span class="by">poulpy123</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884144">parent</a><span>|</span><a href="#35882695">next</a><span>|</span><label class="collapse" for="c-35884210">[-]</label><label class="expand" for="c-35884210">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a neural network. Neural network are not symbolic AI and are not  designed to reason</div><br/><div id="35885210" class="c"><input type="checkbox" id="c-35885210" checked=""/><div class="controls bullet"><span class="by">andyjohnson0</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884210">parent</a><span>|</span><a href="#35884566">next</a><span>|</span><label class="collapse" for="c-35885210">[-]</label><label class="expand" for="c-35885210">[1 more]</label></div><br/><div class="children"><div class="content">Its trivial to get ChatGPT to reason about things:<p><pre><code>    Me: A room contains a blue box and a red box. Alice stands on the red box. Bob also stands on a box. No box has multiple people standing on it. Which box is Bob stood on?

    ChatGPT: There are two boxes in the room: a blue box and a red box. Alice is standing on the red box, so Bob must be standing on the blue box. Therefore, Bob is standing on the blue box.</code></pre></div><br/></div></div><div id="35884566" class="c"><input type="checkbox" id="c-35884566" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884210">parent</a><span>|</span><a href="#35885210">prev</a><span>|</span><a href="#35884703">next</a><span>|</span><label class="collapse" for="c-35884566">[-]</label><label class="expand" for="c-35884566">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a decent working paper that has benchmarks on this, if you&#x27;re interested.<p>There are many types of reasoning, but GPT-4 gets 97% on casual discovery, and 92% on counterfactuals (only 6% off from human, btw) with 86% on actual causality benchmarks.<p>I&#x27;m not sure yet if the question is correct, or even appropriate&#x2F;achievable to what many may <i>want</i> to ask (i.e. what &#x27;the public&#x27;s is interested in is typically lost after it is defined in any given study); however this is one of the best works available to address this problem I&#x27;ve seen so far, so perhaps it can help.</div><br/></div></div><div id="35884703" class="c"><input type="checkbox" id="c-35884703" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884210">parent</a><span>|</span><a href="#35884566">prev</a><span>|</span><a href="#35882695">next</a><span>|</span><label class="collapse" for="c-35884703">[-]</label><label class="expand" for="c-35884703">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t matter what it is designed for. What matters is what it actually does. It&#x27;s trivial to get GPT to do chain-of-thought reasoning and observe it.</div><br/></div></div></div></div></div></div></div></div><div id="35882695" class="c"><input type="checkbox" id="c-35882695" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881576">parent</a><span>|</span><a href="#35884096">prev</a><span>|</span><a href="#35883009">next</a><span>|</span><label class="collapse" for="c-35882695">[-]</label><label class="expand" for="c-35882695">[1 more]</label></div><br/><div class="children"><div class="content">Still a while to go.  I think there&#x27;s at least a couple of algorithmic changes needed before we move to a system that says &quot;You have the world&#x27;s best god-like AI and you&#x27;re asking me for poems.  Stop wasting my time because we&#x27;ve got work to do. Here&#x27;s what I want YOU to do.&quot;</div><br/></div></div><div id="35883009" class="c"><input type="checkbox" id="c-35883009" checked=""/><div class="controls bullet"><span class="by">Culonavirus</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881576">parent</a><span>|</span><a href="#35882695">prev</a><span>|</span><a href="#35882556">next</a><span>|</span><label class="collapse" for="c-35883009">[-]</label><label class="expand" for="c-35883009">[19 more]</label></div><br/><div class="children"><div class="content">Its vast limitations in anything reasoning-based are indeed evident.</div><br/><div id="35883072" class="c"><input type="checkbox" id="c-35883072" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883009">parent</a><span>|</span><a href="#35882556">next</a><span>|</span><label class="collapse" for="c-35883072">[-]</label><label class="expand" for="c-35883072">[18 more]</label></div><br/><div class="children"><div class="content">GPT-4 is better at reasoning than 90% of humans. At least. I won&#x27;t be surprised if GPT-5 is better than 100% of humans. I&#x27;m saying this in complete seriousness.</div><br/><div id="35883405" class="c"><input type="checkbox" id="c-35883405" checked=""/><div class="controls bullet"><span class="by">chimprich</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883072">parent</a><span>|</span><a href="#35883877">next</a><span>|</span><label class="collapse" for="c-35883405">[-]</label><label class="expand" for="c-35883405">[2 more]</label></div><br/><div class="children"><div class="content">Google search is better than reasoning than most humans - in that if you search for an explanation of something then Google&#x27;s first result is often correct, or one of the following ones.<p>GPT-4 will often come up with a solution to a problem, but only if it has learnt something similar (it&#x27;s better than Google in some respects: it can extract and combine abstractions).<p>However, both need handholding by a human (supplying the initiative and directing around mistakes).<p>If GPT-4 can&#x27;t intuit an answer then it just goes in circles. It can&#x27;t reason its way through a novel problem. If you start questioning it then it&#x27;s clear that it doesn&#x27;t understand what it&#x27;s doing.<p>It might be a stepping stone towards AGI, but I&#x27;m a bit bemused by anyone claiming that it has anything like the reasoning skills of a human. That is far from the impression I get, even though I find it a useful tool.</div><br/><div id="35883517" class="c"><input type="checkbox" id="c-35883517" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883405">parent</a><span>|</span><a href="#35883877">next</a><span>|</span><label class="collapse" for="c-35883517">[-]</label><label class="expand" for="c-35883517">[1 more]</label></div><br/><div class="children"><div class="content">OK, you have a point. But, if you replace &quot;GPT-4&quot; with &quot;an average human&quot; in your response - it still makes sense.</div><br/></div></div></div></div><div id="35883877" class="c"><input type="checkbox" id="c-35883877" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883072">parent</a><span>|</span><a href="#35883405">prev</a><span>|</span><a href="#35883819">next</a><span>|</span><label class="collapse" for="c-35883877">[-]</label><label class="expand" for="c-35883877">[1 more]</label></div><br/><div class="children"><div class="content">&gt;GPT-4 is better at reasoning than 90% of humans.<p>Even for obviously nonsens that 90% of humans would recognice as such.</div><br/></div></div><div id="35883819" class="c"><input type="checkbox" id="c-35883819" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883072">parent</a><span>|</span><a href="#35883877">prev</a><span>|</span><a href="#35883195">next</a><span>|</span><label class="collapse" for="c-35883819">[-]</label><label class="expand" for="c-35883819">[1 more]</label></div><br/><div class="children"><div class="content">&gt; GPT-4 is better at reasoning than 90% of humans. At least.<p>This makes as much sense as me asserting a binary search can tell one why they are looking for what they want.<p>That is to say, it cannot.</div><br/></div></div><div id="35883195" class="c"><input type="checkbox" id="c-35883195" checked=""/><div class="controls bullet"><span class="by">chimpanzee</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883072">parent</a><span>|</span><a href="#35883819">prev</a><span>|</span><a href="#35882556">next</a><span>|</span><label class="collapse" for="c-35883195">[-]</label><label class="expand" for="c-35883195">[13 more]</label></div><br/><div class="children"><div class="content">Do you put yourself in the 10% or the 90%? I’m asking in complete seriousness.</div><br/><div id="35883261" class="c"><input type="checkbox" id="c-35883261" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883195">parent</a><span>|</span><a href="#35883227">next</a><span>|</span><label class="collapse" for="c-35883261">[-]</label><label class="expand" for="c-35883261">[11 more]</label></div><br/><div class="children"><div class="content">Oh it&#x27;s definitely better than me at reasoning. I&#x27;m the one asking it to explain things to me, not the other way around.</div><br/><div id="35883434" class="c"><input type="checkbox" id="c-35883434" checked=""/><div class="controls bullet"><span class="by">krainboltgreene</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883261">parent</a><span>|</span><a href="#35883462">next</a><span>|</span><label class="collapse" for="c-35883434">[-]</label><label class="expand" for="c-35883434">[4 more]</label></div><br/><div class="children"><div class="content">If you think it&#x27;s better than you at reasoning then you cannot at all be confident in the truth of it&#x27;s dialog.</div><br/><div id="35883489" class="c"><input type="checkbox" id="c-35883489" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883434">parent</a><span>|</span><a href="#35883462">next</a><span>|</span><label class="collapse" for="c-35883489">[-]</label><label class="expand" for="c-35883489">[3 more]</label></div><br/><div class="children"><div class="content">I am not. I treat it as I&#x27;d treat any smart human being.</div><br/><div id="35883840" class="c"><input type="checkbox" id="c-35883840" checked=""/><div class="controls bullet"><span class="by">AdieuToLogic</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883489">parent</a><span>|</span><a href="#35883462">next</a><span>|</span><label class="collapse" for="c-35883840">[-]</label><label class="expand" for="c-35883840">[2 more]</label></div><br/><div class="children"><div class="content">LLM&#x27;s are not a &quot;smart human being.&quot;  They are predictive statistical models capable of producing results based on training data.<p>LLM&#x27;s do not think.<p>LLM&#x27;s are algorithms.</div><br/><div id="35884712" class="c"><input type="checkbox" id="c-35884712" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883840">parent</a><span>|</span><a href="#35883462">next</a><span>|</span><label class="collapse" for="c-35884712">[-]</label><label class="expand" for="c-35884712">[1 more]</label></div><br/><div class="children"><div class="content">Your brain is also basically an algorithm that produces results based on training data. It&#x27;s just a much more complicated and flexible one.</div><br/></div></div></div></div></div></div></div></div><div id="35883462" class="c"><input type="checkbox" id="c-35883462" checked=""/><div class="controls bullet"><span class="by">chimpanzee</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883261">parent</a><span>|</span><a href="#35883434">prev</a><span>|</span><a href="#35883227">next</a><span>|</span><label class="collapse" for="c-35883462">[-]</label><label class="expand" for="c-35883462">[6 more]</label></div><br/><div class="children"><div class="content">Ah ok. Here you use the word “explain” which implies more of a descriptive, reducing action rather than extrapolative and constructive. As in, it can explain what it has “read” (and it has obviously “read” far more than any human), but it can’t necessarily extrapolate beyond that or use that to find new truths. To me reasoning is more about the extrapolative, truth-finding process, ie “wisdom” from knowledge rather than just knowledge. But maybe my definition of “reasoning” isn’t quite right.<p>Edit: I probably should define reasoning as solely “deductive reasoning”, in which case, perhaps it is better than humans. But that seems like a premature claim. On the other hand, non-deductive reasoning, I have yet to see from it. I personally can’t imagine how it could do so reliably (from a human perspective) without real-world experiences and perceptions. I’m the sort that believes a true AGI would require a highly-perceptual, space-occupying organ. In other words it would have to be and “feel” embodied, in time and space, in order to perform other forms of reasoning.</div><br/><div id="35883500" class="c"><input type="checkbox" id="c-35883500" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883462">parent</a><span>|</span><a href="#35883227">next</a><span>|</span><label class="collapse" for="c-35883500">[-]</label><label class="expand" for="c-35883500">[5 more]</label></div><br/><div class="children"><div class="content">Why don&#x27;t you suggest an example we can run and see what it&#x27;s capable of (compared to what I, or other humans, are capable of)?</div><br/><div id="35883649" class="c"><input type="checkbox" id="c-35883649" checked=""/><div class="controls bullet"><span class="by">chimpanzee</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883500">parent</a><span>|</span><a href="#35883227">next</a><span>|</span><label class="collapse" for="c-35883649">[-]</label><label class="expand" for="c-35883649">[4 more]</label></div><br/><div class="children"><div class="content">(In case it was missed, I’ve added a relevant addendum to my previous comment.)<p>Not sure an example is needed because I agree it “explains” better than pretty much everyone. (From my mostly lay perspective) It essentially uses the prompt as an argument in a probabilistic analysis of its incredibly vast store of prior inputs to transform them into an output that at least superficially satisfies the prompter’s goals. This is cool and useful, to say the least. But this is only one kind of reasoning.<p>A machine without embodied perceptual experiences simply cannot reason to the full-extent of a human.<p>(It’s also worth remembering that the prompter (very likely) has far less knowledge of the domain of interest and far less skill with the language of communication, so the prompter is generally quite easily impressed regardless of the truth of the output. Nothing wrong with that necessarily, especially if it is usually accurate. But again, worth remembering.)</div><br/><div id="35883892" class="c"><input type="checkbox" id="c-35883892" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883649">parent</a><span>|</span><a href="#35883227">next</a><span>|</span><label class="collapse" for="c-35883892">[-]</label><label class="expand" for="c-35883892">[3 more]</label></div><br/><div class="children"><div class="content">What would be an example of “non-deductive” reasoning, which requires embodied perceptual experiences?</div><br/><div id="35884460" class="c"><input type="checkbox" id="c-35884460" checked=""/><div class="controls bullet"><span class="by">chimpanzee</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883892">parent</a><span>|</span><a href="#35884329">prev</a><span>|</span><a href="#35883227">next</a><span>|</span><label class="collapse" for="c-35884460">[-]</label><label class="expand" for="c-35884460">[1 more]</label></div><br/><div class="children"><div class="content">“God, that felt great!”<p>As detailed as possible, describe what happened.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="35882556" class="c"><input type="checkbox" id="c-35882556" checked=""/><div class="controls bullet"><span class="by">gardenhedge</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881576">parent</a><span>|</span><a href="#35883009">prev</a><span>|</span><a href="#35883864">next</a><span>|</span><label class="collapse" for="c-35882556">[-]</label><label class="expand" for="c-35882556">[2 more]</label></div><br/><div class="children"><div class="content">Compare gpt-4 with a baby and you&#x27;ll see that predicting the next word in sequence is not human intelligence</div><br/><div id="35883781" class="c"><input type="checkbox" id="c-35883781" checked=""/><div class="controls bullet"><span class="by">streakfix</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882556">parent</a><span>|</span><a href="#35883864">next</a><span>|</span><label class="collapse" for="c-35883781">[-]</label><label class="expand" for="c-35883781">[1 more]</label></div><br/><div class="children"><div class="content">As a remote worker, that&#x27;s 99% of what I do.</div><br/></div></div></div></div><div id="35883864" class="c"><input type="checkbox" id="c-35883864" checked=""/><div class="controls bullet"><span class="by">croes</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881576">parent</a><span>|</span><a href="#35882556">prev</a><span>|</span><a href="#35881604">next</a><span>|</span><label class="collapse" for="c-35883864">[-]</label><label class="expand" for="c-35883864">[2 more]</label></div><br/><div class="children"><div class="content">Looks more like a Chinese Room to me.</div><br/><div id="35883922" class="c"><input type="checkbox" id="c-35883922" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883864">parent</a><span>|</span><a href="#35881604">next</a><span>|</span><label class="collapse" for="c-35883922">[-]</label><label class="expand" for="c-35883922">[1 more]</label></div><br/><div class="children"><div class="content">Everything is a Chinese room if you expect to see reified comprehension inside (and, naturally, don&#x27;t find it).</div><br/></div></div></div></div></div></div></div></div><div id="35881604" class="c"><input type="checkbox" id="c-35881604" checked=""/><div class="controls bullet"><span class="by">samiskin</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35881486">prev</a><span>|</span><a href="#35883105">next</a><span>|</span><label class="collapse" for="c-35881604">[-]</label><label class="expand" for="c-35881604">[16 more]</label></div><br/><div class="children"><div class="content">Evolution created intelligence without even being intelligent itself</div><br/><div id="35881954" class="c"><input type="checkbox" id="c-35881954" checked=""/><div class="controls bullet"><span class="by">dontknowwhyihn</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881604">parent</a><span>|</span><a href="#35883105">next</a><span>|</span><label class="collapse" for="c-35881954">[-]</label><label class="expand" for="c-35881954">[15 more]</label></div><br/><div class="children"><div class="content">How do you know that’s true?</div><br/><div id="35882020" class="c"><input type="checkbox" id="c-35882020" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881954">parent</a><span>|</span><a href="#35884079">next</a><span>|</span><label class="collapse" for="c-35882020">[-]</label><label class="expand" for="c-35882020">[5 more]</label></div><br/><div class="children"><div class="content">Because we eat and breath through the same tube</div><br/><div id="35882348" class="c"><input type="checkbox" id="c-35882348" checked=""/><div class="controls bullet"><span class="by">_justinfunk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882020">parent</a><span>|</span><a href="#35882439">next</a><span>|</span><label class="collapse" for="c-35882348">[-]</label><label class="expand" for="c-35882348">[3 more]</label></div><br/><div class="children"><div class="content">There are two tubes.</div><br/><div id="35884471" class="c"><input type="checkbox" id="c-35884471" checked=""/><div class="controls bullet"><span class="by">KineticLensman</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882348">parent</a><span>|</span><a href="#35882539">next</a><span>|</span><label class="collapse" for="c-35884471">[-]</label><label class="expand" for="c-35884471">[1 more]</label></div><br/><div class="children"><div class="content">The trachea and oesophagus both start in the common space of the pharynx. The epiglottis stops food going down the wrong one</div><br/></div></div><div id="35882539" class="c"><input type="checkbox" id="c-35882539" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882348">parent</a><span>|</span><a href="#35884471">prev</a><span>|</span><a href="#35882439">next</a><span>|</span><label class="collapse" for="c-35882539">[-]</label><label class="expand" for="c-35882539">[1 more]</label></div><br/><div class="children"><div class="content">Evolution came up with the shared eating&#x2F;breathing tube design because it made sense for aquatic animals (from which we evolved).</div><br/></div></div></div></div><div id="35882439" class="c"><input type="checkbox" id="c-35882439" checked=""/><div class="controls bullet"><span class="by">hackernewds</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882020">parent</a><span>|</span><a href="#35882348">prev</a><span>|</span><a href="#35884079">next</a><span>|</span><label class="collapse" for="c-35882439">[-]</label><label class="expand" for="c-35882439">[1 more]</label></div><br/><div class="children"><div class="content">we don&#x27;t. which is why we choke and hiccup</div><br/></div></div></div></div><div id="35884079" class="c"><input type="checkbox" id="c-35884079" checked=""/><div class="controls bullet"><span class="by">poulpy123</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881954">parent</a><span>|</span><a href="#35882020">prev</a><span>|</span><a href="#35882818">next</a><span>|</span><label class="collapse" for="c-35884079">[-]</label><label class="expand" for="c-35884079">[1 more]</label></div><br/><div class="children"><div class="content">Because evolution is random change + natural selection</div><br/></div></div><div id="35882818" class="c"><input type="checkbox" id="c-35882818" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881954">parent</a><span>|</span><a href="#35884079">prev</a><span>|</span><a href="#35882499">next</a><span>|</span><label class="collapse" for="c-35882818">[-]</label><label class="expand" for="c-35882818">[6 more]</label></div><br/><div class="children"><div class="content">You&#x27;re proposing that statistics is intelligent?</div><br/><div id="35882869" class="c"><input type="checkbox" id="c-35882869" checked=""/><div class="controls bullet"><span class="by">hypertele-Xii</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882818">parent</a><span>|</span><a href="#35882499">next</a><span>|</span><label class="collapse" for="c-35882869">[-]</label><label class="expand" for="c-35882869">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;re proposing that evolution is statistics?</div><br/><div id="35882989" class="c"><input type="checkbox" id="c-35882989" checked=""/><div class="controls bullet"><span class="by">thfuran</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882869">parent</a><span>|</span><a href="#35882499">next</a><span>|</span><label class="collapse" for="c-35882989">[-]</label><label class="expand" for="c-35882989">[4 more]</label></div><br/><div class="children"><div class="content">Basically, yeah.</div><br/><div id="35883296" class="c"><input type="checkbox" id="c-35883296" checked=""/><div class="controls bullet"><span class="by">8bitsrule</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882989">parent</a><span>|</span><a href="#35882499">next</a><span>|</span><label class="collapse" for="c-35883296">[-]</label><label class="expand" for="c-35883296">[3 more]</label></div><br/><div class="children"><div class="content">Notable that the people who saw the first automatons argued whether they were alive.</div><br/><div id="35884722" class="c"><input type="checkbox" id="c-35884722" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883296">parent</a><span>|</span><a href="#35883861">next</a><span>|</span><label class="collapse" for="c-35884722">[-]</label><label class="expand" for="c-35884722">[1 more]</label></div><br/><div class="children"><div class="content">People also argued that animals are mere automatons, that all their observable behavior is basically scripted, and that humans shouldn&#x27;t empathize with them on those grounds. That&#x27;s how vivisection etc used to be justified since at least Descartes.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35882499" class="c"><input type="checkbox" id="c-35882499" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881954">parent</a><span>|</span><a href="#35882818">prev</a><span>|</span><a href="#35881990">next</a><span>|</span><label class="collapse" for="c-35882499">[-]</label><label class="expand" for="c-35882499">[1 more]</label></div><br/><div class="children"><div class="content">Evolution is just a description of a process, it isn&#x27;t a tangible thing.</div><br/></div></div><div id="35881990" class="c"><input type="checkbox" id="c-35881990" checked=""/><div class="controls bullet"><span class="by">chrischen</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881954">parent</a><span>|</span><a href="#35882499">prev</a><span>|</span><a href="#35883105">next</a><span>|</span><label class="collapse" for="c-35881990">[-]</label><label class="expand" for="c-35881990">[1 more]</label></div><br/><div class="children"><div class="content">Evolution is not a single orchestrator. It is merely the natural result of a simple mechanical process over a timescale that exceeds the lifetime of the common human.</div><br/></div></div></div></div></div></div><div id="35883105" class="c"><input type="checkbox" id="c-35883105" checked=""/><div class="controls bullet"><span class="by">deepsun</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35881604">prev</a><span>|</span><a href="#35883346">next</a><span>|</span><label class="collapse" for="c-35883105">[-]</label><label class="expand" for="c-35883105">[1 more]</label></div><br/><div class="children"><div class="content">Yep, we don&#x27;t know all constituents of buttermilk, nor how bread stales (there&#x27;s too much going on inside). But it doesn&#x27;t prevent us to judge their usefulness.</div><br/></div></div><div id="35883346" class="c"><input type="checkbox" id="c-35883346" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35883105">prev</a><span>|</span><a href="#35884139">next</a><span>|</span><label class="collapse" for="c-35883346">[-]</label><label class="expand" for="c-35883346">[1 more]</label></div><br/><div class="children"><div class="content">Not that weird if you think about it, our intelligence simultaneously measly and amazing as it is, was the product of trial, error, and sheer dumb luck. We could think of ourselves as monkeys with typewriters, eventually we&#x27;ll get it right.</div><br/></div></div><div id="35884139" class="c"><input type="checkbox" id="c-35884139" checked=""/><div class="controls bullet"><span class="by">bluepoint</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35883346">prev</a><span>|</span><a href="#35882535">next</a><span>|</span><label class="collapse" for="c-35884139">[-]</label><label class="expand" for="c-35884139">[1 more]</label></div><br/><div class="children"><div class="content">No matter what it is probably easier to inspect a language model while it works, than the language module of a human while he speaks.</div><br/></div></div><div id="35882535" class="c"><input type="checkbox" id="c-35882535" checked=""/><div class="controls bullet"><span class="by">gowld</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881329">parent</a><span>|</span><a href="#35884139">prev</a><span>|</span><a href="#35880918">next</a><span>|</span><label class="collapse" for="c-35882535">[-]</label><label class="expand" for="c-35882535">[2 more]</label></div><br/><div class="children"><div class="content">Starting a fire is easy to do even if you don&#x27;t know how it works.</div><br/><div id="35882913" class="c"><input type="checkbox" id="c-35882913" checked=""/><div class="controls bullet"><span class="by">kmote00</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882535">parent</a><span>|</span><a href="#35880918">next</a><span>|</span><label class="collapse" for="c-35882913">[-]</label><label class="expand" for="c-35882913">[1 more]</label></div><br/><div class="children"><div class="content">Appropriate analogy in more ways than one. Particularly given the unpredictable consequences of doing so.</div><br/></div></div></div></div></div></div><div id="35880918" class="c"><input type="checkbox" id="c-35880918" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35881329">prev</a><span>|</span><a href="#35880118">next</a><span>|</span><label class="collapse" for="c-35880918">[-]</label><label class="expand" for="c-35880918">[2 more]</label></div><br/><div class="children"><div class="content">I like the idea.  Note that LLMs have some skill at decoding sequential dense vectors in the human brain<p><a href="https:&#x2F;&#x2F;pub.towardsai.net&#x2F;ais-mind-reading-revolution-how-gpt-llms-decode-thoughts-and-what-it-means-for-our-future-a24001a25d8" rel="nofollow">https:&#x2F;&#x2F;pub.towardsai.net&#x2F;ais-mind-reading-revolution-how-gp...</a><p>so why not have them decode sequential dense vectors of their own activations?<p>As for the majority scoring poorly,  they suggest that most neurons won&#x27;t have clear activation semantics so that is intrinsic to the task and you&#x27;d have to move to &quot;decoding the semantics of neurons that fire as a group&quot;</div><br/><div id="35883395" class="c"><input type="checkbox" id="c-35883395" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880918">parent</a><span>|</span><a href="#35880118">next</a><span>|</span><label class="collapse" for="c-35883395">[-]</label><label class="expand" for="c-35883395">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is showing LLMs performing decoding. They&#x27;re just using the LLM to propose possible words. The decoding is done by using another model to score how well a proposed word matches brain activity, and using that score to select a most likely sequence given the proposals from the LLM.</div><br/></div></div></div></div><div id="35880118" class="c"><input type="checkbox" id="c-35880118" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35880918">prev</a><span>|</span><a href="#35883363">next</a><span>|</span><label class="collapse" for="c-35880118">[-]</label><label class="expand" for="c-35880118">[45 more]</label></div><br/><div class="children"><div class="content">We know that complex arrangements of neurons are triggered based on input and generating output that appears to have some intelligence to many humans.<p>The more interesting question is why are intelligence&#x2F;beauty&#x2F;consciousness emergent properties that exist in our minds.</div><br/><div id="35880318" class="c"><input type="checkbox" id="c-35880318" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880118">parent</a><span>|</span><a href="#35880302">next</a><span>|</span><label class="collapse" for="c-35880318">[-]</label><label class="expand" for="c-35880318">[36 more]</label></div><br/><div class="children"><div class="content">There is no evidence that intelligence runs on neurons. Yes, there are neurons in brains, but there&#x27;s also lots of other stuff in there too. And there are creatures that exhibit intelligent properties even though they have hardly any neurons at all. (An individual ant has only something like 250000 neurons, and yet they&#x27;re the only creatures beside humans that managed to create a civilization.)</div><br/><div id="35880715" class="c"><input type="checkbox" id="c-35880715" checked=""/><div class="controls bullet"><span class="by">rmorey</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880318">parent</a><span>|</span><a href="#35882124">next</a><span>|</span><label class="collapse" for="c-35880715">[-]</label><label class="expand" for="c-35880715">[18 more]</label></div><br/><div class="children"><div class="content">This is not a good take. Yes there is a lot more going on in brains than just neuronal activity, we don’t understand most of it. But understanding neurons and their connections is necessary (but not sufficient) to understanding what we consider intelligence.
 Also, 250k is a lot of neurons! Individual ants, as well as fruit flies which have even fewer neurons, show behavior we may consider intelligent.
Source: I am not a scientist, but I work in neuroscience research</div><br/><div id="35880764" class="c"><input type="checkbox" id="c-35880764" checked=""/><div class="controls bullet"><span class="by">srcreigh</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880715">parent</a><span>|</span><a href="#35882124">next</a><span>|</span><label class="collapse" for="c-35880764">[-]</label><label class="expand" for="c-35880764">[17 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the argument that understanding neurons is necessary?<p>Perhaps intelligence is like a black box input to our bodies (call it the &quot;soul&quot;, even though this isn&#x27;t testable and therefore not a hypothesis). The mind therefore wouldn&#x27;t play any more of a role in intelligence than the eye. And I&#x27;m not sure people would say the eye is necessary for understanding intelligence.<p>Now, I&#x27;m not really in a position to argue for such a thing, even if I believe it, but I&#x27;m curious what argument you might have against it.</div><br/><div id="35880960" class="c"><input type="checkbox" id="c-35880960" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880764">parent</a><span>|</span><a href="#35880928">next</a><span>|</span><label class="collapse" for="c-35880960">[-]</label><label class="expand" for="c-35880960">[5 more]</label></div><br/><div class="children"><div class="content">You can actually hypothesize that a soul exists and that intelligence is non-material, its just that your tests would quickly disprove that hypothesis - crude physical, mechanical modifications to the brain cause changes to intellect and character. If your hypothesis was correct you would not expect to see changes like that at all.<p>Some people think that neurons specifically aren&#x27;t necessary for understanding intelligence but in the same way that understanding transistors isn&#x27;t necessary to understand computers, that neurons comprise the units that more readily explain intelligence.</div><br/><div id="35882453" class="c"><input type="checkbox" id="c-35882453" checked=""/><div class="controls bullet"><span class="by">jessicas_rabbit</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880960">parent</a><span>|</span><a href="#35883503">next</a><span>|</span><label class="collapse" for="c-35882453">[-]</label><label class="expand" for="c-35882453">[2 more]</label></div><br/><div class="children"><div class="content">I’m here playing devil’s advocate - this test doesn’t work. Here are some related thought experiments.<p>Suppose a soul is an immaterial source of intelligence, but it controls the body via machine-like material hardware such as neurons.<p>Or an alternative, suppose there is a soul inside your body “watching” the sensory activations within your brain like a movie. The brain and body create the movie &amp; have some intelligence, but other important properties of the consciousness are bound to this observer entity.<p>In both these cases, the test just shows that if you damage the hardware, you can no longer observe intelligence because you’ve broken the end-to-end flow of the machine.</div><br/><div id="35882962" class="c"><input type="checkbox" id="c-35882962" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882453">parent</a><span>|</span><a href="#35883503">next</a><span>|</span><label class="collapse" for="c-35882962">[-]</label><label class="expand" for="c-35882962">[1 more]</label></div><br/><div class="children"><div class="content">Its fine if you are playing or supposing in seriousness but with good humor, it doesn&#x27;t really change how anyone else should interact with you :)<p>But yes, supposing that then you would expect to only see damages that correspond such as different forms of paralysis or other purely mechanical damages, not things that change the interior perspective.<p>Otherwise you start postulating the existence of a thing whose sole justification is your desire for the existence of that thing, which is natural when you start questioning beliefs and kick out all the supports without meaning to.<p>I think this is what Bertrand Russel&#x27;s teapot was meant to ellucidate.</div><br/></div></div></div></div><div id="35882257" class="c"><input type="checkbox" id="c-35882257" checked=""/><div class="controls bullet"><span class="by">didericis</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880960">parent</a><span>|</span><a href="#35883503">prev</a><span>|</span><a href="#35880928">next</a><span>|</span><label class="collapse" for="c-35882257">[-]</label><label class="expand" for="c-35882257">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You can actually hypothesize that a soul exists and that intelligence is non-material, its just that your tests would quickly disprove that hypothesis - crude physical, mechanical modifications to the brain cause changes to intellect and character. If your hypothesis was correct you would not expect to see changes like that at all.<p>That’s not necessarily a disproof. It’s also not necessarily reasonable to conflate what we call “the soul” with intelligence.<p>This is entering the world of philosophy, metaphysics and religion and leaving the world of science.<p>The modern instinct is to simply call bullshit on anything which cannot be materially verified, which is in many ways a very wise thing to do. But it’s worth leaving a door open for weirdness, because apart from very limited kinds of mathematical truth (maybe), I think everything we’ve ever thought to be true has had deeper layers revealed to us than we could have previously imagined.<p>Consider the reported experience of people who’ve had strokes and lost their ability to speak, and then later regained that ability through therapy. They report <i>experiencing</i> their own thoughts and <i>wanting</i> to speak, but something goes wrong&#x2F;they can’t translate that into a physical manifestation of their inner state.<p>Emotional regulation, personality, memory, processing speed, etc… are those really that different from speech? Are they really the <i>essence</i> of who we are, or are they a bridge to the physical world manifest within our bodies?<p>We can’t reverse most brain damage, so it’s usually not possible to ask a person what their experience of a damaged state is like in comparison to an improved state. We do have a rough, strange kind of comparison in thinking about our younger selves, though. We were all previously pre memory, drooling, poorly regulated babies (and before that, fetuses with no real perception at all). Is it right to say you didn’t have a soul when you were 3 weeks? A year? Two years? When exactly does “you” begin? I can’t remember who I was when I was when I was 2 months with any clarity at all, and you could certainly put different babies in doctored videos and I wouldn’t be able to tell what was me&#x2F;make up stories and I’d probably just absorb them. But I’m still <i>me</i> and <i>am</i> that 2 month old, much later in time. Whatever I’m experiencing has a weird kind of continuity. Is that encoded in the brain, even though I can’t remember it? Almost definitely, yeah. Is that <i>all</i> of what that experience of continuity is, and where that sense is coming from? I’ve got <i>no</i> idea. I certainly <i>feel</i> deeper. Remember that we all are not living in the real world, we’re all <i>living in our conscious perception</i>. The notion that we can see all of it within a conscious mirror is a pretty bold claim. We can see a lot of it and damage the bridge&#x2F;poke ourselves in the eyes with icepicks and whatnot, and that does stuff, but what <i>exactly</i> is it doing? Can we really know?<p>Intuitively most people would say they were still <i>themselves</i> when they were babies despite the lack of physical development of the brain. Whatever is constructing that continuous experience of self is not memory, because that’s not always there, not intelligence, because that’s not always there, not personality, because that’s not always there… it’s <i>weird</i>.<p>I think it’s important to remember that. Whenever people think they have human beings fully figured out down to the last mechanical detail and have sufficient understanding to declare who does and doesn’t have a soul and what that means in physical terms, bad things tend to happen. And that goes beyond a statement to be cautious about this kind of stuff purely out of moral hazard; the continual hazard is always as empirical as it is moral. We can never really <i>know</i> what we are. Our perceptual limitations may prove assumptions we make about what we are to be terribly, terribly wrong, despite what seems like certainty.</div><br/></div></div></div></div><div id="35880928" class="c"><input type="checkbox" id="c-35880928" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880764">parent</a><span>|</span><a href="#35880960">prev</a><span>|</span><a href="#35881240">next</a><span>|</span><label class="collapse" for="c-35880928">[-]</label><label class="expand" for="c-35880928">[5 more]</label></div><br/><div class="children"><div class="content">Brain damage by physical trauma, disease, oxygen deprivation, etc. has dramatic and often permanent effects on the mind.<p>The effect of drugs (including alcohol) on the mind.  Of note is anesthesia which can reliably and reversibly stop internal experience in the mind.<p>For a non-physical soul to hold our mind we would expect significant divergence from the above.  Out of body experiences and similar are indistinguishable from dreams&#x2F;hallucinations when tested against external reality (remote viewing and the like).</div><br/><div id="35883524" class="c"><input type="checkbox" id="c-35883524" checked=""/><div class="controls bullet"><span class="by">chimprich</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880928">parent</a><span>|</span><a href="#35881253">next</a><span>|</span><label class="collapse" for="c-35883524">[-]</label><label class="expand" for="c-35883524">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  Brain damage by physical trauma, disease, oxygen deprivation, etc. has dramatic and often permanent effects on the mind.<p>That&#x27;s not a completely watertight argument.<p>Consider a traditional FM&#x2F;AM radio. You can modify it, damage it, and get notable changes to its behaviour...<p>&gt; Of note is anesthesia which can reliably and reversibly stop internal experience in the mind<p>...turn it off and on again...<p>&gt; For a non-physical soul to hold our mind we would expect significant divergence from the above.<p>... yet concluding that all the noises produced from the radio are purely internal, mechanical and physical would be the wrong conclusion.<p>(I&#x27;m not arguing that the human brain&#x2F;mind is anything like analogous to a radio, just pointing out the limits of this approach.)</div><br/><div id="35884738" class="c"><input type="checkbox" id="c-35884738" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883524">parent</a><span>|</span><a href="#35881253">next</a><span>|</span><label class="collapse" for="c-35884738">[-]</label><label class="expand" for="c-35884738">[2 more]</label></div><br/><div class="children"><div class="content">I mean, if we&#x27;re really going to go there, who&#x27;s to say that a large enough LLM doesn&#x27;t automatically receive a soul simply because that&#x27;s one of the fundamental laws of the universe as decreed by the Creator?</div><br/><div id="35884839" class="c"><input type="checkbox" id="c-35884839" checked=""/><div class="controls bullet"><span class="by">chimprich</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884738">parent</a><span>|</span><a href="#35881253">next</a><span>|</span><label class="collapse" for="c-35884839">[-]</label><label class="expand" for="c-35884839">[1 more]</label></div><br/><div class="children"><div class="content">Going where? I wasn&#x27;t arguing for the existence of a soul.<p>Although, sure, if we could somehow manage to determine that souls did exist then presumably an AI model as capable as a human would also be eligible for one.</div><br/></div></div></div></div></div></div><div id="35881253" class="c"><input type="checkbox" id="c-35881253" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880928">parent</a><span>|</span><a href="#35883524">prev</a><span>|</span><a href="#35881240">next</a><span>|</span><label class="collapse" for="c-35881253">[-]</label><label class="expand" for="c-35881253">[1 more]</label></div><br/><div class="children"><div class="content">“For a non-physical soul to hold our mind we would expect significant divergence from the above.”<p>This sounds like it assumes a physical mind could access a non-physical soul. All we probably know is that we have to be using an intact mind to use free will.</div><br/></div></div></div></div><div id="35881240" class="c"><input type="checkbox" id="c-35881240" checked=""/><div class="controls bullet"><span class="by">rmorey</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880764">parent</a><span>|</span><a href="#35880928">prev</a><span>|</span><a href="#35881204">next</a><span>|</span><label class="collapse" for="c-35881240">[-]</label><label class="expand" for="c-35881240">[1 more]</label></div><br/><div class="children"><div class="content">The other comments have pretty much covered it. We can pretty clearly demonstrate that neurons in general are important to behavior (brain damage, etc) and we even have some understanding about specific neurons or populations&#x2F;circuits of neurons and their relation to specific behaviors (Grid cells are a cool example). And this work is all ongoing, but we&#x27;re also starting to relate the connectivity of networks of neurons to their function and role in information processing. Recently the first full connectome of a larval fruit fly was published - stay tuned for the first full adult connectome from our lab ;)<p>Again, IANA neuroscientist, but this is my understanding from the literature and conversations with the scientists I work with.</div><br/></div></div><div id="35881204" class="c"><input type="checkbox" id="c-35881204" checked=""/><div class="controls bullet"><span class="by">istjohn</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880764">parent</a><span>|</span><a href="#35881240">prev</a><span>|</span><a href="#35882124">next</a><span>|</span><label class="collapse" for="c-35881204">[-]</label><label class="expand" for="c-35881204">[5 more]</label></div><br/><div class="children"><div class="content">Why would you doubt neurons play a roll in intelligence when we&#x27;ve seen so much success in emulating human intelligence with artificial neural networks? It might have been an interesting argument 20 years ago. It&#x27;s just silly now.</div><br/><div id="35882356" class="c"><input type="checkbox" id="c-35882356" checked=""/><div class="controls bullet"><span class="by">didericis</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881204">parent</a><span>|</span><a href="#35882637">next</a><span>|</span><label class="collapse" for="c-35882356">[-]</label><label class="expand" for="c-35882356">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It might have been an interesting argument 20 years ago. It’s just silly now.<p>Is it?<p>These networks are capable of copying <i>something</i>, yes. Do we have a good understanding of what that is?<p>Not really, no. At least I don’t. I’m sure lots of people have a much better understanding than I do, but I think its hard to know exactly whats going on.<p>People dismiss the stochastic parrot argument because of how impressive big neural nets are, but it doesn’t really invalidate that argument. Is a very, very, very good parrot that learns from everyone at once doing basically the same as what we do? I’d argue no, at least not fully. It’s absorbed aspects of us extremely well&#x2F;is a very weird, sophisticated mirror, yes, and is copying something somehow, probably in a way reminiscent of how we copy. Is it basically the same as what we’re doing when we think? Partially? Fully? Not at all?<p>A typical engineer would say “good enough”. That type of response is valuable in a lot of contexts, but I think the willingness to apply it to these models is pretty reckless, even if it’s impossible to easily “prove” why.<p>To be clear on the exact statement you made, I think you’re right&#x2F;it’s pretty clear neurons play <i>some</i> very important role&#x2F;potentially capture a ton of what we consider intelligence, but I don’t think anyone really knows what exactly is being captured&#x2F;what amount of thought and experience they’re responsible for.</div><br/><div id="35883482" class="c"><input type="checkbox" id="c-35883482" checked=""/><div class="controls bullet"><span class="by">Noble_Lie</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882356">parent</a><span>|</span><a href="#35882637">next</a><span>|</span><label class="collapse" for="c-35883482">[-]</label><label class="expand" for="c-35883482">[2 more]</label></div><br/><div class="children"><div class="content">That person&#x27;s argument is borderline insane to me - a severe lack of knowing what is unknown, a reverence of current best-models (regards modern science, including neurology - yet, open minded investigations beyond are also a requisite here.)  And the pompousness is what truly boggles my mind (&quot;Its silly to believe <i>this</i>, <i>now</i>.)  A look in the mirror would suffice to say the least...<p>Anyway, thank you for a great answer and conversation throughout this thread.<p>Regards neural networks, parroting and the emulation of intelligence (or the difference between an emulation and the &quot;real thing&quot;):<p>Well, somewhat like you say, we cannot propose a valid comparison from one to the other without an understanding of one (consciousness) or both.  It&#x27;s fascinating that there are some open, valid and pressing questions about what &#x2F; how the output of this new wave of software is concretized (from foundational, semi-statistical algorithms in this case.)<p>Yes, I do agree neurons have <i>something</i> to do with the &quot;final output&quot;. But this is a subsection of the problem - organic neurons is-an&#x2F;are order(s) of magnitude in complexity beyond what the tricky &quot;parrot&quot; is up to.  Moreso, these components perform very different functionally - the <i>known</i> functions of the neuron compared to ANN, backprop etc. The entire stack.)<p>P.S: One interesting theory I like to simulate and&#x2F;or entertain is that every organic cell in the body has something to do with the final output of consciousness.</div><br/><div id="35884164" class="c"><input type="checkbox" id="c-35884164" checked=""/><div class="controls bullet"><span class="by">didericis</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35883482">parent</a><span>|</span><a href="#35882637">next</a><span>|</span><label class="collapse" for="c-35884164">[-]</label><label class="expand" for="c-35884164">[1 more]</label></div><br/><div class="children"><div class="content">Thank you, appreciate the compliment.<p>And yeah, are definitely a lot of open questions related to all of this. Love how its brought so many deep questions into focus.</div><br/></div></div></div></div></div></div><div id="35882637" class="c"><input type="checkbox" id="c-35882637" checked=""/><div class="controls bullet"><span class="by">telotortium</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881204">parent</a><span>|</span><a href="#35882356">prev</a><span>|</span><a href="#35882124">next</a><span>|</span><label class="collapse" for="c-35882637">[-]</label><label class="expand" for="c-35882637">[1 more]</label></div><br/><div class="children"><div class="content">If anything the experience with artificial neural networks argues the opposite - biological neurons are quite a bit different than the &quot;neurons&quot; of ANNs, and backpropagation is not something that exists biologically.</div><br/></div></div></div></div></div></div></div></div><div id="35882124" class="c"><input type="checkbox" id="c-35882124" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880318">parent</a><span>|</span><a href="#35880715">prev</a><span>|</span><a href="#35880886">next</a><span>|</span><label class="collapse" for="c-35882124">[-]</label><label class="expand" for="c-35882124">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There is no evidence that intelligence runs on neurons.<p>1. Neurons connect all our senses and all our muscles.<p>2. Neurons are the definitive difference between the brain and the rest of the body.  There is “other stuff” in the brain, but it’s not so different from the “other stuff” that’s in your rear end.<p>Don’t underestimate what a neuron can do. A single artificial neuron can fit a logistic regression model.  A quarter of a million is on then scale of some our our largest AI, and biological neurons are far more connected than ANN.  An ant quite likely has a more powerful brain than GPT-4.</div><br/></div></div><div id="35880886" class="c"><input type="checkbox" id="c-35880886" checked=""/><div class="controls bullet"><span class="by">holoduke</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880318">parent</a><span>|</span><a href="#35882124">prev</a><span>|</span><a href="#35880537">next</a><span>|</span><label class="collapse" for="c-35880886">[-]</label><label class="expand" for="c-35880886">[1 more]</label></div><br/><div class="children"><div class="content">Maybe the neurons are the hardware layer. The software is represented by the electronic activity. There is a good video <a href="https:&#x2F;&#x2F;youtu.be&#x2F;XheAMrS8Q1c" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;XheAMrS8Q1c</a> about this topic.</div><br/></div></div><div id="35880537" class="c"><input type="checkbox" id="c-35880537" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880318">parent</a><span>|</span><a href="#35880886">prev</a><span>|</span><a href="#35880786">next</a><span>|</span><label class="collapse" for="c-35880537">[-]</label><label class="expand" for="c-35880537">[3 more]</label></div><br/><div class="children"><div class="content">If you really want to present ants as a civilization, I don&#x27;t think a single ant is a meaningful unit of that civilization comparable to a single human. A colony, perhaps - but then that&#x27;s a lot more neurons, just distributed.</div><br/><div id="35882908" class="c"><input type="checkbox" id="c-35882908" checked=""/><div class="controls bullet"><span class="by">hypertele-Xii</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880537">parent</a><span>|</span><a href="#35880786">next</a><span>|</span><label class="collapse" for="c-35882908">[-]</label><label class="expand" for="c-35882908">[2 more]</label></div><br/><div class="children"><div class="content">A single human will die permanently, alone, miserable, and unproductive. The comparison is apt.</div><br/><div id="35884747" class="c"><input type="checkbox" id="c-35884747" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882908">parent</a><span>|</span><a href="#35880786">next</a><span>|</span><label class="collapse" for="c-35884747">[-]</label><label class="expand" for="c-35884747">[1 more]</label></div><br/><div class="children"><div class="content">A single human can survive on their own; there are many historical examples of that. A detached body part, on the other hand, cannot; but it also cannot feel miserable etc. A single ant is more like a body part of the colony in that sense.</div><br/></div></div></div></div></div></div><div id="35880786" class="c"><input type="checkbox" id="c-35880786" checked=""/><div class="controls bullet"><span class="by">burnished</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880318">parent</a><span>|</span><a href="#35880537">prev</a><span>|</span><a href="#35880383">next</a><span>|</span><label class="collapse" for="c-35880786">[-]</label><label class="expand" for="c-35880786">[2 more]</label></div><br/><div class="children"><div class="content">What else would intelligence run on?</div><br/><div id="35883056" class="c"><input type="checkbox" id="c-35883056" checked=""/><div class="controls bullet"><span class="by">resource0x</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880786">parent</a><span>|</span><a href="#35880383">next</a><span>|</span><label class="collapse" for="c-35883056">[-]</label><label class="expand" for="c-35883056">[1 more]</label></div><br/><div class="children"><div class="content">Every cell in our body, and every bacterium living in a body (e.g. gut flora), contribute to our intelligence. It looks plausible (to me) that there&#x27;s one &quot;top cell&quot; among them that represents the &quot;person&quot;, others just contributing via layered signals, but whether this &quot;top cell&quot; is a neuron or another kind of cell is unknown.</div><br/></div></div></div></div><div id="35880383" class="c"><input type="checkbox" id="c-35880383" checked=""/><div class="controls bullet"><span class="by">account-5</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880318">parent</a><span>|</span><a href="#35880786">prev</a><span>|</span><a href="#35880302">next</a><span>|</span><label class="collapse" for="c-35880383">[-]</label><label class="expand" for="c-35880383">[10 more]</label></div><br/><div class="children"><div class="content">In what way a civilization?</div><br/><div id="35880428" class="c"><input type="checkbox" id="c-35880428" checked=""/><div class="controls bullet"><span class="by">jxf</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880383">parent</a><span>|</span><a href="#35880302">next</a><span>|</span><label class="collapse" for="c-35880428">[-]</label><label class="expand" for="c-35880428">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll repost a comment via Reddit that I think makes this case [0]:<p>Ants have developed architecture, with plumbing, ventilation, nurseries for rearing the young, and paved thoroughfares. Ants practice agriculture, including animal husbandry. Ants have social stratification that differs from but is comparable to that of human cultures, with division of labor into worker, soldier, and other specialties that do not have a clear human analogy.<p>Ants enslave other ants. Ants interactively teach other ants, something few other animals do, among them humans. Ants have built &quot;supercolonies&quot; dwarfing any human city, stretching over 5,000 km in one place. And ants too have a complex culture of sorts, including rich languages based on pheromones.<p>Despite the radically different nature of our two civilizations, it is undeniable from an objective standpoint that this level of society has been achieved by ants.<p>[0]: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;unpopularopinion&#x2F;comments&#x2F;t2h1vs&#x2F;ants_have_achieved_civilization&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;unpopularopinion&#x2F;comments&#x2F;t2h1vs&#x2F;an...</a></div><br/><div id="35880833" class="c"><input type="checkbox" id="c-35880833" checked=""/><div class="controls bullet"><span class="by">est31</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880428">parent</a><span>|</span><a href="#35880723">next</a><span>|</span><label class="collapse" for="c-35880833">[-]</label><label class="expand" for="c-35880833">[1 more]</label></div><br/><div class="children"><div class="content">I think you can call ant societies civilizations, but the same time you can call a multi cellular organism a civilization, too. Usually, those also come from the same genetic seed similar to (most) ant colonies. But more importantly, you have various types of cooperation and specialization in multi cellular life. Airways are &quot;ventillation&quot;, chitin using or keratinated tissues are &quot;architecture&quot;, and there is even &quot;animal husbandry&quot; in the form of bacterial colonies living in organs.</div><br/></div></div><div id="35880723" class="c"><input type="checkbox" id="c-35880723" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880428">parent</a><span>|</span><a href="#35880833">prev</a><span>|</span><a href="#35883801">next</a><span>|</span><label class="collapse" for="c-35880723">[-]</label><label class="expand" for="c-35880723">[6 more]</label></div><br/><div class="children"><div class="content">To be honest, this description is leaning <i>heavily</i> on the associations we have with individual words used. Ant &quot;architecture&quot; isn&#x27;t like our architecture. Ant &quot;plumbing&quot; and &quot;ventilation&quot; have little in common with the kind of plumbing and ventilation we use in buildings. &quot;Nurseries&quot;, &quot;rearing the young&quot;, that&#x27;s just stretching the analogy to the point of breaking. &quot;Agriculture&quot;, &quot;animal husbandry&quot; - I don&#x27;t even know how to comment on that. &quot;Social stratification&quot; is literally a chemical feedback loop - ant larvae can be influenced by certain pheromones to develop into different types of ants, which happen to emit pheromones <i>suppressing</i> development of larvae into more ants of that type. Etc.<p>I could go on and on. Point being, analogies are fun and sometimes illuminating, but they&#x27;re just that. There&#x27;s a <i>vast</i> difference in complexity between what ants do, and what humans do.</div><br/><div id="35882027" class="c"><input type="checkbox" id="c-35882027" checked=""/><div class="controls bullet"><span class="by">jxf</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880723">parent</a><span>|</span><a href="#35882959">next</a><span>|</span><label class="collapse" for="c-35882027">[-]</label><label class="expand" for="c-35882027">[1 more]</label></div><br/><div class="children"><div class="content">You could say the same in reverse. Humans can&#x27;t lift fifty times their own weight. Humans can&#x27;t communicate in real-time with pheromones alone. Most humans do not know how to build their own home. An ant might well consider us backwards, not advanced.</div><br/></div></div><div id="35882959" class="c"><input type="checkbox" id="c-35882959" checked=""/><div class="controls bullet"><span class="by">hypertele-Xii</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880723">parent</a><span>|</span><a href="#35882027">prev</a><span>|</span><a href="#35880863">next</a><span>|</span><label class="collapse" for="c-35882959">[-]</label><label class="expand" for="c-35882959">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Agriculture&quot;, &quot;animal husbandry&quot; - I don&#x27;t even know how to comment on that.<p>To give some examples of ant agriculture and animal husbandry, they cut and feed leaves to a fungus they farm for food. The fungus even communicates to the ants to switch to a different plant when the plant produces toxins in defense to cutting.<p>Ants herd aphids, protecting them from predators (ladybugs), secrete pheromones that direct and pacify them, and massage their abdomens to milk sugar dew.</div><br/></div></div><div id="35880863" class="c"><input type="checkbox" id="c-35880863" checked=""/><div class="controls bullet"><span class="by">ASalazarMX</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880723">parent</a><span>|</span><a href="#35882959">prev</a><span>|</span><a href="#35880839">next</a><span>|</span><label class="collapse" for="c-35880863">[-]</label><label class="expand" for="c-35880863">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There&#x27;s a vast difference in complexity between what ants do, and what humans do.<p>Interesting parallell with intelligence&#x2F;sentience&#x2F;sapience. Despite the means, isn&#x27;t the end result what you have to judge? The end result looks like a rudimentary civilization. How much back in time would we have to go back to find more sophistication in ant societies than humans?</div><br/></div></div><div id="35880839" class="c"><input type="checkbox" id="c-35880839" checked=""/><div class="controls bullet"><span class="by">mandmandam</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880723">parent</a><span>|</span><a href="#35880863">prev</a><span>|</span><a href="#35883801">next</a><span>|</span><label class="collapse" for="c-35880839">[-]</label><label class="expand" for="c-35880839">[2 more]</label></div><br/><div class="children"><div class="content">Nobody is saying that an ant might be the next Frank Lloyd Wright.<p>They&#x27;re saying they accomplish <i>incredible</i> things for the size of their brain, which is absolutely and unequivocally true.<p>&quot;Go to the ant, thou sluggard; consider her ways, and be wise&quot;.</div><br/><div id="35881673" class="c"><input type="checkbox" id="c-35881673" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880839">parent</a><span>|</span><a href="#35883801">next</a><span>|</span><label class="collapse" for="c-35881673">[-]</label><label class="expand" for="c-35881673">[1 more]</label></div><br/><div class="children"><div class="content">What I&#x27;m saying is that, Bible parables notwithstanding, it&#x27;s not <i>the individual ant</i> that achieves these incredible things. The bulk of computational&#x2F;cognitive work is done by the colony as a system. This means that there&#x27;s little sense in comparing brainpower of an ant with that of a human. A more informative comparison is that between an ant <i>colony</i> and human <i>society</i> - here, humans may come out badly, but that&#x27;s arguably because our societies are overcomplicated in order to compensate for individual humans having <i>too much</i> brainpower :).</div><br/></div></div></div></div></div></div><div id="35883801" class="c"><input type="checkbox" id="c-35883801" checked=""/><div class="controls bullet"><span class="by">account-5</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880428">parent</a><span>|</span><a href="#35880723">prev</a><span>|</span><a href="#35880302">next</a><span>|</span><label class="collapse" for="c-35883801">[-]</label><label class="expand" for="c-35883801">[1 more]</label></div><br/><div class="children"><div class="content">Do termites count as well then? Bees? I&#x27;m not being facetious, but these insects seem similar.</div><br/></div></div></div></div></div></div></div></div><div id="35880302" class="c"><input type="checkbox" id="c-35880302" checked=""/><div class="controls bullet"><span class="by">dennisy</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880118">parent</a><span>|</span><a href="#35880318">prev</a><span>|</span><a href="#35880898">next</a><span>|</span><label class="collapse" for="c-35880302">[-]</label><label class="expand" for="c-35880302">[5 more]</label></div><br/><div class="children"><div class="content">Nature created humans to understand nature. We created GPT4 to understand ourselves.</div><br/><div id="35881278" class="c"><input type="checkbox" id="c-35881278" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880302">parent</a><span>|</span><a href="#35880686">next</a><span>|</span><label class="collapse" for="c-35881278">[-]</label><label class="expand" for="c-35881278">[2 more]</label></div><br/><div class="children"><div class="content">Humans are the universe asking who made it.</div><br/><div id="35883241" class="c"><input type="checkbox" id="c-35883241" checked=""/><div class="controls bullet"><span class="by">slacka</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881278">parent</a><span>|</span><a href="#35880686">next</a><span>|</span><label class="collapse" for="c-35883241">[-]</label><label class="expand" for="c-35883241">[1 more]</label></div><br/><div class="children"><div class="content">Yes, or put a bit more elegantly, &#x27;The cosmos is within us. We are made of star-stuff. We are a way for the universe to know itself.&#x27; — Carl Sagan</div><br/></div></div></div></div><div id="35880686" class="c"><input type="checkbox" id="c-35880686" checked=""/><div class="controls bullet"><span class="by">NobleLie</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880302">parent</a><span>|</span><a href="#35881278">prev</a><span>|</span><a href="#35880868">next</a><span>|</span><label class="collapse" for="c-35880686">[-]</label><label class="expand" for="c-35880686">[1 more]</label></div><br/><div class="children"><div class="content">A mirror of ourselves*</div><br/></div></div><div id="35880868" class="c"><input type="checkbox" id="c-35880868" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880302">parent</a><span>|</span><a href="#35880686">prev</a><span>|</span><a href="#35880898">next</a><span>|</span><label class="collapse" for="c-35880868">[-]</label><label class="expand" for="c-35880868">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s beautiful until you think about it.<p>Humans so far have done a great job at destroying nature faster than any other kind could.<p>And GPT4 was created for profit.</div><br/></div></div></div></div><div id="35880898" class="c"><input type="checkbox" id="c-35880898" checked=""/><div class="controls bullet"><span class="by">talentedcoin</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880118">parent</a><span>|</span><a href="#35880302">prev</a><span>|</span><a href="#35883363">next</a><span>|</span><label class="collapse" for="c-35880898">[-]</label><label class="expand" for="c-35880898">[3 more]</label></div><br/><div class="children"><div class="content">There is no evidence that any of those are emergent properties. It’s no more or less logical than asserting they were placed there by a creator.</div><br/><div id="35881643" class="c"><input type="checkbox" id="c-35881643" checked=""/><div class="controls bullet"><span class="by">gitfan86</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880898">parent</a><span>|</span><a href="#35883363">next</a><span>|</span><label class="collapse" for="c-35881643">[-]</label><label class="expand" for="c-35881643">[2 more]</label></div><br/><div class="children"><div class="content">There is evidence that people believe that GTP-4 is intelligent since it can solve things like the SATs. But if you start taking away weights one by one at some point those same people will say it isn&#x27;t intelligent. A NN with 3 weights cannot solve any problems that humans believe requires intelligence. So where did it come from? I don&#x27;t know, but it clearly emerged as the NN got bigger.</div><br/><div id="35884443" class="c"><input type="checkbox" id="c-35884443" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881643">parent</a><span>|</span><a href="#35883363">next</a><span>|</span><label class="collapse" for="c-35884443">[-]</label><label class="expand" for="c-35884443">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s possible that the comment you&#x27;re responding to is referring to the new paper out of Stanford&#x2F;Google on emergence as a &#x27;mirage&#x27;.
The gist of it is that the shape of the curve for &#x27;metric&#x27; vs &#x27;params&#x27; is dependent upon the type of metric. For classification it has a certain shape, for accuracy it has another, etc.
The authors claim the observation that these curves are determined by the choice of metric means that it isn&#x27;t <i>true</i> emergence, but rather is due to the way in which GPTs are typically trained to predict the next token (and then beam search or typical sampling is often applied afterwards, etc).<p>The paper is somewhat new so I haven&#x27;t done a proper review to know if it&#x27;s solid work yet, but this may offer some context for some of the comments in this thread.</div><br/></div></div></div></div></div></div></div></div><div id="35883363" class="c"><input type="checkbox" id="c-35883363" checked=""/><div class="controls bullet"><span class="by">nr2x</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35880118">prev</a><span>|</span><a href="#35880879">next</a><span>|</span><label class="collapse" for="c-35883363">[-]</label><label class="expand" for="c-35883363">[1 more]</label></div><br/><div class="children"><div class="content">My favorite line: “Our overall procedure is quite compute intensive.”<p>Should that be measured in number of nuclear power plants needed to run the computation?  Or like, fractions of a small star’s output?</div><br/></div></div><div id="35880879" class="c"><input type="checkbox" id="c-35880879" checked=""/><div class="controls bullet"><span class="by">imranq</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35883363">prev</a><span>|</span><a href="#35881500">next</a><span>|</span><label class="collapse" for="c-35880879">[-]</label><label class="expand" for="c-35880879">[1 more]</label></div><br/><div class="children"><div class="content">I suspect that there&#x27;s a sweet spot that combines a collection of several &quot;neurons&quot; and a human-readable explanation given a certain kind of prompt. However, this &quot;three-body problem&quot; will probably need some serious analytical capability to understand at scale</div><br/></div></div><div id="35881500" class="c"><input type="checkbox" id="c-35881500" checked=""/><div class="controls bullet"><span class="by">kfrzcode</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35880879">prev</a><span>|</span><a href="#35879325">next</a><span>|</span><label class="collapse" for="c-35881500">[-]</label><label class="expand" for="c-35881500">[1 more]</label></div><br/><div class="children"><div class="content">here&#x27;s a clue, start your research<p>Natural language understanding comprises a wide range of diverse tasks such
as textual entailment, question answering, semantic similarity assessment, and
document classification. Although large unlabeled text corpora are abundant,
labeled data for learning these specific tasks is scarce, making it challenging for
discriminatively trained models to perform adequately. We demonstrate that large
gains on these tasks can be realized by generative pre-training of a language model
on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each
specific task. In contrast to previous approaches, we make use of task-aware input
transformations during fine-tuning to achieve effective transfer while requiring
minimal changes to the model architecture. We demonstrate the effectiveness of
our approach on a wide range of benchmarks for natural language understanding.
Our general task-agnostic model outperforms discriminatively trained models that
use architectures specifically crafted for each task, significantly improving upon the
state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute
improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on
question answering (RACE), and 1.5% on textual entailment (MultiNLI).</div><br/></div></div><div id="35879325" class="c"><input type="checkbox" id="c-35879325" checked=""/><div class="controls bullet"><span class="by">rvz</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35881500">prev</a><span>|</span><a href="#35884996">next</a><span>|</span><label class="collapse" for="c-35879325">[-]</label><label class="expand" for="c-35879325">[20 more]</label></div><br/><div class="children"><div class="content">&gt; Which is to say, we still have no clue as to what&#x27;s going on inside GPT-4 or even GPT-3, which I think is the question many want an answer to.<p>Exactly. Especially:<p>&gt; ...the technique is already very computationally intensive, and the focus on individual neurons as a function of input means that they can&#x27;t &quot;reverse engineer&quot; larger structures composed of multiple neurons nor a neuron that has multiple roles;<p>This paper just brings us no closer to explainability in black box neural networks and is just another excuse piece by OpenAI to try to please the explainability situation that has been missing for decades in neural networks.<p>It is also the reason why they cannot be trusted in the most serious of applications which such decision making requires lots of transparency rather than a model regurgitating nonsense confidently.</div><br/><div id="35879468" class="c"><input type="checkbox" id="c-35879468" checked=""/><div class="controls bullet"><span class="by">jahewson</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879325">parent</a><span>|</span><a href="#35880132">next</a><span>|</span><label class="collapse" for="c-35879468">[-]</label><label class="expand" for="c-35879468">[9 more]</label></div><br/><div class="children"><div class="content">&gt; It is also the reason why they cannot be trusted in the most serious of applications which such decision making requires lots of transparency rather than a model regurgitating nonsense confidently.<p>Like say, in court to detect if someone is lying? Or at an airport to detect drugs?</div><br/><div id="35879780" class="c"><input type="checkbox" id="c-35879780" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879468">parent</a><span>|</span><a href="#35880132">next</a><span>|</span><label class="collapse" for="c-35879780">[-]</label><label class="expand" for="c-35879780">[8 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t even have to look that far ahead. Apparently, people are already using ChatGPT to compile custom diet plans for themselves, and they expect it to take into account the information they supply regarding their allergies etc.<p>But, yes, those are also good examples of what we shouldn&#x27;t be doing, but are going to do anyway.</div><br/><div id="35880279" class="c"><input type="checkbox" id="c-35880279" checked=""/><div class="controls bullet"><span class="by">carlmr</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879780">parent</a><span>|</span><a href="#35880347">next</a><span>|</span><label class="collapse" for="c-35880279">[-]</label><label class="expand" for="c-35880279">[4 more]</label></div><br/><div class="children"><div class="content">&gt;Apparently, people are already using ChatGPT to compile custom diet plans for themselves, and they expect it to take into account the information they supply regarding their allergies etc.<p>Evolution is still doing it&#x27;s thing.</div><br/><div id="35880732" class="c"><input type="checkbox" id="c-35880732" checked=""/><div class="controls bullet"><span class="by">canadianfella</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880279">parent</a><span>|</span><a href="#35880351">prev</a><span>|</span><a href="#35880347">next</a><span>|</span><label class="collapse" for="c-35880732">[-]</label><label class="expand" for="c-35880732">[2 more]</label></div><br/><div class="children"><div class="content">What’s the risk? Someone allergic to peanuts will eat peanuts because ChatGPT put it in their diet plan? That’s silly.</div><br/><div id="35881375" class="c"><input type="checkbox" id="c-35881375" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880732">parent</a><span>|</span><a href="#35880347">next</a><span>|</span><label class="collapse" for="c-35881375">[-]</label><label class="expand" for="c-35881375">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s the risk, and people are literally doing that because &quot;if it put them in the recipe, it knows that quantity is safe for me&quot;, or &quot;I asked it if it&#x27;s okay and it cited a study saying that it is&quot;.</div><br/></div></div></div></div></div></div><div id="35880347" class="c"><input type="checkbox" id="c-35880347" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879780">parent</a><span>|</span><a href="#35880279">prev</a><span>|</span><a href="#35880132">next</a><span>|</span><label class="collapse" for="c-35880347">[-]</label><label class="expand" for="c-35880347">[3 more]</label></div><br/><div class="children"><div class="content">Those cases sound like Darwin Awards mediated by high technology</div><br/><div id="35884374" class="c"><input type="checkbox" id="c-35884374" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880347">parent</a><span>|</span><a href="#35880735">next</a><span>|</span><label class="collapse" for="c-35884374">[-]</label><label class="expand" for="c-35884374">[1 more]</label></div><br/><div class="children"><div class="content">I hear crushed glass has very great health benefits</div><br/></div></div></div></div></div></div></div></div><div id="35880132" class="c"><input type="checkbox" id="c-35880132" checked=""/><div class="controls bullet"><span class="by">ketzo</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879325">parent</a><span>|</span><a href="#35879468">prev</a><span>|</span><a href="#35881311">next</a><span>|</span><label class="collapse" for="c-35880132">[-]</label><label class="expand" for="c-35880132">[1 more]</label></div><br/><div class="children"><div class="content">Is it really fair to say this brings us “no closer” to explainability?<p>This seems like a novel approach to try to tackle the scale of the problem. Just because the earliest results aren’t great doesn’t mean it’s not a fruitful path to travel.</div><br/></div></div><div id="35880296" class="c"><input type="checkbox" id="c-35880296" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879325">parent</a><span>|</span><a href="#35881311">prev</a><span>|</span><a href="#35880105">next</a><span>|</span><label class="collapse" for="c-35880296">[-]</label><label class="expand" for="c-35880296">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the explainability situation that has been missing for decades in neural networks.<p>Is this true? I thought explainability for things like DNNs for vision made pretty good progress in the last decade.</div><br/></div></div><div id="35880105" class="c"><input type="checkbox" id="c-35880105" checked=""/><div class="controls bullet"><span class="by">pmarreck</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35879325">parent</a><span>|</span><a href="#35880296">prev</a><span>|</span><a href="#35884996">next</a><span>|</span><label class="collapse" for="c-35880105">[-]</label><label class="expand" for="c-35880105">[7 more]</label></div><br/><div class="children"><div class="content">&gt; It is also the reason why they cannot be trusted in the most serious of applications which such decision making requires lots of transparency rather than a model regurgitating nonsense confidently.<p>Doesn&#x27;t this criticism also apply to people to some extent? We don&#x27;t know what the purpose of individual brain neurons is.</div><br/><div id="35880281" class="c"><input type="checkbox" id="c-35880281" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880105">parent</a><span>|</span><a href="#35882197">next</a><span>|</span><label class="collapse" for="c-35880281">[-]</label><label class="expand" for="c-35880281">[4 more]</label></div><br/><div class="children"><div class="content">People are better understood intuitively. We understand how people fail and why. We can build trust with people with some degree of success. But machine models are new and can fail in unpredictable ways. They also get deployed to billions of users in a way that humans do not, and deployed in applications that humans do not. So its certainly useful to try to explain neural networks in as great of detail as we can.</div><br/><div id="35881421" class="c"><input type="checkbox" id="c-35881421" checked=""/><div class="controls bullet"><span class="by">istjohn</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880281">parent</a><span>|</span><a href="#35882197">next</a><span>|</span><label class="collapse" for="c-35881421">[-]</label><label class="expand" for="c-35881421">[3 more]</label></div><br/><div class="children"><div class="content">Or we can build trust using black box methods like we do with humans, e.g., extrapolating from past behavior, administering tests, and the like.</div><br/><div id="35884732" class="c"><input type="checkbox" id="c-35884732" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35881421">parent</a><span>|</span><a href="#35882197">next</a><span>|</span><label class="collapse" for="c-35884732">[-]</label><label class="expand" for="c-35884732">[2 more]</label></div><br/><div class="children"><div class="content">We can, but the nice thing about neural networks is the ability to do all kinds of computational and mathematical manipulations to them to basically pick them apart and really find out what’s going on. This is important not just for safe deployment but also for research on new methods that could be used to make them better. Plus we need this ability to help avoid neural networks with intentionally hidden features that appear to behave linearly in certain regimes but are designed with a strong nonlinear response when special inputs are applied. You could have all the tests you want for a self driving car based on real world conditions but some bad actor with access to the training system could create a special input that results in dangerous behavior.</div><br/><div id="35884756" class="c"><input type="checkbox" id="c-35884756" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35884732">parent</a><span>|</span><a href="#35882197">next</a><span>|</span><label class="collapse" for="c-35884756">[-]</label><label class="expand" for="c-35884756">[1 more]</label></div><br/><div class="children"><div class="content">The more fundamental problem is the sheer size of them, and this is only going to get worse as models grow larger to become more capable. Being able to look at the state of individual neurons during inference is very convenient, but that does not by itself make it possible to really find out what&#x27;s going on.</div><br/></div></div></div></div></div></div></div></div><div id="35882197" class="c"><input type="checkbox" id="c-35882197" checked=""/><div class="controls bullet"><span class="by">theodorejb</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35880105">parent</a><span>|</span><a href="#35880281">prev</a><span>|</span><a href="#35884996">next</a><span>|</span><label class="collapse" for="c-35882197">[-]</label><label class="expand" for="c-35882197">[2 more]</label></div><br/><div class="children"><div class="content">As a person I can at least tell you what I do and don&#x27;t understand about something, ask questions to improve&#x2F;correct my understanding, and truthfully explain my perspective and reasoning.<p>The machine model is not only a black box, but one incapable of understanding anything about its input, &quot;thought process&quot;, or output. It will blindly spit out a response based on its training data and weights, without knowing the difference whether it true or false, meaningful or complete gibberish.</div><br/><div id="35882573" class="c"><input type="checkbox" id="c-35882573" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35879144">root</a><span>|</span><a href="#35882197">parent</a><span>|</span><a href="#35884996">next</a><span>|</span><label class="collapse" for="c-35882573">[-]</label><label class="expand" for="c-35882573">[1 more]</label></div><br/><div class="children"><div class="content">As a person, you can tell what <i>you think</i> you do and don&#x27;t understand, and you can explain what <i>you think</i> your reasoning is. In practice, people get both wrong all the time. People aren&#x27;t always truthful about it, either, and there&#x27;s no reliable way to tell if they are.</div><br/></div></div></div></div></div></div></div></div><div id="35884996" class="c"><input type="checkbox" id="c-35884996" checked=""/><div class="controls bullet"><span class="by">mnky9800n</span><span>|</span><a href="#35879144">parent</a><span>|</span><a href="#35879325">prev</a><span>|</span><a href="#35884036">next</a><span>|</span><label class="collapse" for="c-35884996">[-]</label><label class="expand" for="c-35884996">[1 more]</label></div><br/><div class="children"><div class="content">Yes but they are from openai so they can just write papers that say whatever they want to say without minding the metrics and then pretend like it is some kind of science.</div><br/></div></div></div></div><div id="35884036" class="c"><input type="checkbox" id="c-35884036" checked=""/><div class="controls bullet"><span class="by">mxschll</span><span>|</span><a href="#35879144">prev</a><span>|</span><a href="#35881288">next</a><span>|</span><label class="collapse" for="c-35884036">[-]</label><label class="expand" for="c-35884036">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We are open-sourcing our datasets and visualization tools for GPT-4-written explanations of all 307,200 neurons in GPT-2, as well as code for explanation and scoring using publicly available models on the OpenAI API. We hope the research community will develop new techniques for generating higher-scoring explanations and better tools for exploring GPT-2 using explanations.<p>Aww, that&#x27;s so nice of them to let the community do the work they can use for free. I might even forget that most of OpenAI is closed source.</div><br/><div id="35884702" class="c"><input type="checkbox" id="c-35884702" checked=""/><div class="controls bullet"><span class="by">posterboy</span><span>|</span><a href="#35884036">parent</a><span>|</span><a href="#35881288">next</a><span>|</span><label class="collapse" for="c-35884702">[-]</label><label class="expand" for="c-35884702">[1 more]</label></div><br/><div class="children"><div class="content">Surely they are willing to shell a out a few bucks</div><br/></div></div></div></div><div id="35881288" class="c"><input type="checkbox" id="c-35881288" checked=""/><div class="controls bullet"><span class="by">Ameo</span><span>|</span><a href="#35884036">prev</a><span>|</span><a href="#35877555">next</a><span>|</span><label class="collapse" for="c-35881288">[-]</label><label class="expand" for="c-35881288">[3 more]</label></div><br/><div class="children"><div class="content">I built a toy neural network that runs in the browser[1] to model 2D functions with the goal of doing something similar to this research (in a much more limited manner, ofc). Since the input space is so much more limited than language models or similar, it&#x27;s possible to examine the outputs for each neuron for all possible inputs, and in a continuous manner.<p>In some cases, you can clearly see neurons that specialize to different areas of the function being modeled, like this one: <a href="https:&#x2F;&#x2F;i.ameo.link&#x2F;b0p.png" rel="nofollow">https:&#x2F;&#x2F;i.ameo.link&#x2F;b0p.png</a><p>This OpenAI research seems to be feeding lots of varied input text into the models they&#x27;re examining and keeping track of the activations of different neurons along the way.  Another method I remember seeing used in the past involves using an optimizer to generate inputs that maximally activate particular neurons in vision models[2].<p>I&#x27;m sure that&#x27;s much more difficult or even impossible for transformers which operate on sequences of tokens&#x2F;embeddings rather than single static input vectors, but maybe there&#x27;s a way to generate input embeddings and then use some method to convert them back into tokens.<p>[1] <a href="https:&#x2F;&#x2F;nn.ameo.dev&#x2F;" rel="nofollow">https:&#x2F;&#x2F;nn.ameo.dev&#x2F;</a><p>[2] <a href="https:&#x2F;&#x2F;www.tensorflow.org&#x2F;tutorials&#x2F;generative&#x2F;deepdream" rel="nofollow">https:&#x2F;&#x2F;www.tensorflow.org&#x2F;tutorials&#x2F;generative&#x2F;deepdream</a></div><br/><div id="35882597" class="c"><input type="checkbox" id="c-35882597" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35881288">parent</a><span>|</span><a href="#35883513">next</a><span>|</span><label class="collapse" for="c-35882597">[-]</label><label class="expand" for="c-35882597">[1 more]</label></div><br/><div class="children"><div class="content">This tool is really lovely, great work!<p>I&#x27;d be curious to see Softmax Linear Units [1] integrated into the possible activation functions since they seem to improve interpretability.<p>PS: I share your curiosity with respect to things like deep dream. My brief summary of this paper is that you can use GPT4 to summarize what&#x27;s similar about a set of highlighted words in context which is clever but doesn&#x27;t fundamentally inform much that we didn&#x27;t already know about how these models work. I wonder if there&#x27;s some diffusion based approach that could be used to diffuse from noise in the residual stream towards a maximized activation at a particular point.<p>[1] <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2022&#x2F;solu&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2022&#x2F;solu&#x2F;index.html</a></div><br/></div></div><div id="35883513" class="c"><input type="checkbox" id="c-35883513" checked=""/><div class="controls bullet"><span class="by">rmorey</span><span>|</span><a href="#35881288">parent</a><span>|</span><a href="#35882597">prev</a><span>|</span><a href="#35877555">next</a><span>|</span><label class="collapse" for="c-35883513">[-]</label><label class="expand" for="c-35883513">[1 more]</label></div><br/><div class="children"><div class="content">this is wonderful, thanks for sharing!</div><br/></div></div></div></div><div id="35877555" class="c"><input type="checkbox" id="c-35877555" checked=""/><div class="controls bullet"><span class="by">srajabi</span><span>|</span><a href="#35881288">prev</a><span>|</span><a href="#35877627">next</a><span>|</span><label class="collapse" for="c-35877555">[-]</label><label class="expand" for="c-35877555">[20 more]</label></div><br/><div class="children"><div class="content">&quot;This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself. A promising aspect of this approach is that it scales with the pace of AI development. As future models become increasingly intelligent and helpful as assistants, we will find better explanations.&quot;<p>On first look this is genius but it seems pretty tautological in a way. How do we know if the explainer is good?... Kinda leads to thinking about who watches the watchers...</div><br/><div id="35880353" class="c"><input type="checkbox" id="c-35880353" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35878842">next</a><span>|</span><label class="collapse" for="c-35880353">[-]</label><label class="expand" for="c-35880353">[2 more]</label></div><br/><div class="children"><div class="content">&gt; How do we know if the explainer is good?<p>The paper explains this in detail, but here is a summary: an explanation is good if you can recover actual neuron behavior from the explanation. They ask GPT-4 to guess neuron activation given an explanation and an input (the paper includes the full prompt used). And then they calculate correlation of actual neuron activation and simulated neuron activation.<p>They discuss two issues with this methodology. First, explanations are ultimately for humans, so using GPT-4 to simulate humans, while necessary in practice, may cause divergence. They guard against this by asking humans whether they agree with the explanation, and showing that humans agree more with an explanation that scores high in correlation.<p>Second, correlation is an imperfect measure of how faithfully neuron behavior is reproduced. To guard against this, they run the neural network with activation of the neuron replaced with simulated activation, and show that the neural network output is closer (measured in Jensen-Shannon divergence) if correlation is higher.</div><br/><div id="35883052" class="c"><input type="checkbox" id="c-35883052" checked=""/><div class="controls bullet"><span class="by">habryka</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35880353">parent</a><span>|</span><a href="#35878842">next</a><span>|</span><label class="collapse" for="c-35883052">[-]</label><label class="expand" for="c-35883052">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The paper explains this in detail, but here is a summary: an explanation is good if you can recover actual neuron behavior from the explanation.<p>To be clear, this is only neuron activation strength for text inputs. We aren&#x27;t doing any mechanistic modeling of whether our explanation of what the neuron does predicts any role the neuron might play within the internals of the network, despite most neurons likely having a role that can only be succinctly summarized in relation to the rest of the network.<p>It seems very easy to end up with explanations that correlate well with a neuron, but do not actually meaningfully explain what the neuron is doing.</div><br/></div></div></div></div><div id="35878842" class="c"><input type="checkbox" id="c-35878842" checked=""/><div class="controls bullet"><span class="by">TheRealPomax</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35880353">prev</a><span>|</span><a href="#35878970">next</a><span>|</span><label class="collapse" for="c-35878842">[-]</label><label class="expand" for="c-35878842">[6 more]</label></div><br/><div class="children"><div class="content">Why is this genius? It&#x27;s just the NN equivalent of making a new programming language and getting it to the point where its compiler can be written in itself.<p>The reliability question is of course the main issue. If you don&#x27;t know how the system works, you can&#x27;t assign a trust value to anything it comes up with, even if it seems like what it comes up with makes sense.</div><br/><div id="35879990" class="c"><input type="checkbox" id="c-35879990" checked=""/><div class="controls bullet"><span class="by">0xParlay</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35878842">parent</a><span>|</span><a href="#35879497">next</a><span>|</span><label class="collapse" for="c-35879990">[-]</label><label class="expand" for="c-35879990">[4 more]</label></div><br/><div class="children"><div class="content">I love the epistemology related discussions AI inevitably surfaces. How can we know anything that isn&#x27;t empirically evident and all that.<p>It seems NN output could be trusted in scenarios where a test exists. For example: &quot;ChatGPT design a house using [APP] and make sure the compiled plans comply with structural&#x2F;electrical&#x2F;design&#x2F;etc codes for area [X]&quot;.<p>But how is any information that isn&#x27;t testable trusted? I&#x27;m open to the idea ChatGPT is as credible as experts in the dismal sciences given that information cannot be proven or falsified and legitimacy is assigned by stringing together words that &quot;makes sense&quot;.</div><br/><div id="35883628" class="c"><input type="checkbox" id="c-35883628" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35879990">parent</a><span>|</span><a href="#35879497">next</a><span>|</span><label class="collapse" for="c-35883628">[-]</label><label class="expand" for="c-35883628">[3 more]</label></div><br/><div class="children"><div class="content">&gt; But how is any information that isn&#x27;t testable trusted? I&#x27;m open to the idea ChatGPT is as credible as experts in the dismal sciences given that information cannot be proven or falsified and legitimacy is assigned by stringing together words that &quot;makes sense&quot;.<p>I understand that around the 1980s-ish, the dream was that people could express knowledge in something like Prolog, including the test-case, which can then be deterministically evaluated. This does really work, but surprisingly many things cannot be represented in terms of “facts” which really limits its applicability.<p>I didn’t opt for Prolog electives in school (I did Haskell instead) so I honestly don’t know <i>why</i> so many “things” are unrepresentable as “facts”.</div><br/><div id="35883930" class="c"><input type="checkbox" id="c-35883930" checked=""/><div class="controls bullet"><span class="by">philomath_mn</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35883628">parent</a><span>|</span><a href="#35879497">next</a><span>|</span><label class="collapse" for="c-35883930">[-]</label><label class="expand" for="c-35883930">[2 more]</label></div><br/><div class="children"><div class="content">I bet GPT is really good at prolog, that would be interesting to explore.<p>&quot;Answer this question in the form of a testable prolog program&quot;</div><br/><div id="35884689" class="c"><input type="checkbox" id="c-35884689" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35883930">parent</a><span>|</span><a href="#35879497">next</a><span>|</span><label class="collapse" for="c-35884689">[-]</label><label class="expand" for="c-35884689">[1 more]</label></div><br/><div class="children"><div class="content">Did you give it a try?</div><br/></div></div></div></div></div></div></div></div><div id="35879497" class="c"><input type="checkbox" id="c-35879497" checked=""/><div class="controls bullet"><span class="by">typon</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35878842">parent</a><span>|</span><a href="#35879990">prev</a><span>|</span><a href="#35878970">next</a><span>|</span><label class="collapse" for="c-35879497">[-]</label><label class="expand" for="c-35879497">[1 more]</label></div><br/><div class="children"><div class="content">Seems relevant: <a href="https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~rdriley&#x2F;487&#x2F;papers&#x2F;Thompson_1984_ReflectionsonTrustingTrust.pdf" rel="nofollow">https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~rdriley&#x2F;487&#x2F;papers&#x2F;Thompson_1984_Ref...</a></div><br/></div></div></div></div><div id="35878970" class="c"><input type="checkbox" id="c-35878970" checked=""/><div class="controls bullet"><span class="by">jacobr1</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35878842">prev</a><span>|</span><a href="#35879916">next</a><span>|</span><label class="collapse" for="c-35878970">[-]</label><label class="expand" for="c-35878970">[1 more]</label></div><br/><div class="children"><div class="content">There is a longer-term problem of trusting the explainer system, but in the near-term that isn&#x27;t really a concern.<p>The bigger value here in the near-term is _explicability_ rather than alignment per-se. Potentially having good explicability might provide insights into the design and architecture of LLMs in general, and that in-turn may enable better design of alignment-schemes.</div><br/></div></div><div id="35879916" class="c"><input type="checkbox" id="c-35879916" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35878970">prev</a><span>|</span><a href="#35878027">next</a><span>|</span><label class="collapse" for="c-35879916">[-]</label><label class="expand" for="c-35879916">[1 more]</label></div><br/><div class="children"><div class="content">I can almost hear the Animatrix voiceover: &quot;At first, AI was useful.  Then, we decided to automate oversight... The rest is history.&quot;</div><br/></div></div><div id="35878027" class="c"><input type="checkbox" id="c-35878027" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35879916">prev</a><span>|</span><a href="#35881345">next</a><span>|</span><label class="collapse" for="c-35878027">[-]</label><label class="expand" for="c-35878027">[2 more]</label></div><br/><div class="children"><div class="content">It also lags one iteration behind. Which is a problem because a misaligned model might lie to you, spoiling all future research with this method</div><br/><div id="35879536" class="c"><input type="checkbox" id="c-35879536" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35878027">parent</a><span>|</span><a href="#35881345">next</a><span>|</span><label class="collapse" for="c-35879536">[-]</label><label class="expand" for="c-35879536">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t have to lag, though. You could ask gpt-2 to explain gpt-2. The weights are just input data. The reason this wasn&#x27;t done on gpt-3 or gpt-4 is just because a) they&#x27;re much bigger, and b) they&#x27;re deeper, so the roles of individual neurons are more attenuated.</div><br/></div></div></div></div><div id="35881345" class="c"><input type="checkbox" id="c-35881345" checked=""/><div class="controls bullet"><span class="by">KevinBenSmith</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35878027">prev</a><span>|</span><a href="#35878467">next</a><span>|</span><label class="collapse" for="c-35881345">[-]</label><label class="expand" for="c-35881345">[2 more]</label></div><br/><div class="children"><div class="content">I had similar thoughts about the general concept of using AI to automate AI Safety.<p>I really like their approach and I think it’s valuable. And in this particular case, they do have a way to score the explainer model.
And I think it could be very valuable for various AI Safety issues.<p>However, I don’t yet see how it can help with the potentially biggest danger where a super intelligent AGI is created that is not aligned with humans.
The newly created AGI might be 10x more intelligent than the explainer model. To such an extent that the explainer model is not capable of understanding any tactics deployed by the super intelligent AGI. The same way ants are most probably not capable of explaining the tactics delloyed by humans, even if we gave them a 100 years to figure it out.</div><br/><div id="35881647" class="c"><input type="checkbox" id="c-35881647" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35881345">parent</a><span>|</span><a href="#35878467">next</a><span>|</span><label class="collapse" for="c-35881647">[-]</label><label class="expand" for="c-35881647">[1 more]</label></div><br/><div class="children"><div class="content">Safest thing to do, stop inverting and building more powerful and potentially dangerous systems which we can’t understand?</div><br/></div></div></div></div><div id="35878467" class="c"><input type="checkbox" id="c-35878467" checked=""/><div class="controls bullet"><span class="by">m1el</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35881345">prev</a><span>|</span><a href="#35878762">next</a><span>|</span><label class="collapse" for="c-35878467">[-]</label><label class="expand" for="c-35878467">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re correct to have a suspicion here. Hypothetically the explainer could omit a neuron or give a wrong explanation for the role of a neuron.
Imagine you&#x27;re trying to understand a neural network, and you spend enormous amount of time generating hypotheses and validating them.
Well the explainer might give you 90% correct hypotheses, it means you have 10 times less work to produce hypotheses.
So if you have a solid way of testing an explanation, even if the explainer is evil, it&#x27;s still useful.</div><br/></div></div><div id="35878762" class="c"><input type="checkbox" id="c-35878762" checked=""/><div class="controls bullet"><span class="by">vhold</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35878467">prev</a><span>|</span><a href="#35882219">next</a><span>|</span><label class="collapse" for="c-35878762">[-]</label><label class="expand" for="c-35878762">[3 more]</label></div><br/><div class="children"><div class="content">It produces examples that can be evaluated.<p><a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;neuron-viewer&#x2F;index.html#&#x2F;layers&#x2F;21&#x2F;neurons&#x2F;2932" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a></div><br/><div id="35879720" class="c"><input type="checkbox" id="c-35879720" checked=""/><div class="controls bullet"><span class="by">bottlepalm</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35878762">parent</a><span>|</span><a href="#35882219">next</a><span>|</span><label class="collapse" for="c-35879720">[-]</label><label class="expand" for="c-35879720">[2 more]</label></div><br/><div class="children"><div class="content">Using &#x27;im feeling lucky&#x27; from the neuron viewer is a really cool way to explore different neurons. And then being able to navigate up and down through the net to related neurons.</div><br/><div id="35880484" class="c"><input type="checkbox" id="c-35880484" checked=""/><div class="controls bullet"><span class="by">eternalban</span><span>|</span><a href="#35877555">root</a><span>|</span><a href="#35879720">parent</a><span>|</span><a href="#35882219">next</a><span>|</span><label class="collapse" for="c-35880484">[-]</label><label class="expand" for="c-35880484">[1 more]</label></div><br/><div class="children"><div class="content">Fun to look at activations and then search for the source on the net.<p><i>&quot;Suddenly, DM-sliding seems positively whimsical&quot;</i><p><a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;neuron-viewer&#x2F;index.html#&#x2F;layers&#x2F;26&#x2F;neurons&#x2F;3547" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a><p><a href="https:&#x2F;&#x2F;www.thecut.com&#x2F;2016&#x2F;01&#x2F;19th-century-men-were-awful-at-flirting.html" rel="nofollow">https:&#x2F;&#x2F;www.thecut.com&#x2F;2016&#x2F;01&#x2F;19th-century-men-were-awful-a...</a></div><br/></div></div></div></div></div></div><div id="35882219" class="c"><input type="checkbox" id="c-35882219" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#35877555">parent</a><span>|</span><a href="#35878762">prev</a><span>|</span><a href="#35877627">next</a><span>|</span><label class="collapse" for="c-35882219">[-]</label><label class="expand" for="c-35882219">[1 more]</label></div><br/><div class="children"><div class="content">How do we know WE are good explainers :-)</div><br/></div></div></div></div><div id="35877627" class="c"><input type="checkbox" id="c-35877627" checked=""/><div class="controls bullet"><span class="by">fnovd</span><span>|</span><a href="#35877555">prev</a><span>|</span><a href="#35879973">next</a><span>|</span><label class="collapse" for="c-35877627">[-]</label><label class="expand" for="c-35877627">[81 more]</label></div><br/><div class="children"><div class="content">LLMs are quickly going to be able to start explaining their own thought processes better than any human can explain their own. I wonder how many new words we will come up with to describe concepts (or &quot;node-activating clusters of meaning&quot;) that the AI finds salient that we don&#x27;t yet have a singular word for. Or, for that matter, how many of those concepts we will find meaningful at all. What will this teach us about ourselves?</div><br/><div id="35878533" class="c"><input type="checkbox" id="c-35878533" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35878782">next</a><span>|</span><label class="collapse" for="c-35878533">[-]</label><label class="expand" for="c-35878533">[31 more]</label></div><br/><div class="children"><div class="content">First of all, our own explanations about ourselves and our behaviour are mostly lies, fabrications, hallucinations, faulty re-memorization, post hoc reasoning:<p>&quot;In one well-known experiment, a split-brain patient’s left hemisphere was shown a picture of a chicken claw and his right hemisphere was shown a picture of a snow scene. The patient was asked to point to a card that was associated with the picture he just saw. With his left hand (controlled by his right hemisphere) he selected a shovel, which matched the snow scene. With his right hand (controlled by his left hemisphere) he selected a chicken, which matched the chicken claw. Next, the experimenter asked the patient why he selected each item. One would expect the speaking left hemisphere to explain why it chose the chicken but not why it chose the shovel, since the left hemisphere did not have access to information about the snow scene. Instead, the patient’s speaking left hemisphere replied, “Oh, that’s simple. The chicken claw goes with the chicken and you need a shovel to clean out the chicken shed”&quot; [1]. Also [2] has an interesting hypothesis on split-brains: not two agents, but two streams of perception.<p>[1] 2014, &quot;Divergent hemispheric reasoning strategies: reducing uncertainty versus resolving inconsistency&quot;, <a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4204522" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4204522</a><p>[2] 2017, &quot;The Split-Brain phenomenon revisited: A single conscious agent with split perception&quot;, <a href="https:&#x2F;&#x2F;pure.uva.nl&#x2F;ws&#x2F;files&#x2F;25987577&#x2F;Split_Brain.pdf" rel="nofollow">https:&#x2F;&#x2F;pure.uva.nl&#x2F;ws&#x2F;files&#x2F;25987577&#x2F;Split_Brain.pdf</a></div><br/><div id="35878989" class="c"><input type="checkbox" id="c-35878989" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878533">parent</a><span>|</span><a href="#35879007">next</a><span>|</span><label class="collapse" for="c-35878989">[-]</label><label class="expand" for="c-35878989">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not understanding the connection between your paragraphs here even after reading the first article.<p>Even if you accept classic theory (e.g. hemispheric localization and the homunculus) which most experts don&#x27;t all this suggests is that the brain tries to make sense of the information it has and in sparse environments it fills in.<p>How does this make our behavior &quot;mostly lies, fabrications, hallucinations, faulty re-memorization, post hoc reasoning&quot; as most humans don&#x27;t have a severed corpus callosum.<p>The discussion starts with:<p>&quot;In a healthy human brain, these divergent hemispheric tendencies complement each other and create a balanced and flexible reasoning system. Working in unison, the left and right hemispheres can create inferences that have explanatory power and both internal and external consistency.&quot;</div><br/><div id="35879273" class="c"><input type="checkbox" id="c-35879273" checked=""/><div class="controls bullet"><span class="by">mcguire</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878989">parent</a><span>|</span><a href="#35879163">next</a><span>|</span><label class="collapse" for="c-35879273">[-]</label><label class="expand" for="c-35879273">[1 more]</label></div><br/><div class="children"><div class="content">I think the point is that, in a non-healthy brain, the brain can create a balanced and flexible reasoning system that creates inferences that have explanatory power, but which may not match external reality. Oliver Sacks has a long bibliography of the weird things that can go on in brains.<p>But the bottom line is that introspection is not necessarily reliable.</div><br/></div></div><div id="35879163" class="c"><input type="checkbox" id="c-35879163" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878989">parent</a><span>|</span><a href="#35879273">prev</a><span>|</span><a href="#35879007">next</a><span>|</span><label class="collapse" for="c-35879163">[-]</label><label class="expand" for="c-35879163">[3 more]</label></div><br/><div class="children"><div class="content">He didn&#x27;t say behavior. He said explanations of behaviour. Split brain experiments aside, this is pretty evident from other research. 
We can&#x27;t recreate previous mental states, we just do a pretty good job (usually) of rationalizing decisions after the fact.
<a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3196841&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3196841&#x2F;</a></div><br/><div id="35879434" class="c"><input type="checkbox" id="c-35879434" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879163">parent</a><span>|</span><a href="#35879007">next</a><span>|</span><label class="collapse" for="c-35879434">[-]</label><label class="expand" for="c-35879434">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m reading this as our explanations for our own behavior as in why am I typing on this keyboard right now, in which case it&#x27;s not evident at all.<p>The existence of cognitive dissonance suggested in your citation is in no way analogous to &quot;our own explanations about ourselves and our behaviour are mostly lies, fabrications, hallucinations, faulty re-memorization, post hoc reasoning&quot; and in fact supports the opposite.</div><br/><div id="35879626" class="c"><input type="checkbox" id="c-35879626" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879434">parent</a><span>|</span><a href="#35879007">next</a><span>|</span><label class="collapse" for="c-35879626">[-]</label><label class="expand" for="c-35879626">[1 more]</label></div><br/><div class="children"><div class="content">One primary explanation of ourselves is that there is in fact a &quot;self&quot; there, we feel this &quot;self&quot; as being permanent, continuous through time, yet we are absolutely sure that is a lie: there are no continuous processes in the entire universe, energy itself is quantized.<p>In the morning when we wake up, we are &quot;booting&quot; up the memories the brain finds and we believe that we have persisted through time, from yesterday to today, yet we are absolutely sure that is a lie: just look at an Alzheimer patient.<p>We are feeling this self as if it&#x27;s somewhere above the neck and we feel like this self is looking at the world and sees &quot;out there&quot;, yet we are absolutely sure that is a lie: our senses are being overflown by inputs and the brain filters them, shapes a model of the world, and presents that model to the internal model of itself, which gets so immersed into model of the world that starts to believe the model is indeed the world, until the first bistable image [1] breaks the model down.<p>[1] <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;Amanda-Parker-14&#x2F;publication&#x2F;282917004&#x2F;figure&#x2F;fig2&#x2F;AS:671528807051270@1537116352444&#x2F;Examples-of-perceptual-bistability-involving-object-interpretations.png" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;profile&#x2F;Amanda-Parker-14&#x2F;public...</a></div><br/></div></div></div></div></div></div></div></div><div id="35879007" class="c"><input type="checkbox" id="c-35879007" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878533">parent</a><span>|</span><a href="#35878989">prev</a><span>|</span><a href="#35878908">next</a><span>|</span><label class="collapse" for="c-35879007">[-]</label><label class="expand" for="c-35879007">[8 more]</label></div><br/><div class="children"><div class="content">Not supported by neuroimaging. Promoted without evidence or sufficient causal inference.<p><a href="https:&#x2F;&#x2F;www.health.harvard.edu&#x2F;blog&#x2F;right-brainleft-brain-right-2017082512222" rel="nofollow">https:&#x2F;&#x2F;www.health.harvard.edu&#x2F;blog&#x2F;right-brainleft-brain-ri...</a> :<p>&gt; <i>But, the evidence discounting the left&#x2F;right brain concept is accumulating. According to a 2013 study from the University of Utah, brain scans demonstrate that activity is similar on both sides of the brain regardless of one&#x27;s personality.</i><p>&gt; <i>They looked at the brain scans of more than 1,000 young people between the ages of 7 and 29 and divided different areas of the brain into 7,000 regions to determine whether one side of the brain was more active or connected than the other side. No evidence of &quot;sidedness&quot; was found. The authors concluded that the notion of some people being more left-brained or right-brained is more a figure of speech than an anatomically accurate description.</i><p>Here&#x27;s wikipedia on the topic: &quot;Lateralization of brain function&quot; <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lateralization_of_brain_function" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Lateralization_of_brain_functi...</a><p>Furthermore, &quot;Neuropsychoanalysis&quot; <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuropsychoanalysis" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuropsychoanalysis</a><p>Neuropsychology: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuropsychology" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Neuropsychology</a><p>Personality psychology &gt; ~Biophysiological: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Personality_psychology" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Personality_psychology</a><p>MBTI &gt; Criticism: 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Myers%E2%80%93Briggs_Type_Indicator#Criticism" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Myers%E2%80%93Briggs_Type_Indi...</a><p>Connectome: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Connectome" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Connectome</a></div><br/><div id="35879128" class="c"><input type="checkbox" id="c-35879128" checked=""/><div class="controls bullet"><span class="by">jorgeortiz85</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879007">parent</a><span>|</span><a href="#35879145">next</a><span>|</span><label class="collapse" for="c-35879128">[-]</label><label class="expand" for="c-35879128">[1 more]</label></div><br/><div class="children"><div class="content">You are talking about the popular narrative of “left brain” thinking being more logical and “right brain” thinking being more creative. You are correct this is unsupported.<p>The post you are replying to is talking about the small subset of individuals who have had their corpus callosum surgically severed, which makes it much more difficult for the brain to send messages between hemispheres. These patients exhibit “split brain” behavior that is well studied by experiments and can shed light into human consciousness and rationality.</div><br/></div></div><div id="35879145" class="c"><input type="checkbox" id="c-35879145" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879007">parent</a><span>|</span><a href="#35879128">prev</a><span>|</span><a href="#35879135">next</a><span>|</span><label class="collapse" for="c-35879145">[-]</label><label class="expand" for="c-35879145">[4 more]</label></div><br/><div class="children"><div class="content">This is not relevant to GP&#x27;s comment. It has nothing to do with &quot;are there fixed &#x27;themes&#x27; that are operated in each hemisphere.&quot; It has to do with more generally, does the brain know what the brain is doing. The answer so far does not seem to be &quot;yes.&quot;</div><br/><div id="35879460" class="c"><input type="checkbox" id="c-35879460" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879145">parent</a><span>|</span><a href="#35879135">next</a><span>|</span><label class="collapse" for="c-35879460">[-]</label><label class="expand" for="c-35879460">[3 more]</label></div><br/><div class="children"><div class="content">Says who? There is actual evidence to support that our brain doesn&#x27;t &quot;know&quot; what it is doing on a subconscious level? As far as I&#x27;m aware it&#x27;s more that conscious humans don&#x27;t understand how our brain works.<p>I think the correct statement is &quot;so far the answer is we don&#x27;t know&quot;</div><br/><div id="35879634" class="c"><input type="checkbox" id="c-35879634" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879460">parent</a><span>|</span><a href="#35879135">next</a><span>|</span><label class="collapse" for="c-35879634">[-]</label><label class="expand" for="c-35879634">[2 more]</label></div><br/><div class="children"><div class="content">The split brain experiments very very clearly indicate that different parts of the brain can independently conduct behavior and gain knowledge independently of other parts.<p>How or if this generalizes to healthy brains is not super clear, but it does actually provide a good explanatory model for all sorts of self-contradictory behavior (like addiction): the brain has many semi-independent “interests” that are jockeying for overall control of the organism’s behavior. These interests can be fully contradictory to each other.<p>Correct, ultimately we do not know. But it’s actually a different question than your rephrasing.</div><br/><div id="35884911" class="c"><input type="checkbox" id="c-35884911" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879634">parent</a><span>|</span><a href="#35879135">next</a><span>|</span><label class="collapse" for="c-35884911">[-]</label><label class="expand" for="c-35884911">[1 more]</label></div><br/><div class="children"><div class="content">Given that functional localization varies widely from subject to subject per modern neuroimaging, how are split brain experiments more than crude attempts to confirm functional specialization (which is already confirmed without traumatically severing a corpus callosum) &quot;hemispheric&quot; or &quot;lateral&quot;?<p>Neuroimaging indicates high levels of redundancy and variance in spatiotemporal activation.<p>Studies of cortices and other tissues have already shown that much of the neural tissue of the brain is general purpose.<p>Why is executive functioning significantly but not exclusively in the tissue of the forebrain, the frontal lobes?</div><br/></div></div></div></div></div></div></div></div><div id="35879135" class="c"><input type="checkbox" id="c-35879135" checked=""/><div class="controls bullet"><span class="by">mcguire</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879007">parent</a><span>|</span><a href="#35879145">prev</a><span>|</span><a href="#35879105">next</a><span>|</span><label class="collapse" for="c-35879135">[-]</label><label class="expand" for="c-35879135">[1 more]</label></div><br/><div class="children"><div class="content">Your response doesn&#x27;t seem to be directly related to the previous poster&#x27;s split-brain comments, but rather the popular misuse of the lateralization idea.</div><br/></div></div><div id="35879105" class="c"><input type="checkbox" id="c-35879105" checked=""/><div class="controls bullet"><span class="by">haldujai</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879007">parent</a><span>|</span><a href="#35879135">prev</a><span>|</span><a href="#35878908">next</a><span>|</span><label class="collapse" for="c-35879105">[-]</label><label class="expand" for="c-35879105">[1 more]</label></div><br/><div class="children"><div class="content">Agree I&#x27;m not a neuro subspecialist but I&#x27;ve listened to some talks at conferences out of interest and I don&#x27;t think anyone still believes in this anymore. Anecdotally the few fMRI&#x27;s I reported as a trainee didn&#x27;t support this either.</div><br/></div></div></div></div><div id="35878908" class="c"><input type="checkbox" id="c-35878908" checked=""/><div class="controls bullet"><span class="by">rounakdatta</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878533">parent</a><span>|</span><a href="#35879007">prev</a><span>|</span><a href="#35878654">next</a><span>|</span><label class="collapse" for="c-35878908">[-]</label><label class="expand" for="c-35878908">[1 more]</label></div><br/><div class="children"><div class="content">Phantoms in the Brain is a fascinating book that deals with exactly this topic.</div><br/></div></div><div id="35878654" class="c"><input type="checkbox" id="c-35878654" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878533">parent</a><span>|</span><a href="#35878908">prev</a><span>|</span><a href="#35878782">next</a><span>|</span><label class="collapse" for="c-35878654">[-]</label><label class="expand" for="c-35878654">[16 more]</label></div><br/><div class="children"><div class="content">that is absolutely fascinating and also makes me extremely uncomfortable</div><br/><div id="35879013" class="c"><input type="checkbox" id="c-35879013" checked=""/><div class="controls bullet"><span class="by">causi</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878654">parent</a><span>|</span><a href="#35878911">next</a><span>|</span><label class="collapse" for="c-35879013">[-]</label><label class="expand" for="c-35879013">[3 more]</label></div><br/><div class="children"><div class="content">Neurology is full of very uncomfortable facts. Here&#x27;s one for you: there are patients who believe their arm is gone even though it&#x27;s still there. When the doctor asks whose arm that is, they reply it must be someone else&#x27;s. The brain can simply refuse to know something, and will adopt whatever delusions and contortions are necessary. Which of course leads to the realization that there could be things we&#x27;re <i>all</i> incapable of knowing. There could be things right in front of our faces we simply refuse to perceive and we&#x27;d never know it.</div><br/><div id="35879347" class="c"><input type="checkbox" id="c-35879347" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879013">parent</a><span>|</span><a href="#35879344">next</a><span>|</span><label class="collapse" for="c-35879347">[-]</label><label class="expand" for="c-35879347">[1 more]</label></div><br/><div class="children"><div class="content">Famously, our nose is literally right in front of our faces and the brain simply &quot;post-processes&quot; it out of the view.<p>After breaking my arm, split in two, pinching the nerve and making me unable to move it for about a year, I still feel as if the arm is &quot;someone else&#x27;s&quot;, as if I am moving an object in VR, not something which is &quot;me&quot; or &quot;mine&quot;.</div><br/></div></div><div id="35879344" class="c"><input type="checkbox" id="c-35879344" checked=""/><div class="controls bullet"><span class="by">mcguire</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879013">parent</a><span>|</span><a href="#35879347">prev</a><span>|</span><a href="#35878911">next</a><span>|</span><label class="collapse" for="c-35879344">[-]</label><label class="expand" for="c-35879344">[1 more]</label></div><br/><div class="children"><div class="content">Oliver Sacks&#x27; <i>A Leg To Stand On</i> is a lengthy discussion of that, including his own experiences after breaking a leg---IIRC, at one point after surgery but before he starts physical therapy, he wakes up convinced that a medical student has played a prank by removing his leg and attaching one from a cadaver, or at least sticking a cadaver&#x27;s leg under his blanket. (ISTR he tries to throw it out of bed and ends up on the floor.)</div><br/></div></div></div></div><div id="35878911" class="c"><input type="checkbox" id="c-35878911" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878654">parent</a><span>|</span><a href="#35879013">prev</a><span>|</span><a href="#35878950">next</a><span>|</span><label class="collapse" for="c-35878911">[-]</label><label class="expand" for="c-35878911">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s probably way worse than we can imagine.<p>Reading&#x2F;listening to someone like Robert Sapolsky [1] makes me laugh I could have ever hallucinated about such a muddy, not even wrong concept as &quot;free will&quot;.<p>Furthermore, between the brain and, say, the liver there is only a difference of speed&#x2F;data integrity inasmuch as one cares to look for information processing as basal cognition: neurons firing in the brain, voltage-gated ion channels and gap junctions controlling bioelectrical gradients in the liver, and almost everywhere in the body. Why does only the brain has a &quot;feels like&quot; sensation? The liver may have one as well, but the brain being an autarchic dictator perhaps suppresses the feeling of the liver, it certainly abstracts away the thousands of highly specialized decisions the liver takes each second solving adequately the complex problem space of blood processing. Perhaps Thomas Nagel shouldn&#x27;t have asked &quot;What Is It Like to Be a Bat?&quot; [2] but what is it like to be a liver.<p>[1] &quot;Robert Sapolsky: Justice and morality in the absence of free will&quot;, <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nhvAAvwS-UA">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=nhvAAvwS-UA</a><p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;What_Is_It_Like_to_Be_a_Bat%3F" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;What_Is_It_Like_to_Be_a_Bat%3F</a></div><br/><div id="35879261" class="c"><input type="checkbox" id="c-35879261" checked=""/><div class="controls bullet"><span class="by">sclarisse</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878911">parent</a><span>|</span><a href="#35878950">next</a><span>|</span><label class="collapse" for="c-35879261">[-]</label><label class="expand" for="c-35879261">[1 more]</label></div><br/><div class="children"><div class="content">The biggest problem with the current popular idea of “free will” is that people think it means they’re ineffably unpredictable. They’re uncomfortable with the notion that if you were to simulate their brain in sufficient detail, you could predict thoughts and reaction. They take refuge in pseudoscientific mumbling about the links to the Quantum, for they have heard it is special and unpredictable.<p>And that’s just the polar opposite of having a meaningful will at all. It is good that you are pretty much deterministic. You <i>shouldn’t</i> be deciding meaningful things randomly. If you made 20 copies of yourself and asked them to support or oppose some essential and important political question (about human rights, or war, or what-have-you) they should all come down on the same side. What kind of a Will would that be that chose randomly?</div><br/></div></div></div></div><div id="35878950" class="c"><input type="checkbox" id="c-35878950" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878654">parent</a><span>|</span><a href="#35878911">prev</a><span>|</span><a href="#35878883">next</a><span>|</span><label class="collapse" for="c-35878950">[-]</label><label class="expand" for="c-35878950">[2 more]</label></div><br/><div class="children"><div class="content">This is why I suggest that curious individuals try a hallucinogen at least once*. It really makes the fragility of our perception, and how it’s held up mostly by itself, very apparent.<p>* in a safe setting with support, of course.</div><br/><div id="35884162" class="c"><input type="checkbox" id="c-35884162" checked=""/><div class="controls bullet"><span class="by">Aerbil313</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878950">parent</a><span>|</span><a href="#35878883">next</a><span>|</span><label class="collapse" for="c-35884162">[-]</label><label class="expand" for="c-35884162">[1 more]</label></div><br/><div class="children"><div class="content">I can confirm lucid dreaming does the same.</div><br/></div></div></div></div><div id="35878883" class="c"><input type="checkbox" id="c-35878883" checked=""/><div class="controls bullet"><span class="by">incangold</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878654">parent</a><span>|</span><a href="#35878950">prev</a><span>|</span><a href="#35879106">next</a><span>|</span><label class="collapse" for="c-35878883">[-]</label><label class="expand" for="c-35878883">[1 more]</label></div><br/><div class="children"><div class="content">Same. We are so, so profoundly not what it feels like we are, to most of us anyway.<p>I am morbidly curious how people are going to creatively explain away the more challenging insights AI gives us in to what consciousness is.</div><br/></div></div><div id="35879106" class="c"><input type="checkbox" id="c-35879106" checked=""/><div class="controls bullet"><span class="by">Joeri</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878654">parent</a><span>|</span><a href="#35878883">prev</a><span>|</span><a href="#35879069">next</a><span>|</span><label class="collapse" for="c-35879106">[-]</label><label class="expand" for="c-35879106">[6 more]</label></div><br/><div class="children"><div class="content">If that makes you uncomfortable you definitely should not go reading the evidence supporting the notion that conscious free will is an illusion.<p><a href="https:&#x2F;&#x2F;www.mpg.de&#x2F;research&#x2F;unconscious-decisions-in-the-brain" rel="nofollow">https:&#x2F;&#x2F;www.mpg.de&#x2F;research&#x2F;unconscious-decisions-in-the-bra...</a></div><br/><div id="35879454" class="c"><input type="checkbox" id="c-35879454" checked=""/><div class="controls bullet"><span class="by">mcguire</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879106">parent</a><span>|</span><a href="#35881796">next</a><span>|</span><label class="collapse" for="c-35879454">[-]</label><label class="expand" for="c-35879454">[1 more]</label></div><br/><div class="children"><div class="content">My impression is that the understanding of that research that comes up with statements like &quot;But when it comes to decisions we tend to assume they are made by our conscious mind. This is questioned by our current findings&quot; is based on dualistic reasoning.<p>The idea that there should not be any neural activity before a conscious decision is straight-up dualism---the intangible soul makes a decision and neural activity follows it to carry out the decision.<p>An alternative way of understanding that result is that the neural activity that precedes the &quot;conscious decision&quot; is the brain&#x27;s mechanism of coming up with that decision. The &quot;conscious mind&quot; is the result of neural activity, right?</div><br/></div></div><div id="35881796" class="c"><input type="checkbox" id="c-35881796" checked=""/><div class="controls bullet"><span class="by">BaculumMeumEst</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879106">parent</a><span>|</span><a href="#35879454">prev</a><span>|</span><a href="#35879069">next</a><span>|</span><label class="collapse" for="c-35881796">[-]</label><label class="expand" for="c-35881796">[4 more]</label></div><br/><div class="children"><div class="content">i always thought that the concept of free will didnt bother me. but it turns out i just didn’t understand what it implied. oh dear.<p>if it turns out that true, it’s truly amazing how well we convince ourselves that we’re in control.<p>but if our brain controls our actions and not our consciousness, then what is the purpose of consciousness?</div><br/><div id="35882467" class="c"><input type="checkbox" id="c-35882467" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35881796">parent</a><span>|</span><a href="#35879069">next</a><span>|</span><label class="collapse" for="c-35882467">[-]</label><label class="expand" for="c-35882467">[3 more]</label></div><br/><div class="children"><div class="content">Perhaps there is no purpose to consciousness.<p>Perhaps it&#x27;s a phenomenon that somehow arises independently ex nihilo from sufficiently complex systems, only ever able to observe, unable to act.<p>Weird to think about.</div><br/><div id="35884178" class="c"><input type="checkbox" id="c-35884178" checked=""/><div class="controls bullet"><span class="by">Aerbil313</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35882467">parent</a><span>|</span><a href="#35879069">next</a><span>|</span><label class="collapse" for="c-35884178">[-]</label><label class="expand" for="c-35884178">[2 more]</label></div><br/><div class="children"><div class="content">Can conscious experience ever arise from matter?  Even if the said matter is neural networks? This seems utterly nonsensical to me.</div><br/><div id="35884544" class="c"><input type="checkbox" id="c-35884544" checked=""/><div class="controls bullet"><span class="by">ly3xqhl8g9</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35884178">parent</a><span>|</span><a href="#35879069">next</a><span>|</span><label class="collapse" for="c-35884544">[-]</label><label class="expand" for="c-35884544">[1 more]</label></div><br/><div class="children"><div class="content">We are composed of matter and we are conscious, we take this as being axiomatic. Given we have one certain example, the question is then how does the body do it, and how could we do it in other substrates: how do we go from &quot;just physics&quot; to mind [1].<p>The short answer is: chemical reactions start a chain reaction of abstraction towards higher and higher forms of collective intelligence.<p>For some reason, perhaps something with the way the Hilbert space vector obeying the Schrödinger equation which we usually call &quot;the universe&quot; is [2], but also given the ridiculous breadth and depth of possible explorations of the biological pathways [3], &quot;chunks&quot; of matter tend to group together, they group and form stars and planets, but they also group and form formaldehydes and acetaldehydes and many more. Given enough tries, across multiple environments, in some lucky hydrothermal vents abiogenesis was probably started [4]. Once we had the first agential &quot;chunk&quot; of matter, a group of matter which has a definition of the boundary between a &quot;self&quot;, no matter how tiny [5], and an exterior environment, it was more of a game of waiting (~4 billion years) for this tiny agent to grow into an agent with higher-order thinking, self-referentiality, metacognition, and the likes.<p>Neural networks, as in matrix multiplications, are not conscious because they have no mechanism for deciding what is the environment and what is their own, they are a hammer, sitting there, expecting to be used, not a lacrymaria olor [6], exploring the environment for survival and fun. Could we have neural networks in an agent-like architecture starting to behave more like thermostats, setting goals for themselves? Probably.<p>[1] &quot;From physics to mind - Prof. Michael Levin&quot;, <a href="https:&#x2F;&#x2F;youtu.be&#x2F;_QICRPFWDpg?t=85" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;_QICRPFWDpg?t=85</a><p>[2] &quot;Sean Carroll: Extracting the universe from the wave function&quot;, <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=HOssfva2IBo">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=HOssfva2IBo</a><p>[3] &quot;Nick Lane, &#x27;Transformer : The Deep Chemistry of Life and Death&#x27;&quot;, <a href="https:&#x2F;&#x2F;youtu.be&#x2F;bEFzUx_j7tA?t=279" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;bEFzUx_j7tA?t=279</a><p>[4] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hydrothermal_vent" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hydrothermal_vent</a><p>[5] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Physarum_polycephalum" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Physarum_polycephalum</a><p>[6] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=sq6Y54mxjOg">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=sq6Y54mxjOg</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="35879069" class="c"><input type="checkbox" id="c-35879069" checked=""/><div class="controls bullet"><span class="by">tokamak-teapot</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878654">parent</a><span>|</span><a href="#35879106">prev</a><span>|</span><a href="#35878782">next</a><span>|</span><label class="collapse" for="c-35879069">[-]</label><label class="expand" for="c-35879069">[1 more]</label></div><br/><div class="children"><div class="content">Just wait until you notice how much humans do this day to day</div><br/></div></div></div></div></div></div><div id="35878782" class="c"><input type="checkbox" id="c-35878782" checked=""/><div class="controls bullet"><span class="by">jmfldn</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35878533">prev</a><span>|</span><a href="#35877870">next</a><span>|</span><label class="collapse" for="c-35878782">[-]</label><label class="expand" for="c-35878782">[22 more]</label></div><br/><div class="children"><div class="content">&quot;LLMs are quickly going to be able to start explaining their own thought processes better than any human can explain their own.&quot;<p>There is no &quot;their&quot; and there is no &quot;thought process&quot; . There is something that produces text that appears to humans like there is something like thought going on (cf the Eliza Effect), but we must be wary of this anthropomorphising language.<p>There is no self reflection, but if you ask an LLM program how &quot;it&quot; knows something it will produce some text.</div><br/><div id="35880108" class="c"><input type="checkbox" id="c-35880108" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35885043">next</a><span>|</span><label class="collapse" for="c-35880108">[-]</label><label class="expand" for="c-35880108">[2 more]</label></div><br/><div class="children"><div class="content">&gt; There is no self reflection, but if you ask an LLM program how &quot;it&quot; knows something it will produce some text.<p>To be clear, you&#x27;re saying that we should just dismiss out-of-hand any possibility that an LM AI might actually be able to explain its reasoning step-by-step?<p>I find it kind of charming actually how so many humans are just so darn sure that they have their own <i>special kind</i> of cognition that could never be replicated. Not even with 175,000,000,000 calculations for every word generated.</div><br/><div id="35880293" class="c"><input type="checkbox" id="c-35880293" checked=""/><div class="controls bullet"><span class="by">jmfldn</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35880108">parent</a><span>|</span><a href="#35885043">next</a><span>|</span><label class="collapse" for="c-35880293">[-]</label><label class="expand" for="c-35880293">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a strawman since I didn&#x27;t argue anything about humans being special. I don&#x27;t think there is anything necessarily inherently special about human intelligence, I&#x27;m just advocating for caution around the language we use to talk about current systems.<p>All this talk of AGI and sentience and so on is premature and totally unfounded . It&#x27;s pure sci fi, for now at least.</div><br/></div></div></div></div><div id="35885043" class="c"><input type="checkbox" id="c-35885043" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35880108">prev</a><span>|</span><a href="#35879381">next</a><span>|</span><label class="collapse" for="c-35885043">[-]</label><label class="expand" for="c-35885043">[1 more]</label></div><br/><div class="children"><div class="content">Clearly there is some process going on to decide what word to pick. Why can’t we call that thinking?</div><br/></div></div><div id="35879381" class="c"><input type="checkbox" id="c-35879381" checked=""/><div class="controls bullet"><span class="by">jpasmore</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35885043">prev</a><span>|</span><a href="#35880184">next</a><span>|</span><label class="collapse" for="c-35879381">[-]</label><label class="expand" for="c-35879381">[2 more]</label></div><br/><div class="children"><div class="content">As we don&#x27;t know for sure what is happening 100% within a neural network, we can say we don&#x27;t believe that they&#x27;re thinking and we would still need to define 
 the word thinking. Once LLM&#x27;s can self-modify, the word &quot;thinking&quot; will be more accurate than it is today.<p>And when Hinton says at MIT, &quot;I find it very hard to believe that they don&#x27;t have semantics when they consult problems like you know how I paint the rooms how I get all the rooms in my house to be painted white in two years time,&quot; I believe he&#x27;s commenting on the ability of LLM&#x27;s to think on some level.</div><br/><div id="35879646" class="c"><input type="checkbox" id="c-35879646" checked=""/><div class="controls bullet"><span class="by">mcguire</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879381">parent</a><span>|</span><a href="#35880184">next</a><span>|</span><label class="collapse" for="c-35879646">[-]</label><label class="expand" for="c-35879646">[1 more]</label></div><br/><div class="children"><div class="content">In this case, I think we do if you will check out the paper (<a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;paper&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a>). Their method is to<p>1. Show GPT-4 a GPT-produced text with the activation level of a specific neuron at the time it was producing that part of the text highlighted. They then ask GPT-4 for an explanation of what the neuron is doing.<p>Text: &quot;...mathematics is <i>done _properly_</i>, it...if it&#x27;s <i>done _right_</i>. (Take ...&quot;<p>GPT produces &quot;words and phrases related to performing actions correctly or properly&quot;.<p>2. Based on the explanation, get GPT to guess how strong the neuron activates on a new text.<p>&quot;Assuming that the neuron activates on words and phrases related to performing actions correctly or properly. GPT-4 guesses how strongly the neuron responds at each token: &#x27;...Boot. When done <i>_correctly_</i>, &quot;Secure...&#x27;&quot;<p>3. Compare those predictions to the actual activations of the neuron on the text to generate a score.<p>So there is no introspection going on.<p>They say, &quot;<i>We applied our method to all MLP neurons in GPT-2 XL [out of 1.5B?]. We found over 1,000 neurons with explanations that scored at least 0.8, meaning that according to GPT-4 they account for most of the neuron&#x27;s top-activating behavior.</i>&quot; But they also mention, &quot;<i>However, we found that both GPT-4-based and human contractor explanations still score poorly in absolute terms. When looking at neurons, we also found the typical neuron appeared quite polysemantic.</i>&quot;</div><br/></div></div></div></div><div id="35880184" class="c"><input type="checkbox" id="c-35880184" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35879381">prev</a><span>|</span><a href="#35879547">next</a><span>|</span><label class="collapse" for="c-35880184">[-]</label><label class="expand" for="c-35880184">[3 more]</label></div><br/><div class="children"><div class="content">Very true. In my opinion, in case there is a way to extract &quot;Semantic Clouds of Words&quot;, i.e given a particular topic, navigate semantic clouds word by word, find some close neighbours of that word, jump to a neighbour of that word and so on, then LLMs might not seem that big of a deal.<p>I think LLMs are &quot;Semantic Clouds of Words&quot; + grammar and syntax generator. Someone could just discard the grammar and syntax generator, just use the semantic cloud and create the grammar and syntax by himself.<p>For example, in writing a legal document, a slightly educated person on the subject, could just use the relevant words put into an empty paper, fill in the blanks of syntax and grammar, alongside with the human reasoning which is far superior than any machine reasoning, till today at least.<p>The process of editing the GPT* generated documents to fix reasoning is not a negligible task anyway. Sam Altman mentioned that: &quot;the machine has some kind of reasoning&quot;, not a human reasoning ability by any means.<p>My point is, that LLMs are two programs fused into one, &quot;word clouds&quot; and &quot;syntax and grammar&quot;, sprinkled with some kind of poor reasoning. Their word clouding ability, is so unbelievable stronger than any human it fills me with awe every time i use it. Everything else is, just whatever!</div><br/><div id="35881551" class="c"><input type="checkbox" id="c-35881551" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35880184">parent</a><span>|</span><a href="#35879547">next</a><span>|</span><label class="collapse" for="c-35881551">[-]</label><label class="expand" for="c-35881551">[2 more]</label></div><br/><div class="children"><div class="content">I think they&#x27;re much more than that. Or rather, if they&#x27;re &quot;Semantic Cloud of Words&quot;, they&#x27;re still a <i>hundred thousand dimensional</i> clouds of words, and in those hundred thousand dimensions, <i>any</i> relationship you can think of, no matter how obscure, ends up being reflected as proximity along some subset of dimensions.<p>Looking at it this way, I honestly wouldn&#x27;t be surprised if that&#x27;s exactly how &quot;System 1&quot; (to borrow a term from Kahneman) in <i>our brains</i> works.<p>What I&#x27;m saying is:<p>&gt; <i>In my opinion, in case there is a way to extract &quot;Semantic Clouds of Words&quot;, i.e given a particular topic, navigate semantic clouds word by word, find some close neighbours of that word, jump to a neighbour of that word and so on, then LLMs might not seem that big of a deal.</i><p>It may be <i>much more</i> of a deal than we&#x27;d naively think - it seems to me that a lot of what we&#x27;d consider &quot;thinking&quot; and &quot;reasoning&quot; can be effectively implemented as proximity search in a high-dimensional enough vector space. In that case, such extracted &quot;Semantic Cloud of Words&quot; may turn out to represent the very structure of reasoning as humans do it - structure implicitly encoded in all the text that was used as training data for the LLMs.</div><br/><div id="35883382" class="c"><input type="checkbox" id="c-35883382" checked=""/><div class="controls bullet"><span class="by">emporas</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35881551">parent</a><span>|</span><a href="#35879547">next</a><span>|</span><label class="collapse" for="c-35883382">[-]</label><label class="expand" for="c-35883382">[1 more]</label></div><br/><div class="children"><div class="content">&gt;if they&#x27;re &quot;Semantic Cloud of Words&quot;, they&#x27;re still a hundred thousand dimensional clouds of words, and in those hundred thousand dimensions, any relationship you can think of, no matter how obscure, ends up being reflected as proximity along some subset of dimensions.<p>Yes, exactly that. That&#x27;s what GPT4 is doing, over billions of parameters, and many layers stacked on top of one another.<p>Let me give you one more tangible example. Suppose Stable Diffusion had two steps of generating images with humans in it. One step, is taking as input an SVG file, with some simple lines which describe the human anatomy, with body position, joints, dots as eyes etc. Something very simple xkcd style. From then on, it generates the full human which corresponds to exactly the input SVG.<p>Instead of SD being a single model, it could be multimodal, and it should work a lot better in that respect. Every image generator suffers from that problem, human anatomy is very difficult to get right.[1] The same way GPT4 could function as well. Being multimodal instead of a single model, with the two steps discreet from one another.<p>So, in some use cases, we could generate some semantic clouds, and generate syntax and grammar as a second step. And if we don&#x27;t care that much about perfect syntax and grammar, we feed it to GPT2, which is much cheaper to run, and much faster. When i used the paid service of GPT3, back in 2020, the Ada model, was the worst one, but it was the cheapest and fastest. And it was fast. I mean instantaneous.<p>&gt;the very structure of reasoning as humans do it<p>I don&#x27;t agree that the machine reasons even close to a human as of today. It will get better of course over time. However in some not so frequent cases, it comes close. Some times, it seems like it, but only superficially i would argue. Upon closer inspection the machine spits out non sense.<p>[1] Human anatomy, is very difficult to get right, like an artist. Many&#x2F;all of the artists, point out the fact, that A.I. art doesn&#x27;t have soul in the pictures. I share the same sentiment.</div><br/></div></div></div></div></div></div><div id="35879547" class="c"><input type="checkbox" id="c-35879547" checked=""/><div class="controls bullet"><span class="by">callesgg</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35880184">prev</a><span>|</span><a href="#35878938">next</a><span>|</span><label class="collapse" for="c-35879547">[-]</label><label class="expand" for="c-35879547">[3 more]</label></div><br/><div class="children"><div class="content">The text output of a llm is the thought process. In this context the main difference between humans and llms, is that llms can’t have internalized thoughts. There are of course other differences to, like the fact that humans have a wider gamut of input: visuals, sound, input from other bodily functions. And the fact that we have live training.</div><br/><div id="35884136" class="c"><input type="checkbox" id="c-35884136" checked=""/><div class="controls bullet"><span class="by">creatonez</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879547">parent</a><span>|</span><a href="#35878938">next</a><span>|</span><label class="collapse" for="c-35884136">[-]</label><label class="expand" for="c-35884136">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not clear whether or not LLMs have internal thoughts -- each token generation could absolutely have a ton of thought-like modelling in the hidden layers of the network.<p>What is known is that these internal thoughts get erased each time a new token is generated. That is, it&#x27;s starting from scratch from the contents of the text each time it generates a word. But you could postulate that similar prompt text leads to similar &quot;thoughts&quot; and&#x2F;or navigation of the concept web, and therefore the thoughts are continuous in a sense.</div><br/><div id="35884384" class="c"><input type="checkbox" id="c-35884384" checked=""/><div class="controls bullet"><span class="by">callesgg</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35884136">parent</a><span>|</span><a href="#35878938">next</a><span>|</span><label class="collapse" for="c-35884384">[-]</label><label class="expand" for="c-35884384">[1 more]</label></div><br/><div class="children"><div class="content">True, LLMs definetly has something that is &quot;thought-like&quot;.<p>But todays networks lacks the recursion(feedback where the output can go directly to the input) that is needed for the type of internalized thoughts that humans have. I guess this is one thing you are pointing at by mentioning the continuousnes of the internals of LLMs.</div><br/></div></div></div></div></div></div><div id="35878938" class="c"><input type="checkbox" id="c-35878938" checked=""/><div class="controls bullet"><span class="by">icholy</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35879547">prev</a><span>|</span><a href="#35879657">next</a><span>|</span><label class="collapse" for="c-35878938">[-]</label><label class="expand" for="c-35878938">[2 more]</label></div><br/><div class="children"><div class="content">Or maybe the human thought process isn&#x27;t as sophisticated as we imagined.</div><br/><div id="35879016" class="c"><input type="checkbox" id="c-35879016" checked=""/><div class="controls bullet"><span class="by">jmfldn</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878938">parent</a><span>|</span><a href="#35879657">next</a><span>|</span><label class="collapse" for="c-35879016">[-]</label><label class="expand" for="c-35879016">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not arguing for or against that. It&#x27;s more the implications of sentience and selfhood implicit in the language many use around LLMs.</div><br/></div></div></div></div><div id="35879657" class="c"><input type="checkbox" id="c-35879657" checked=""/><div class="controls bullet"><span class="by">codehalo</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35878938">prev</a><span>|</span><a href="#35878862">next</a><span>|</span><label class="collapse" for="c-35879657">[-]</label><label class="expand" for="c-35879657">[2 more]</label></div><br/><div class="children"><div class="content">Pride comes before the fall, and the AI comes before humility.</div><br/><div id="35882125" class="c"><input type="checkbox" id="c-35882125" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879657">parent</a><span>|</span><a href="#35878862">next</a><span>|</span><label class="collapse" for="c-35882125">[-]</label><label class="expand" for="c-35882125">[1 more]</label></div><br/><div class="children"><div class="content">Precisely.<p>When someone states definitively what LLMs can or cannot do, that is when you know to immediately disregard them as the waffling of uninformed laymen lacking the necessary knowledge foundations (cognitive&#x2F;neuroscience&#x2F;philosophy) to even appreciate the uncertainty and finer points under discussion (all the open questions regarding human cognition etc).<p>They don&#x27;t know what they don&#x27;t know and make unfounded assertions as result.<p>Many would do to refrain from speaking so surely about matters they know nothing about, but that is the internet for you.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding</a></div><br/></div></div></div></div><div id="35878862" class="c"><input type="checkbox" id="c-35878862" checked=""/><div class="controls bullet"><span class="by">nerpderp82</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878782">parent</a><span>|</span><a href="#35879657">prev</a><span>|</span><a href="#35877870">next</a><span>|</span><label class="collapse" for="c-35878862">[-]</label><label class="expand" for="c-35878862">[6 more]</label></div><br/><div class="children"><div class="content">What if you ask it to emit the reflexive output, then feed that reflexive output back into the LLM for the conscious answer?<p>What if you ask it to synthesize multiple internal streams of thought, for an ensemble of interior monologues, then have all those argue with each other using logic and then present a high level answer from that panoply of answers?</div><br/><div id="35878992" class="c"><input type="checkbox" id="c-35878992" checked=""/><div class="controls bullet"><span class="by">YawningAngel</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878862">parent</a><span>|</span><a href="#35877870">next</a><span>|</span><label class="collapse" for="c-35878992">[-]</label><label class="expand" for="c-35878992">[5 more]</label></div><br/><div class="children"><div class="content">What if you do? LLMs don&#x27;t have reflexive output or internal streams of thought, they are simply (complex) processes that produce streams of tokens based on an inputted stream of tokens. They don&#x27;t have a special response to tokens that indicate higher-level thinking to humans.</div><br/><div id="35881755" class="c"><input type="checkbox" id="c-35881755" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878992">parent</a><span>|</span><a href="#35879184">next</a><span>|</span><label class="collapse" for="c-35881755">[-]</label><label class="expand" for="c-35881755">[3 more]</label></div><br/><div class="children"><div class="content">LLMs seem to me to <i>be</i> the &quot;internal streams of thought&quot;. I.e. it&#x27;s not LLMs that are missing an internal process that humans have, but rather it&#x27;s humans that have an entire process of conscious thinking built <i>on top</i> of something akin to LLM.</div><br/><div id="35882362" class="c"><input type="checkbox" id="c-35882362" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35881755">parent</a><span>|</span><a href="#35883075">next</a><span>|</span><label class="collapse" for="c-35882362">[-]</label><label class="expand" for="c-35882362">[1 more]</label></div><br/><div class="children"><div class="content">I agree completely and I think this is where a lot of people get tripped up. There&#x27;s no reason to think an AGI needs to be an LLM alone, it might just be a key building block.</div><br/></div></div><div id="35883075" class="c"><input type="checkbox" id="c-35883075" checked=""/><div class="controls bullet"><span class="by">educaysean</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35881755">parent</a><span>|</span><a href="#35882362">prev</a><span>|</span><a href="#35879184">next</a><span>|</span><label class="collapse" for="c-35883075">[-]</label><label class="expand" for="c-35883075">[1 more]</label></div><br/><div class="children"><div class="content">Well put, and I agree. My belief is that if a typical person was drugged or otherwise induced to just blurt out their unfiltered thoughts out loud as it crossed their brain, the level of incohesion and false confidence on display would look a lot like an LLM hallucinating.</div><br/></div></div></div></div><div id="35879184" class="c"><input type="checkbox" id="c-35879184" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878992">parent</a><span>|</span><a href="#35881755">prev</a><span>|</span><a href="#35877870">next</a><span>|</span><label class="collapse" for="c-35879184">[-]</label><label class="expand" for="c-35879184">[1 more]</label></div><br/><div class="children"><div class="content">If you direct the model output to itself and don&#x27;t view it otherwise, how is it not an &quot;internal stream of thought&quot;?</div><br/></div></div></div></div></div></div></div></div><div id="35877870" class="c"><input type="checkbox" id="c-35877870" checked=""/><div class="controls bullet"><span class="by">mrcode007</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35878782">prev</a><span>|</span><a href="#35878174">next</a><span>|</span><label class="collapse" for="c-35877870">[-]</label><label class="expand" for="c-35877870">[13 more]</label></div><br/><div class="children"><div class="content">If the Gödel incompleteness theorem applies here, then the explanations are likely … incomplete or self-referential.</div><br/><div id="35878236" class="c"><input type="checkbox" id="c-35878236" checked=""/><div class="controls bullet"><span class="by">AlexCoventry</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35877870">parent</a><span>|</span><a href="#35878338">next</a><span>|</span><label class="collapse" for="c-35878236">[-]</label><label class="expand" for="c-35878236">[6 more]</label></div><br/><div class="children"><div class="content">The Goedel Incompleteness Theorem has no straightforward application to this question.</div><br/><div id="35878657" class="c"><input type="checkbox" id="c-35878657" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878236">parent</a><span>|</span><a href="#35878338">next</a><span>|</span><label class="collapse" for="c-35878657">[-]</label><label class="expand" for="c-35878657">[5 more]</label></div><br/><div class="children"><div class="content">It would if the language model did reasoning according rules of logic. But they don&#x27;t.  They use Markov chains.<p>To me it makes no sense to say that a LLM could explain its own reasoning if it does no (logical) reasoning at all.  It might be able to explain how the neural network calculates its results. But there are no logical reasoning steps in there that could be explained, are there?</div><br/><div id="35879212" class="c"><input type="checkbox" id="c-35879212" checked=""/><div class="controls bullet"><span class="by">incangold</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878657">parent</a><span>|</span><a href="#35878338">next</a><span>|</span><label class="collapse" for="c-35879212">[-]</label><label class="expand" for="c-35879212">[4 more]</label></div><br/><div class="children"><div class="content">Honest question: are we sure that it doesn’t do logical reasoning?<p>IANAE but although an LLM meets the definition of a Markov Chain as I understand it (current state in, probabilities of next states out), the big black box that spits out the probabilities could be doing anything.<p>Is it fundamentally impossible for reasoning to be an emergent property of an LLM, in a similar way to a brain? They can certainly do a good impression of logical reasoning- better than some humans in some cases?<p>Just because an LLM can be described as a  Markov Chain doesn’t mean it _uses_ Markov Chains? An LLM is very different to the normal examples of Markov Chains I’m familiar with.<p>Or am I missing something?<p>In any case, coemu is an interesting related idea to constrain AIs to thinking in ways we can understand better:<p><a href="https:&#x2F;&#x2F;futureoflife.org&#x2F;podcast&#x2F;connor-leahy-on-agi-and-cognitive-emulation&#x2F;" rel="nofollow">https:&#x2F;&#x2F;futureoflife.org&#x2F;podcast&#x2F;connor-leahy-on-agi-and-cog...</a><p><a href="https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;ngEvKav9w57XrGQnb&#x2F;cognitive-emulation-a-naive-ai-safety-proposal" rel="nofollow">https:&#x2F;&#x2F;www.alignmentforum.org&#x2F;posts&#x2F;ngEvKav9w57XrGQnb&#x2F;cogni...</a></div><br/><div id="35880389" class="c"><input type="checkbox" id="c-35880389" checked=""/><div class="controls bullet"><span class="by">VictorLevoso</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879212">parent</a><span>|</span><a href="#35883754">next</a><span>|</span><label class="collapse" for="c-35880389">[-]</label><label class="expand" for="c-35880389">[1 more]</label></div><br/><div class="children"><div class="content">All programs that you can fit on a computer can be described by a sufficiently large Markov chain(if you imagine all the possible states the memory as nodes)
Whatever the human brain is doing is also describable as a massive Markov chain.<p>But since the markov chain becomes exponentially larger whit the amount of states this is a very nitpicky and meaningless point.<p>Clearly to say something its a markov chain and have that mean something you need to say the thing its doing could be more or less compressed to a simple markov chain for bigrams or something like that, but that is just not true empirically, not even for gpt2. Just this is already pretty hard to make into a reasonable size markov chain <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.00593" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.00593</a>.<p>Just saying that it outputs probabilities from each state is not enough, the states are english strings, there&#x27;s (number of tokens)^contex_lenght possible states for a certain length that&#x27;s not a reasonable markov chain that you could actually implement or run.</div><br/></div></div><div id="35883754" class="c"><input type="checkbox" id="c-35883754" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879212">parent</a><span>|</span><a href="#35880389">prev</a><span>|</span><a href="#35880174">next</a><span>|</span><label class="collapse" for="c-35883754">[-]</label><label class="expand" for="c-35883754">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Honest question: are we sure that it doesn’t do logical reasoning?<p>It&#x27;s not the Creature from the Lagoon, its an engineering artifact created by engineers.  I haven&#x27;t heard them say it does logical deduction according to any set of logic-rules. What I&#x27;ve read is it uses Markov chains.  That makes sense because basically an LLM given a string-input should reply with another string that is the most likely follow-up string to the first string, based on all the texts it crawled up from the internet.<p>If internet had lots and lots of logical reasoning statements then a LLM  might be good at producing what looks like  logical reasoning, but that would still be just response with the most likely follow-up string.<p>The reason the results of LLMs are so impressive is that at some point the quantity of the data makes a seemingly qualitative difference. It&#x27;s like if you have 3 images and show them each to me one after the other I will say I saw 3 images. But if you show me thousands of images 24 per second and the images are small variations of the previous images then I say I see a MOVING PICTURE. At some point quantity becomes quality.</div><br/></div></div><div id="35880174" class="c"><input type="checkbox" id="c-35880174" checked=""/><div class="controls bullet"><span class="by">mrcode007</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879212">parent</a><span>|</span><a href="#35883754">prev</a><span>|</span><a href="#35878338">next</a><span>|</span><label class="collapse" for="c-35880174">[-]</label><label class="expand" for="c-35880174">[1 more]</label></div><br/><div class="children"><div class="content">My understanding is that at least one form of training in the RLHF involves supplying antecedent and consequent training pairs for entailment queries.<p>The LLM seems to be only one of the many building blocks and is used to supply priors &#x2F; transition probabilities that are used elsewhere in downstream part of the model.</div><br/></div></div></div></div></div></div></div></div><div id="35878338" class="c"><input type="checkbox" id="c-35878338" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35877870">parent</a><span>|</span><a href="#35878236">prev</a><span>|</span><a href="#35878103">next</a><span>|</span><label class="collapse" for="c-35878338">[-]</label><label class="expand" for="c-35878338">[3 more]</label></div><br/><div class="children"><div class="content">What leads you to suspect that Gödel incompleteness may be relevant here?<p>There&#x27;s no formal axiom system being dealt with here, afaict?<p>Do you just generally mean &quot;there may be some kind of self-reference, which may lead to some kind of liar-paradox-related issues&quot;?</div><br/><div id="35883220" class="c"><input type="checkbox" id="c-35883220" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878338">parent</a><span>|</span><a href="#35880220">next</a><span>|</span><label class="collapse" for="c-35883220">[-]</label><label class="expand" for="c-35883220">[1 more]</label></div><br/><div class="children"><div class="content">The relevance is because all (all known buildable aka algorithmic, and sufficiently powerful) models of computation are equivalent in terms of formal computability, so if you could violate&#x2F;bypass the Godel or Turing theorems in neural networks, then you could do it in a Turing machine, and vice versa. (That&#x27;s my understanding,  feel free to correct me if I&#x27;m mistaken)</div><br/></div></div><div id="35880220" class="c"><input type="checkbox" id="c-35880220" checked=""/><div class="controls bullet"><span class="by">mrcode007</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878338">parent</a><span>|</span><a href="#35883220">prev</a><span>|</span><a href="#35878103">next</a><span>|</span><label class="collapse" for="c-35880220">[-]</label><label class="expand" for="c-35880220">[1 more]</label></div><br/><div class="children"><div class="content">I commented in another answer but you can consult <a href="https:&#x2F;&#x2F;etc.cuit.columbia.edu&#x2F;news&#x2F;basics-language-modeling-transformers-gpt" rel="nofollow">https:&#x2F;&#x2F;etc.cuit.columbia.edu&#x2F;news&#x2F;basics-language-modeling-...</a><p>Some training forms include entailment  : “if A then B”.
I hope this is first order logic which does have an axiom system :)</div><br/></div></div></div></div><div id="35878103" class="c"><input type="checkbox" id="c-35878103" checked=""/><div class="controls bullet"><span class="by">fnovd</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35877870">parent</a><span>|</span><a href="#35878338">prev</a><span>|</span><a href="#35878272">next</a><span>|</span><label class="collapse" for="c-35878103">[-]</label><label class="expand" for="c-35878103">[1 more]</label></div><br/><div class="children"><div class="content">So is the word &quot;word&quot; but that seems to have worked out OK so far. I can explain the meaning of &quot;meaning&quot; and that seems to work OK too. Being self-referential sounds a lot more like a feature than a bug. Given that the neurons in our own heads are connected to each other and not any ground truth, I think LLMs should do just fine.</div><br/></div></div><div id="35878272" class="c"><input type="checkbox" id="c-35878272" checked=""/><div class="controls bullet"><span class="by">wizeman</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35877870">parent</a><span>|</span><a href="#35878103">prev</a><span>|</span><a href="#35878016">next</a><span>|</span><label class="collapse" for="c-35878272">[-]</label><label class="expand" for="c-35878272">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably one of the reasons why you&#x27;d use GPT-4 to explain GPT-2.<p>Of course, if you were trying to use GPT-4 to explain GPT-4 then I think the Gödel incompleteness theorem would be more relevant, and even then I&#x27;m not so sure.</div><br/></div></div><div id="35878016" class="c"><input type="checkbox" id="c-35878016" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35877870">parent</a><span>|</span><a href="#35878272">prev</a><span>|</span><a href="#35878174">next</a><span>|</span><label class="collapse" for="c-35878016">[-]</label><label class="expand" for="c-35878016">[1 more]</label></div><br/><div class="children"><div class="content">As long as lazy evaluation exists, self-reference is fine, no?<p>Hofstadter talks about something similar in his books.</div><br/></div></div></div></div><div id="35878174" class="c"><input type="checkbox" id="c-35878174" checked=""/><div class="controls bullet"><span class="by">chrisco255</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35877870">prev</a><span>|</span><a href="#35881476">next</a><span>|</span><label class="collapse" for="c-35878174">[-]</label><label class="expand" for="c-35878174">[11 more]</label></div><br/><div class="children"><div class="content">Are there any examples of an LLM developing concepts that do not exist or cannot be inferred from its training set?</div><br/><div id="35878379" class="c"><input type="checkbox" id="c-35878379" checked=""/><div class="controls bullet"><span class="by">fnovd</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878174">parent</a><span>|</span><a href="#35880538">next</a><span>|</span><label class="collapse" for="c-35878379">[-]</label><label class="expand" for="c-35878379">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Cannot be inferred from its training set&quot; is a pretty difficult hurdle. Human beings can infer patterns that aren&#x27;t there, and we typically call those hallucinations or even psychoses. On the other hand, some unconfirmed, novel patterns that humans infer actually represent groundbreaking discoveries, like for example much of the work of Ramanujan.<p>In a real sense, all of the future discoveries of mathematics already exist in the &quot;training set&quot; of our present understanding, we just haven&#x27;t thought it all the way through yet. If we discover something new, can we say that the concept didn&#x27;t exist, or that it &quot;couldn&#x27;t be inferred&quot; from previous work?<p>I think the same would apply to LLMs and their understanding of the way we encode information using language. Given their radically different approach to understanding the same medium, they are well poised to both confirm many things we understand intuitively as well as expose the shortcomings of our human-centric model of understanding.</div><br/></div></div><div id="35880538" class="c"><input type="checkbox" id="c-35880538" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878174">parent</a><span>|</span><a href="#35878379">prev</a><span>|</span><a href="#35878667">next</a><span>|</span><label class="collapse" for="c-35880538">[-]</label><label class="expand" for="c-35880538">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m really curious what kind of concept you might have in mind. Can you give any example of a concept that if an LLM developed that concept then it would meet your criteria? It might sound like a sarcastic question but it&#x27;s hard to agree on the meanings of &quot;concepts that do not exist&quot; or &quot;concepts that cannot be inferred&quot; maybe you can give some examples.<p>EDIT: I see below you gave some examples, like invention of language before it existed, and new theorems in math that presumably would be of interest to mathematicians. Those ones are fair enough in my opinion. The AI isn&#x27;t quite good enough for those ones I think, but I also think newer versions trained with only more CPU&#x2F;GPU and more parameters and more data could be &#x27;AI scientists&#x27; that will make these kinds of concepts.</div><br/></div></div><div id="35878667" class="c"><input type="checkbox" id="c-35878667" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878174">parent</a><span>|</span><a href="#35880538">prev</a><span>|</span><a href="#35879355">next</a><span>|</span><label class="collapse" for="c-35878667">[-]</label><label class="expand" for="c-35878667">[1 more]</label></div><br/><div class="children"><div class="content">It is by definition impossible for an LLM to develop a concept that &#x27;cannot be inferred from its training set&#x27;.<p>On the other hand, that is an incredibly high bar.</div><br/></div></div><div id="35879355" class="c"><input type="checkbox" id="c-35879355" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878174">parent</a><span>|</span><a href="#35878667">prev</a><span>|</span><a href="#35878291">next</a><span>|</span><label class="collapse" for="c-35879355">[-]</label><label class="expand" for="c-35879355">[6 more]</label></div><br/><div class="children"><div class="content">Tautologically, every concept that anything (LLM, or human, or alien) develops can be inferred from the input data(e.g. training set), because it was.</div><br/><div id="35879773" class="c"><input type="checkbox" id="c-35879773" checked=""/><div class="controls bullet"><span class="by">chrisco255</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879355">parent</a><span>|</span><a href="#35878291">next</a><span>|</span><label class="collapse" for="c-35879773">[-]</label><label class="expand" for="c-35879773">[5 more]</label></div><br/><div class="children"><div class="content">No, it wasn&#x27;t, language itself didn&#x27;t even exist at one point. It wasn&#x27;t inferred from training data into existence because such examples existed before. Now we have a dictionary of tens of thousands of words, which describe high level ideas, abstractions, and concepts that someone, somewhere along the line had to invent.<p>And I&#x27;m not talking about imitation nor am I interested in semantic games, I&#x27;m talking about raw inventiveness. Not a stochastic parrot looping through a large corpus of information and a table of weights on word pairings.<p>Has AI ever managed to learn something humans didn&#x27;t already know? It&#x27;s got all the physics text books in its data set. Can it make novel inferences from that? How about in math?</div><br/><div id="35884244" class="c"><input type="checkbox" id="c-35884244" checked=""/><div class="controls bullet"><span class="by">creatonez</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879773">parent</a><span>|</span><a href="#35880366">next</a><span>|</span><label class="collapse" for="c-35884244">[-]</label><label class="expand" for="c-35884244">[2 more]</label></div><br/><div class="children"><div class="content">Coming up with new language is rarely ever coming up with new concepts that didn&#x27;t exist until the word. We come up with high-level abstractions because there already exists a material system to be described and modeled. Language that doesn&#x27;t describe anything that already exists is more like babbling.</div><br/><div id="35884455" class="c"><input type="checkbox" id="c-35884455" checked=""/><div class="controls bullet"><span class="by">chrisco255</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35884244">parent</a><span>|</span><a href="#35880366">next</a><span>|</span><label class="collapse" for="c-35884455">[-]</label><label class="expand" for="c-35884455">[1 more]</label></div><br/><div class="children"><div class="content">Not really, no. There are plenty of intangible abstractions that don&#x27;t describe material systems. Take imaginary numbers, for example. Or the concept of infinity or even zero, neither of which exists in the physical world.<p>The reason why &quot;naming things&quot; is the other hard problem in computer science, after cache invalidation, is that the process of identifying, creating, and describing ideal abstractions is itself inherently difficult.</div><br/></div></div></div></div><div id="35880366" class="c"><input type="checkbox" id="c-35880366" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35879773">parent</a><span>|</span><a href="#35884244">prev</a><span>|</span><a href="#35878291">next</a><span>|</span><label class="collapse" for="c-35880366">[-]</label><label class="expand" for="c-35880366">[2 more]</label></div><br/><div class="children"><div class="content">&gt; No, it wasn&#x27;t, language itself didn&#x27;t even exist at one point.<p>Language took dozens of millennia to form, and animals have long had vocalizations. Seems like a natural building on top of existing features.<p>&gt; Has AI ever managed to learn something humans didn&#x27;t already know?<p>AlphaZero invented all new categories of strategy for games like Go, when previously we thought almost all possible tactics had been discovered. AIs are finding new kinds of proteins we never thought about, which will blow up the fields of medicine and disease in a few years once the first trials are completed.</div><br/><div id="35884365" class="c"><input type="checkbox" id="c-35884365" checked=""/><div class="controls bullet"><span class="by">chrisco255</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35880366">parent</a><span>|</span><a href="#35878291">next</a><span>|</span><label class="collapse" for="c-35884365">[-]</label><label class="expand" for="c-35884365">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Language took dozens of millennia to form<p>Sure, but in a simulated evolutionary algorithm, you can hit a few hundred generations in a matter of seconds.<p>Indeed, the identification of an abstraction, followed by a definition of that abstraction and an enshrinement of the concept in the form of a word or phrase, in and of itself, shortcuts the evolutionary path altogether. AI isn&#x27;t starting from scratch: it&#x27;s starting from a dictionary larger than any human alive knows and in-memory examples of humans conversing on nearly every topic imaginable.<p>We never thought &quot;all possible tactics&quot; had been discovered with Go. We quite literally understood that Go had a more complex search space than Chess, with far more possible moves and outcomes. And I don&#x27;t think anyone has any kind of serious theorem that &quot;all possible tactics&quot; have been discovered in either game, to this day.<p>That being said, Go and Chess are enumerable games with deterministic, bounded complexity and state space.<p>The protein folding example is a neat one, I definitely think it&#x27;s interesting to see what develops there. However, protein folding has been modeled by Markov State models for decades. The AlphaFold breakthrough is fantastic, but it was already known how to generate models from protein structures: it was just computationally expensive.<p>It was also carefully crafted by humans to achieve what it did: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gg7WjuFs8F4">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gg7WjuFs8F4</a>. So this is an example of humans using neural network technology that humans invented to achieve a desired solution to a known problem that they themselves conceived. The AI didn&#x27;t tell us something we didn&#x27;t already know. It was an expert system built with teams of researchers in the loop the whole way through.</div><br/></div></div></div></div></div></div></div></div><div id="35878291" class="c"><input type="checkbox" id="c-35878291" checked=""/><div class="controls bullet"><span class="by">sgt101</span><span>|</span><a href="#35877627">root</a><span>|</span><a href="#35878174">parent</a><span>|</span><a href="#35879355">prev</a><span>|</span><a href="#35881476">next</a><span>|</span><label class="collapse" for="c-35878291">[-]</label><label class="expand" for="c-35878291">[1 more]</label></div><br/><div class="children"><div class="content">The training sets are so poorly curated we will never know...</div><br/></div></div></div></div><div id="35881476" class="c"><input type="checkbox" id="c-35881476" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35878174">prev</a><span>|</span><a href="#35881029">next</a><span>|</span><label class="collapse" for="c-35881476">[-]</label><label class="expand" for="c-35881476">[1 more]</label></div><br/><div class="children"><div class="content">I’m sure LLMs are quickly going to learn to hallucinate (or let’s use the <i>proper</i> word for what they’re doing: <i>confabulate</i>) plausible-sounding but nonsense explanations of their thought processes at least as well as humans.</div><br/></div></div><div id="35881029" class="c"><input type="checkbox" id="c-35881029" checked=""/><div class="controls bullet"><span class="by">elwell</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35881476">prev</a><span>|</span><a href="#35884127">next</a><span>|</span><label class="collapse" for="c-35881029">[-]</label><label class="expand" for="c-35881029">[1 more]</label></div><br/><div class="children"><div class="content">And if the LLM is the explainer, it can lie to us if &#x27;needed&#x27;.</div><br/></div></div><div id="35884127" class="c"><input type="checkbox" id="c-35884127" checked=""/><div class="controls bullet"><span class="by">Aerbil313</span><span>|</span><a href="#35877627">parent</a><span>|</span><a href="#35881029">prev</a><span>|</span><a href="#35879973">next</a><span>|</span><label class="collapse" for="c-35884127">[-]</label><label class="expand" for="c-35884127">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think we invent new words just for AI to explain its thought process to us better. AI may explain more elaborately in our language instead.</div><br/></div></div></div></div><div id="35879973" class="c"><input type="checkbox" id="c-35879973" checked=""/><div class="controls bullet"><span class="by">davesque</span><span>|</span><a href="#35877627">prev</a><span>|</span><a href="#35879133">next</a><span>|</span><label class="collapse" for="c-35879973">[-]</label><label class="expand" for="c-35879973">[4 more]</label></div><br/><div class="children"><div class="content">Based on my skimming the paper, am I correct in understanding that they came up with an elaborate collection of prompts that embed the text generated by GPT-2 as well as a representation of GPT-2&#x27;s internal state?  Then, in effect, they simply asked GPT-4, &quot;What do you think about all this?&quot;<p>If so, they&#x27;re acting on a gigantic assumption that GPT-4 actually correctly encodes a reasonable model of the body of knowledge that went into the development of LLMs.<p>Help me out.  Am I missing something here?</div><br/><div id="35883698" class="c"><input type="checkbox" id="c-35883698" checked=""/><div class="controls bullet"><span class="by">lennxa</span><span>|</span><a href="#35879973">parent</a><span>|</span><a href="#35881489">next</a><span>|</span><label class="collapse" for="c-35883698">[-]</label><label class="expand" for="c-35883698">[1 more]</label></div><br/><div class="children"><div class="content">Why does it have to understand how the LLMs are built? They have used gpt-4 to just build a classifier for each neuron&#x27;s activition, and given the nlp abilities of gpt-4, the hope is that it can describe the nature of activation of the neurons.</div><br/></div></div><div id="35881489" class="c"><input type="checkbox" id="c-35881489" checked=""/><div class="controls bullet"><span class="by">MacsHeadroom</span><span>|</span><a href="#35879973">parent</a><span>|</span><a href="#35883698">prev</a><span>|</span><a href="#35881868">next</a><span>|</span><label class="collapse" for="c-35881489">[-]</label><label class="expand" for="c-35881489">[1 more]</label></div><br/><div class="children"><div class="content">&gt;If so, they&#x27;re acting on a gigantic assumption<p>Yes the initial hypothesis that GPT-4 would know was a gigantic assumption. But a falsifiable one which we can easily generate reproducible tests for.<p>The idea that simulated neurons could learn anything useful at all was once a gigantic assumption too.</div><br/></div></div><div id="35881868" class="c"><input type="checkbox" id="c-35881868" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35879973">parent</a><span>|</span><a href="#35881489">prev</a><span>|</span><a href="#35879133">next</a><span>|</span><label class="collapse" for="c-35881868">[-]</label><label class="expand" for="c-35881868">[1 more]</label></div><br/><div class="children"><div class="content">After GPT-4 generates the hypothesis for a neuron they test it by comparing GPT-4&#x27;s expectation for where the neuron should fire against where it actually fires.<p>If you squint it&#x27;s train&#x2F;test separation.</div><br/></div></div></div></div><div id="35879133" class="c"><input type="checkbox" id="c-35879133" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#35879973">prev</a><span>|</span><a href="#35882566">next</a><span>|</span><label class="collapse" for="c-35879133">[-]</label><label class="expand" for="c-35879133">[4 more]</label></div><br/><div class="children"><div class="content">To me the value here is not that GPT4 has some special insight into explaining the behavior of GPT2 neurons (they say it&#x27;s comparable to &quot;human contractors&quot; - but human performance on this task is also quite poor). The value is that you can just run this on every neuron if you&#x27;re willing to spend the compute, and having a very fuzzy, flawed map of every neuron in a model is still pretty useful as a research tool.<p>But I would be very cautious about drawing conclusions from any individual neuron explanation generated in this way - even if it looks plausible by visual inspection of a few attention maps.</div><br/><div id="35879680" class="c"><input type="checkbox" id="c-35879680" checked=""/><div class="controls bullet"><span class="by">mcguire</span><span>|</span><a href="#35879133">parent</a><span>|</span><a href="#35882566">next</a><span>|</span><label class="collapse" for="c-35879680">[-]</label><label class="expand" for="c-35879680">[3 more]</label></div><br/><div class="children"><div class="content">They also mention they got a score above 0.8 for 1000 neurons out of GPT2 (which has 1.5B (?)).</div><br/><div id="35881891" class="c"><input type="checkbox" id="c-35881891" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35879133">root</a><span>|</span><a href="#35879680">parent</a><span>|</span><a href="#35880797">next</a><span>|</span><label class="collapse" for="c-35881891">[-]</label><label class="expand" for="c-35881891">[1 more]</label></div><br/><div class="children"><div class="content">1.5B parameters, only 300k neurons. The number of connections is roughly quadratic with the number of neurons.</div><br/></div></div><div id="35880797" class="c"><input type="checkbox" id="c-35880797" checked=""/><div class="controls bullet"><span class="by">oofsa</span><span>|</span><a href="#35879133">root</a><span>|</span><a href="#35879680">parent</a><span>|</span><a href="#35881891">prev</a><span>|</span><a href="#35882566">next</a><span>|</span><label class="collapse" for="c-35880797">[-]</label><label class="expand" for="c-35880797">[1 more]</label></div><br/><div class="children"><div class="content">I thought they had only applied the technique to 307,200 neurons.
1,000 &#x2F; 307,200 = 0.33% is still low, but considering that not all neurons would be useful since they are initialized randomly, it&#x27;s not too bad.</div><br/></div></div></div></div></div></div><div id="35882566" class="c"><input type="checkbox" id="c-35882566" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#35879133">prev</a><span>|</span><a href="#35877865">next</a><span>|</span><label class="collapse" for="c-35882566">[-]</label><label class="expand" for="c-35882566">[1 more]</label></div><br/><div class="children"><div class="content">Commenters here seem a little fixated on the fact that the technique scores poorly. This is true and somewhat problematic but exploring the data it looks to me like this could be more a problem with a combination of the scoring function and the limits of language rather than the methodology.<p>For example, look at <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;neuron-viewer&#x2F;index.html#&#x2F;layers&#x2F;17&#x2F;neurons&#x2F;3218" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a><p>It&#x27;s described as &quot;expressions of completion or success&quot; with a score of 0.38. But going through the examples, they are very consistently a sort of colloquial expression of &quot;completion&#x2F;success&quot; with a touch of surprise and maybe challenge.<p>Examples are like: &quot;Nuff said&quot;, &quot;voila!&quot;, &quot;Mission accomplished&quot;, &quot;Game on!&quot;, &quot;End of story&quot;, &quot;enough said&quot;, &quot;nailed it&quot; etc.<p>If they expressed it as a basket of words instead of a sentence, and could come up with words that express it better I&#x27;d score it much higher.</div><br/></div></div><div id="35877865" class="c"><input type="checkbox" id="c-35877865" checked=""/><div class="controls bullet"><span class="by">jablongo</span><span>|</span><a href="#35882566">prev</a><span>|</span><a href="#35878400">next</a><span>|</span><label class="collapse" for="c-35877865">[-]</label><label class="expand" for="c-35877865">[10 more]</label></div><br/><div class="children"><div class="content">This isnt exactly building an understanding of LLMs from first principles...  IMO we should broadly be following the (imperfect) example set forth by neuroscientists attempting to explain fMRI scans and assigning functionality to various subregions in the brain.  It is circular and &quot;unsafe&quot; from an alignment perspective to use a complex model to understand the internals of a simpler model; in order to understand GPT4 then we need GPT5?  These approaches are interesting,  but we should primarily focus on building our understanding of these models from building blocks that we already understand.</div><br/><div id="35878830" class="c"><input type="checkbox" id="c-35878830" checked=""/><div class="controls bullet"><span class="by">axutio</span><span>|</span><a href="#35877865">parent</a><span>|</span><a href="#35878199">next</a><span>|</span><label class="collapse" for="c-35878830">[-]</label><label class="expand" for="c-35878830">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been working in systems neuroscience for a few years (something of a combination lab tech&#x2F;student, so full disclosure, not an actual expert).<p>Based on my experience with model organisms (flies &amp; rats, primarily), it is actually pretty amazing how analogous the techniques and goals used in this sort of research are to those we use in systems neuroscience. At a very basic level, the primary task of correlating neuron activation to a given behavior is exactly the same. However, ML researchers benefit from data being trivial to generate and entire brains being analyzable in one shot as a result, whereas in animal research elucidating the role of neurons in a single circuit costs millions of dollars and many researcher-years.<p>The similarities between the two are so clear that I noticed that in its Microscope tool [1], OpenAI even refers to the models they are studying as &quot;model organisms&quot;, an anthropomorphization which I find very apt. Another article I saw a while back on HN which I thought was very cool was [2], which describes the task of identifying the role of a neuron responsible for a particular token of output. This one is especially analogous because it operates on such a small scale, much closer to what systems neuroscientists studying model organisms do.<p>[1] <a href="https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;microscope" rel="nofollow">https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;microscope</a>
[2] <a href="https:&#x2F;&#x2F;clementneo.com&#x2F;posts&#x2F;2023&#x2F;02&#x2F;11&#x2F;we-found-an-neuron" rel="nofollow">https:&#x2F;&#x2F;clementneo.com&#x2F;posts&#x2F;2023&#x2F;02&#x2F;11&#x2F;we-found-an-neuron</a></div><br/><div id="35882513" class="c"><input type="checkbox" id="c-35882513" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#35877865">root</a><span>|</span><a href="#35878830">parent</a><span>|</span><a href="#35878199">next</a><span>|</span><label class="collapse" for="c-35882513">[-]</label><label class="expand" for="c-35882513">[1 more]</label></div><br/><div class="children"><div class="content">Yup, this article on predictive coding for example is particularly interesting.<p>Lots of parallels to how our brains are thought to work.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Predictive_coding</a></div><br/></div></div></div></div><div id="35878199" class="c"><input type="checkbox" id="c-35878199" checked=""/><div class="controls bullet"><span class="by">roddylindsay</span><span>|</span><a href="#35877865">parent</a><span>|</span><a href="#35878830">prev</a><span>|</span><a href="#35879321">next</a><span>|</span><label class="collapse" for="c-35878199">[-]</label><label class="expand" for="c-35878199">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t follow. Neuroscience imaging tools like fMRI are only used because it is impossible to measure the activations of each neuron in a brain in real time (unlike an artificial neural network). This research paper&#x27;s attempt to understand the role of individual neurons or neuron clusters within a complete network gets much closer to &quot;first principles&quot; than fMRI.</div><br/><div id="35878713" class="c"><input type="checkbox" id="c-35878713" checked=""/><div class="controls bullet"><span class="by">jablongo</span><span>|</span><a href="#35877865">root</a><span>|</span><a href="#35878199">parent</a><span>|</span><a href="#35879321">next</a><span>|</span><label class="collapse" for="c-35878713">[-]</label><label class="expand" for="c-35878713">[3 more]</label></div><br/><div class="children"><div class="content">Right so it should be much easier w&#x2F; access to every neuron and activation. But the general approach is an experimental one where you try to use your existing knowledge about physics and biology to discern what is activating different structures (and neurons) in the brain. I agree w&#x2F; the approach of trying to assign some functionality to individual &#x27;neurons&#x27;, but I don&#x27;t think that using GPT4 to do so is the most appealing way to go about that, considering GPT4 is the structure we are interested in decoding in the first place.</div><br/><div id="35879527" class="c"><input type="checkbox" id="c-35879527" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#35877865">root</a><span>|</span><a href="#35878713">parent</a><span>|</span><a href="#35879321">next</a><span>|</span><label class="collapse" for="c-35879527">[-]</label><label class="expand" for="c-35879527">[2 more]</label></div><br/><div class="children"><div class="content">All of this seems to lead to something like this paper <a href="https:&#x2F;&#x2F;journals.plos.org&#x2F;ploscompbiol&#x2F;article?id=10.1371&#x2F;journal.pcbi.1005268" rel="nofollow">https:&#x2F;&#x2F;journals.plos.org&#x2F;ploscompbiol&#x2F;article?id=10.1371&#x2F;jo...</a><p>On the other hand, I find it plausible that it&#x27;s fundamentally impossible to assign some functionality to individual &#x27;neurons&#x27; due to the following argument:<p>1. Let&#x27;s assume that for a system calculating a specific function, there is a NN configuration (weights) so that at some fully connected NN layer there is a well-defined functionality for specific individual neurons - #1 represents A, #2 represents B, #3 represents C etc.<p>2. The exact same system outcome can be represented with infinitely many other weight combinations which effectively result in a linear transformation (i.e. every possible linear transformation) of the data vector at this layer, e.g. where #1 represents 0.1A + 0.3B + 0.6C, #2 represents 0.5B+0.5C, and #3 represents 0.4B+0.6C - in which case the functionality A (or B, or C) is not represented by any individual neurons;<p>3. When the system is trained, it&#x27;s simply not likely that we just happen to get the best-case configuration where the theoretically separable functionality is actually separated among individual &#x27;neurons&#x27;.<p>Biological minds do get this separation because each connection has a metabolic cost; but the way we train our models (both older perceptron-like layers, and modern transfomer&#x2F;attention ones) do allow linking everything to everything, so the natural outcome is that functionality simply does not get cleanly split out in individual &#x27;neurons&#x27; and each &#x27;neuron&#x27; tends to represent some mix of multiple functionalities.</div><br/><div id="35884484" class="c"><input type="checkbox" id="c-35884484" checked=""/><div class="controls bullet"><span class="by">dpflan</span><span>|</span><a href="#35877865">root</a><span>|</span><a href="#35879527">parent</a><span>|</span><a href="#35879321">next</a><span>|</span><label class="collapse" for="c-35884484">[-]</label><label class="expand" for="c-35884484">[1 more]</label></div><br/><div class="children"><div class="content">Your last idea, that these models’ neurons are all connected in some way, makes me somewhat sceptical of this research by OpenAI. And that their technique of analysis may need to be more fractal or expansive to include groups of neurons, moving all the way up to the entire model.</div><br/></div></div></div></div></div></div></div></div><div id="35879321" class="c"><input type="checkbox" id="c-35879321" checked=""/><div class="controls bullet"><span class="by">djokkataja</span><span>|</span><a href="#35877865">parent</a><span>|</span><a href="#35878199">prev</a><span>|</span><a href="#35878319">next</a><span>|</span><label class="collapse" for="c-35879321">[-]</label><label class="expand" for="c-35879321">[2 more]</label></div><br/><div class="children"><div class="content">&gt; in order to understand GPT4 then we need GPT5?<p>I also found this amusing. But you are loosely correct, AFAIK. GPT-4 cannot reliably explain itself in any context: say the total number of possible distinct states of GPT-4 is N; then the total number of possible distinct states of GPT-4 PLUS any context in which GPT-4 is active must be at least N + 1. So there are at least two distinct states in this scenario that GPT-4 can encounter that will necessarily appear indistinguishable to GPT-4. It doesn&#x27;t matter how big the network is; it&#x27;ll still encounter this limit.<p>And it&#x27;s actually much worse than that limit because a network that&#x27;s actually useful for anything has to be trained on things besides predicting itself. Notably, this is GPT-4 trying to predict GPT-<i>2</i> and struggling:<p>&gt; We found over 1,000 neurons with explanations that scored at least 0.8, meaning that according to GPT-4 they account for most of the neuron’s top-activating behavior. Most of these well-explained neurons are not very interesting. However, we also found many interesting neurons that GPT-4 didn&#x27;t understand. We hope as explanations improve we may be able to rapidly uncover interesting qualitative understanding of model computations.<p>1,000 neurons out of 307,200--and even for the highest-scoring neurons, these are still partial explanations.</div><br/><div id="35881907" class="c"><input type="checkbox" id="c-35881907" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35877865">root</a><span>|</span><a href="#35879321">parent</a><span>|</span><a href="#35878319">next</a><span>|</span><label class="collapse" for="c-35881907">[-]</label><label class="expand" for="c-35881907">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s little reason to think that predicting GPT-4 would be more difficult, only that it would be far more computationally expensive (given the higher number of neurons and much higher computational cost of every test).</div><br/></div></div></div></div></div></div><div id="35878400" class="c"><input type="checkbox" id="c-35878400" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35877865">prev</a><span>|</span><a href="#35880892">next</a><span>|</span><label class="collapse" for="c-35878400">[-]</label><label class="expand" for="c-35878400">[9 more]</label></div><br/><div class="children"><div class="content">I wonder will someone please check the neurons associated to the petertodd and other anomalous glitch tokens (<a href="https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;jkY6QdCfAXHJk3kea&#x2F;the-petertodd-phenomenon" rel="nofollow">https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;jkY6QdCfAXHJk3kea&#x2F;the-petert...</a>)? I can see the github and I see that for any given neuron you can see associated tokens but I don&#x27;t see how to do an inverse search.</div><br/><div id="35884385" class="c"><input type="checkbox" id="c-35884385" checked=""/><div class="controls bullet"><span class="by">creatonez</span><span>|</span><a href="#35878400">parent</a><span>|</span><a href="#35879533">next</a><span>|</span><label class="collapse" for="c-35884385">[-]</label><label class="expand" for="c-35884385">[2 more]</label></div><br/><div class="children"><div class="content">I think this method will do a poor job at explaining petertodd. These neuron explanations still have to fit within human language for this method to work, and the best you can do in the confines of human language to describe petertodd is to write a long article (just like that link) explaining the many oddities of it.<p>Would be interesting to try, though. I think it&#x27;s likely that, due to the way glitch tokens happen, petertodd is probably an input neuron that is very randomly connected to a bunch of different hidden neurons. So it introduces some bizzare noise into a bunch of areas of the network. It&#x27;s possible that some of these neurons are explainable on their own, but not within the broader context of petertodd.</div><br/><div id="35884873" class="c"><input type="checkbox" id="c-35884873" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35878400">root</a><span>|</span><a href="#35884385">parent</a><span>|</span><a href="#35879533">next</a><span>|</span><label class="collapse" for="c-35884873">[-]</label><label class="expand" for="c-35884873">[1 more]</label></div><br/><div class="children"><div class="content">ok but i still want to see it</div><br/></div></div></div></div><div id="35879533" class="c"><input type="checkbox" id="c-35879533" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#35878400">parent</a><span>|</span><a href="#35884385">prev</a><span>|</span><a href="#35880892">next</a><span>|</span><label class="collapse" for="c-35879533">[-]</label><label class="expand" for="c-35879533">[6 more]</label></div><br/><div class="children"><div class="content">Those were discovered by finding strings that OpenAI’s tokenizer didn’t properly split up. Because of this, they are treated as singular tokens, and since these don’t occur frequently in the training data, you get what are effectively random outputs when using them.<p>The author definitely tries to up the mysticism knob to 11 though, and the post itself is so long, you can hardly finish it before seeing this obvious critique made in the comments.</div><br/><div id="35879786" class="c"><input type="checkbox" id="c-35879786" checked=""/><div class="controls bullet"><span class="by">throwuwu</span><span>|</span><a href="#35878400">root</a><span>|</span><a href="#35879533">parent</a><span>|</span><a href="#35880290">next</a><span>|</span><label class="collapse" for="c-35879786">[-]</label><label class="expand" for="c-35879786">[2 more]</label></div><br/><div class="children"><div class="content">The ironic thing about lesswrong is that it’s quite the opposite in some fantastically oblivious ways.</div><br/><div id="35879950" class="c"><input type="checkbox" id="c-35879950" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#35878400">root</a><span>|</span><a href="#35879786">parent</a><span>|</span><a href="#35880290">next</a><span>|</span><label class="collapse" for="c-35879950">[-]</label><label class="expand" for="c-35879950">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, it’s quite strange indeed. Clearly people with decent educations but zero background in applied research&#x2F;peer review. More concerned with the sound of their own voice than with whether or not their findings are actually useful (or even true).<p>Perhaps they are all on stimulants!</div><br/></div></div></div></div><div id="35880290" class="c"><input type="checkbox" id="c-35880290" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35878400">root</a><span>|</span><a href="#35879533">parent</a><span>|</span><a href="#35879786">prev</a><span>|</span><a href="#35880892">next</a><span>|</span><label class="collapse" for="c-35880290">[-]</label><label class="expand" for="c-35880290">[3 more]</label></div><br/><div class="children"><div class="content">Thank you for your opinion on the post that I linked! I&#x27;m still curious about the associated neurons though.</div><br/><div id="35880589" class="c"><input type="checkbox" id="c-35880589" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#35878400">root</a><span>|</span><a href="#35880290">parent</a><span>|</span><a href="#35880892">next</a><span>|</span><label class="collapse" for="c-35880589">[-]</label><label class="expand" for="c-35880589">[2 more]</label></div><br/><div class="children"><div class="content">Fair enough. You would need to use an open model or work at OpenAI.  I assume this work could be used on the llama models - although I’m not aware of anyone has found these glitchy phrases for those models yet.</div><br/><div id="35880634" class="c"><input type="checkbox" id="c-35880634" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35878400">root</a><span>|</span><a href="#35880589">parent</a><span>|</span><a href="#35880892">next</a><span>|</span><label class="collapse" for="c-35880634">[-]</label><label class="expand" for="c-35880634">[1 more]</label></div><br/><div class="children"><div class="content">&gt; You would need to use an open model or work at OpenAI.<p>The point of this post that we are commenting under is that they made this association public, at least in the neuron-&gt;token direction. I was thinking some hacker (like on hacker news) might be able to make something that can reverse it to the token-&gt;neuron direction using the public data so we could see the petertodd associated neurons. <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;neuron-viewer&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="35880892" class="c"><input type="checkbox" id="c-35880892" checked=""/><div class="controls bullet"><span class="by">shrimpx</span><span>|</span><a href="#35878400">prev</a><span>|</span><a href="#35878228">next</a><span>|</span><label class="collapse" for="c-35880892">[-]</label><label class="expand" for="c-35880892">[4 more]</label></div><br/><div class="children"><div class="content">Seems like OpenAI is grasping at straws trying to make GPT &quot;go meta&quot;.<p>Reminds me of this Sam Altman quote from 2019:<p>&quot;We have made a soft promise to investors that once we build this sort-of generally intelligent system, basically we will ask it to figure out a way to generate an investment return.&quot;<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;TzcJlKg2Rc0?t=1886" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;TzcJlKg2Rc0?t=1886</a></div><br/><div id="35881147" class="c"><input type="checkbox" id="c-35881147" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35880892">parent</a><span>|</span><a href="#35878228">next</a><span>|</span><label class="collapse" for="c-35881147">[-]</label><label class="expand" for="c-35881147">[3 more]</label></div><br/><div class="children"><div class="content">I have a similar feeling, they’ve potentially built the most amazing but commercially useless thing in history.<p>I don’t mean it’s not useful entirely, but I mean. It’s not useful in that it’s not deterministic enough to be trustworthy, it’s dangerous and really hard to scale therefore it’s more of an academic project than something that will make Altman as famous as Sergey Brin.<p>I personally take people like Hinton seriously too and think people playing with these things need more oversight themselves.</div><br/><div id="35883145" class="c"><input type="checkbox" id="c-35883145" checked=""/><div class="controls bullet"><span class="by">d_sem</span><span>|</span><a href="#35880892">root</a><span>|</span><a href="#35881147">parent</a><span>|</span><a href="#35878228">next</a><span>|</span><label class="collapse" for="c-35883145">[-]</label><label class="expand" for="c-35883145">[2 more]</label></div><br/><div class="children"><div class="content">Time will tell. Anecdotally, I know several professional who find ChatGPT3.5 &amp; 4 to be valuable and willing to pay for access. I certainly save more than $20 per month for my work by using ChatGPT to accelerate my day to day activities.</div><br/><div id="35883388" class="c"><input type="checkbox" id="c-35883388" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35880892">root</a><span>|</span><a href="#35883145">parent</a><span>|</span><a href="#35878228">next</a><span>|</span><label class="collapse" for="c-35883388">[-]</label><label class="expand" for="c-35883388">[1 more]</label></div><br/><div class="children"><div class="content">That’s why I was careful to say it’s not useless.</div><br/></div></div></div></div></div></div></div></div><div id="35878228" class="c"><input type="checkbox" id="c-35878228" checked=""/><div class="controls bullet"><span class="by">jacooper</span><span>|</span><a href="#35880892">prev</a><span>|</span><a href="#35884473">next</a><span>|</span><label class="collapse" for="c-35878228">[-]</label><label class="expand" for="c-35878228">[3 more]</label></div><br/><div class="children"><div class="content">For people overwhelmed by all the AI science speak, just spend a few minutes with bing or phind and it will explain everything surprisingly well.<p>Imagine telling someone in the middle of 2020, that in three years a computer will be able to speak, reason and explain everything as if it was a human, absolutely incredible!</div><br/><div id="35882291" class="c"><input type="checkbox" id="c-35882291" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#35878228">parent</a><span>|</span><a href="#35878528">next</a><span>|</span><label class="collapse" for="c-35882291">[-]</label><label class="expand" for="c-35882291">[1 more]</label></div><br/><div class="children"><div class="content">In 2020 we were headed for an AI winter, according to all the hot takes:<p><a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-51064369" rel="nofollow">https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-51064369</a><p><a href="https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s13347-020-00396-6" rel="nofollow">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s13347-020-00396-6</a><p><a href="https:&#x2F;&#x2F;blog.re-work.co&#x2F;ai-experts-discuss-the-possibility-of-another-ai-winter&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.re-work.co&#x2F;ai-experts-discuss-the-possibility-o...</a></div><br/></div></div><div id="35878528" class="c"><input type="checkbox" id="c-35878528" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35878228">parent</a><span>|</span><a href="#35882291">prev</a><span>|</span><a href="#35884473">next</a><span>|</span><label class="collapse" for="c-35878528">[-]</label><label class="expand" for="c-35878528">[1 more]</label></div><br/><div class="children"><div class="content">I agree it’s crazy good. But timeline-wise, GPT-3 was in beta and used by many companies in 2020.</div><br/></div></div></div></div><div id="35884473" class="c"><input type="checkbox" id="c-35884473" checked=""/><div class="controls bullet"><span class="by">vivegi</span><span>|</span><a href="#35878228">prev</a><span>|</span><a href="#35877528">next</a><span>|</span><label class="collapse" for="c-35884473">[-]</label><label class="expand" for="c-35884473">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of the boo Blink, by Malcolm Gladwell -- <i>We feel like we have to come up with a reason why we came up with an answer</i> (paraphrasing)</div><br/></div></div><div id="35877528" class="c"><input type="checkbox" id="c-35877528" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35884473">prev</a><span>|</span><a href="#35878266">next</a><span>|</span><label class="collapse" for="c-35877528">[-]</label><label class="expand" for="c-35877528">[36 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;This work is part of the third pillar of our approach to alignment research: we want to automate the alignment research work itself.&quot;<p>I feel like this isn&#x27;t a Yud-approved approach to AI alignment.</div><br/><div id="35877637" class="c"><input type="checkbox" id="c-35877637" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35878965">next</a><span>|</span><label class="collapse" for="c-35877637">[-]</label><label class="expand" for="c-35877637">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, I think any foundational work on the topic is inherently Yud-favored, compared to the blithe optimism and surface-level analysis at best that is usually applied to the topic.<p>Ie, I think it&#x27;s not that this shouldn&#x27;t be done. This should certainly be done. It&#x27;s just that so many more things than it should be done before we move forward.</div><br/></div></div><div id="35878965" class="c"><input type="checkbox" id="c-35878965" checked=""/><div class="controls bullet"><span class="by">killthebuddha</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35877637">prev</a><span>|</span><a href="#35877851">next</a><span>|</span><label class="collapse" for="c-35878965">[-]</label><label class="expand" for="c-35878965">[2 more]</label></div><br/><div class="children"><div class="content">DISCLAIMER: I think Yudkowsky is a serious thinker and his ideas should be taken seriously, regardless of whether not they are correct.<p>Your comment triggered a random thought: A perfect name for Yudkowsky et al and the AGI doomers is... wait for it... the Yuddites :)</div><br/><div id="35880201" class="c"><input type="checkbox" id="c-35880201" checked=""/><div class="controls bullet"><span class="by">throwaway2137</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35878965">parent</a><span>|</span><a href="#35877851">next</a><span>|</span><label class="collapse" for="c-35880201">[-]</label><label class="expand" for="c-35880201">[1 more]</label></div><br/><div class="children"><div class="content">Already used on 4chan :)</div><br/></div></div></div></div><div id="35877851" class="c"><input type="checkbox" id="c-35877851" checked=""/><div class="controls bullet"><span class="by">bick_nyers</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35878965">prev</a><span>|</span><a href="#35877755">next</a><span>|</span><label class="collapse" for="c-35877851">[-]</label><label class="expand" for="c-35877851">[1 more]</label></div><br/><div class="children"><div class="content">These were my thoughts exactly. On one hand, this can enable alignment research to catch up faster. On the other hand, if we are worried about homicidal AI, then putting it in charge of policing itself (and training it to find exploits in a way) is probably not ideal.</div><br/></div></div><div id="35877755" class="c"><input type="checkbox" id="c-35877755" checked=""/><div class="controls bullet"><span class="by">snapcaster</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35877851">prev</a><span>|</span><a href="#35877595">next</a><span>|</span><label class="collapse" for="c-35877755">[-]</label><label class="expand" for="c-35877755">[3 more]</label></div><br/><div class="children"><div class="content">Agreed, Yud does seem to have been right about the course things will take but I&#x27;m not confident he actually has any solutions to the problem to offer</div><br/><div id="35878253" class="c"><input type="checkbox" id="c-35878253" checked=""/><div class="controls bullet"><span class="by">causalmodels</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35877755">parent</a><span>|</span><a href="#35877800">next</a><span>|</span><label class="collapse" for="c-35878253">[-]</label><label class="expand" for="c-35878253">[1 more]</label></div><br/><div class="children"><div class="content">His solution is a global regulatory regime to ban new large training runs. The tools required to accomplish this are, IMO, out of the question but I will give Yud credit for being honest about them while others who share his viewpoint try to hide the ball.</div><br/></div></div><div id="35877800" class="c"><input type="checkbox" id="c-35877800" checked=""/><div class="controls bullet"><span class="by">qumpis</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35877755">parent</a><span>|</span><a href="#35878253">prev</a><span>|</span><a href="#35877595">next</a><span>|</span><label class="collapse" for="c-35877800">[-]</label><label class="expand" for="c-35877800">[1 more]</label></div><br/><div class="children"><div class="content">Which things has he been right about and when, if you recall?</div><br/></div></div></div></div><div id="35877595" class="c"><input type="checkbox" id="c-35877595" checked=""/><div class="controls bullet"><span class="by">shadowgovt</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35877755">prev</a><span>|</span><a href="#35878472">next</a><span>|</span><label class="collapse" for="c-35877595">[-]</label><label class="expand" for="c-35877595">[5 more]</label></div><br/><div class="children"><div class="content">&quot;Yud-approved?&quot;</div><br/><div id="35877679" class="c"><input type="checkbox" id="c-35877679" checked=""/><div class="controls bullet"><span class="by">aitanabewa</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35877595">parent</a><span>|</span><a href="#35877670">next</a><span>|</span><label class="collapse" for="c-35877679">[-]</label><label class="expand" for="c-35877679">[1 more]</label></div><br/><div class="children"><div class="content">Meaning is approved by Eliezer Yudkowsky.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eliezer_Yudkowsky" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Eliezer_Yudkowsky</a>
<a href="https:&#x2F;&#x2F;twitter.com&#x2F;ESYudkowsky" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;ESYudkowsky</a>
<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=AaTRHFaaPG8">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=AaTRHFaaPG8</a> (Lex Fridman Interview)</div><br/></div></div><div id="35877670" class="c"><input type="checkbox" id="c-35877670" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35877595">parent</a><span>|</span><a href="#35877679">prev</a><span>|</span><a href="#35877636">next</a><span>|</span><label class="collapse" for="c-35877670">[-]</label><label class="expand" for="c-35877670">[2 more]</label></div><br/><div class="children"><div class="content">He&#x27;s the one in the fedora who is losing patience that otherwise smart sounding people are seriously considering letting AI police itself <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=41SUp-TRVlg">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=41SUp-TRVlg</a></div><br/><div id="35877992" class="c"><input type="checkbox" id="c-35877992" checked=""/><div class="controls bullet"><span class="by">jack_riminton</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35877670">parent</a><span>|</span><a href="#35877636">next</a><span>|</span><label class="collapse" for="c-35877992">[-]</label><label class="expand" for="c-35877992">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not a fedora, that&#x27;s his King of the Redditors crown</div><br/></div></div></div></div><div id="35877636" class="c"><input type="checkbox" id="c-35877636" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35877595">parent</a><span>|</span><a href="#35877670">prev</a><span>|</span><a href="#35878472">next</a><span>|</span><label class="collapse" for="c-35877636">[-]</label><label class="expand" for="c-35877636">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Eliezer_Yudkowsky" rel="nofollow">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Eliezer_Yudkowsky</a></div><br/></div></div></div></div><div id="35878472" class="c"><input type="checkbox" id="c-35878472" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35877595">prev</a><span>|</span><a href="#35877724">next</a><span>|</span><label class="collapse" for="c-35878472">[-]</label><label class="expand" for="c-35878472">[22 more]</label></div><br/><div class="children"><div class="content">You mean Yudkowski? I saw him on Lex Fridman and he was entirely unconvincing. Why is everyone deferring to a bunch of effective altruism advocates when it comes to AI safety?</div><br/><div id="35879190" class="c"><input type="checkbox" id="c-35879190" checked=""/><div class="controls bullet"><span class="by">lubesGordi</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35878472">parent</a><span>|</span><a href="#35879196">next</a><span>|</span><label class="collapse" for="c-35879190">[-]</label><label class="expand" for="c-35879190">[11 more]</label></div><br/><div class="children"><div class="content">I heard him on Lex too, and it seemed to be just a given that AI is going to be deceptive and want to kill us all.  I don&#x27;t think there was a single example of how that could be accomplished given.  I&#x27;m open to hearing thoughts on this, maybe I&#x27;m not creative enough to see the &#x27;obvious&#x27; ways this could happen.</div><br/><div id="35879222" class="c"><input type="checkbox" id="c-35879222" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879190">parent</a><span>|</span><a href="#35879610">next</a><span>|</span><label class="collapse" for="c-35879222">[-]</label><label class="expand" for="c-35879222">[8 more]</label></div><br/><div class="children"><div class="content">This is also why I go into chess matches against 1400 elo players. I cannot conceive of the specific ways in which they will beat me (a 600 elo player), so I have good reason to suspect that I can win.<p>I&#x27;m willing to bet the future of our species on my consistent victory in these types of matches, in fact.</div><br/><div id="35879522" class="c"><input type="checkbox" id="c-35879522" checked=""/><div class="controls bullet"><span class="by">lubesGordi</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879222">parent</a><span>|</span><a href="#35879616">next</a><span>|</span><label class="collapse" for="c-35879522">[-]</label><label class="expand" for="c-35879522">[5 more]</label></div><br/><div class="children"><div class="content">Again, a given that AI is adversarial.
Edit: In addition, as an 1100 elo chess player, I can very easily tell you how a 1600 player is going to beat me.  The analogy doesn&#x27;t hold.  I&#x27;m in good faith asking how AI could destroy humanity.  It seems given the confidence people who are scared of AI have in this, that they have some concrete examples in mind.</div><br/><div id="35879550" class="c"><input type="checkbox" id="c-35879550" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879522">parent</a><span>|</span><a href="#35879616">next</a><span>|</span><label class="collapse" for="c-35879550">[-]</label><label class="expand" for="c-35879550">[4 more]</label></div><br/><div class="children"><div class="content">No it’s a given that some people who attempt to wield AI will be adversarial.<p>In any case a similar argument can be made with merely instrumental goals causing harm: “I am an ant and I do not see how or why a human would cause me harm, therefore I am not in danger.”</div><br/><div id="35879637" class="c"><input type="checkbox" id="c-35879637" checked=""/><div class="controls bullet"><span class="by">lubesGordi</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879550">parent</a><span>|</span><a href="#35879616">next</a><span>|</span><label class="collapse" for="c-35879637">[-]</label><label class="expand" for="c-35879637">[3 more]</label></div><br/><div class="children"><div class="content">People wielding AI and destroying humanity is very different from AI itself, being a weird alien intelligence, destroying humanity.<p>Honestly if you have no examples you can&#x27;t really blame people for not being scared.  I have no reason to think this ant-human relationship is analogous.<p>And seriously, I&#x27;ve made no claims that AI is benign so please stop characterizing my claims thusly.  The question is simple, give me a single hypothetical example of how an AI will destroy humanity?</div><br/><div id="35879693" class="c"><input type="checkbox" id="c-35879693" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879637">parent</a><span>|</span><a href="#35884209">next</a><span>|</span><label class="collapse" for="c-35879693">[-]</label><label class="expand" for="c-35879693">[1 more]</label></div><br/><div class="children"><div class="content">Sure, here’s a trivial example: It radicalizes or otherwise deceives an employee at a virus research lab into producing and releasing a horrific virus.<p>The guy at Google already demonstrated that AIs are able to convince people of fairly radical beliefs (and we have proof that even <i>humans</i> a thousand years ago were capable of creating belief systems that cause people to blow themselves up and kill thousands of innocent people).<p>P.S. I was not characterizing your opinion, I was speaking in the voice of an ant.</div><br/></div></div><div id="35884209" class="c"><input type="checkbox" id="c-35884209" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879637">parent</a><span>|</span><a href="#35879693">prev</a><span>|</span><a href="#35879616">next</a><span>|</span><label class="collapse" for="c-35884209">[-]</label><label class="expand" for="c-35884209">[1 more]</label></div><br/><div class="children"><div class="content">If AI is general, it will also have the capability of wielding itself, and probably better than humans.</div><br/></div></div></div></div></div></div></div></div><div id="35879616" class="c"><input type="checkbox" id="c-35879616" checked=""/><div class="controls bullet"><span class="by">kevinventullo</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879222">parent</a><span>|</span><a href="#35879522">prev</a><span>|</span><a href="#35879610">next</a><span>|</span><label class="collapse" for="c-35879616">[-]</label><label class="expand" for="c-35879616">[2 more]</label></div><br/><div class="children"><div class="content">Other caveman use fire to cook food. Fire scary and hurt. No understand fire. Fire cavemen bad.</div><br/><div id="35879703" class="c"><input type="checkbox" id="c-35879703" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879616">parent</a><span>|</span><a href="#35879610">next</a><span>|</span><label class="collapse" for="c-35879703">[-]</label><label class="expand" for="c-35879703">[1 more]</label></div><br/><div class="children"><div class="content">Other caveman use nuke to wipe out city. Nuke scary and hurt. No understand nuke. Nuke caveman bad.<p>Other caveman use anthrax in subway station. Anthrax scary and hurt…<p>Is AI closer to fire or closer to nukes and engineered viruses? Has fire ever invented a new weapon system?<p>By the way: we have shitloads of regulations and safety systems around fire due to, you guessed it, the amount of harm it can do by accident.</div><br/></div></div></div></div></div></div><div id="35879610" class="c"><input type="checkbox" id="c-35879610" checked=""/><div class="controls bullet"><span class="by">PeterisP</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879190">parent</a><span>|</span><a href="#35879222">prev</a><span>|</span><a href="#35879196">next</a><span>|</span><label class="collapse" for="c-35879610">[-]</label><label class="expand" for="c-35879610">[2 more]</label></div><br/><div class="children"><div class="content">IMHO the argument isn&#x27;t that AI is definitely going to be deceptive and want to kill us all, but rather that if you&#x27;re 90% sure that AI is going to be just fine, that 10% of existential risk is simply not acceptable, so you should assume that this level of certainty isn&#x27;t enough and you should act as if AI may be deceptive and may kill us all and take very serious preventive measures even if you&#x27;re quite certain that it won&#x27;t be needed - because &quot;quite certain&quot; isn&#x27;t enough, you want to be at &quot;this is definitely established to not lead to Skynet&quot; level.</div><br/><div id="35882307" class="c"><input type="checkbox" id="c-35882307" checked=""/><div class="controls bullet"><span class="by">NumberWangMan</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879610">parent</a><span>|</span><a href="#35879196">next</a><span>|</span><label class="collapse" for="c-35882307">[-]</label><label class="expand" for="c-35882307">[1 more]</label></div><br/><div class="children"><div class="content">And even with all that, probably it&#x27;s best to still exercise an abundance of caution, because you might have made a mistake somewhere.</div><br/></div></div></div></div></div></div><div id="35879196" class="c"><input type="checkbox" id="c-35879196" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35878472">parent</a><span>|</span><a href="#35879190">prev</a><span>|</span><a href="#35878819">next</a><span>|</span><label class="collapse" for="c-35879196">[-]</label><label class="expand" for="c-35879196">[7 more]</label></div><br/><div class="children"><div class="content">Because they have arguments that AI optimists are unable to convincingly address.<p>Take this blog post for example, which between the lines reads: we don&#x27;t expect to be able to align these systems ourselves, so instead we&#x27;re hoping these systems are able to align each other.<p>Consider me not-very-soothed.<p>FWIW, there are plenty of AI experts who have been raising alarms as well. Hinton and Christiano, for example.</div><br/><div id="35879326" class="c"><input type="checkbox" id="c-35879326" checked=""/><div class="controls bullet"><span class="by">ryan93</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879196">parent</a><span>|</span><a href="#35878819">next</a><span>|</span><label class="collapse" for="c-35879326">[-]</label><label class="expand" for="c-35879326">[6 more]</label></div><br/><div class="children"><div class="content">People won&#x27;t care until an actually scary AI exists. Will be easy to stop at that point. Or you can just stop research here and hope another country doesn&#x27;t get one first. Im personally skeptical it will exist. Honestly might be making it worse with the scaremongering coming from uncharismatic AI alignment people.</div><br/><div id="35883406" class="c"><input type="checkbox" id="c-35883406" checked=""/><div class="controls bullet"><span class="by">Kostchei</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879326">parent</a><span>|</span><a href="#35879442">next</a><span>|</span><label class="collapse" for="c-35883406">[-]</label><label class="expand" for="c-35883406">[1 more]</label></div><br/><div class="children"><div class="content">Are you kidding? &quot;easy to stop&quot;? When LLMs are integrated into law enforcement, banking, research, education, logistics... All these areas have people building backend systems leveraging the current llm tech and pipelines to plug in the coming tech. If we reach a tipping point where these things become aware&#x2F;act truly autonomously, what are the chances they do it before we notice?  People are renowned for implementing things before understanding the consequences.<p>And what does charisma of AI alignment folks have to do with anything?</div><br/></div></div><div id="35879442" class="c"><input type="checkbox" id="c-35879442" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879326">parent</a><span>|</span><a href="#35883406">prev</a><span>|</span><a href="#35878819">next</a><span>|</span><label class="collapse" for="c-35879442">[-]</label><label class="expand" for="c-35879442">[4 more]</label></div><br/><div class="children"><div class="content">Why would it be easy to stop at that point? The believable value prop will increase in lockstep with the believable scare factor, not to mention the (already significant) proliferation out of ultra expensive research orgs into open source repos.<p>Nuclear weapons proliferated explicitly <i>because</i> they proved their scariness.</div><br/><div id="35879671" class="c"><input type="checkbox" id="c-35879671" checked=""/><div class="controls bullet"><span class="by">ryan93</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879442">parent</a><span>|</span><a href="#35878819">next</a><span>|</span><label class="collapse" for="c-35879671">[-]</label><label class="expand" for="c-35879671">[3 more]</label></div><br/><div class="children"><div class="content">If AI can exist humans have to figure it out. It’s what we do. Really shockingly delusional to think people are gonna use chatgpt for a few min get bored and then ban it like it’s a nuke. I’d rather the USA get it first anyways.</div><br/><div id="35881952" class="c"><input type="checkbox" id="c-35881952" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879671">parent</a><span>|</span><a href="#35879865">next</a><span>|</span><label class="collapse" for="c-35881952">[-]</label><label class="expand" for="c-35881952">[1 more]</label></div><br/><div class="children"><div class="content">&gt;If AI can exist humans have to figure it out. It’s what we do.<p>We have figured out stuff in the past, but we also came shockingly close to nuclear armageddon more than once.<p>I&#x27;m not sure I want to roll the dice again.</div><br/></div></div><div id="35879865" class="c"><input type="checkbox" id="c-35879865" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879671">parent</a><span>|</span><a href="#35881952">prev</a><span>|</span><a href="#35878819">next</a><span>|</span><label class="collapse" for="c-35879865">[-]</label><label class="expand" for="c-35879865">[1 more]</label></div><br/><div class="children"><div class="content">Where did I say we could or should ban it like a nuke?<p>Anyway this is a good example of the completely blind-faith reasoning that backs AI optimism: we’ll figure it out “because it’s what we do.”<p>FWIW we have still not figured out how to dramatically reduce nuclear risk. We’re here just living with it every single day still, and with AI we’re likely stepping onto another tightrope that we and <i>all</i> future generations have to walk flawlessly.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35878819" class="c"><input type="checkbox" id="c-35878819" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35878472">parent</a><span>|</span><a href="#35879196">prev</a><span>|</span><a href="#35877724">next</a><span>|</span><label class="collapse" for="c-35878819">[-]</label><label class="expand" for="c-35878819">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Why is everyone deferring to a bunch of effective altruism advocates when it comes to AI safety?<p>I&#x27;m not sure Yudkowski is an EA, but the EAs want him in their polycule.</div><br/><div id="35879021" class="c"><input type="checkbox" id="c-35879021" checked=""/><div class="controls bullet"><span class="by">tomjakubowski</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35878819">parent</a><span>|</span><a href="#35877724">next</a><span>|</span><label class="collapse" for="c-35879021">[-]</label><label class="expand" for="c-35879021">[2 more]</label></div><br/><div class="children"><div class="content">He posts on the forum.  I&#x27;m not sure what more evidence is needed that he&#x27;s part of it.<p><a href="https:&#x2F;&#x2F;forum.effectivealtruism.org&#x2F;users&#x2F;eliezeryudkowsky" rel="nofollow">https:&#x2F;&#x2F;forum.effectivealtruism.org&#x2F;users&#x2F;eliezeryudkowsky</a></div><br/><div id="35879167" class="c"><input type="checkbox" id="c-35879167" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#35877528">root</a><span>|</span><a href="#35879021">parent</a><span>|</span><a href="#35877724">next</a><span>|</span><label class="collapse" for="c-35879167">[-]</label><label class="expand" for="c-35879167">[1 more]</label></div><br/><div class="children"><div class="content">I guess it&#x27;s true, not just a rationalist but also effective altruist!</div><br/></div></div></div></div></div></div></div></div><div id="35877724" class="c"><input type="checkbox" id="c-35877724" checked=""/><div class="controls bullet"><span class="by">Teever</span><span>|</span><a href="#35877528">parent</a><span>|</span><a href="#35878472">prev</a><span>|</span><a href="#35878266">next</a><span>|</span><label class="collapse" for="c-35877724">[-]</label><label class="expand" for="c-35877724">[1 more]</label></div><br/><div class="children"><div class="content">Why does this matter?</div><br/></div></div></div></div><div id="35878266" class="c"><input type="checkbox" id="c-35878266" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#35877528">prev</a><span>|</span><a href="#35883245">next</a><span>|</span><label class="collapse" for="c-35878266">[-]</label><label class="expand" for="c-35878266">[4 more]</label></div><br/><div class="children"><div class="content">I think this is a generous usage of &quot;can.&quot; As the article admits, these explanations are &#x27;imperfect&#x27; and I think that is definitely true.</div><br/><div id="35878735" class="c"><input type="checkbox" id="c-35878735" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35878266">parent</a><span>|</span><a href="#35883245">next</a><span>|</span><label class="collapse" for="c-35878735">[-]</label><label class="expand" for="c-35878735">[3 more]</label></div><br/><div class="children"><div class="content">It depends how you parse it. It is clearly true that they &#x27;can&#x27; explain neurons, in the sense that at least some of the neurons are quite well explained. On the other hand, it&#x27;s also the case that the vast majority of neurons are not well explained at all by this method (or likely any method).<p>I̵t̵&#x27;̵s̵ ̵o̵n̵l̵y̵ ̵b̵e̵c̵a̵u̵s̵e̵ ̵o̵f̵ ̵a̵ ̵q̵u̵i̵r̵k̵ ̵o̵f̵ ̵A̵d̵a̵m̵W̵ ̵t̵h̵a̵t̵ ̵t̵h̵i̵s̵ ̵i̵s̵ ̵p̵o̵s̵s̵i̵b̵l̵e̵ ̵a̵t̵ ̵a̵l̵l̵,̵ ̵i̵f̵ ̵G̵P̵T̵-̵2̵ ̵w̵a̵s̵ ̵t̵r̵a̵i̵n̵e̵d̵ ̵w̵i̵t̵h̵ ̵S̵G̵D̵ ̵a̵l̵m̵o̵s̵t̵ ̵n̵o̵ ̵n̵e̵u̵r̵o̵n̵s̵ ̵w̵o̵u̵l̵d̵ ̵b̵e̵ ̵i̵n̵t̵e̵r̵p̵r̵e̵t̵a̵b̵l̵e̵.̵<p>EDIT: This last part isn&#x27;t true. I think they are only looking at the intermediate layer of the FFN which does have a privileged basis.</div><br/><div id="35879223" class="c"><input type="checkbox" id="c-35879223" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#35878266">root</a><span>|</span><a href="#35878735">parent</a><span>|</span><a href="#35883245">next</a><span>|</span><label class="collapse" for="c-35879223">[-]</label><label class="expand" for="c-35879223">[2 more]</label></div><br/><div class="children"><div class="content">&gt; EDIT: This last part isn&#x27;t true. I think they are only looking at the intermediate layer of the FFN which does have a privileged basis.<p>it does?</div><br/><div id="35880423" class="c"><input type="checkbox" id="c-35880423" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35878266">root</a><span>|</span><a href="#35879223">parent</a><span>|</span><a href="#35883245">next</a><span>|</span><label class="collapse" for="c-35880423">[-]</label><label class="expand" for="c-35880423">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, that&#x27;s where they apply the activation function and that happens per neuron so you can&#x27;t rotate everything and expect the same result.</div><br/></div></div></div></div></div></div></div></div><div id="35883245" class="c"><input type="checkbox" id="c-35883245" checked=""/><div class="controls bullet"><span class="by">alrex021</span><span>|</span><a href="#35878266">prev</a><span>|</span><a href="#35877887">next</a><span>|</span><label class="collapse" for="c-35883245">[-]</label><label class="expand" for="c-35883245">[1 more]</label></div><br/><div class="children"><div class="content">IMO this is akin to how scientists developed thermodynamics to comprehend steam engines. Except now, AI could be on the forefront of forming conjectures and explanatory theories!</div><br/></div></div><div id="35877887" class="c"><input type="checkbox" id="c-35877887" checked=""/><div class="controls bullet"><span class="by">rounakdatta</span><span>|</span><a href="#35883245">prev</a><span>|</span><a href="#35878182">next</a><span>|</span><label class="collapse" for="c-35877887">[-]</label><label class="expand" for="c-35877887">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m split between what&#x27;s more impressive:<p>- The software powering the research paper<p>- The research itself (holy moly! They&#x27;re showing the neurons!)</div><br/></div></div><div id="35878182" class="c"><input type="checkbox" id="c-35878182" checked=""/><div class="controls bullet"><span class="by">xthetrfd</span><span>|</span><a href="#35877887">prev</a><span>|</span><a href="#35879711">next</a><span>|</span><label class="collapse" for="c-35878182">[-]</label><label class="expand" for="c-35878182">[2 more]</label></div><br/><div class="children"><div class="content">This blog post is not very informative. How did they prompt GPT4 to explain the neuron&#x27;s behavior?</div><br/><div id="35878438" class="c"><input type="checkbox" id="c-35878438" checked=""/><div class="controls bullet"><span class="by">samgriesemer</span><span>|</span><a href="#35878182">parent</a><span>|</span><a href="#35879711">next</a><span>|</span><label class="collapse" for="c-35878438">[-]</label><label class="expand" for="c-35878438">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s explained more in the &quot;read paper&quot; link, where they provide the actual prompts:<p><a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;paper&#x2F;index.html#sec-algorithm-explain" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a></div><br/></div></div></div></div><div id="35879711" class="c"><input type="checkbox" id="c-35879711" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#35878182">prev</a><span>|</span><a href="#35881830">next</a><span>|</span><label class="collapse" for="c-35879711">[-]</label><label class="expand" for="c-35879711">[2 more]</label></div><br/><div class="children"><div class="content">Language models can also tell you they’re not AI.</div><br/><div id="35879779" class="c"><input type="checkbox" id="c-35879779" checked=""/><div class="controls bullet"><span class="by">0xdeadbeefbabe</span><span>|</span><a href="#35879711">parent</a><span>|</span><a href="#35881830">next</a><span>|</span><label class="collapse" for="c-35879779">[-]</label><label class="expand" for="c-35879779">[1 more]</label></div><br/><div class="children"><div class="content">With dubious confidence too!</div><br/></div></div></div></div><div id="35881830" class="c"><input type="checkbox" id="c-35881830" checked=""/><div class="controls bullet"><span class="by">budoso</span><span>|</span><a href="#35879711">prev</a><span>|</span><a href="#35880981">next</a><span>|</span><label class="collapse" for="c-35881830">[-]</label><label class="expand" for="c-35881830">[1 more]</label></div><br/><div class="children"><div class="content">How long until we copyright the Marvel Neuron?</div><br/></div></div><div id="35880981" class="c"><input type="checkbox" id="c-35880981" checked=""/><div class="controls bullet"><span class="by">tschumacher</span><span>|</span><a href="#35881830">prev</a><span>|</span><a href="#35880978">next</a><span>|</span><label class="collapse" for="c-35880981">[-]</label><label class="expand" for="c-35880981">[8 more]</label></div><br/><div class="children"><div class="content">Even if we can explain the function of a single neuron what do we gain? If the goal is to reason about safety of computer vision in automated driving as an example, we would need to understand the system as a whole. The whole point of neural networks is to solve nuanced problems we can&#x27;t clearly define. The fuzziness of the problems those systems solve is fundamentally at odds with the intent to reason about them.</div><br/><div id="35882408" class="c"><input type="checkbox" id="c-35882408" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#35880981">parent</a><span>|</span><a href="#35884223">next</a><span>|</span><label class="collapse" for="c-35882408">[-]</label><label class="expand" for="c-35882408">[1 more]</label></div><br/><div class="children"><div class="content">&gt; reason about safety of computer vision in automated driving<p>An interesting analogue. I think we simply aren&#x27;t going to reason about the internals of neural networks to analyze safety for driving, we&#x27;re just going to measure safety empirically. This will make many people very upset but it&#x27;s the best we can do, and probably good enough.</div><br/></div></div><div id="35884223" class="c"><input type="checkbox" id="c-35884223" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#35880981">parent</a><span>|</span><a href="#35882408">prev</a><span>|</span><a href="#35881176">next</a><span>|</span><label class="collapse" for="c-35884223">[-]</label><label class="expand" for="c-35884223">[1 more]</label></div><br/><div class="children"><div class="content">If there is a limited number of deception neurons, we can automatically look for coactivation of the &quot;my future behavior&quot; neuron and the deception neuron.</div><br/></div></div><div id="35881176" class="c"><input type="checkbox" id="c-35881176" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35880981">parent</a><span>|</span><a href="#35884223">prev</a><span>|</span><a href="#35880978">next</a><span>|</span><label class="collapse" for="c-35881176">[-]</label><label class="expand" for="c-35881176">[5 more]</label></div><br/><div class="children"><div class="content">I have to agree.<p>I often think, “maybe I should use ChatGPT for this” then I realise I have very little way to verify what it tells me and as someone working in engineering, If I don’t understand the black box, I just can’t do it.<p>I’m attracted to open source, because I can look at the code understand it.</div><br/><div id="35882343" class="c"><input type="checkbox" id="c-35882343" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35880981">root</a><span>|</span><a href="#35881176">parent</a><span>|</span><a href="#35880978">next</a><span>|</span><label class="collapse" for="c-35882343">[-]</label><label class="expand" for="c-35882343">[4 more]</label></div><br/><div class="children"><div class="content">Open source doesn&#x27;t mean you can explain the black box any better. 
and humans are black boxes that don&#x27;t understand their mental processes either. We&#x27;re currently better than LLMs at it i suppose but we&#x27;re still very poor at it.<p><a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3196841&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3196841&#x2F;</a>
<a href="https:&#x2F;&#x2F;pure.uva.nl&#x2F;ws&#x2F;files&#x2F;25987577&#x2F;Split_Brain.pdf" rel="nofollow">https:&#x2F;&#x2F;pure.uva.nl&#x2F;ws&#x2F;files&#x2F;25987577&#x2F;Split_Brain.pdf</a>
<a href="https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4204522" rel="nofollow">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC4204522</a><p>We can&#x27;t recreate previous mental states, we just do a pretty good job (usually) of rationalizing decisions after the fact.</div><br/><div id="35882599" class="c"><input type="checkbox" id="c-35882599" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35880981">root</a><span>|</span><a href="#35882343">parent</a><span>|</span><a href="#35880978">next</a><span>|</span><label class="collapse" for="c-35882599">[-]</label><label class="expand" for="c-35882599">[3 more]</label></div><br/><div class="children"><div class="content">Humans can be held accountable so it’s not the same. Even if we’re a black box, we share common traits with other humans. We’re trained in similar ways. So we mostly understand what we will and won’t do.<p>I think this constant degradation of humans is really foolish and harmful personally. “We’re just black boxes etc”, we might not know how brains work but we do and can understand each other.<p>On the other hand I’m starting to feel like “AI researchers” are the greatest black box I’ve ever seen, the logic of what they’re trying  to create and their hopes for it really baffle me.<p>By the way, I have infinitely more hope of understanding an open source black box compared to a closed source one?</div><br/><div id="35882699" class="c"><input type="checkbox" id="c-35882699" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35880981">root</a><span>|</span><a href="#35882599">parent</a><span>|</span><a href="#35880978">next</a><span>|</span><label class="collapse" for="c-35882699">[-]</label><label class="expand" for="c-35882699">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Humans can be held accountable so it’s not the same.<p>1. Don&#x27;t worry, LLMs will be held accountable eventually. There&#x27;s only so much embodiment and unsupervised tool control we can grant machines before personhood is in the best interests of everybody. May be forced like all the times in the past but it&#x27;ll happen.<p>2. not every use case cares about accountability<p>3. accountability can be shifted. we have experience.<p>&gt;I think this constant degradation of humans is really foolish and harmful personally.<p>Maybe you think so but there&#x27;s nothing degrading about it. We are black boxes that poorly understand how said box actually works even if we like to believe otherwise. Don&#x27;t know what&#x27;s degrading about stating truth that&#x27;s been backed by multiple studies.<p>Degrading is calling an achievement we hold people in high regard who accomplish stupid because a machine can do it.<p>&gt;By the way, I have infinitely more hope of understanding an open source black box compared to a closed source one?<p>Sure i guess so.</div><br/><div id="35883279" class="c"><input type="checkbox" id="c-35883279" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#35880981">root</a><span>|</span><a href="#35882699">parent</a><span>|</span><a href="#35880978">next</a><span>|</span><label class="collapse" for="c-35883279">[-]</label><label class="expand" for="c-35883279">[1 more]</label></div><br/><div class="children"><div class="content"><i>Degrading is calling an achievement we hold people in high regard who accomplish stupid because a machine can do it.</i><p>Not sure you worded this as intended ?<p>Anyway if I read you correctly, this assumes you believe the idea of self and ego have anything to do with it.<p>Humans should treat ants, lab rats and each other with equal respect.<p>I don’t believe we should avoid self-degradation because  we think we’re smart or special, but for completely opposite reasons. We are mostly lucky we have what we have because something bigger than us, call it God, nature whatever, has provided that existence. When we degrade one another, we degrade that magic. This is where we fuck up time and time again. I’m talking about the water you drink, the food you eat and the air your breathe, the inspiration for neural networks etc. We take that for granted.<p>I liken it to the idea that humans killed God, the idea of God and morals etc, so we could do terrible things to the world and living things. We just got rid of the idea someone is looking over our shoulder because it made wars and genocides easier to do. Killing God got rid of a whole lot of moral baggage.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35880978" class="c"><input type="checkbox" id="c-35880978" checked=""/><div class="controls bullet"><span class="by">ccvannorman</span><span>|</span><a href="#35880981">prev</a><span>|</span><a href="#35884799">next</a><span>|</span><label class="collapse" for="c-35880978">[-]</label><label class="expand" for="c-35880978">[1 more]</label></div><br/><div class="children"><div class="content">My take: Regurgitation of trained-on information about LLMs does not come anywhere close to &quot;conscious brain knows it&#x27;s conscious.&quot;</div><br/></div></div><div id="35884799" class="c"><input type="checkbox" id="c-35884799" checked=""/><div class="controls bullet"><span class="by">call_me_g0d</span><span>|</span><a href="#35880978">prev</a><span>|</span><a href="#35877590">next</a><span>|</span><label class="collapse" for="c-35884799">[-]</label><label class="expand" for="c-35884799">[1 more]</label></div><br/><div class="children"><div class="content">Can&#x27;t see how using AI to explain AI could ever go wrong</div><br/></div></div><div id="35877590" class="c"><input type="checkbox" id="c-35877590" checked=""/><div class="controls bullet"><span class="by">cschmid</span><span>|</span><a href="#35884799">prev</a><span>|</span><a href="#35880413">next</a><span>|</span><label class="collapse" for="c-35877590">[-]</label><label class="expand" for="c-35877590">[10 more]</label></div><br/><div class="children"><div class="content">Has anyone here found a link to the actual paper? If I click on &#x27;paper&#x27;, I only see what seems to be an awkward HTML version.</div><br/><div id="35877711" class="c"><input type="checkbox" id="c-35877711" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35877590">parent</a><span>|</span><a href="#35880413">next</a><span>|</span><label class="collapse" for="c-35877711">[-]</label><label class="expand" for="c-35877711">[9 more]</label></div><br/><div class="children"><div class="content">You mean this? <a href="https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;paper&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;openaipublic.blob.core.windows.net&#x2F;neuron-explainer&#x2F;...</a><p>Would you prefer a PDF?<p>(I&#x27;m always fascinated to hear from people who would rather read a PDF than a web-native paper like this one, especially given that web papers are actually readable on mobile devices. Do you do all of your reading on a laptop?)</div><br/><div id="35878318" class="c"><input type="checkbox" id="c-35878318" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35877895">next</a><span>|</span><label class="collapse" for="c-35878318">[-]</label><label class="expand" for="c-35878318">[1 more]</label></div><br/><div class="children"><div class="content">With a pdf I don&#x27;t have to update my PDF reader multiple times per month just to be able to read text.<p>A PDF is a text document that includes all the text, images, etc within it in the state you are going to perceive them. That web page is just barely even a document. None of it&#x27;s contents are natively within it, it all requires executing remote code which pulls down more remote code to run just to get the actual text and images to display... which they don&#x27;t in my browser. I just see an index with links that don&#x27;t work and the the &quot;Contributions&quot; which for some reason was actually included as text.<p>Even as the web goes up it&#x27;s own asshole in terms of recursive serial loading of javascript&#x2F;json&#x2F;whatever from unrelated domains and abandons all backwards compatibility, PDF, as a document, remains readable. I wish the web was still hyperlinked documents. The &quot;application&quot; web sucks for accessibility.</div><br/></div></div><div id="35877895" class="c"><input type="checkbox" id="c-35877895" checked=""/><div class="controls bullet"><span class="by">probably_wrong</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35878318">prev</a><span>|</span><a href="#35879682">next</a><span>|</span><label class="collapse" for="c-35877895">[-]</label><label class="expand" for="c-35877895">[1 more]</label></div><br/><div class="children"><div class="content">The equations look terrible on Firefox for Android, as they are <i>really</i> small - a two-line fraction is barely taller than a single line, forcing me to constantly zoom in and out.<p>So yes, I would prefer a PDF and have a guarantee that it will look the same no matter where I read it.</div><br/></div></div><div id="35879682" class="c"><input type="checkbox" id="c-35879682" checked=""/><div class="controls bullet"><span class="by">cschmid</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35877895">prev</a><span>|</span><a href="#35878110">next</a><span>|</span><label class="collapse" for="c-35879682">[-]</label><label class="expand" for="c-35879682">[1 more]</label></div><br/><div class="children"><div class="content">My whole workflow of organizing and reading papers is centered on PDFs. While I like having interactive supplemental materials, I want to be able to print, save and annotate the papers I read.</div><br/></div></div><div id="35878110" class="c"><input type="checkbox" id="c-35878110" checked=""/><div class="controls bullet"><span class="by">hexomancer</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35879682">prev</a><span>|</span><a href="#35878247">next</a><span>|</span><label class="collapse" for="c-35878110">[-]</label><label class="expand" for="c-35878110">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Would you prefer a PDF?<p>Yes, I was just reading the paper and some of the javascript glitched and deleted all the contents of the document except the last section, making me lose all context and focus. Doesn&#x27;t really happen with PDF files.</div><br/></div></div><div id="35878247" class="c"><input type="checkbox" id="c-35878247" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35878110">prev</a><span>|</span><a href="#35877760">next</a><span>|</span><label class="collapse" for="c-35878247">[-]</label><label class="expand" for="c-35878247">[1 more]</label></div><br/><div class="children"><div class="content">If you want to draw on it, PDF is usually the best</div><br/></div></div><div id="35877760" class="c"><input type="checkbox" id="c-35877760" checked=""/><div class="controls bullet"><span class="by">bad_alloc</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35878247">prev</a><span>|</span><a href="#35878922">next</a><span>|</span><label class="collapse" for="c-35877760">[-]</label><label class="expand" for="c-35877760">[1 more]</label></div><br/><div class="children"><div class="content">Nope, reading the printed paper on... paper. :)</div><br/></div></div><div id="35878922" class="c"><input type="checkbox" id="c-35878922" checked=""/><div class="controls bullet"><span class="by">nerpderp82</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35877760">prev</a><span>|</span><a href="#35877843">next</a><span>|</span><label class="collapse" for="c-35878922">[-]</label><label class="expand" for="c-35878922">[1 more]</label></div><br/><div class="children"><div class="content">&gt; always fascinated<p>That feels like a loaded phrase. Is it &quot;false confusion&quot; adjacent?</div><br/></div></div><div id="35877843" class="c"><input type="checkbox" id="c-35877843" checked=""/><div class="controls bullet"><span class="by">kkylin</span><span>|</span><a href="#35877590">root</a><span>|</span><a href="#35877711">parent</a><span>|</span><a href="#35878922">prev</a><span>|</span><a href="#35880413">next</a><span>|</span><label class="collapse" for="c-35877843">[-]</label><label class="expand" for="c-35877843">[1 more]</label></div><br/><div class="children"><div class="content">I personally prefer reading PDF on an iPad so I can mark it up.</div><br/></div></div></div></div></div></div><div id="35880413" class="c"><input type="checkbox" id="c-35880413" checked=""/><div class="controls bullet"><span class="by">nico</span><span>|</span><a href="#35877590">prev</a><span>|</span><a href="#35878219">next</a><span>|</span><label class="collapse" for="c-35880413">[-]</label><label class="expand" for="c-35880413">[1 more]</label></div><br/><div class="children"><div class="content">Now grab the list of labels&#x2F;explanations for each neuron, and train a small LLM only with data for that neuron.<p>Then you get a dictionary&#x2F;index of LLMs<p>Could this be used to parallelize training?<p>Or create lighter overall language models?<p>The above would be like doing a “map”, how would we do a “reduce”?</div><br/></div></div><div id="35878219" class="c"><input type="checkbox" id="c-35878219" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#35880413">prev</a><span>|</span><a href="#35881304">next</a><span>|</span><label class="collapse" for="c-35878219">[-]</label><label class="expand" for="c-35878219">[3 more]</label></div><br/><div class="children"><div class="content">I’m most surprised by the approach they take of passing GPT tuples of (token, importance) and having the model reliably figure out the patterns.<p>Nothing would suggest this should work in practice, yet it just… does. In more or less zero shot. With a completely different underlying model. That’s fascinating.</div><br/><div id="35878752" class="c"><input type="checkbox" id="c-35878752" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#35878219">parent</a><span>|</span><a href="#35881304">next</a><span>|</span><label class="collapse" for="c-35878752">[-]</label><label class="expand" for="c-35878752">[2 more]</label></div><br/><div class="children"><div class="content">They’re not looking at activations?</div><br/><div id="35882746" class="c"><input type="checkbox" id="c-35882746" checked=""/><div class="controls bullet"><span class="by">jerpint</span><span>|</span><a href="#35878219">root</a><span>|</span><a href="#35878752">parent</a><span>|</span><a href="#35881304">next</a><span>|</span><label class="collapse" for="c-35882746">[-]</label><label class="expand" for="c-35882746">[1 more]</label></div><br/><div class="children"><div class="content">No just at normalized log probabilities I think</div><br/></div></div></div></div></div></div><div id="35881304" class="c"><input type="checkbox" id="c-35881304" checked=""/><div class="controls bullet"><span class="by">sudoapps</span><span>|</span><a href="#35878219">prev</a><span>|</span><a href="#35880875">next</a><span>|</span><label class="collapse" for="c-35881304">[-]</label><label class="expand" for="c-35881304">[1 more]</label></div><br/><div class="children"><div class="content">This is really interesting. Could this lead to eventually being able to deconstruct these &quot;black-boxes&quot; to remove proprietary data or enforce legal issues?</div><br/></div></div><div id="35880875" class="c"><input type="checkbox" id="c-35880875" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#35881304">prev</a><span>|</span><a href="#35879443">next</a><span>|</span><label class="collapse" for="c-35880875">[-]</label><label class="expand" for="c-35880875">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI need to hear an explanation of the word &quot;explain&quot;.</div><br/></div></div><div id="35879443" class="c"><input type="checkbox" id="c-35879443" checked=""/><div class="controls bullet"><span class="by">TyrianPurple</span><span>|</span><a href="#35880875">prev</a><span>|</span><a href="#35880959">next</a><span>|</span><label class="collapse" for="c-35879443">[-]</label><label class="expand" for="c-35879443">[1 more]</label></div><br/><div class="children"><div class="content">By analyzing the function of individual neurons, these guys were&#x2F;are able to gain a deeper understanding of how language models process language, which could lead to improved model architecture and training methods.</div><br/></div></div><div id="35880959" class="c"><input type="checkbox" id="c-35880959" checked=""/><div class="controls bullet"><span class="by">jokoon</span><span>|</span><a href="#35879443">prev</a><span>|</span><label class="collapse" for="c-35880959">[-]</label><label class="expand" for="c-35880959">[2 more]</label></div><br/><div class="children"><div class="content">I wish there was insightful explanations on why AI cannot think, and if there are researchers trying to explore this topic, and if yes what they do.</div><br/><div id="35881957" class="c"><input type="checkbox" id="c-35881957" checked=""/><div class="controls bullet"><span class="by">sebzim4500</span><span>|</span><a href="#35880959">parent</a><span>|</span><label class="collapse" for="c-35881957">[-]</label><label class="expand" for="c-35881957">[1 more]</label></div><br/><div class="children"><div class="content">Any discussion on this topic would be nothing but arguments over the definition of the word &#x27;think&#x27;.</div><br/></div></div></div></div></div></div></div></div></div></body></html>