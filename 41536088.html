<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1726304449836" as="style"/><link rel="stylesheet" href="styles.css?v=1726304449836"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://louwrentius.com/my-71-tib-zfs-nas-after-10-years-and-zero-drive-failures.html">My 71 TiB ZFS NAS After 10 Years and Zero Drive Failures</a> <span class="domain">(<a href="https://louwrentius.com">louwrentius.com</a>)</span></div><div class="subtext"><span>louwrentius</span> | <span>158 comments</span></div><br/><div><div id="41536897" class="c"><input type="checkbox" id="c-41536897" checked=""/><div class="controls bullet"><span class="by">snvzz</span><span>|</span><a href="#41538213">next</a><span>|</span><label class="collapse" for="c-41536897">[-]</label><label class="expand" for="c-41536897">[22 more]</label></div><br/><div class="children"><div class="content">&gt;but for residential usage, it&#x27;s totally reasonable to accept the risk.<p>Polite disagree. Data integrity is the natural expectation humans have from computers, and thus we should stick to filesystems with data checksums such as ZFS, as well as ECC memory.</div><br/><div id="41538018" class="c"><input type="checkbox" id="c-41538018" checked=""/><div class="controls bullet"><span class="by">Timshel</span><span>|</span><a href="#41536897">parent</a><span>|</span><a href="#41537288">next</a><span>|</span><label class="collapse" for="c-41538018">[-]</label><label class="expand" for="c-41538018">[6 more]</label></div><br/><div class="children"><div class="content">Just checked my scrub history, for 20TB on consumer hardware during the last two years it repaired twice around 2 and 4 blocks each time.<p>So not much but at the same time with a special kind of luck might have been on an encrypted archive ^^.</div><br/><div id="41538063" class="c"><input type="checkbox" id="c-41538063" checked=""/><div class="controls bullet"><span class="by">nolok</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538018">parent</a><span>|</span><a href="#41537288">next</a><span>|</span><label class="collapse" for="c-41538063">[-]</label><label class="expand" for="c-41538063">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s all fine and good until that one random lone broken block stops you from opening that file you really need.</div><br/><div id="41538148" class="c"><input type="checkbox" id="c-41538148" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538063">parent</a><span>|</span><a href="#41537288">next</a><span>|</span><label class="collapse" for="c-41538148">[-]</label><label class="expand" for="c-41538148">[4 more]</label></div><br/><div class="children"><div class="content">Or in my case, a key filesystem metadata block that ruins everything. :s</div><br/><div id="41538268" class="c"><input type="checkbox" id="c-41538268" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538148">parent</a><span>|</span><a href="#41537288">next</a><span>|</span><label class="collapse" for="c-41538268">[-]</label><label class="expand" for="c-41538268">[3 more]</label></div><br/><div class="children"><div class="content">I only know about FAT but these &quot;key file metadata blocks&quot; are redundant, so you need really special double-plus bad luck to do that.</div><br/><div id="41538462" class="c"><input type="checkbox" id="c-41538462" checked=""/><div class="controls bullet"><span class="by">Szpadel</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538268">parent</a><span>|</span><a href="#41538468">next</a><span>|</span><label class="collapse" for="c-41538462">[-]</label><label class="expand" for="c-41538462">[1 more]</label></div><br/><div class="children"><div class="content">so I can consider myself very lucky and unlucky at the same time.
I had data corruption on zfs filesystem that destroyed whole pool to unrecoverable state (zfs was segfaulting while trying to import, all recovery zfs features where crashing zfs module and required reboot)
the lucky part is that this happened just after (something like next day) I migrated whole pool to another (bigger) server&#x2F;pool so that system was already scheduled for full disk wipe</div><br/></div></div><div id="41538468" class="c"><input type="checkbox" id="c-41538468" checked=""/><div class="controls bullet"><span class="by">lazide</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538268">parent</a><span>|</span><a href="#41538462">prev</a><span>|</span><a href="#41537288">next</a><span>|</span><label class="collapse" for="c-41538468">[-]</label><label class="expand" for="c-41538468">[1 more]</label></div><br/><div class="children"><div class="content">It was ext4, and I’ve had it happen two different times - in fact, I’ve never had it happen in a ‘good’ recoverable way before that I’ve ever seen.<p>It triggered a kernel panic in every machine that I mounted it in, and it wasn’t a media issue either.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41537288" class="c"><input type="checkbox" id="c-41537288" checked=""/><div class="controls bullet"><span class="by">naming_the_user</span><span>|</span><a href="#41536897">parent</a><span>|</span><a href="#41538018">prev</a><span>|</span><a href="#41537331">next</a><span>|</span><label class="collapse" for="c-41537288">[-]</label><label class="expand" for="c-41537288">[3 more]</label></div><br/><div class="children"><div class="content">I agree.<p>I think that the author may not have experienced these sorts of errors before.<p>Yes, the average person may not care about experiencing a couple of bit flips per year and losing the odd pixel or block of a JPEG, but they will care if some cable somewhere or transfer or bad RAM chip or whatever else manages to destroy a significant amount of data before they notice it.</div><br/><div id="41537605" class="c"><input type="checkbox" id="c-41537605" checked=""/><div class="controls bullet"><span class="by">AndrewDavis</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537288">parent</a><span>|</span><a href="#41537331">next</a><span>|</span><label class="collapse" for="c-41537605">[-]</label><label class="expand" for="c-41537605">[2 more]</label></div><br/><div class="children"><div class="content">I had a significant data loss years ago.<p>I was young and only had a deskop, so all my data was there.<p>So I purchased a 300GB external usb drive to use for periodic backup. It was all manual copy&#x2F;paste files across with no real schedule, but it was fine for the time and life was good.<p>Over time my data grew and the 300GB drive wasn&#x27;t large enough to store it all. For a while some of it wasnt backed up (I was young with much less disposable income).<p>Eventually I purchased a 500GB drive.<p>But what I didn&#x27;t know is my desktop drive was dying. Bits were flipping, a lot of them.<p>So when I did my first backup with the new drive I copied all my data off my desktop along with the corruption.<p>It was months before I realised a huge amount of my files were corrupted. By that point I&#x27;d wiped the old backup drive to give to my Mum to do her own backups. My data was long gone.<p>Once I discovered ZFS I jumped on it. It was the exact thing that would have prevented this because I could have detected the corruption when I purchased the new backup drive and did the initial backup to it.<p>(I made up the drive sizes because I can&#x27;t remember, but the ratios will be about right).</div><br/><div id="41538374" class="c"><input type="checkbox" id="c-41538374" checked=""/><div class="controls bullet"><span class="by">369548684892826</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537605">parent</a><span>|</span><a href="#41537331">next</a><span>|</span><label class="collapse" for="c-41538374">[-]</label><label class="expand" for="c-41538374">[1 more]</label></div><br/><div class="children"><div class="content">There’s something disturbing about the idea of silent data loss, it totally undermines the peace of mind of having backups. ZFS is good, but you can also just run rsync periodically with checksum and dryrun args and check the output for diffs.</div><br/></div></div></div></div></div></div><div id="41537331" class="c"><input type="checkbox" id="c-41537331" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#41536897">parent</a><span>|</span><a href="#41537288">prev</a><span>|</span><a href="#41538213">next</a><span>|</span><label class="collapse" for="c-41537331">[-]</label><label class="expand" for="c-41537331">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Data integrity is the natural expectation humans have from computers<p>I&#x27;ve said it once, and I&#x27;ll say it again: the only reason ZFS isn&#x27;t the norm is because we all once lived through a primordial era when it didn&#x27;t exist.  No serious person designing a filesystem today would say it&#x27;s okay to misplace your data.<p>Not long ago, on this forum, someone told me that ZFS is only good because <i>it had no competitors</i> in its space.  Which is kind of like saying the heavyweight champ is only good because no one else could compete.</div><br/><div id="41537710" class="c"><input type="checkbox" id="c-41537710" checked=""/><div class="controls bullet"><span class="by">wolrah</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537331">parent</a><span>|</span><a href="#41538069">next</a><span>|</span><label class="collapse" for="c-41537710">[-]</label><label class="expand" for="c-41537710">[1 more]</label></div><br/><div class="children"><div class="content">To paraphrase, &quot;ZFS is the worst filesystem, except for all those other filesystems that have been tried from time to time.&quot;<p>It&#x27;s far from perfect, but it has no peers.<p>I spent many years stubbornly using btrfs and lost data multiple times.  Never once did the redundancy I had supposedly configured actually do anything to help me.  ZFS has identified corruption caused by bad memory and a bad CPU and let me know immediately which files were damaged.</div><br/></div></div><div id="41538069" class="c"><input type="checkbox" id="c-41538069" checked=""/><div class="controls bullet"><span class="by">KMag</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537331">parent</a><span>|</span><a href="#41537710">prev</a><span>|</span><a href="#41537772">next</a><span>|</span><label class="collapse" for="c-41538069">[-]</label><label class="expand" for="c-41538069">[3 more]</label></div><br/><div class="children"><div class="content">&gt;  No serious person designing a filesystem today would say it&#x27;s okay to misplace your data.<p>Former LimeWire developer here... the LimeWire splash screen at startup was due to experiences with silent data corruption.   We got some impossible bug reports, so we created a stub executable that would show a splash screen while computing the SHA-1 checksums of the actual application DLLs and JARs.  Once everything checked out, that stub would use Java reflection to start the actual application.   After moving to that, those impossible bug reports stopped happening.  With 60 million simultaneous users, there were always some of them with silent disk corruption that they would blame on LimeWire.<p>When Microsoft was offering free Win7 pre-release install ISOs for download, I was having install issues.   I didn&#x27;t want to get my ISO illegally, so I found a torrent of the ISO, and wrote a Python script to download the ISO from Microsoft, but use the torrent file to verify chunks and re-download any corrupted chunks.   Something was very wrong on some device between my desktop and Microsoft&#x27;s servers, but it eventually got a non-corrupted ISO.<p>It annoys me to no end that ECC isn&#x27;t the norm for all devices with more than 1 GB of RAM.  Silent bit flips are just not okay.<p>Edit: side note: it&#x27;s interesting to see the number of complaints I still see from people who blame hard drive failures on LimeWire stressing their drives.   From very early on, LimeWire allowed bandwidth limiting, which I used to keep heat down on machines that didn&#x27;t cool their drives properly.   Beyond heat issues that I would blame on machine vendors, failures from write volume I would lay at the feet of drive manufacturers.<p>Though, I&#x27;m biased.  Any blame for drive wear that didn&#x27;t fall on either the drive manufacturers or the filesystem implementers not dealing well with random writes would probably fall at my feet.  I&#x27;m the one who implemented randomized chunk order downloading in order to rapidly increase availability of rare content, which would increase the number of hard drive head seeks on non-log-based filesystems.  I always intended to go back and (1) use sequential downloads if tens of copies of the file were in the swarm, to reduce hard drive seeks and (2) implement randomized downloading of rarest chunks first, rather than the naive randomization in the initial implementation.   I say naive, but the initial implementation did have some logic to randomize chunk download order in a way to reduce the size of the messages that swarms used to advertise which peers had which chunks.  As it turns out, there were always more pressing things to implement and the initial implementation was good enough.<p>(Though, really, all read-write filesystems should be copy-on-write log-based, at least for recent writes, maybe having some background process using a count-min-sketch to estimate locality for frequently read data and optimize read locality for rarely changing data that&#x27;s also frequently read.)<p>Edit: Also, it&#x27;s really a shame that TCP over IPv6 doesn&#x27;t use CRC-32C (to intentionally use a different CRC polynomial than Ethernet, to catch more error patterns) to end-to-end checksum data in each packet.   Yes, it&#x27;s a layering abstraction violation, but IPv6 was a convenient point to introduce a needed change.  On the gripping hand, it&#x27;s probably best in the big picture to raise flow control, corruption&#x2F;loss detection, retransmission (and add forward error correction) in libraries at the application layer (a la QUIC, etc.) and move everything to UDP.   I was working on Google&#x27;s indexing system infra when they switched transatlantic search index distribution from multiple parallel transatlantic TCP streams to reserving dedicated bandwidth from the routers and blasting UDP using rateless forward error codes.  Provided that everyone is implementing responsible (read TCP-compatible) flow control, it&#x27;s really good to have the rapid evolution possible by just using UDP and raising other concerns to libraries at the application layer.  (N parallel TCP streams are useful because they typically don&#x27;t simultaneously hit exponential backoff, so for long-fat networks, you get both higher utilization and lower variance than a single TCP stream at N times the bandwidth.)</div><br/><div id="41538320" class="c"><input type="checkbox" id="c-41538320" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538069">parent</a><span>|</span><a href="#41537772">next</a><span>|</span><label class="collapse" for="c-41538320">[-]</label><label class="expand" for="c-41538320">[2 more]</label></div><br/><div class="children"><div class="content">It sounds like a fun comp sci exercise to optimise the algo for randomised block download to reduce disk operations but maintain resilience. Presumably it would vary significantly by disk cache sizes.<p>It&#x27;s not my field, but my impression is that it would be equally resilient to just randomise the start block (adjust spacing of start blocks according to user bandwidth?) then let users just run through the download serially; maybe stopping when they hit blocks that have multiple sources and then skipping to a new start block?<p>It&#x27;s kinda mindbogglingly to me too think of all the processes that go into a &#x27;simple&#x27; torrent download at the logical level.<p>If AIs get good enough before I die then asking it to create simulations on silly things like this will probably keep me happy for all my spare time!</div><br/><div id="41538428" class="c"><input type="checkbox" id="c-41538428" checked=""/><div class="controls bullet"><span class="by">KMag</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538320">parent</a><span>|</span><a href="#41537772">next</a><span>|</span><label class="collapse" for="c-41538428">[-]</label><label class="expand" for="c-41538428">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the algorithm in the initial implementation was to first check the file extension against a list of extensions likely to be accessed by the user while still downloading (mp3, ogg, mpeg, avi, wma, asf, etc.).<p>For the case where the user is unlikely to access the content until the download is finished (the general case algorithm), look at the number of extents (continuous ranges of bytes the user already has).  If the number of extents is less than 4, pick any block randomly from the list of blocks that peers were offering from download.  If there are 4 or more extents available locally, for each end of each extent available locally, check the block before it and the block after it to see if they&#x27;re available from download from peers.   If this list of available adjacent blocks is non-empty, then randomly chose one of those adjacent blocks for download.   If the list of available adjacent blocks is empty, then uniformly randomly chose from one of the blocks available from peers.<p>In the case of file types likely to be viewed while being downloaded, it would download from the front of the file until the download was 50% complete, and then randomly either download the first needed block, or else use the previously described algorithm, with the probability of using the previous (randomized) algorithm increasing as the percentage of the download completed increased.   There was also some logic to get the last few chunks of files very early in the download for file formats that required information from a file footer in order to start using them (IIRC, ASF and&#x2F;or WMA relied on footer information to start playing).<p>For the completely randomized algorithm, my initial prototype always appended to the last chosen extent if the next block for that extent was available on the network.   This was to reduce drive seeks, but looked at the statistical bias introduced in block availability and decided to randomly grow in either direction from a randomly selected extent.<p>Internally, there was also logic to check if a chunk was corrupted (using a Merkle tree using the Tiger hash algorithm).  We would ignore the corrupted chunks when calculating the percentage completed, but would remove them from the list of blocks we needed to download, unless such removal resulted in an empty list of blocks needed for download.   In this way, we would avoid re-downloading corrupted blocks unless we had nothing else to do, because this would avoid the case where one peer had a corrupted block and we just kept getting the corrupted block from the peer.  There was some logic to alert the user if too many corrupted blocks were detected and give the user options to stop the download early and delete it, just keep trying, or else just keep the corrupted file and live with it.</div><br/></div></div></div></div></div></div><div id="41537772" class="c"><input type="checkbox" id="c-41537772" checked=""/><div class="controls bullet"><span class="by">Too</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537331">parent</a><span>|</span><a href="#41538069">prev</a><span>|</span><a href="#41538079">next</a><span>|</span><label class="collapse" for="c-41537772">[-]</label><label class="expand" for="c-41537772">[4 more]</label></div><br/><div class="children"><div class="content">The reason ZFS isn&#x27;t the norm is because it historically was difficult to set up. Outside of NAS solutions, it&#x27;s only since Ubuntu 20.04 it has been supported out of the box on any high profile customer facing OS. The reliability of the early versions was also questionable, with high zsys cpu usage and some times arcane commands needed to rebuild pools. Anecdotally, I&#x27;ve had to support lots of friends with zfs issues, never so with other file systems. The data always comes back, it&#x27;s just that it needs petting.<p>Earlier, there used to be lot of fears around the license, with Torvalds advising against its use, both for that reason and for lack of maintainers. Now i believe that has been mostly ironed out and should be less of an issue.</div><br/><div id="41538037" class="c"><input type="checkbox" id="c-41538037" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537772">parent</a><span>|</span><a href="#41538115">next</a><span>|</span><label class="collapse" for="c-41538037">[-]</label><label class="expand" for="c-41538037">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The reason ZFS isn&#x27;t the norm is because it historically was difficult to set up. Outside of NAS solutions, it&#x27;s only since Ubuntu 20.04 it has been supported out of the box on any high profile customer facing OS.<p>In this one very narrow sense, we are agreed, if we are talking about Linux on root.  IMHO it should also have been virtually everywhere else.  It should have been in MacOS, etc.<p>However, I think your particular comment may miss the forest for the trees.  Yes, ZFS was difficult to set up for Linux, because Linux people disfavored its use (which you do touch upon later).<p>People sometimes imagine that purely technical considerations govern the technical choices of remote groups.  However, I think when people say &quot;all tech is political&quot; in the cultural-war-ing American politics sense, they may be right, but they are absolutely right in the small ball open source politics sense.<p>Linux communities were convinced not to include or build ZFS support.  Because licensing was a problem.  Because btrfs was coming and would be better.  Because Linus said ZFS was mostly marketing.  So they didn&#x27;t care to build support.  Of course, this was all BS or FUD or NIH, but it was what happened, not that ZFS had new and different recovery tool, or was less reliable in the arbitrary past.  It was because the Linux community engaged in its own (successful) FUD campaign against another FOSS project.</div><br/></div></div><div id="41538115" class="c"><input type="checkbox" id="c-41538115" checked=""/><div class="controls bullet"><span class="by">zvr</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537772">parent</a><span>|</span><a href="#41538037">prev</a><span>|</span><a href="#41538079">next</a><span>|</span><label class="collapse" for="c-41538115">[-]</label><label class="expand" for="c-41538115">[2 more]</label></div><br/><div class="children"><div class="content">Was there any change in the license that made you believe that it should be less than a issue?<p>Or do you think people simply stopped paying attention?</div><br/><div id="41538242" class="c"><input type="checkbox" id="c-41538242" checked=""/><div class="controls bullet"><span class="by">Too</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538115">parent</a><span>|</span><a href="#41538079">next</a><span>|</span><label class="collapse" for="c-41538242">[-]</label><label class="expand" for="c-41538242">[1 more]</label></div><br/><div class="children"><div class="content">Canonical took a team of lawyers to deeply review the license in 2016. It&#x27;s beyond my legal skills to say if the conclusion made it more or less of an issue, at least the boundaries should now be more clear, for those who understand these matters more.<p><a href="https:&#x2F;&#x2F;canonical.com&#x2F;blog&#x2F;zfs-licensing-and-linux" rel="nofollow">https:&#x2F;&#x2F;canonical.com&#x2F;blog&#x2F;zfs-licensing-and-linux</a><p><a href="https:&#x2F;&#x2F;softwarefreedom.org&#x2F;resources&#x2F;2016&#x2F;linux-kernel-cddl.html" rel="nofollow">https:&#x2F;&#x2F;softwarefreedom.org&#x2F;resources&#x2F;2016&#x2F;linux-kernel-cddl...</a></div><br/></div></div></div></div></div></div><div id="41538079" class="c"><input type="checkbox" id="c-41538079" checked=""/><div class="controls bullet"><span class="by">KMag</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41537331">parent</a><span>|</span><a href="#41537772">prev</a><span>|</span><a href="#41538213">next</a><span>|</span><label class="collapse" for="c-41538079">[-]</label><label class="expand" for="c-41538079">[3 more]</label></div><br/><div class="children"><div class="content">How are the memory overheads of ZFS these days?  In the old days, I remember balking at the extra memory required to run ZFS on the little ARM board I was using for a NAS.</div><br/><div id="41538112" class="c"><input type="checkbox" id="c-41538112" checked=""/><div class="controls bullet"><span class="by">doublepg23</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538079">parent</a><span>|</span><a href="#41538213">next</a><span>|</span><label class="collapse" for="c-41538112">[-]</label><label class="expand" for="c-41538112">[2 more]</label></div><br/><div class="children"><div class="content">That was always FUD more or less. ZFS uses RAM as its primary cache…like every other filesystem, so it if you have very little RAM for caching the performance will degrade…like every other filesystem.</div><br/><div id="41538330" class="c"><input type="checkbox" id="c-41538330" checked=""/><div class="controls bullet"><span class="by">KMag</span><span>|</span><a href="#41536897">root</a><span>|</span><a href="#41538112">parent</a><span>|</span><a href="#41538213">next</a><span>|</span><label class="collapse" for="c-41538330">[-]</label><label class="expand" for="c-41538330">[1 more]</label></div><br/><div class="children"><div class="content">But if you have a single board computer with 1 GB of RAM and several TB of ZFS, will it just be slow, or actually not run?  Granted, my use case was abnormal, and I was evaluating in the early days when there were both license and quality concerns with ZFS on Linux.  However, my understanding at the time was that it wouldn&#x27;t actually work to have several TB in a ZFS pool with 1 GB of RAM.<p>My understanding is that ZFS has its own cache apart from the page cache, and the minimum cache size scales with the storage size.  Did I misundertand&#x2F;is my information outdated?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41538213" class="c"><input type="checkbox" id="c-41538213" checked=""/><div class="controls bullet"><span class="by">manuel_w</span><span>|</span><a href="#41536897">prev</a><span>|</span><a href="#41537401">next</a><span>|</span><label class="collapse" for="c-41538213">[-]</label><label class="expand" for="c-41538213">[5 more]</label></div><br/><div class="children"><div class="content">Discussions on checksumming filesystems usually revolve around ZFS and BTRFS, but has someone any experience with bcachefs? It&#x27;s upstreamed in the linux kernel, I learned, and is supposed to have full checksumming. The author also seems to take filesystem responsibility seriously.<p>Is anyone using it around here?<p><a href="https:&#x2F;&#x2F;bcachefs.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bcachefs.org&#x2F;</a></div><br/><div id="41538250" class="c"><input type="checkbox" id="c-41538250" checked=""/><div class="controls bullet"><span class="by">olavgg</span><span>|</span><a href="#41538213">parent</a><span>|</span><a href="#41538253">next</a><span>|</span><label class="collapse" for="c-41538250">[-]</label><label class="expand" for="c-41538250">[1 more]</label></div><br/><div class="children"><div class="content">It is marked experimental, and since it was merged into the kernel there have been a few major issues that has been resolved.
I wouldn&#x27;t risk production data on it, but for a home lab it could be fine. 
But you need to ask yourself, how much time are you willing to spend if something should go wrong? I have also been running ZFS for 15+ years, and I&#x27;ve seen a lot of crap because of bad hardware. But with good enterprise hardware it has been working flawless.</div><br/></div></div><div id="41538253" class="c"><input type="checkbox" id="c-41538253" checked=""/><div class="controls bullet"><span class="by">clan</span><span>|</span><a href="#41538213">parent</a><span>|</span><a href="#41538250">prev</a><span>|</span><a href="#41538340">next</a><span>|</span><label class="collapse" for="c-41538253">[-]</label><label class="expand" for="c-41538253">[2 more]</label></div><br/><div class="children"><div class="content">That was a decision Linus regretted[1]. There has been some recent discussion about this here on Hacker News[2].<p>[1] <a href="https:&#x2F;&#x2F;linuxiac.com&#x2F;torvalds-expresses-regret-over-merging-bcachefs-into-kernel&#x2F;" rel="nofollow">https:&#x2F;&#x2F;linuxiac.com&#x2F;torvalds-expresses-regret-over-merging-...</a><p>[2] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41407768">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41407768</a></div><br/><div id="41538426" class="c"><input type="checkbox" id="c-41538426" checked=""/><div class="controls bullet"><span class="by">Ygg2</span><span>|</span><a href="#41538213">root</a><span>|</span><a href="#41538253">parent</a><span>|</span><a href="#41538340">next</a><span>|</span><label class="collapse" for="c-41538426">[-]</label><label class="expand" for="c-41538426">[1 more]</label></div><br/><div class="children"><div class="content">Context. Linux regrets it because bcachefs doesn&#x27;t have same commitment to stability as Linux.<p>Kent wants to fix a bug with large PR<p>Linux doesn&#x27;t want to merge and review PR that touches so many non-bcachefs things.<p>They&#x27;re both right in a way.</div><br/></div></div></div></div><div id="41538340" class="c"><input type="checkbox" id="c-41538340" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#41538213">parent</a><span>|</span><a href="#41538253">prev</a><span>|</span><a href="#41537401">next</a><span>|</span><label class="collapse" for="c-41538340">[-]</label><label class="expand" for="c-41538340">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using it.  It&#x27;s been ok so far, but you should have all your data backed up anyway, just in case.<p>I&#x27;m trying a combination where I have an SSD (of about 2TiB) in front of a big hard drive (about 8 TiB) and using the SSD as a cache.</div><br/></div></div></div></div><div id="41537401" class="c"><input type="checkbox" id="c-41537401" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#41538213">prev</a><span>|</span><a href="#41536279">next</a><span>|</span><label class="collapse" for="c-41537401">[-]</label><label class="expand" for="c-41537401">[5 more]</label></div><br/><div class="children"><div class="content">Do you have a drive rotation schedule?<p>24 drives. Same model. Likely the same batch. Similar wear. Imagine most of them failing at the same time, and the rest failing as you&#x27;re rebuilding it due to the increased load, because they&#x27;re already almost at the same point.<p>Reliable storage is tricky.</div><br/><div id="41537549" class="c"><input type="checkbox" id="c-41537549" checked=""/><div class="controls bullet"><span class="by">otras</span><span>|</span><a href="#41537401">parent</a><span>|</span><a href="#41537414">next</a><span>|</span><label class="collapse" for="c-41537549">[-]</label><label class="expand" for="c-41537549">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of the HN outage where two SSDs both failed after 40k hours: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32031243">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=32031243</a></div><br/></div></div><div id="41537414" class="c"><input type="checkbox" id="c-41537414" checked=""/><div class="controls bullet"><span class="by">winrid</span><span>|</span><a href="#41537401">parent</a><span>|</span><a href="#41537549">prev</a><span>|</span><a href="#41537743">next</a><span>|</span><label class="collapse" for="c-41537414">[-]</label><label class="expand" for="c-41537414">[1 more]</label></div><br/><div class="children"><div class="content">This. I had just two drives in raid 1, and the 2nd drive failed <i>immediately</i> after silvering a new drive to re-create the array. very lucky :D</div><br/></div></div><div id="41537743" class="c"><input type="checkbox" id="c-41537743" checked=""/><div class="controls bullet"><span class="by">sschueller</span><span>|</span><a href="#41537401">parent</a><span>|</span><a href="#41537414">prev</a><span>|</span><a href="#41537986">next</a><span>|</span><label class="collapse" for="c-41537743">[-]</label><label class="expand" for="c-41537743">[1 more]</label></div><br/><div class="children"><div class="content">Reminds me of the time back in the day when Dell shipped us a server with drives serial numbers being consecutive.<p>Of course both failed at the same time and I spent an all nighter doing a restore.</div><br/></div></div><div id="41537986" class="c"><input type="checkbox" id="c-41537986" checked=""/><div class="controls bullet"><span class="by">madduci</span><span>|</span><a href="#41537401">parent</a><span>|</span><a href="#41537743">prev</a><span>|</span><a href="#41536279">next</a><span>|</span><label class="collapse" for="c-41537986">[-]</label><label class="expand" for="c-41537986">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why you buy different drives from different stores, so you can reduce the chances to get HDDs from the same batch</div><br/></div></div></div></div><div id="41536279" class="c"><input type="checkbox" id="c-41536279" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#41537401">prev</a><span>|</span><a href="#41537789">next</a><span>|</span><label class="collapse" for="c-41536279">[-]</label><label class="expand" for="c-41536279">[23 more]</label></div><br/><div class="children"><div class="content">I’ve heard the exact opposite advice (keep the drives running to reduce wear from power cycling).<p>Not sure what to believe, but I like having my ZFS NAS running so it can regularly run scrubs and check the data. FWIW, I’ve run my 4 drive system for 10 years with 2 drive failures in that time, but they were not enterprise grade drives (WD Green).</div><br/><div id="41536751" class="c"><input type="checkbox" id="c-41536751" checked=""/><div class="controls bullet"><span class="by">CTDOCodebases</span><span>|</span><a href="#41536279">parent</a><span>|</span><a href="#41536654">next</a><span>|</span><label class="collapse" for="c-41536751">[-]</label><label class="expand" for="c-41536751">[3 more]</label></div><br/><div class="children"><div class="content">I think a lot of the advice around keeping the drives running is about avoiding wear caused by spin downs and startups i.e. keeping the &quot;Start Stop Cycles&quot; low.<p>Theres a difference between spinning a drive up&#x2F;down once or twice a day and spinning it down every 15 minutes or less.<p>Also WD Green drives are not recommended for NAS usage. I know in the past they used to park the read&#x2F;write head every few seconds or so which is fine if data is being accessed infrequently but continuously however a server this can result in continuous wear which leads to premature failure.</div><br/><div id="41537819" class="c"><input type="checkbox" id="c-41537819" checked=""/><div class="controls bullet"><span class="by">cm2187</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536751">parent</a><span>|</span><a href="#41537578">next</a><span>|</span><label class="collapse" for="c-41537819">[-]</label><label class="expand" for="c-41537819">[1 more]</label></div><br/><div class="children"><div class="content">Agree. I do weekly backups, the backup NAS is only switched on and off 52 times a year. After 5-6 years the disks are probably close to new in term of usage vs disks that have been running continuously over that same period.<p>Which leads to another strategy which is to swap the primary and the backup after 5 years to get a good 10y out of the two NAS.</div><br/></div></div><div id="41537578" class="c"><input type="checkbox" id="c-41537578" checked=""/><div class="controls bullet"><span class="by">larusso</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536751">parent</a><span>|</span><a href="#41537819">prev</a><span>|</span><a href="#41536654">next</a><span>|</span><label class="collapse" for="c-41537578">[-]</label><label class="expand" for="c-41537578">[1 more]</label></div><br/><div class="children"><div class="content">There used to be some tutorials going around to flash the firmware to turn greens into reds I believe. Which simply disables the head parking.</div><br/></div></div></div></div><div id="41536654" class="c"><input type="checkbox" id="c-41536654" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#41536279">parent</a><span>|</span><a href="#41536751">prev</a><span>|</span><a href="#41536859">next</a><span>|</span><label class="collapse" for="c-41536654">[-]</label><label class="expand" for="c-41536654">[8 more]</label></div><br/><div class="children"><div class="content">&gt;Not sure what to believe<p>Keep them running.<p>Why?:<p>* The read&#x2F;write heads experience literally next to no wear while they are floating above the platters. They physically land onto shelves or onto landing zones on the platters themselves when turned off; landing and takeoff are by far the most wear the heads will suffer.<p>* Following on the above, in the worst case the read&#x2F;write heads might be torn off during takeoff due to stiction.<p>* Bearings will last longer; they might also seize up if left stationary for too long. Likewise the drive motor.<p>* The rush of current when turning on is an electrical stressor, no matter how minimal.<p>The only reasons to turn your hard drives off are to save power, reduce noise, or transport them.</div><br/><div id="41536913" class="c"><input type="checkbox" id="c-41536913" checked=""/><div class="controls bullet"><span class="by">telgareith</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536654">parent</a><span>|</span><a href="#41536895">next</a><span>|</span><label class="collapse" for="c-41536913">[-]</label><label class="expand" for="c-41536913">[1 more]</label></div><br/><div class="children"><div class="content">Citations needed.<p>Counterpoints for each:
Heads don&#x27;t suffer wear when parking. The armature does.<p>If the platters are not spinning fast enough, or the air density is too low: the heads will crash into the sides of the platters.<p>The main wear on platter bearings is vibration, it takes an extremely long time for the lube to &quot;gum up.&quot; If its still a thing at all. I suspect it used to happen because they were petroleum distilate lubes. So, shorter chains would evaporate&#x2F;sublimate leaving longer more viscous chains. Or straight polymerize.<p>With fully synthetic PAO oils, and other options they won&#x27;t do that anymore.<p>What inrush? They&#x27;re polyphase steppers. The only reason for inrush is that the engineers didn&#x27;t think it&#x27;d affect lifetime.<p>Counter: turn your drives off, thebsaved power of 8 drives being off half the day easily totals $80 a year- enough to replace all but the highest capacities.</div><br/></div></div><div id="41536895" class="c"><input type="checkbox" id="c-41536895" checked=""/><div class="controls bullet"><span class="by">tomxor</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536654">parent</a><span>|</span><a href="#41536913">prev</a><span>|</span><a href="#41538120">next</a><span>|</span><label class="collapse" for="c-41536895">[-]</label><label class="expand" for="c-41536895">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Keep them running [...] Bearings will last longer; they might also seize up if left stationary for too long. Likewise the drive motor.<p>All HDD failures I&#x27;ve ever seen in person (5 across 3 decades), were bearing failures, in machine that were almost always on with drives spun up. It&#x27;s difficult to know for sure without proper A-B comparisons, but I&#x27;ve never seen a bearing failure in a machine where drives were spun down automatically.<p>It also seems intuitive that for mechanical bearings the longer they are spun up the greater the wear and the greater the chance of failure.</div><br/><div id="41537002" class="c"><input type="checkbox" id="c-41537002" checked=""/><div class="controls bullet"><span class="by">manwe150</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536895">parent</a><span>|</span><a href="#41538120">next</a><span>|</span><label class="collapse" for="c-41537002">[-]</label><label class="expand" for="c-41537002">[2 more]</label></div><br/><div class="children"><div class="content">I think I have lost half a dozen hard drives (and a couple DVD-RW drives) over the decades because they sat in a box for a couple years on a shelf (I recall that one recovered working with a higher amperage 12V supply, but only long enough to copy off most of the data)</div><br/><div id="41537087" class="c"><input type="checkbox" id="c-41537087" checked=""/><div class="controls bullet"><span class="by">mulmen</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41537002">parent</a><span>|</span><a href="#41538120">next</a><span>|</span><label class="collapse" for="c-41537087">[-]</label><label class="expand" for="c-41537087">[1 more]</label></div><br/><div class="children"><div class="content">My experience with optical drives is similar to ink jet printers.  They work once when new then never again.</div><br/></div></div></div></div></div></div><div id="41538120" class="c"><input type="checkbox" id="c-41538120" checked=""/><div class="controls bullet"><span class="by">philjohn</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536654">parent</a><span>|</span><a href="#41536895">prev</a><span>|</span><a href="#41537943">next</a><span>|</span><label class="collapse" for="c-41538120">[-]</label><label class="expand" for="c-41538120">[1 more]</label></div><br/><div class="children"><div class="content">Yes - although it&#x27;s worth bearing in mind the number of load&#x2F;unload cycles a drive is rated for over its lifetime.<p>In the case of the IronWolf NAS drives in my home server, that&#x27;s 600,000.<p>I spin the drives down after 20 minutes of no activity, which I feel is a good balance between having them be too thrashy and saving energy. After 3 years I&#x27;m at about 60,000 load unload cycles.</div><br/></div></div><div id="41537943" class="c"><input type="checkbox" id="c-41537943" checked=""/><div class="controls bullet"><span class="by">bigiain</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536654">parent</a><span>|</span><a href="#41538120">prev</a><span>|</span><a href="#41537317">next</a><span>|</span><label class="collapse" for="c-41537943">[-]</label><label class="expand" for="c-41537943">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The only reasons to turn your hard drives off are to save power, reduce noise, or transport them.<p>One reason some of my drives get powered down 99+% of the time is that its a way to guard against the whole network getting cryptolockered. i have a weekly backup run by a script that powers up a pair of raid1 usb drives, does and incremental no-delete backup, then unmounts and powers them back down again. Even in a busy week theyre rarely running for more than an hour or two. I&#x27;d have to get unlucky enough to not have the powerup script detect being cryptolockered (it checks md5 hashes of a few &quot;canary files&quot;) and powering up the bav=ckup drives anyway. I figure that&#x27;s a worthwhile reason to spin them down weekly...</div><br/></div></div><div id="41537317" class="c"><input type="checkbox" id="c-41537317" checked=""/><div class="controls bullet"><span class="by">akira2501</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536654">parent</a><span>|</span><a href="#41537943">prev</a><span>|</span><a href="#41536859">next</a><span>|</span><label class="collapse" for="c-41537317">[-]</label><label class="expand" for="c-41537317">[1 more]</label></div><br/><div class="children"><div class="content">Using power creates heat.  Thermal cycles are never good.  Heating parts up and cooling them down often reduces their life.</div><br/></div></div></div></div><div id="41536859" class="c"><input type="checkbox" id="c-41536859" checked=""/><div class="controls bullet"><span class="by">foobarian</span><span>|</span><a href="#41536279">parent</a><span>|</span><a href="#41536654">prev</a><span>|</span><a href="#41536306">next</a><span>|</span><label class="collapse" for="c-41536859">[-]</label><label class="expand" for="c-41536859">[7 more]</label></div><br/><div class="children"><div class="content">&gt; regularly run scrubs and check the data<p>Does this include some kind of built-in hash&#x2F;checksum system to record e.g. md5 sums of each file and periodically test them?  I have a couple of big drives for family media I&#x27;d love to protect with a bit more assurance than &quot;the drive did not fail&quot;.</div><br/><div id="41536901" class="c"><input type="checkbox" id="c-41536901" checked=""/><div class="controls bullet"><span class="by">Cyph0n</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536859">parent</a><span>|</span><a href="#41536883">next</a><span>|</span><label class="collapse" for="c-41536901">[-]</label><label class="expand" for="c-41536901">[1 more]</label></div><br/><div class="children"><div class="content">Yep, ZFS reads everything in the array and validates checksums. ZFS (at least on Linux) ships with scrub systemd timers: <a href="https:&#x2F;&#x2F;openzfs.github.io&#x2F;openzfs-docs&#x2F;man&#x2F;master&#x2F;8&#x2F;zpool-scrub.8.html#PERIODIC_SCRUB" rel="nofollow">https:&#x2F;&#x2F;openzfs.github.io&#x2F;openzfs-docs&#x2F;man&#x2F;master&#x2F;8&#x2F;zpool-sc...</a></div><br/></div></div><div id="41536883" class="c"><input type="checkbox" id="c-41536883" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536859">parent</a><span>|</span><a href="#41536901">prev</a><span>|</span><a href="#41536885">next</a><span>|</span><label class="collapse" for="c-41536883">[-]</label><label class="expand" for="c-41536883">[3 more]</label></div><br/><div class="children"><div class="content">Yes, zfs includes file-level checksums.</div><br/><div id="41537138" class="c"><input type="checkbox" id="c-41537138" checked=""/><div class="controls bullet"><span class="by">jclulow</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536883">parent</a><span>|</span><a href="#41537125">next</a><span>|</span><label class="collapse" for="c-41537138">[-]</label><label class="expand" for="c-41537138">[1 more]</label></div><br/><div class="children"><div class="content">This is not strictly accurate.  ZFS records checksums of the records of data that make up the file storage.  If you want an end to end file-level checksum (like a SHA-256 digest of the contents of the file) you still need to layer that on top.  Which is not to say it&#x27;s bad, and it&#x27;s certainly something I rely on a lot, but it&#x27;s not quite the same!</div><br/></div></div><div id="41537125" class="c"><input type="checkbox" id="c-41537125" checked=""/><div class="controls bullet"><span class="by">giantrobot</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536883">parent</a><span>|</span><a href="#41537138">prev</a><span>|</span><a href="#41536885">next</a><span>|</span><label class="collapse" for="c-41537125">[-]</label><label class="expand" for="c-41537125">[1 more]</label></div><br/><div class="children"><div class="content">Block level checksums.</div><br/></div></div></div></div><div id="41536885" class="c"><input type="checkbox" id="c-41536885" checked=""/><div class="controls bullet"><span class="by">ghostly_s</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536859">parent</a><span>|</span><a href="#41536883">prev</a><span>|</span><a href="#41536870">next</a><span>|</span><label class="collapse" for="c-41536885">[-]</label><label class="expand" for="c-41536885">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ZFS?wprov=sfti1#Resilvering_and_scrub_(array_syncing_and_integrity_checking)" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ZFS?wprov=sfti1#Resilvering_an...</a></div><br/></div></div><div id="41536870" class="c"><input type="checkbox" id="c-41536870" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536859">parent</a><span>|</span><a href="#41536885">prev</a><span>|</span><a href="#41536306">next</a><span>|</span><label class="collapse" for="c-41536870">[-]</label><label class="expand" for="c-41536870">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s ZFS, so that&#x27;s built-in. A scrub does precisely that.</div><br/></div></div></div></div><div id="41536306" class="c"><input type="checkbox" id="c-41536306" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536279">parent</a><span>|</span><a href="#41536859">prev</a><span>|</span><a href="#41537320">next</a><span>|</span><label class="collapse" for="c-41536306">[-]</label><label class="expand" for="c-41536306">[3 more]</label></div><br/><div class="children"><div class="content">Hard drives are often configured to spin down when idle for a certain time. This can cause many spinups and spindowns per day. So I don&#x27;t buy this at all. But I don&#x27;t have supporting evidence that back up this notion.</div><br/><div id="41536412" class="c"><input type="checkbox" id="c-41536412" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536306">parent</a><span>|</span><a href="#41536610">next</a><span>|</span><label class="collapse" for="c-41536412">[-]</label><label class="expand" for="c-41536412">[1 more]</label></div><br/><div class="children"><div class="content">I think NAS systems in particular are often configured to prevent drives from spinning down</div><br/></div></div><div id="41536610" class="c"><input type="checkbox" id="c-41536610" checked=""/><div class="controls bullet"><span class="by">max-ibel</span><span>|</span><a href="#41536279">root</a><span>|</span><a href="#41536306">parent</a><span>|</span><a href="#41536412">prev</a><span>|</span><a href="#41537320">next</a><span>|</span><label class="collapse" for="c-41536610">[-]</label><label class="expand" for="c-41536610">[1 more]</label></div><br/><div class="children"><div class="content">There seems to be a huge difference between spin-down while NAS is up vs shutting the whole NAS down and restart. When I start my NAS, it takes a bunch of time to be back up: it seems to do a lot of checking on&#x2F; syncing off the drives and puts a fair amount of load on them (same is true for CPU as well, just look at your CPU load right after startup).<p>OTOH, when the NAS spins up a single disk again, I haven&#x27;t notice any additional load. Presumably, the read operation just waits until the disk is ready.</div><br/></div></div></div></div><div id="41537320" class="c"><input type="checkbox" id="c-41537320" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#41536279">parent</a><span>|</span><a href="#41536306">prev</a><span>|</span><a href="#41537789">next</a><span>|</span><label class="collapse" for="c-41537320">[-]</label><label class="expand" for="c-41537320">[1 more]</label></div><br/><div class="children"><div class="content">This is completely dependant on access frequency. Do you have a bunch of different people accessing many files frequently? Are you doing frequent backups?<p>If so then yes, keeping them spinning may help improve lifespan by reducing frequent disk jerk. This is really only applicable when you&#x27;re at a pretty consistent high load and you&#x27;re trying to prevent your disks from spinning up and down every few minutes or something.<p>For a homelab, you&#x27;re probably wasting way more money in electricity than you are saving in disk maintenance by leaving your disks spin.</div><br/></div></div></div></div><div id="41537789" class="c"><input type="checkbox" id="c-41537789" checked=""/><div class="controls bullet"><span class="by">ffsm8</span><span>|</span><a href="#41536279">prev</a><span>|</span><a href="#41536716">next</a><span>|</span><label class="collapse" for="c-41537789">[-]</label><label class="expand" for="c-41537789">[12 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Losing the system due to power shenanigans is a risk I accept.</i><p>There is another (very rare) failure an ups protects against, and that&#x27;s imbalance in the electricity.<p>You can get a spike (up or down, both can be destructive) if there is construction in your area and something happens with the electricity, or lightning hits a pylon close enough to your house.<p>First job I worked at had multiple servers die like that, roughly 10 yrs ago. it&#x27;s the only time I&#x27;ve ever heard of such an issue however<p>To my understanding, an ups protects from such spikes as well, as it will die before letting your servers get damaged</div><br/><div id="41538285" class="c"><input type="checkbox" id="c-41538285" checked=""/><div class="controls bullet"><span class="by">JonChesterfield</span><span>|</span><a href="#41537789">parent</a><span>|</span><a href="#41537933">next</a><span>|</span><label class="collapse" for="c-41538285">[-]</label><label class="expand" for="c-41538285">[1 more]</label></div><br/><div class="children"><div class="content">Lightning took out a modem and some nearby hardware here about a week ago. Residential. The distribution of dead vs damaged vs nominally unharmed hardware points very directly at the copper wire carrying vdsl. Modem was connected via ethernet to everything else.<p>I think the proper fix for that is probably to convert to optical, run along a fibre for a bit, then convert back. It seems likely that electricity will take a different route in preference to the glass. That turns out to be disproportionately annoying to spec (not a networking guy, gave up after an hour trying to distinguish products) so I&#x27;ve put a wifi bridge between the vdsl modem and everything else. Hopefully that&#x27;s the failure mode contained for the next storm.<p>Mainly posting because I have a ZFS array that was wired to the same modem as everything else. It seems to have survived the experience but that seems like luck.</div><br/></div></div><div id="41537933" class="c"><input type="checkbox" id="c-41537933" checked=""/><div class="controls bullet"><span class="by">danw1979</span><span>|</span><a href="#41537789">parent</a><span>|</span><a href="#41538285">prev</a><span>|</span><a href="#41537917">next</a><span>|</span><label class="collapse" for="c-41537933">[-]</label><label class="expand" for="c-41537933">[2 more]</label></div><br/><div class="children"><div class="content">I’ve had firsthand experience of a lightning strike hitting some gear that I maintained…<p>My parent’s house got hit right on the TV antenna, which was connected via coax down the the booster&#x2F;splitter unit in comms cupboard … then somehow it got onto the nearby network patch panel and fried every wired ethernet controller attached to the network, including those built into switch ports, APs, etc.  In the network switch, the current destroyed the device’s power supply too, as it was trying to get to ground I guess.<p>Still a bit of a mystery how it got from the coax to the cat5. maybe a close parallel run the electricians put in somewhere ?<p>Total network refit required, but thankfully there were no wired computers on site… I can imagine storage devices wouldn’t have fared very well.</div><br/><div id="41538347" class="c"><input type="checkbox" id="c-41538347" checked=""/><div class="controls bullet"><span class="by">nuancebydefault</span><span>|</span><a href="#41537789">root</a><span>|</span><a href="#41537933">parent</a><span>|</span><a href="#41537917">next</a><span>|</span><label class="collapse" for="c-41538347">[-]</label><label class="expand" for="c-41538347">[1 more]</label></div><br/><div class="children"><div class="content">Well this is an other order of magnitude than &#x27;spikes on the net&#x27;. The electrical field is so intense that current will easily cross large air gaps.</div><br/></div></div></div></div><div id="41538114" class="c"><input type="checkbox" id="c-41538114" checked=""/><div class="controls bullet"><span class="by">manmal</span><span>|</span><a href="#41537789">parent</a><span>|</span><a href="#41537917">prev</a><span>|</span><a href="#41537865">next</a><span>|</span><label class="collapse" for="c-41538114">[-]</label><label class="expand" for="c-41538114">[1 more]</label></div><br/><div class="children"><div class="content">We’ve had such spikes in an old apartment we were living in. I had no servers back then, but LED lamps annoyingly failed every few weeks. It was an old building from the 60s and our own apartment had some iffy quick fixes in the installation.</div><br/></div></div><div id="41537865" class="c"><input type="checkbox" id="c-41537865" checked=""/><div class="controls bullet"><span class="by">int0x29</span><span>|</span><a href="#41537789">parent</a><span>|</span><a href="#41538114">prev</a><span>|</span><a href="#41537813">next</a><span>|</span><label class="collapse" for="c-41537865">[-]</label><label class="expand" for="c-41537865">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this what a surge protector is for?</div><br/><div id="41538100" class="c"><input type="checkbox" id="c-41538100" checked=""/><div class="controls bullet"><span class="by">acstapleton</span><span>|</span><a href="#41537789">root</a><span>|</span><a href="#41537865">parent</a><span>|</span><a href="#41537902">next</a><span>|</span><label class="collapse" for="c-41538100">[-]</label><label class="expand" for="c-41538100">[1 more]</label></div><br/><div class="children"><div class="content">Nothing is really going to protect you from a direct lightning strike. Lightning strikes are on the order of millions of volts and thousands of amps. It will arc between circuits that are close enough and it will raise the ground voltage by thousands of volts too. You basically need a lighting rod buried deep into the earth to prevent it hitting your house directly and then you’re still probably going to deal with fried electronics (but your house will survive). Surge protectors are for faulty power supplies and much milder transient events on the grid and maybe a lightning strike a mile or so away.</div><br/></div></div><div id="41537902" class="c"><input type="checkbox" id="c-41537902" checked=""/><div class="controls bullet"><span class="by">bayindirh</span><span>|</span><a href="#41537789">root</a><span>|</span><a href="#41537865">parent</a><span>|</span><a href="#41538100">prev</a><span>|</span><a href="#41537903">next</a><span>|</span><label class="collapse" for="c-41537902">[-]</label><label class="expand" for="c-41537902">[1 more]</label></div><br/><div class="children"><div class="content">Yes. In most cases, assuming you live in a 220V country, a surge protector will absorb the upwards spike, and the voltage range (a universal PSU can go as low as 107V) will handle the brownout voltage dip.</div><br/></div></div><div id="41537903" class="c"><input type="checkbox" id="c-41537903" checked=""/><div class="controls bullet"><span class="by">Kerb_</span><span>|</span><a href="#41537789">root</a><span>|</span><a href="#41537865">parent</a><span>|</span><a href="#41537902">prev</a><span>|</span><a href="#41537813">next</a><span>|</span><label class="collapse" for="c-41537903">[-]</label><label class="expand" for="c-41537903">[1 more]</label></div><br/><div class="children"><div class="content">Pretty sure surge protectors are less effective against dips than they are spikes</div><br/></div></div></div></div><div id="41537813" class="c"><input type="checkbox" id="c-41537813" checked=""/><div class="controls bullet"><span class="by">Gud</span><span>|</span><a href="#41537789">parent</a><span>|</span><a href="#41537865">prev</a><span>|</span><a href="#41536716">next</a><span>|</span><label class="collapse" for="c-41537813">[-]</label><label class="expand" for="c-41537813">[2 more]</label></div><br/><div class="children"><div class="content">Electronics is absolutely sensitive to this.<p>Please use filters.</div><br/><div id="41537887" class="c"><input type="checkbox" id="c-41537887" checked=""/><div class="controls bullet"><span class="by">bboygravity</span><span>|</span><a href="#41537789">root</a><span>|</span><a href="#41537813">parent</a><span>|</span><a href="#41536716">next</a><span>|</span><label class="collapse" for="c-41537887">[-]</label><label class="expand" for="c-41537887">[1 more]</label></div><br/><div class="children"><div class="content">Filters won&#x27;t help against prolonged periods of higher&#x2F;lower voltages though.</div><br/></div></div></div></div></div></div><div id="41536716" class="c"><input type="checkbox" id="c-41536716" checked=""/><div class="controls bullet"><span class="by">rnxrx</span><span>|</span><a href="#41537789">prev</a><span>|</span><a href="#41536664">next</a><span>|</span><label class="collapse" for="c-41536716">[-]</label><label class="expand" for="c-41536716">[14 more]</label></div><br/><div class="children"><div class="content">In my experience the environment where the drives are running makes a huge difference in longevity. There&#x27;s a ton more variability in residential contexts than in data center (or even office) space.  Potential temperature and humidity variability is a notable challenge but what surprised me was the marked effect of even small amounts of dust.<p>Many years ago I was running an 8x500G array in an old Dell server in my basement.  The drives were all factory-new Seagates - 7200RPM and may have been the &quot;enterprise&quot; versions (i.e. not cheap).  Over 5 years I ended up averaging a drive failure every 6 months.  I ran with 2 parity drives, kept spares around and RMA&#x27;d the drives as they broke.<p>I moved houses and ended up with a room dedicated to lab stuff.  With the same setup I ended up going another 5 years without a single failure.  It wasn&#x27;t a surprise that the new environment was better, but it was surprising how <i>much</i> better a cleaner, more stable environment ended up being.</div><br/><div id="41536995" class="c"><input type="checkbox" id="c-41536995" checked=""/><div class="controls bullet"><span class="by">kalleboo</span><span>|</span><a href="#41536716">parent</a><span>|</span><a href="#41536970">next</a><span>|</span><label class="collapse" for="c-41536995">[-]</label><label class="expand" for="c-41536995">[5 more]</label></div><br/><div class="children"><div class="content">A drive failure every 6 months almost sounds more like dirty power than dust, I’ve always kept my NAS&#x2F;file servers in dusty residential environments (I have a nice fuzzy gray Synology logo visible right now) and never seen anything like that</div><br/><div id="41537135" class="c"><input type="checkbox" id="c-41537135" checked=""/><div class="controls bullet"><span class="by">bitexploder</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41536995">parent</a><span>|</span><a href="#41536970">next</a><span>|</span><label class="collapse" for="c-41537135">[-]</label><label class="expand" for="c-41537135">[4 more]</label></div><br/><div class="children"><div class="content">Drives are sealed anyway. Humidity maybe. Dust can’t really get in. Power or bad batch of drives.</div><br/><div id="41537557" class="c"><input type="checkbox" id="c-41537557" checked=""/><div class="controls bullet"><span class="by">userbinator</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41537135">parent</a><span>|</span><a href="#41537377">next</a><span>|</span><label class="collapse" for="c-41537557">[-]</label><label class="expand" for="c-41537557">[2 more]</label></div><br/><div class="children"><div class="content">Except for the helium-filled ones, they aren&#x27;t sealed; there is a very fine filter that equalises atmospheric pressure. (This is also why they have a maximum operating altitude --- the head needs a certain amount of atmospheric pressure to float.)</div><br/><div id="41537709" class="c"><input type="checkbox" id="c-41537709" checked=""/><div class="controls bullet"><span class="by">aaronmdjones</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41537557">parent</a><span>|</span><a href="#41537377">next</a><span>|</span><label class="collapse" for="c-41537709">[-]</label><label class="expand" for="c-41537709">[1 more]</label></div><br/><div class="children"><div class="content">Yup, this is why the label will say something along the lines of &quot;DO NOT COVER DRIVE HOLES&quot;.</div><br/></div></div></div></div><div id="41537377" class="c"><input type="checkbox" id="c-41537377" checked=""/><div class="controls bullet"><span class="by">rkagerer</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41537135">parent</a><span>|</span><a href="#41537557">prev</a><span>|</span><a href="#41536970">next</a><span>|</span><label class="collapse" for="c-41537377">[-]</label><label class="expand" for="c-41537377">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t know the details, but dust could have been impeding the effectiveness of his fans or clumping to create other hotspots in the system (including in the PSU).</div><br/></div></div></div></div></div></div><div id="41536970" class="c"><input type="checkbox" id="c-41536970" checked=""/><div class="controls bullet"><span class="by">ylee</span><span>|</span><a href="#41536716">parent</a><span>|</span><a href="#41536995">prev</a><span>|</span><a href="#41536934">next</a><span>|</span><label class="collapse" for="c-41536970">[-]</label><label class="expand" for="c-41536970">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Many years ago I was running an 8x500G array in an old Dell server in my basement. The drives were all factory-new Seagates - 7200RPM and may have been the &quot;enterprise&quot; versions (i.e. not cheap). Over 5 years I ended up averaging a drive failure every 6 months. I ran with 2 parity drives, kept spares around and RMA&#x27;d the drives as they broke.<p>Hah! I had a 16x500GB Seagate array and also averaged an RMA every six months. I think there was a firmware issue with that generation.</div><br/></div></div><div id="41536934" class="c"><input type="checkbox" id="c-41536934" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#41536716">parent</a><span>|</span><a href="#41536970">prev</a><span>|</span><a href="#41536950">next</a><span>|</span><label class="collapse" for="c-41536934">[-]</label><label class="expand" for="c-41536934">[4 more]</label></div><br/><div class="children"><div class="content">How does dust affect things? The drives are airtight.</div><br/><div id="41536954" class="c"><input type="checkbox" id="c-41536954" checked=""/><div class="controls bullet"><span class="by">kenhwang</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41536934">parent</a><span>|</span><a href="#41537146">next</a><span>|</span><label class="collapse" for="c-41536954">[-]</label><label class="expand" for="c-41536954">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re airtight now (at the high end or enterprise level). They weren&#x27;t airtight not very long ago and had filters to regulate the air exchange.</div><br/><div id="41537555" class="c"><input type="checkbox" id="c-41537555" checked=""/><div class="controls bullet"><span class="by">Kirby64</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41536954">parent</a><span>|</span><a href="#41537146">next</a><span>|</span><label class="collapse" for="c-41537555">[-]</label><label class="expand" for="c-41537555">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not airtight in the true sense (besides the helium filled ones nowadays), but every drive made in the past... 30? 40 years is airtight in the sense that no dust can ever get into the drive. There&#x27;s a breather hole somewhere (with a big warning to not cover it!) to equalize pressure, and a filter that doesn&#x27;t allow essentially any particles in.</div><br/></div></div></div></div><div id="41537146" class="c"><input type="checkbox" id="c-41537146" checked=""/><div class="controls bullet"><span class="by">deafpolygon</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41536934">parent</a><span>|</span><a href="#41536954">prev</a><span>|</span><a href="#41536950">next</a><span>|</span><label class="collapse" for="c-41537146">[-]</label><label class="expand" for="c-41537146">[1 more]</label></div><br/><div class="children"><div class="content">everything else isn&#x27;t. the dust can get into power supplies and cause irregularities.</div><br/></div></div></div></div><div id="41536950" class="c"><input type="checkbox" id="c-41536950" checked=""/><div class="controls bullet"><span class="by">Loughla</span><span>|</span><a href="#41536716">parent</a><span>|</span><a href="#41536934">prev</a><span>|</span><a href="#41537154">next</a><span>|</span><label class="collapse" for="c-41536950">[-]</label><label class="expand" for="c-41536950">[1 more]</label></div><br/><div class="children"><div class="content">Are your platters open to air? Or was it the cooling system? I&#x27;m confused.</div><br/></div></div><div id="41537154" class="c"><input type="checkbox" id="c-41537154" checked=""/><div class="controls bullet"><span class="by">daniel-s</span><span>|</span><a href="#41536716">parent</a><span>|</span><a href="#41536950">prev</a><span>|</span><a href="#41536664">next</a><span>|</span><label class="collapse" for="c-41537154">[-]</label><label class="expand" for="c-41537154">[2 more]</label></div><br/><div class="children"><div class="content">Does dust matter for SSD drives?</div><br/><div id="41537354" class="c"><input type="checkbox" id="c-41537354" checked=""/><div class="controls bullet"><span class="by">earleybird</span><span>|</span><a href="#41536716">root</a><span>|</span><a href="#41537154">parent</a><span>|</span><a href="#41536664">next</a><span>|</span><label class="collapse" for="c-41537354">[-]</label><label class="expand" for="c-41537354">[1 more]</label></div><br/><div class="children"><div class="content">Only when checking for finger prints :-)</div><br/></div></div></div></div></div></div><div id="41536664" class="c"><input type="checkbox" id="c-41536664" checked=""/><div class="controls bullet"><span class="by">throw0101c</span><span>|</span><a href="#41536716">prev</a><span>|</span><a href="#41536293">next</a><span>|</span><label class="collapse" for="c-41536664">[-]</label><label class="expand" for="c-41536664">[8 more]</label></div><br/><div class="children"><div class="content">&gt; <i>This NAS is very quiet for a NAS (video with audio).</i><p>Big (large radius) fans can move a lot of air even at low RPM. And be much more energy efficient.<p>Oxide Computer, in one of their presentations, talks about using 80mm fans, as they are quiet and (more importantly) don&#x27;t use much power. They observed, in other servers, as much as 25% of the power went just to powering the fans, versus the ~1% of theirs:<p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;hTJYY_Y1H9Q" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;hTJYY_Y1H9Q</a><p>* <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4vVXClXVuzE" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4vVXClXVuzE</a></div><br/><div id="41537283" class="c"><input type="checkbox" id="c-41537283" checked=""/><div class="controls bullet"><span class="by">daemonologist</span><span>|</span><a href="#41536664">parent</a><span>|</span><a href="#41536690">next</a><span>|</span><label class="collapse" for="c-41537283">[-]</label><label class="expand" for="c-41537283">[3 more]</label></div><br/><div class="children"><div class="content">Interesting - I&#x27;m used to desktop&#x2F;workstation hardware where 80mm is the <i>smallest</i> standard fan (aside from 40mm&#x27;s in the near-extinct Flex ATX PSU), and even that is kind of rare.  Mostly you see 120mm or 140mm.</div><br/><div id="41537399" class="c"><input type="checkbox" id="c-41537399" checked=""/><div class="controls bullet"><span class="by">mustache_kimono</span><span>|</span><a href="#41536664">root</a><span>|</span><a href="#41537283">parent</a><span>|</span><a href="#41538427">next</a><span>|</span><label class="collapse" for="c-41537399">[-]</label><label class="expand" for="c-41537399">[1 more]</label></div><br/><div class="children"><div class="content">&gt; 80mm is the smallest standard fan (aside from 40mm&#x27;s in the near-extinct Flex ATX PSU)<p>Those 40mm PSU fans, and the PSU, are what they are replacing with a DC bus bar.</div><br/></div></div><div id="41538427" class="c"><input type="checkbox" id="c-41538427" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#41536664">root</a><span>|</span><a href="#41537283">parent</a><span>|</span><a href="#41537399">prev</a><span>|</span><a href="#41536690">next</a><span>|</span><label class="collapse" for="c-41538427">[-]</label><label class="expand" for="c-41538427">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. In a home environment you should absolutely use desktop gear. I have 5 80mm and one 120mm PWM fans in my NAS and they are essentially silent as they can&#x27;t be heard over the sound of the drives (which is essentially the noise floor for a NAS).<p>It is necessary to use good PWM fans though if concerned about noise as cheaper ones can &quot;tick&quot; annoyingly. Two brands I know to be good in this respect are Be Quiet! and Noctua. DC would in theory be better but most motherboards don&#x27;t support it (would require an external controller and thermal sensors I think).</div><br/></div></div></div></div><div id="41536690" class="c"><input type="checkbox" id="c-41536690" checked=""/><div class="controls bullet"><span class="by">sss111</span><span>|</span><a href="#41536664">parent</a><span>|</span><a href="#41537283">prev</a><span>|</span><a href="#41536677">next</a><span>|</span><label class="collapse" for="c-41536690">[-]</label><label class="expand" for="c-41536690">[3 more]</label></div><br/><div class="children"><div class="content">just curious, are you associated with them, as these are very obscure youtube videos :D<p>Love it though, even the reduction in fan noise is amazing. I wonder why nobody had thought of it before, it seems so simple.</div><br/><div id="41536709" class="c"><input type="checkbox" id="c-41536709" checked=""/><div class="controls bullet"><span class="by">throw0101c</span><span>|</span><a href="#41536664">root</a><span>|</span><a href="#41536690">parent</a><span>|</span><a href="#41536677">next</a><span>|</span><label class="collapse" for="c-41536709">[-]</label><label class="expand" for="c-41536709">[2 more]</label></div><br/><div class="children"><div class="content">&gt; <i>just curious, are you associated with them, as these are very obscure youtube videos :D</i><p>Unassociated, but tech-y videos are often recommended to me, and these videos got pushed to me. (I have viewed other, unrelated Tech Day videos, so probably why I got that short. Also an old Solaris admin, so aware of Cantril, especially his rants.)<p>&gt; <i>Love it though, even the reduction in fan noise is amazing. I wonder why nobody had thought of it before, it seems so simple.</i><p>Depends on the size of the server: can&#x27;t really expand fans with 1U or even 2U pizza boxes. And for general purpose servers, I&#x27;m not sure how many 4U+ systems are purchased—perhaps some more now that perhaps GPUs cards may be a popular add-on.<p>For a while chassis systems (e.g., HP c7000) were popular, but I&#x27;m not sure how they are nowadays.</div><br/><div id="41537094" class="c"><input type="checkbox" id="c-41537094" checked=""/><div class="controls bullet"><span class="by">baby_souffle</span><span>|</span><a href="#41536664">root</a><span>|</span><a href="#41536709">parent</a><span>|</span><a href="#41536677">next</a><span>|</span><label class="collapse" for="c-41537094">[-]</label><label class="expand" for="c-41537094">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m not sure how many 4U+ systems are purchased—perhaps some more now that perhaps GPUs cards may be a popular add-on.<p>Going from what i see at eCycle places, 4U dried up years ago. Everything is either 1 or 2U or massive blade receptacles (10+ U).<p>We (the home-lab on a budget people) may see a return to 4U now that GPUs are in vogue but i&#x27;d bet that the hyper scalers are going to drive that back down to something that&#x27;ll be 3U with water cooling or so over the longer term.<p>We may also see similar with storage systems too; it&#x27;s only a matter of time before SSD gets &quot;close enough&quot; to spinning rust on the $&#x2F;gig&#x2F;unit-volume metrics.</div><br/></div></div></div></div></div></div><div id="41536677" class="c"><input type="checkbox" id="c-41536677" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536664">parent</a><span>|</span><a href="#41536690">prev</a><span>|</span><a href="#41536293">next</a><span>|</span><label class="collapse" for="c-41536677">[-]</label><label class="expand" for="c-41536677">[1 more]</label></div><br/><div class="children"><div class="content">+1 for mentioning 0xide. I love that they went this route and that stat is interesting. I hate the typical DC high RPM small fan whine.<p>I also hope that they do something &#x27;smart&#x27; when they control the fan speed ;-)</div><br/></div></div></div></div><div id="41536293" class="c"><input type="checkbox" id="c-41536293" checked=""/><div class="controls bullet"><span class="by">mvanbaak</span><span>|</span><a href="#41536664">prev</a><span>|</span><a href="#41536120">next</a><span>|</span><label class="collapse" for="c-41536293">[-]</label><label class="expand" for="c-41536293">[13 more]</label></div><br/><div class="children"><div class="content">the &#x27;secret&#x27; is not that you turn them off. it&#x27;s simply luck.<p>I have 4TB HGST drives running 24&#x2F;7 for over a decade. ok, not 24 but 8, and also 0 failures.
But I&#x27;m also lucky, like you.
Some of the people I know have several RMAs with the same drives so there&#x27;s that.<p>My main question is: What is it that takes 71TB but can be turned off most of the time? Is this the server you store backups?</div><br/><div id="41536518" class="c"><input type="checkbox" id="c-41536518" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#41536293">parent</a><span>|</span><a href="#41536333">next</a><span>|</span><label class="collapse" for="c-41536518">[-]</label><label class="expand" for="c-41536518">[1 more]</label></div><br/><div class="children"><div class="content">In fact, the conventional wisdom for a long time was to not turn them off if you want longevity.  Bearings seize when cold for instance.</div><br/></div></div><div id="41536333" class="c"><input type="checkbox" id="c-41536333" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536293">parent</a><span>|</span><a href="#41536518">prev</a><span>|</span><a href="#41536120">next</a><span>|</span><label class="collapse" for="c-41536333">[-]</label><label class="expand" for="c-41536333">[11 more]</label></div><br/><div class="children"><div class="content">It can be luck, but with 24 drives, it feels very lucky. Somebody with proper statistics knowledge can probably calculate the risk with a guestimated 1% yearly failure rate how likely it would be to have all 24 drives remaining.<p>And remember, my previous NAS with 20 drives also didn&#x27;t have any failures. So N=44, how lucky must I be?<p>It&#x27;s for residential usage, and if I need some data, I often just copy it over 10Gbit to a system that uses much less power and this NAS is then turned off again.</div><br/><div id="41536417" class="c"><input type="checkbox" id="c-41536417" checked=""/><div class="controls bullet"><span class="by">the_gorilla</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536333">parent</a><span>|</span><a href="#41536540">next</a><span>|</span><label class="collapse" for="c-41536417">[-]</label><label class="expand" for="c-41536417">[6 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t really have to guess. Backblaze posted their stats for 4 TB HGST drives for 2024, and of their 10,000 drives, 5 failed. If OP&#x27;s 2014 4 TB HGST drives are anything like this, then this is just snake oil and magic rituals and it doesn&#x27;t really matter what you do.</div><br/><div id="41536485" class="c"><input type="checkbox" id="c-41536485" checked=""/><div class="controls bullet"><span class="by">toast0</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536417">parent</a><span>|</span><a href="#41536474">next</a><span>|</span><label class="collapse" for="c-41536485">[-]</label><label class="expand" for="c-41536485">[1 more]</label></div><br/><div class="children"><div class="content">&gt; If OP&#x27;s 2014 4 TB HGST drives are anything like this, then this is just snake oil and magic rituals and it doesn&#x27;t really matter what you do.<p>It might matter what you do, but we only have public data for people in datacenters. Not a whole lot of people with 10,000 drives are going to have them mostly turned off, and none of them shared their data.</div><br/></div></div><div id="41536474" class="c"><input type="checkbox" id="c-41536474" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536417">parent</a><span>|</span><a href="#41536485">prev</a><span>|</span><a href="#41536540">next</a><span>|</span><label class="collapse" for="c-41536474">[-]</label><label class="expand" for="c-41536474">[4 more]</label></div><br/><div class="children"><div class="content">5 drives failed in Q1 2024. 8 died in Q2. That is still a very low failure rate.</div><br/><div id="41536515" class="c"><input type="checkbox" id="c-41536515" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536474">parent</a><span>|</span><a href="#41536540">next</a><span>|</span><label class="collapse" for="c-41536515">[-]</label><label class="expand" for="c-41536515">[3 more]</label></div><br/><div class="children"><div class="content">Drives have a bathtub curve, but if you want you can be conservative and estimate first year failure rates throughout. So that&#x27;s p=5&#x2F;10000 for drive failure. So chance of no-failure per year (because of our assumption) is 1-p. So, chance of no-failure per ten year is (1-p)^10 or about 99.5%</div><br/><div id="41536537" class="c"><input type="checkbox" id="c-41536537" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536515">parent</a><span>|</span><a href="#41536540">next</a><span>|</span><label class="collapse" for="c-41536537">[-]</label><label class="expand" for="c-41536537">[2 more]</label></div><br/><div class="children"><div class="content">Is that for one drive, or for all 24 drives to survive 10 years?</div><br/><div id="41536707" class="c"><input type="checkbox" id="c-41536707" checked=""/><div class="controls bullet"><span class="by">dn3500</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536537">parent</a><span>|</span><a href="#41536540">next</a><span>|</span><label class="collapse" for="c-41536707">[-]</label><label class="expand" for="c-41536707">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s for one drive. For all 24 it&#x27;s about 88.7%.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41536540" class="c"><input type="checkbox" id="c-41536540" checked=""/><div class="controls bullet"><span class="by">dastbe</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536333">parent</a><span>|</span><a href="#41536417">prev</a><span>|</span><a href="#41536443">next</a><span>|</span><label class="collapse" for="c-41536540">[-]</label><label class="expand" for="c-41536540">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s (1-p)^24^10, where p is drive failure rate per year (assuming it doesn&#x27;t go up over time). so at 1% that&#x27;s about 9% or a 1&#x2F;10 chance of this result. Not exactly great, but not impossible.<p>the backblaze rates are all over the place, but it does appear they have drives that this rate or lower: <a href="https:&#x2F;&#x2F;www.backblaze.com&#x2F;blog&#x2F;backblaze-drive-stats-for-q2-2024&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.backblaze.com&#x2F;blog&#x2F;backblaze-drive-stats-for-q2-...</a></div><br/><div id="41536605" class="c"><input type="checkbox" id="c-41536605" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536540">parent</a><span>|</span><a href="#41536443">next</a><span>|</span><label class="collapse" for="c-41536605">[-]</label><label class="expand" for="c-41536605">[1 more]</label></div><br/><div class="children"><div class="content">I can see that, my assumption that my result (no drive failure over 10 year) was rare is wrong. So I&#x27;ve updated my blog about that.</div><br/></div></div></div></div><div id="41536443" class="c"><input type="checkbox" id="c-41536443" checked=""/><div class="controls bullet"><span class="by">manquer</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536333">parent</a><span>|</span><a href="#41536540">prev</a><span>|</span><a href="#41536817">next</a><span>|</span><label class="collapse" for="c-41536443">[-]</label><label class="expand" for="c-41536443">[1 more]</label></div><br/><div class="children"><div class="content">The failure rate is not truly random with a nice normal distribution of failures over time.  There are sometimes higher rates in specific batches or they can start failing altogether etc.<p>Backblaze reports always are interesting insights into how consumers drives behave under constant load.</div><br/></div></div><div id="41536817" class="c"><input type="checkbox" id="c-41536817" checked=""/><div class="controls bullet"><span class="by">CTDOCodebases</span><span>|</span><a href="#41536293">root</a><span>|</span><a href="#41536333">parent</a><span>|</span><a href="#41536443">prev</a><span>|</span><a href="#41536120">next</a><span>|</span><label class="collapse" for="c-41536817">[-]</label><label class="expand" for="c-41536817">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what the &quot;Stop&#x2F;Stop cycle count&quot; is on these drives and roughly how many times per week&#x2F;day you are accessing the server.</div><br/></div></div></div></div></div></div><div id="41536120" class="c"><input type="checkbox" id="c-41536120" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#41536293">prev</a><span>|</span><a href="#41537920">next</a><span>|</span><label class="collapse" for="c-41536120">[-]</label><label class="expand" for="c-41536120">[7 more]</label></div><br/><div class="children"><div class="content">There have been drives where power cycling was hazardous. So, whilst I agree to the model, it shouldn&#x27;t be assumed this is always good, all the time, for all people. Some SSD need to be powered periodically. The duty cycle for a NAS probably meets that burden.<p>Probably good, definitely cheaper power costs. Those extra grease on the axle drives were a blip in time.<p>I wonder if backblaze do a drive on-off lifetime stats model? I think they are in the always on problem space.</div><br/><div id="41536190" class="c"><input type="checkbox" id="c-41536190" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536120">parent</a><span>|</span><a href="#41537920">next</a><span>|</span><label class="collapse" for="c-41536190">[-]</label><label class="expand" for="c-41536190">[6 more]</label></div><br/><div class="children"><div class="content">&gt; There have been drives where power cycling was hazardous.<p>I know about this story from 30+ years ago. It may have been true then. It may be even true now.<p>Yet, in my case, I don&#x27;t power cycle these drives often. At most a few times a month. I can&#x27;t say or prove it&#x27;s a huge risk. I only believe it&#x27;s not. I have accepted this risk for over 15+ years.<p>Update: remember that hard drives have an option to spin down when idle. So hard drives can handle many spinups a day.</div><br/><div id="41536322" class="c"><input type="checkbox" id="c-41536322" checked=""/><div class="controls bullet"><span class="by">neilv</span><span>|</span><a href="#41536120">root</a><span>|</span><a href="#41536190">parent</a><span>|</span><a href="#41536463">next</a><span>|</span><label class="collapse" for="c-41536322">[-]</label><label class="expand" for="c-41536322">[2 more]</label></div><br/><div class="children"><div class="content">In the early &#x27;90s, some Quantum 105S hard drives had a &quot;stiction&quot; problem, and were shipped with Sun SPARCstations.<p>IME at the time, power off a bunch of workstations, such as for building electrical work, and probably at least one of them wouldn&#x27;t spin back up the next business day.<p>Pulling the drive sled, and administering percussive maintenance against the desktop, could work.<p><a href="https:&#x2F;&#x2F;sunmanagers.org&#x2F;1992&#x2F;0383.html" rel="nofollow">https:&#x2F;&#x2F;sunmanagers.org&#x2F;1992&#x2F;0383.html</a></div><br/><div id="41536344" class="c"><input type="checkbox" id="c-41536344" checked=""/><div class="controls bullet"><span class="by">ghaff</span><span>|</span><a href="#41536120">root</a><span>|</span><a href="#41536322">parent</a><span>|</span><a href="#41536463">next</a><span>|</span><label class="collapse" for="c-41536344">[-]</label><label class="expand" for="c-41536344">[1 more]</label></div><br/><div class="children"><div class="content">Stiction was definitely a thing back in that general period when you&#x27;d sometimes knock a drive to get it back to working again.</div><br/></div></div></div></div><div id="41536463" class="c"><input type="checkbox" id="c-41536463" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#41536120">root</a><span>|</span><a href="#41536190">parent</a><span>|</span><a href="#41536322">prev</a><span>|</span><a href="#41537920">next</a><span>|</span><label class="collapse" for="c-41536463">[-]</label><label class="expand" for="c-41536463">[3 more]</label></div><br/><div class="children"><div class="content">I debated posting because it felt like shitstirring. I think overwhelmingly what you&#x27;re doing is right. And if a remote power on eg WOL works on the device, so much the better. If I could wish for one thing, it&#x27;s mods to code or documentation of how to handle drive power down on zfs. The rumour mill is zfs doesn&#x27;t like spindown.</div><br/><div id="41537583" class="c"><input type="checkbox" id="c-41537583" checked=""/><div class="controls bullet"><span class="by">Kirby64</span><span>|</span><a href="#41536120">root</a><span>|</span><a href="#41536463">parent</a><span>|</span><a href="#41536549">next</a><span>|</span><label class="collapse" for="c-41537583">[-]</label><label class="expand" for="c-41537583">[1 more]</label></div><br/><div class="children"><div class="content">What is there to handle? I have a ZFS array that works just fine with hard drives that automatically spin down. ZFS handles this without an issue.<p>The main gotchas tend to be: if you use the array for many things, especially stuff that throws off log files, you will constantly be accessing that array and resetting the spin down timers. Or you might be just at the threshold for spindown and you&#x27;ll put a ton of cycles on it as it bounces from spindown to access to spin up.<p>For a static file server (rarely accessed backups or media), partitioned correctly, it works great.</div><br/></div></div><div id="41536549" class="c"><input type="checkbox" id="c-41536549" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536120">root</a><span>|</span><a href="#41536463">parent</a><span>|</span><a href="#41537583">prev</a><span>|</span><a href="#41537920">next</a><span>|</span><label class="collapse" for="c-41536549">[-]</label><label class="expand" for="c-41536549">[1 more]</label></div><br/><div class="children"><div class="content">I did try using HDD spindown on ZFS but I remember (It&#x27;s a long time ago) that I encountered too many vague errors that scared me and I just disabled spindown all together.</div><br/></div></div></div></div></div></div></div></div><div id="41537920" class="c"><input type="checkbox" id="c-41537920" checked=""/><div class="controls bullet"><span class="by">tie-in</span><span>|</span><a href="#41536120">prev</a><span>|</span><a href="#41538014">next</a><span>|</span><label class="collapse" for="c-41537920">[-]</label><label class="expand" for="c-41537920">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve been using a multi-TB PostgreSQL database on ZFS for quite a few years in production and have encountered zero problems so far, including no bit flips. In case anyone is interested, our experience is documented here:<p><a href="https:&#x2F;&#x2F;lackofimagination.org&#x2F;2022&#x2F;04&#x2F;our-experience-with-postgresql-on-zfs&#x2F;" rel="nofollow">https:&#x2F;&#x2F;lackofimagination.org&#x2F;2022&#x2F;04&#x2F;our-experience-with-po...</a></div><br/></div></div><div id="41538014" class="c"><input type="checkbox" id="c-41538014" checked=""/><div class="controls bullet"><span class="by">lifeisstillgood</span><span>|</span><a href="#41537920">prev</a><span>|</span><a href="#41536432">next</a><span>|</span><label class="collapse" for="c-41538014">[-]</label><label class="expand" for="c-41538014">[2 more]</label></div><br/><div class="children"><div class="content">My takeaway is that there is a difference between residential and industrial usage, just as there is a difference between residential car ownership and 24&#x2F;7 taxi &#x2F; industrial use<p>And that no matter how amazing the industrial revolution has been, we can build reliability at the residential level but not the industrial level.<p>And certainly at the price points.<p>The whole “At FAANG scale” is a misnomer - we aren’t supposed to use residential quality (possibly the only quality) at that scale - maybe we are supposed to park our cars in our garages and drive them on a Sunday<p>Maybe we should keep our servers at home, just like we keep our insurance documents and our notebooks</div><br/><div id="41538074" class="c"><input type="checkbox" id="c-41538074" checked=""/><div class="controls bullet"><span class="by">bofadeez</span><span>|</span><a href="#41538014">parent</a><span>|</span><a href="#41536432">next</a><span>|</span><label class="collapse" for="c-41538074">[-]</label><label class="expand" for="c-41538074">[1 more]</label></div><br/><div class="children"><div class="content">I might be interested in buying storage at 1&#x2F;10 of the price if the only tradeoff was a 5 minute wait to power on a hard drive.</div><br/></div></div></div></div><div id="41536432" class="c"><input type="checkbox" id="c-41536432" checked=""/><div class="controls bullet"><span class="by">rkagerer</span><span>|</span><a href="#41538014">prev</a><span>|</span><a href="#41537312">next</a><span>|</span><label class="collapse" for="c-41536432">[-]</label><label class="expand" for="c-41536432">[2 more]</label></div><br/><div class="children"><div class="content">I have a similar-sized array which I also only power on nightly to receive backups, or occasionally when I need access to it for a week or two at a time.<p>It&#x27;s a whitebox RAID6 running NTFS (tried ReFS, didn&#x27;t like it), and has been around for 12+ years, although  I&#x27;ve upgraded the drives a couple times (2TB --&gt; 4TB --&gt; 16TB) - the older Areca RAID controllers make it super simple to do this.  Tools like Hard Disk Sentinel are awesome as well, to help catch drives before they fail.<p>I have an additional, smaller array that runs 24x7, which has been through similar upgrade cycles, plus a handful of clients with whitebox storage arrays that have lasted over a decade.  Usually the client ones are more abused (poor temperature control when they delay fixing their serveroom A&#x2F;C for months but keep cramming in new heat-generating equipment, UPS batteries not replaced diligently after staff turnover, etc...).<p>Do I notice a difference in drive lifespan between the ones that are mostly-off vs. the ones that are always-on?  Hard to say.  It&#x27;s too small a sample size and possibly too much variance in &#x27;abuse&#x27; between them.  But definitely seen a failure rate differential between the ones that have been maintained and kept cool, vs. allowed to get hotter than is healthy.<p>I <i>can</i> attest those 4TB HGST drives mentioned in the article were tanks.  Anecdotally, they&#x27;re the most reliable ones I&#x27;ve ever owned.  And I have a more reasonable sample size there as I was buying dozens at a time for various clients back in the day.</div><br/><div id="41536520" class="c"><input type="checkbox" id="c-41536520" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536432">parent</a><span>|</span><a href="#41537312">next</a><span>|</span><label class="collapse" for="c-41536520">[-]</label><label class="expand" for="c-41536520">[1 more]</label></div><br/><div class="children"><div class="content">I bought the HGSTs specifically because they showed good stats in those Backblaze drive stats that they just started to publish back then.</div><br/></div></div></div></div><div id="41537312" class="c"><input type="checkbox" id="c-41537312" checked=""/><div class="controls bullet"><span class="by">hi_hi</span><span>|</span><a href="#41536432">prev</a><span>|</span><a href="#41538029">next</a><span>|</span><label class="collapse" for="c-41537312">[-]</label><label class="expand" for="c-41537312">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had the exact same NAS for over 15 years. It&#x27;s had 5 hard drives replaced, 2 new enclosures and 1 new power supply, but it&#x27;s still as good as new...</div><br/></div></div><div id="41538029" class="c"><input type="checkbox" id="c-41538029" checked=""/><div class="controls bullet"><span class="by">tobiasbischoff</span><span>|</span><a href="#41537312">prev</a><span>|</span><a href="#41537602">next</a><span>|</span><label class="collapse" for="c-41538029">[-]</label><label class="expand" for="c-41538029">[1 more]</label></div><br/><div class="children"><div class="content">Let me tell you powering these drives on and off is far more dangerous then just keeping them running. 10 years is well in the MTBF of these enterprise drives. (I worked for 10 years as enterprise storage technician, i saw a lot if sh*).</div><br/></div></div><div id="41537602" class="c"><input type="checkbox" id="c-41537602" checked=""/><div class="controls bullet"><span class="by">fulafel</span><span>|</span><a href="#41538029">prev</a><span>|</span><a href="#41536955">next</a><span>|</span><label class="collapse" for="c-41537602">[-]</label><label class="expand" for="c-41537602">[1 more]</label></div><br/><div class="children"><div class="content">Regular reminder: RAID (and ZFS) don&#x27;t replace backups. It&#x27;s an availability solution to reduce downtime in event of disk failure. Many things can go wrong with your files and filesystem besides disk failure, eg user error, userspace software&#x2F;script bugs, driver or FS or hardware bugs, ransomware, etc)<p>The article mentions backups near the end saying eg &quot;most of the data is not important&quot; and the &quot;most important&quot; data is backed up. Feeling lucky I guess.</div><br/></div></div><div id="41536955" class="c"><input type="checkbox" id="c-41536955" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#41537602">prev</a><span>|</span><a href="#41537291">next</a><span>|</span><label class="collapse" for="c-41536955">[-]</label><label class="expand" for="c-41536955">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m confused about optimizing 7 watts as important -- rough numbers, 7 watts is 61 kWh&#x2F;y. If you assume US-average prices of $0.16&#x2F;kWh that&#x27;s about $10&#x2F;year.<p>edit: looks like for the netherlands (where he lives) this is more significant -- $0.50&#x2F;kWh is the average price, so ~$32&#x2F;year</div><br/><div id="41536994" class="c"><input type="checkbox" id="c-41536994" checked=""/><div class="controls bullet"><span class="by">yumraj</span><span>|</span><a href="#41536955">parent</a><span>|</span><a href="#41537291">next</a><span>|</span><label class="collapse" for="c-41536994">[-]</label><label class="expand" for="c-41536994">[1 more]</label></div><br/><div class="children"><div class="content">CA is average ~$0.5&#x2F;kWh.<p>Where in US matters a lot. I think OR is pretty cheap.</div><br/></div></div></div></div><div id="41537291" class="c"><input type="checkbox" id="c-41537291" checked=""/><div class="controls bullet"><span class="by">ztravis</span><span>|</span><a href="#41536955">prev</a><span>|</span><a href="#41536603">next</a><span>|</span><label class="collapse" for="c-41537291">[-]</label><label class="expand" for="c-41537291">[1 more]</label></div><br/><div class="children"><div class="content">Around 12 years ago I helped design and set up a 48-drive, 9U, ~120TB NAS in the Chenbro RM91250 chassis (still going strong! but plenty of drive failures along the way...). This looks like it&#x27;s probably the 24-drive&#x2F;4U entry in the same line (or similar). IIRC the fans were very noisy in their original hot-swappable mounts but replacing them with fixed (screw) mounts made a big difference. I can&#x27;t tell from the picture if this has hot-swappable fans, though - I think I remember ours having purple plastic hardware.</div><br/></div></div><div id="41536603" class="c"><input type="checkbox" id="c-41536603" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#41537291">prev</a><span>|</span><a href="#41536531">next</a><span>|</span><label class="collapse" for="c-41536603">[-]</label><label class="expand" for="c-41536603">[10 more]</label></div><br/><div class="children"><div class="content">Surprised to not find &#x27;ecc&#x27; on that page.<p>I know it&#x27;s not a guarantee of no-corruption, and ZFS without ECC is probably no more dangerous than any other file system without ECC, but if data corruption is a major concern for you, and you&#x27;re building out a <i>pretty hefty</i> system like this, I can&#x27;t imagine not using ECC.<p>Slow on-disk data corruption resulting from gradual and near-silent RAM failures may be like doing regular 3-2-1 backups -- you either mitigate against the problem because you&#x27;ve been stung previously, or you&#x27;re in that blissful pre-sting phase of your life.<p>EDIT:  I found TFA&#x27;s link to the original build out - and happily they are in fact running a Xeon with ECC. Surprisingly it&#x27;s a 16GB box (I thought ZFS was much hungrier on the RAM : disk ratio.)   Obviously it hasn&#x27;t helped for physical disk failures, but the success of <i>the storage array</i> owes a lot to this component.</div><br/><div id="41536891" class="c"><input type="checkbox" id="c-41536891" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#41536603">parent</a><span>|</span><a href="#41536662">next</a><span>|</span><label class="collapse" for="c-41536891">[-]</label><label class="expand" for="c-41536891">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s difficult as a home user to find ECC memory, harder to make sure it actually works in your hardware configuration, and near-impossible to find ECC memory that doesn&#x27;t require lower speeds than what you can get for $50 on amazon.<p>I would very much like to put ECC memory in my home server, but I couldn&#x27;t figure it out this generation. After four hours I decided I had better things to do with my time.</div><br/><div id="41537038" class="c"><input type="checkbox" id="c-41537038" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#41536603">root</a><span>|</span><a href="#41536891">parent</a><span>|</span><a href="#41536662">next</a><span>|</span><label class="collapse" for="c-41537038">[-]</label><label class="expand" for="c-41537038">[2 more]</label></div><br/><div class="children"><div class="content">Indeed. I&#x27;d started to add an aside to the effect of &#x27;ten years ago it was probably <i>easier</i> to go ECC&#x27;.  I&#x27;ll add it here instead.<p>A decade ago if you wanted ECC your choice was basically Xeon, and all(<i>) Xeon motherboards would accept ECC.<p>I agree that these days it&#x27;s much more complex, since you are ineluctably going get sucked into the despair-spiral of trying to work out what combination of Ryzen + motherboard + ECC RAM will give you </i>actual, demonstrable* ECC (with correction, not just detection).</div><br/><div id="41537425" class="c"><input type="checkbox" id="c-41537425" checked=""/><div class="controls bullet"><span class="by">rpcope1</span><span>|</span><a href="#41536603">root</a><span>|</span><a href="#41537038">parent</a><span>|</span><a href="#41536662">next</a><span>|</span><label class="collapse" for="c-41537425">[-]</label><label class="expand" for="c-41537425">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like the answer is to just buy another Xeon then, even if it&#x27;s a little older and maybe secondhand. I think there&#x27;s a reason the vast majority of Supermicro motherboards are still just Intel only.</div><br/></div></div></div></div></div></div><div id="41536662" class="c"><input type="checkbox" id="c-41536662" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#41536603">parent</a><span>|</span><a href="#41536891">prev</a><span>|</span><a href="#41536624">next</a><span>|</span><label class="collapse" for="c-41536662">[-]</label><label class="expand" for="c-41536662">[1 more]</label></div><br/><div class="children"><div class="content">Accidentally unplugged my raid 5 array and thought I damaged the raid card. Hours after boot I’d get problems. I glitched a RAM chip and the array was picking it up as disk corruption.</div><br/></div></div><div id="41536624" class="c"><input type="checkbox" id="c-41536624" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536603">parent</a><span>|</span><a href="#41536662">prev</a><span>|</span><a href="#41536531">next</a><span>|</span><label class="collapse" for="c-41536624">[-]</label><label class="expand" for="c-41536624">[5 more]</label></div><br/><div class="children"><div class="content">The system is using ECC and I specifically - unrelated to ZFS - wanted to use ECC memory to reduce risk of data&#x2F;fs corruption. I&#x27;ve also added &#x27;ecc&#x27; to the original blog post to clarify.<p>Edit: ZFS for home usage doesn&#x27;t need a ton of RAM as far as I&#x27;ve learned. There is the 1 GB of RAM per 1TB of storage rule of thumb, but that was for a specific context. Maybe the ill-fated data deduplication feature, or was it just to sustain performance?</div><br/><div id="41537026" class="c"><input type="checkbox" id="c-41537026" checked=""/><div class="controls bullet"><span class="by">Jedd</span><span>|</span><a href="#41536603">root</a><span>|</span><a href="#41536624">parent</a><span>|</span><a href="#41536781">next</a><span>|</span><label class="collapse" for="c-41537026">[-]</label><label class="expand" for="c-41537026">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, and all good - it was my fault for not following the link in this story to your post about the actual build, before starting on my mini-rant.<p>I&#x27;d heard the original ZFS memory estimations were somewhat exuberant, and recommendations had come down a lot since the early days, but I&#x27;d imagine given your usage pattern - powered on periodically - a performance hit for whatever operations you&#x27;re doing during that time wouldn&#x27;t be problematic.<p>I used to use mdadm for software RAID, but for several years now my home boxes are all hardware RAID.  LVM2 provides the other features I need, so I haven&#x27;t really ever explored zfs as a replacement for both - though everyone I know that uses it, loves it.</div><br/></div></div><div id="41536781" class="c"><input type="checkbox" id="c-41536781" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#41536603">root</a><span>|</span><a href="#41536624">parent</a><span>|</span><a href="#41537026">prev</a><span>|</span><a href="#41536531">next</a><span>|</span><label class="collapse" for="c-41536781">[-]</label><label class="expand" for="c-41536781">[3 more]</label></div><br/><div class="children"><div class="content">It was a handwavey rule of estimation for dedup, handwavey because dedup scales on number of records, which is going to vary wildly by recordsize.</div><br/><div id="41536923" class="c"><input type="checkbox" id="c-41536923" checked=""/><div class="controls bullet"><span class="by">InvaderFizz</span><span>|</span><a href="#41536603">root</a><span>|</span><a href="#41536781">parent</a><span>|</span><a href="#41536531">next</a><span>|</span><label class="collapse" for="c-41536923">[-]</label><label class="expand" for="c-41536923">[2 more]</label></div><br/><div class="children"><div class="content">Additionally unless it&#x27;s changed in the last six years, you should pretend ZFS dedupe doesn&#x27;t exist.</div><br/><div id="41537367" class="c"><input type="checkbox" id="c-41537367" checked=""/><div class="controls bullet"><span class="by">rincebrain</span><span>|</span><a href="#41536603">root</a><span>|</span><a href="#41536923">parent</a><span>|</span><a href="#41536531">next</a><span>|</span><label class="collapse" for="c-41537367">[-]</label><label class="expand" for="c-41537367">[1 more]</label></div><br/><div class="children"><div class="content">Not in a stable release yet, but check out <a href="https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;discussions&#x2F;15896">https:&#x2F;&#x2F;github.com&#x2F;openzfs&#x2F;zfs&#x2F;discussions&#x2F;15896</a> if you have a need for that.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="41536531" class="c"><input type="checkbox" id="c-41536531" checked=""/><div class="controls bullet"><span class="by">anjel</span><span>|</span><a href="#41536603">prev</a><span>|</span><a href="#41537500">next</a><span>|</span><label class="collapse" for="c-41536531">[-]</label><label class="expand" for="c-41536531">[4 more]</label></div><br/><div class="children"><div class="content">Ca&#x27;t help but wonder how much electricity would have been consumed if you had left it on 24&#x2F;7 for ten years...</div><br/><div id="41536614" class="c"><input type="checkbox" id="c-41536614" checked=""/><div class="controls bullet"><span class="by">nine_k</span><span>|</span><a href="#41536531">parent</a><span>|</span><a href="#41536578">next</a><span>|</span><label class="collapse" for="c-41536614">[-]</label><label class="expand" for="c-41536614">[1 more]</label></div><br/><div class="children"><div class="content">Why wonder, let&#x27;s approximate.<p>A typical 7200 rpm disk consumes about 5W when idle. For 24 drives, it&#x27;s 120W. Rather substantial, but not an electric kettle level. At $0.25 &#x2F; kWh, it&#x27;s $0.72 &#x2F; day, or about $22 &#x2F; mo, or slightly more than $260 &#x2F; year. But this is only the disks; the CPU + mobo can easily consume half as much on average, so it would be more like $30-35 &#x2F; mo.<p>And if you have electricity at a lower price, the numbers change accordingly.<p>This is why my ancient NAS uses 5400 RPM disks, and a future upgrade could use even slower disks if these were available. The reading bandwidth is multiplied by the number of disks involved.</div><br/></div></div><div id="41536596" class="c"><input type="checkbox" id="c-41536596" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536531">parent</a><span>|</span><a href="#41536578">prev</a><span>|</span><a href="#41537500">next</a><span>|</span><label class="collapse" for="c-41536596">[-]</label><label class="expand" for="c-41536596">[1 more]</label></div><br/><div class="children"><div class="content">On the original blog it states that the machine used 200W idle. Thats 4,8 KWh a day. 17520 KWh over 10 years? At around 0.30 euro per KWh that&#x27;s 5K+ if I&#x27;m not mistaken.</div><br/></div></div></div></div><div id="41537500" class="c"><input type="checkbox" id="c-41537500" checked=""/><div class="controls bullet"><span class="by">tedk-42</span><span>|</span><a href="#41536531">prev</a><span>|</span><a href="#41536213">next</a><span>|</span><label class="collapse" for="c-41537500">[-]</label><label class="expand" for="c-41537500">[1 more]</label></div><br/><div class="children"><div class="content">Really a non article as it feels like an edge case for usage.<p>It&#x27;s not on 24&#x2F;7.<p>No mention of I&#x2F;O metrics or data stored.<p>For all we know, OP is storing their photos and videos and never actually need to have 80% of the drives actually on and connected.</div><br/></div></div><div id="41536213" class="c"><input type="checkbox" id="c-41536213" checked=""/><div class="controls bullet"><span class="by">russfink</span><span>|</span><a href="#41537500">prev</a><span>|</span><a href="#41536702">next</a><span>|</span><label class="collapse" for="c-41536213">[-]</label><label class="expand" for="c-41536213">[9 more]</label></div><br/><div class="children"><div class="content">What does one do with all this storage?</div><br/><div id="41536505" class="c"><input type="checkbox" id="c-41536505" checked=""/><div class="controls bullet"><span class="by">Nadya</span><span>|</span><a href="#41536213">parent</a><span>|</span><a href="#41536298">next</a><span>|</span><label class="collapse" for="c-41536505">[-]</label><label class="expand" for="c-41536505">[1 more]</label></div><br/><div class="children"><div class="content">If you prefer to own media instead of streaming it, are into photography, video editing, 3D modelling, any AI-related stuff (models add up) or are a digital hoarder&#x2F;archivist you blow through storage rather quickly. I&#x27;m sure there are some other hobbies that routinely work with large file sizes.<p>Storage is cheap enough that rather than deleting 1000&#x27;s of photos and never be able to reclaim or look at them again I&#x27;d rather buy another drive. I&#x27;d rather have a RAW of an 8 year old photo that I overlooked and decide I really like and want to edit&#x2F;work with than a 87kb resized and compressed JPG of the same file. Same for a mostly-edited 240GB video file. What if I want or need to make some changes to it in the future? May as well hold onto it than have to re-edit the video or re-shoot the video if the original footage was also deleted.<p>Content creators have deleted their content often enough that if I enjoyed a video and think future me might enjoy rewatching the video - I download it rather than trust that I can still watch it in the future. Sites have been taken offline frequently enough that I download things. News sites keep restructuring and breaking all their old article links so I download the articles locally. JP artists are notorious for deleting their entire accounts and restarting under a new alias that I routinely archive entire Pixiv&#x2F;Twitter accounts if I like their art as there is no guarantee it will still be there to enjoy the next day.<p>It all adds up and I&#x27;m approaching 2 million well-organized and (mostly) tagged media files in my Hydrus client [0]. I have many scripts to automate downloading and tagging content for these purposes. I very, very rarely delete things. My most frequent reason for deleting anything is &quot;found in higher quality&quot; which conceptually isn&#x27;t <i>really</i> deleting.<p>Until storage costs become unreasonable I don&#x27;t see my habits changing anytime soon. On the contrary - storage keeps getting cheaper and cheaper and new formats keep getting created to encode data more and more efficiently.<p>[0] <a href="https:&#x2F;&#x2F;hydrusnetwork.github.io&#x2F;hydrus&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;hydrusnetwork.github.io&#x2F;hydrus&#x2F;index.html</a></div><br/></div></div><div id="41536298" class="c"><input type="checkbox" id="c-41536298" checked=""/><div class="controls bullet"><span class="by">flounder3</span><span>|</span><a href="#41536213">parent</a><span>|</span><a href="#41536505">prev</a><span>|</span><a href="#41536307">next</a><span>|</span><label class="collapse" for="c-41536298">[-]</label><label class="expand" for="c-41536298">[3 more]</label></div><br/><div class="children"><div class="content">This is a drop in the bucket for photographers, videographers, and general backups of RAW &#x2F; high resolution videos from mobile devices. 80TB [usable] was &quot;just enough&quot; for my household in 2016.</div><br/><div id="41536452" class="c"><input type="checkbox" id="c-41536452" checked=""/><div class="controls bullet"><span class="by">patchymcnoodles</span><span>|</span><a href="#41536213">root</a><span>|</span><a href="#41536298">parent</a><span>|</span><a href="#41536307">next</a><span>|</span><label class="collapse" for="c-41536452">[-]</label><label class="expand" for="c-41536452">[2 more]</label></div><br/><div class="children"><div class="content">Exactly that. I&#x27;m not even shooting in ProRes or similar &quot;raw&quot; video. But one video project easily takes 3TB. And I&#x27;m not even a professional.</div><br/><div id="41536688" class="c"><input type="checkbox" id="c-41536688" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#41536213">root</a><span>|</span><a href="#41536452">parent</a><span>|</span><a href="#41536307">next</a><span>|</span><label class="collapse" for="c-41536688">[-]</label><label class="expand" for="c-41536688">[1 more]</label></div><br/><div class="children"><div class="content">Holy cow, what are you shooting with!?<p>I have a Nikon Z8 that can output up to 8.3K @ 60 fps raw video, and my biggest project is just 1 TB! Most are on the order of 20 GB, if that.</div><br/></div></div></div></div></div></div><div id="41536307" class="c"><input type="checkbox" id="c-41536307" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#41536213">parent</a><span>|</span><a href="#41536298">prev</a><span>|</span><a href="#41536302">next</a><span>|</span><label class="collapse" for="c-41536307">[-]</label><label class="expand" for="c-41536307">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s still not enough to hold a local copy of sci-hub, but could probably hold quite a few recorded conference talks (or similar multimedia files) or a good selection of huggingface models.</div><br/></div></div><div id="41536302" class="c"><input type="checkbox" id="c-41536302" checked=""/><div class="controls bullet"><span class="by">rhcom2</span><span>|</span><a href="#41536213">parent</a><span>|</span><a href="#41536307">prev</a><span>|</span><a href="#41536300">next</a><span>|</span><label class="collapse" for="c-41536302">[-]</label><label class="expand" for="c-41536302">[1 more]</label></div><br/><div class="children"><div class="content">For me I have about 35TB and growing in Unraid for Plex&#x2F;Torrents&#x2F;Backups&#x2F;Docker</div><br/></div></div><div id="41536300" class="c"><input type="checkbox" id="c-41536300" checked=""/><div class="controls bullet"><span class="by">andrelaszlo</span><span>|</span><a href="#41536213">parent</a><span>|</span><a href="#41536302">prev</a><span>|</span><a href="#41536525">next</a><span>|</span><label class="collapse" for="c-41536300">[-]</label><label class="expand" for="c-41536300">[1 more]</label></div><br/><div class="children"><div class="content">Especially since it&#x27;s mostly turned off and it seems like the author is the only user</div><br/></div></div><div id="41536525" class="c"><input type="checkbox" id="c-41536525" checked=""/><div class="controls bullet"><span class="by">denkmoon</span><span>|</span><a href="#41536213">parent</a><span>|</span><a href="#41536300">prev</a><span>|</span><a href="#41536702">next</a><span>|</span><label class="collapse" for="c-41536525">[-]</label><label class="expand" for="c-41536525">[1 more]</label></div><br/><div class="children"><div class="content">avoid giving disney money</div><br/></div></div></div></div><div id="41536702" class="c"><input type="checkbox" id="c-41536702" checked=""/><div class="controls bullet"><span class="by">leighleighleigh</span><span>|</span><a href="#41536213">prev</a><span>|</span><a href="#41537717">next</a><span>|</span><label class="collapse" for="c-41536702">[-]</label><label class="expand" for="c-41536702">[2 more]</label></div><br/><div class="children"><div class="content">Regarding the custom PID controller script: I could have sworn the Linux kernel had a generic PID controller available as a module, which you could setup via the device tree, but I can&#x27;t seem to find it! (grepping for &#x27;PID&#x27; doesn&#x27;t provide very helpful results lol).<p>I think it was used on nVidia Tegra systems, maybe? I&#x27;d be interested to find it again, if anyone knows. :)</div><br/><div id="41536730" class="c"><input type="checkbox" id="c-41536730" checked=""/><div class="controls bullet"><span class="by">ewalk153</span><span>|</span><a href="#41536702">parent</a><span>|</span><a href="#41537717">next</a><span>|</span><label class="collapse" for="c-41536730">[-]</label><label class="expand" for="c-41536730">[1 more]</label></div><br/><div class="children"><div class="content">Maybe related to this?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;torvalds&#x2F;linux&#x2F;blob&#x2F;master&#x2F;tools&#x2F;thermal&#x2F;tmon&#x2F;pid.c">https:&#x2F;&#x2F;github.com&#x2F;torvalds&#x2F;linux&#x2F;blob&#x2F;master&#x2F;tools&#x2F;thermal&#x2F;...</a></div><br/></div></div></div></div><div id="41537717" class="c"><input type="checkbox" id="c-41537717" checked=""/><div class="controls bullet"><span class="by">yread</span><span>|</span><a href="#41536702">prev</a><span>|</span><a href="#41536695">next</a><span>|</span><label class="collapse" for="c-41537717">[-]</label><label class="expand" for="c-41537717">[2 more]</label></div><br/><div class="children"><div class="content">Nowadays you could almost fit all that on a single 61TB SSD and not bother with 24 disks</div><br/><div id="41537724" class="c"><input type="checkbox" id="c-41537724" checked=""/><div class="controls bullet"><span class="by">tmikaeld</span><span>|</span><a href="#41537717">parent</a><span>|</span><a href="#41536695">next</a><span>|</span><label class="collapse" for="c-41537724">[-]</label><label class="expand" for="c-41537724">[1 more]</label></div><br/><div class="children"><div class="content">And loose all of it when it fails.</div><br/></div></div></div></div><div id="41536695" class="c"><input type="checkbox" id="c-41536695" checked=""/><div class="controls bullet"><span class="by">bearjaws</span><span>|</span><a href="#41537717">prev</a><span>|</span><a href="#41536217">next</a><span>|</span><label class="collapse" for="c-41536695">[-]</label><label class="expand" for="c-41536695">[1 more]</label></div><br/><div class="children"><div class="content">I feel like 10 years is when my drives started failing the most.<p>I run a 8x8tb array zraid2 redundancy, initially it was a 8x2tb array but drives started failing once every 4 months, after 3 drives failed I upgraded the remaining ones.<p>Only downside to hosting your own is power consumption. OS upgrades have been surprisingly easy.</div><br/></div></div><div id="41536217" class="c"><input type="checkbox" id="c-41536217" checked=""/><div class="controls bullet"><span class="by">fiddlerwoaroof</span><span>|</span><a href="#41536695">prev</a><span>|</span><a href="#41536426">next</a><span>|</span><label class="collapse" for="c-41536217">[-]</label><label class="expand" for="c-41536217">[2 more]</label></div><br/><div class="children"><div class="content">This sounds to me like it&#x27;s just a matter of luck and not really a model to be imitated.</div><br/><div id="41536693" class="c"><input type="checkbox" id="c-41536693" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#41536217">parent</a><span>|</span><a href="#41536426">next</a><span>|</span><label class="collapse" for="c-41536693">[-]</label><label class="expand" for="c-41536693">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s likely that you are right and that I misjudged the likelihood of this result being special.<p>You can still imitate the model to save money on power, but your drives may not last longer, no evidence for that indeed.</div><br/></div></div></div></div><div id="41536426" class="c"><input type="checkbox" id="c-41536426" checked=""/><div class="controls bullet"><span class="by">naming_the_user</span><span>|</span><a href="#41536217">prev</a><span>|</span><a href="#41536768">next</a><span>|</span><label class="collapse" for="c-41536426">[-]</label><label class="expand" for="c-41536426">[1 more]</label></div><br/><div class="children"><div class="content">For what it&#x27;s worth this isn&#x27;t that uncommon. Most drives fail in the first few years, if you get through that then annualized failure rates are about 1-2%.<p>I&#x27;ve had the (small) SSD in a NAS fail before any of the drives due to TBW.</div><br/></div></div><div id="41536768" class="c"><input type="checkbox" id="c-41536768" checked=""/><div class="controls bullet"><span class="by">ed_mercer</span><span>|</span><a href="#41536426">prev</a><span>|</span><a href="#41536750">next</a><span>|</span><label class="collapse" for="c-41536768">[-]</label><label class="expand" for="c-41536768">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Losing the system due to power shenanigans is a risk I accept.<p>A UPS provides more than just that, it delivers constant energy without fluctuations and thus makes your hardware last longer.</div><br/><div id="41536874" class="c"><input type="checkbox" id="c-41536874" checked=""/><div class="controls bullet"><span class="by">vunderba</span><span>|</span><a href="#41536768">parent</a><span>|</span><a href="#41536750">next</a><span>|</span><label class="collapse" for="c-41536874">[-]</label><label class="expand" for="c-41536874">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, this definitely caused me to raise an eyebrow. UPS covers brown outs and obviously the occasional temporary power outage. All those drives spinning at full speed suddenly coming to a grinding halt as the power is suddenly cut, and you&#x27;re quibbling over a paltry additional 10 watts? I can only assume that the data is not that important.</div><br/></div></div></div></div><div id="41536750" class="c"><input type="checkbox" id="c-41536750" checked=""/><div class="controls bullet"><span class="by">lvl155</span><span>|</span><a href="#41536768">prev</a><span>|</span><a href="#41536389">next</a><span>|</span><label class="collapse" for="c-41536750">[-]</label><label class="expand" for="c-41536750">[1 more]</label></div><br/><div class="children"><div class="content">I have a similar approach but I don’t use ZFS. It’s a bit superfluous especially if you’re using your storage periodically (turn on and off). I use redundant NVMEs in two stages and periodically save important data into multiple HDDs (cold storage). Worth noting, it’s important to prune your data.<p>I also do not backup photos and videos locally. It’s a major headache and they just take up a crap ton of space when Amazon Prime will give you photo storage for free.<p>Anecdotally, only drives that failed on me were enterprise-grade HDDs. And they all failed within a year and in an always-on system. I also think RAIDs are over-utilized and frankly a big money pit outside of enterprise-level environments.</div><br/></div></div><div id="41537894" class="c"><input type="checkbox" id="c-41537894" checked=""/><div class="controls bullet"><span class="by">sneak</span><span>|</span><a href="#41536389">prev</a><span>|</span><label class="collapse" for="c-41537894">[-]</label><label class="expand" for="c-41537894">[1 more]</label></div><br/><div class="children"><div class="content">My home NAS is about 200TB, runs 24&#x2F;7, is very loud and power inefficient, does a full scrub every Sunday, and also hasn’t had any drive failures.  It’s only been 4 or 5 years, however.</div><br/></div></div></div></div></div></div></div></body></html>