<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1725440450794" as="style"/><link rel="stylesheet" href="styles.css?v=1725440450794"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://llmstxt.org/">Llms.txt</a> <span class="domain">(<a href="https://llmstxt.org">llmstxt.org</a>)</span></div><div class="subtext"><span>polyrand</span> | <span>88 comments</span></div><br/><div><div id="41441178" class="c"><input type="checkbox" id="c-41441178" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#41441104">next</a><span>|</span><label class="collapse" for="c-41441178">[-]</label><label class="expand" for="c-41441178">[16 more]</label></div><br/><div class="children"><div class="content">This is not how these kinds of things should be designed for the web.<p>Instead of putting resources in the root of the web, this is what &#x2F;.well-known&#x2F; was designed for. See RFC 5785:<p><a href="https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc5785" rel="nofollow">https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc5785</a><p>Instead of munging URLs to get alternate formats, this is what content negotiation or rel=alternate were designed for.<p>I’m not sure making it easier to consume content is something that is needed. I think it might be more useful to define script type=llm that would expose function calling to LLMs embedded in browsers.</div><br/><div id="41441547" class="c"><input type="checkbox" id="c-41441547" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#41441178">parent</a><span>|</span><a href="#41441384">next</a><span>|</span><label class="collapse" for="c-41441547">[-]</label><label class="expand" for="c-41441547">[2 more]</label></div><br/><div class="children"><div class="content">Fwiw the active RFC is 8615 as RFC5785 is obsolete.<p><a href="https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc8615" rel="nofollow">https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc8615</a></div><br/><div id="41443240" class="c"><input type="checkbox" id="c-41443240" checked=""/><div class="controls bullet"><span class="by">pasiaj</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441547">parent</a><span>|</span><a href="#41441384">next</a><span>|</span><label class="collapse" for="c-41443240">[-]</label><label class="expand" for="c-41443240">[1 more]</label></div><br/><div class="children"><div class="content">Having two different RFCs on this is clearly causing confusion!<p>I think I&#x27;ll create a new RFC to supersede both to clear up the situation.</div><br/></div></div></div></div><div id="41441384" class="c"><input type="checkbox" id="c-41441384" checked=""/><div class="controls bullet"><span class="by">Joker_vD</span><span>|</span><a href="#41441178">parent</a><span>|</span><a href="#41441547">prev</a><span>|</span><a href="#41441672">next</a><span>|</span><label class="collapse" for="c-41441384">[-]</label><label class="expand" for="c-41441384">[9 more]</label></div><br/><div class="children"><div class="content">If only this RFC was well-known among the people who actually put stuff out on the Web.</div><br/><div id="41441661" class="c"><input type="checkbox" id="c-41441661" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441384">parent</a><span>|</span><a href="#41441453">next</a><span>|</span><label class="collapse" for="c-41441661">[-]</label><label class="expand" for="c-41441661">[1 more]</label></div><br/><div class="children"><div class="content">I think it is. Lots of actually used standards use it. I see it in the wild all the time.<p>Certainly more well used than content negotiation which grandparent also mentioned.</div><br/></div></div><div id="41441453" class="c"><input type="checkbox" id="c-41441453" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441384">parent</a><span>|</span><a href="#41441661">prev</a><span>|</span><a href="#41441429">next</a><span>|</span><label class="collapse" for="c-41441453">[-]</label><label class="expand" for="c-41441453">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI was good about using well known for plugins</div><br/></div></div><div id="41441429" class="c"><input type="checkbox" id="c-41441429" checked=""/><div class="controls bullet"><span class="by">russellbeattie</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441384">parent</a><span>|</span><a href="#41441453">prev</a><span>|</span><a href="#41441402">next</a><span>|</span><label class="collapse" for="c-41441429">[-]</label><label class="expand" for="c-41441429">[5 more]</label></div><br/><div class="children"><div class="content">If only that RFC didn&#x27;t make it a hidden directory.<p>I can think of a dozen reasons why hiding that folder is a horrible idea, and not a single one for why it would be a good thing to do.</div><br/><div id="41443251" class="c"><input type="checkbox" id="c-41443251" checked=""/><div class="controls bullet"><span class="by">darrenf</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441429">parent</a><span>|</span><a href="#41441534">next</a><span>|</span><label class="collapse" for="c-41443251">[-]</label><label class="expand" for="c-41443251">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>If only that RFC didn&#x27;t make it a hidden directory.</i><p>There&#x27;s zero guidance on configuring how URIs under `&#x2F;.well-known&#x2F;` should be served at all, is there? They just reserve&#x2F;sandbox the initial path component for the URI schemes which support it. That&#x27;s it. It&#x27;s the developers&#x27; choice to implement it as a directory - hidden or otherwise - on a filesystem; neither RFC says they SHOULD or MUST be served in such a way.<p>(The updated RFC says &quot;e.g., on a filesystem&quot; in section 4.1, and mentions directories in section 4.4 in a way that, to my eyes, pretty much recommends against making it hidden)</div><br/></div></div><div id="41441534" class="c"><input type="checkbox" id="c-41441534" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441429">parent</a><span>|</span><a href="#41443251">prev</a><span>|</span><a href="#41441669">next</a><span>|</span><label class="collapse" for="c-41441534">[-]</label><label class="expand" for="c-41441534">[1 more]</label></div><br/><div class="children"><div class="content">The reason is because it&#x27;s supposed to be a standard folder that isn&#x27;t in use accidentally for other purposes.<p>It&#x27;s exceedingly unlikely that a website is going to just happen to make content available in a hidden directory path without it being created by automated tooling (which would likely be aware of such standards).<p>The entire point is to avoid adopting a path that people already publicly use for something else. A hidden directory is the best way to do that.</div><br/></div></div><div id="41441669" class="c"><input type="checkbox" id="c-41441669" checked=""/><div class="controls bullet"><span class="by">ftmch</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441429">parent</a><span>|</span><a href="#41441534">prev</a><span>|</span><a href="#41441402">next</a><span>|</span><label class="collapse" for="c-41441669">[-]</label><label class="expand" for="c-41441669">[2 more]</label></div><br/><div class="children"><div class="content">Also &quot;well-known&quot; was always such an awkward name to me.</div><br/><div id="41442198" class="c"><input type="checkbox" id="c-41442198" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441669">parent</a><span>|</span><a href="#41441402">next</a><span>|</span><label class="collapse" for="c-41442198">[-]</label><label class="expand" for="c-41442198">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a joke about things being described as needing to be put at &quot;well known&quot; paths.</div><br/></div></div></div></div></div></div><div id="41441402" class="c"><input type="checkbox" id="c-41441402" checked=""/><div class="controls bullet"><span class="by">seeknotfind</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441384">parent</a><span>|</span><a href="#41441429">prev</a><span>|</span><a href="#41441672">next</a><span>|</span><label class="collapse" for="c-41441402">[-]</label><label class="expand" for="c-41441402">[1 more]</label></div><br/><div class="children"><div class="content">How about an LLM agent which automatically finds inconsistencies in RFCs.</div><br/></div></div></div></div><div id="41441672" class="c"><input type="checkbox" id="c-41441672" checked=""/><div class="controls bullet"><span class="by">KTibow</span><span>|</span><a href="#41441178">parent</a><span>|</span><a href="#41441384">prev</a><span>|</span><a href="#41441448">next</a><span>|</span><label class="collapse" for="c-41441672">[-]</label><label class="expand" for="c-41441672">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s based on the series of robots.txt, humans.txt, security.txt, etc</div><br/><div id="41441774" class="c"><input type="checkbox" id="c-41441774" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441672">parent</a><span>|</span><a href="#41443338">next</a><span>|</span><label class="collapse" for="c-41441774">[-]</label><label class="expand" for="c-41441774">[1 more]</label></div><br/><div class="children"><div class="content">Yes, and robots.txt was reasonable because it was created in the early days of the web. But the others don’t have any excuse (and they were warned about it as soon as they were announced and ignored people).</div><br/></div></div><div id="41443338" class="c"><input type="checkbox" id="c-41443338" checked=""/><div class="controls bullet"><span class="by">13hunteo</span><span>|</span><a href="#41441178">root</a><span>|</span><a href="#41441672">parent</a><span>|</span><a href="#41441774">prev</a><span>|</span><a href="#41441448">next</a><span>|</span><label class="collapse" for="c-41443338">[-]</label><label class="expand" for="c-41443338">[1 more]</label></div><br/><div class="children"><div class="content">FWIW, security.txt is placed in .well-known&#x2F;</div><br/></div></div></div></div><div id="41441448" class="c"><input type="checkbox" id="c-41441448" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#41441178">parent</a><span>|</span><a href="#41441672">prev</a><span>|</span><a href="#41441104">next</a><span>|</span><label class="collapse" for="c-41441448">[-]</label><label class="expand" for="c-41441448">[1 more]</label></div><br/><div class="children"><div class="content">What if each LLM had to register and tell you when&#x2F;where&#x2F;what it scraped&#x2F;ingested from your site&#x2F;page&#x2F;url? And you could look at whatever your LLM trigger log looks like, and have a DELETE_ME link. <i>(right_to_be_un_vectorized)</i></div><br/></div></div></div></div><div id="41441104" class="c"><input type="checkbox" id="c-41441104" checked=""/><div class="controls bullet"><span class="by">internetter</span><span>|</span><a href="#41441178">prev</a><span>|</span><a href="#41441436">next</a><span>|</span><label class="collapse" for="c-41441104">[-]</label><label class="expand" for="c-41441104">[12 more]</label></div><br/><div class="children"><div class="content">To disallow:<p>Amazonbot, anthropic-ai, AwarioRssBot, AwarioSmartBot, Bytespider, CCBot, ChatGPT-User, ClaudeBot, Claude-Web, cohere-ai, DataForSeoBot, Diffbot, Webzio-Extended, FacebookBot, FriendlyCrawler, Google-Extended, GPTBot, 0AI-SearchBot, ImagesiftBot, Meta-ExternalAgent, Meta-ExternalFetcher, omgili, omgilibot, PerplexityBot, Quora-Bot, TurnitinBot<p>For all of these bots,<p>User-agent: &lt;Bot Name&gt;
Disallow: &#x2F;<p>For more information, check <a href="https:&#x2F;&#x2F;darkvisitors.com&#x2F;agents" rel="nofollow">https:&#x2F;&#x2F;darkvisitors.com&#x2F;agents</a><p>If this takes off, I&#x27;ve made my own variant of llms.txt here: <a href="https:&#x2F;&#x2F;boehs.org&#x2F;llms.txt" rel="nofollow">https:&#x2F;&#x2F;boehs.org&#x2F;llms.txt</a> . I hereby release this file to the public domain, if you wish to adapt and reuse it on your own site.<p>Hall of shame: <a href="https:&#x2F;&#x2F;www.404media.co&#x2F;websites-are-blocking-the-wrong-ai-scrapers-because-ai-companies-keep-making-new-ones&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.404media.co&#x2F;websites-are-blocking-the-wrong-ai-s...</a></div><br/><div id="41442320" class="c"><input type="checkbox" id="c-41442320" checked=""/><div class="controls bullet"><span class="by">jraph</span><span>|</span><a href="#41441104">parent</a><span>|</span><a href="#41441655">next</a><span>|</span><label class="collapse" for="c-41442320">[-]</label><label class="expand" for="c-41442320">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen some of these bots take a lot of CPU on my server, especially when browsing my (very small) forgejo instance. I banned them with a 444 error [1] in the reverse proxy settings as a temporary measure that became permanent, and then some more from this list [2], but I will consider yours as well, thanks for sharing.<p><pre><code>    if ($http_user_agent ~ facebook) { return 444; }
    if ($http_user_agent ~ Amazonbot) { return 444; }
    if ($http_user_agent ~ Bytespider) { return 444; }
    if ($http_user_agent ~ GPTBot) { return 444; }
    if ($http_user_agent ~ ClaudeBot) { return 444; }
    if ($http_user_agent ~ ImagesiftBot) { return 444; }
    if ($http_user_agent ~ CCBot) { return 444; }
    if ($http_user_agent ~ ChatGPT-User) { return 444; }
    if ($http_user_agent ~ omgili) { return 444; }
    if ($http_user_agent ~ Diffbot) { return 444; }
    if ($http_user_agent ~ Claude-Web) { return 444; }
    if ($http_user_agent ~ PerplexityBot) { return 444; }
</code></pre>
(edit: see replies to do it in a cleaner way)<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_HTTP_status_codes#nginx" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_HTTP_status_codes#ngin...</a><p>[2] <a href="https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.cloudflare.com&#x2F;declaring-your-aindependence-blo...</a></div><br/><div id="41442479" class="c"><input type="checkbox" id="c-41442479" checked=""/><div class="controls bullet"><span class="by">otherme123</span><span>|</span><a href="#41441104">root</a><span>|</span><a href="#41442320">parent</a><span>|</span><a href="#41441655">next</a><span>|</span><label class="collapse" for="c-41442479">[-]</label><label class="expand" for="c-41442479">[3 more]</label></div><br/><div class="children"><div class="content">In your nginx.conf, http block, add<p><pre><code>    include &#x2F;etc&#x2F;nginx&#x2F;useragent.rules;
</code></pre>
In &#x2F;etc&#x2F;nginx&#x2F;useragent.rules<p><pre><code>    map $http_user_agent $badagent {
        default 0;
        ~facebook 1;
        [...]
        ~PerplexityBot 1;
    }
</code></pre>
In your site.conf, server block, add<p><pre><code>    if ($badagent) {
        return 444;
    }</code></pre></div><br/><div id="41443123" class="c"><input type="checkbox" id="c-41443123" checked=""/><div class="controls bullet"><span class="by">nilsherzig</span><span>|</span><a href="#41441104">root</a><span>|</span><a href="#41442479">parent</a><span>|</span><a href="#41442662">next</a><span>|</span><label class="collapse" for="c-41443123">[-]</label><label class="expand" for="c-41443123">[1 more]</label></div><br/><div class="children"><div class="content">Anyone knows of a crowd sourced list of these user agents? With the current state of AI startups it will be hard to keep this up to date by myself</div><br/></div></div><div id="41442662" class="c"><input type="checkbox" id="c-41442662" checked=""/><div class="controls bullet"><span class="by">jraph</span><span>|</span><a href="#41441104">root</a><span>|</span><a href="#41442479">parent</a><span>|</span><a href="#41443123">prev</a><span>|</span><a href="#41441655">next</a><span>|</span><label class="collapse" for="c-41442662">[-]</label><label class="expand" for="c-41442662">[1 more]</label></div><br/><div class="children"><div class="content">Ah, nice, way better than my string of ifs.</div><br/></div></div></div></div></div></div><div id="41441655" class="c"><input type="checkbox" id="c-41441655" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41441104">parent</a><span>|</span><a href="#41442320">prev</a><span>|</span><a href="#41441770">next</a><span>|</span><label class="collapse" for="c-41441655">[-]</label><label class="expand" for="c-41441655">[3 more]</label></div><br/><div class="children"><div class="content">As much as these companies <i>should</i> respect our preferences, it&#x27;s very clear that they won&#x27;t. It wouldn&#x27;t matter to these companies if it was outright illegal, &quot;pretty please&quot; certainly isn&#x27;t going to cut it. You can&#x27;t stop scraping and the harder people try the worse their sites become for everyone else. Throwing up a robots.txt or llms.txt that calls out their bad behavior isn&#x27;t a bad idea, but it&#x27;s not likely to help anything either.</div><br/><div id="41442515" class="c"><input type="checkbox" id="c-41442515" checked=""/><div class="controls bullet"><span class="by">otherme123</span><span>|</span><a href="#41441104">root</a><span>|</span><a href="#41441655">parent</a><span>|</span><a href="#41441770">next</a><span>|</span><label class="collapse" for="c-41442515">[-]</label><label class="expand" for="c-41442515">[2 more]</label></div><br/><div class="children"><div class="content">In one of my robots.txt I have &quot;Crawl-Delay: 20&quot; for all User-Agents. Pretty much every search bot respect that Crawl-Delay, even the shaddy ones. But one of the most known AI bots launched a crawl requesting about 2 pages per second. It was so intense that it got banned by the &quot;limit_req_<i>&quot; and &quot;limit_rate_</i>&quot; of the nginx config. Now I have it configured to always get a 444 by user agent and ip range no matter how much they request.</div><br/><div id="41443379" class="c"><input type="checkbox" id="c-41443379" checked=""/><div class="controls bullet"><span class="by">dns_snek</span><span>|</span><a href="#41441104">root</a><span>|</span><a href="#41442515">parent</a><span>|</span><a href="#41441770">next</a><span>|</span><label class="collapse" for="c-41443379">[-]</label><label class="expand" for="c-41443379">[1 more]</label></div><br/><div class="children"><div class="content">&gt; a crawl requesting about 2 pages per second. It was so intense [...]<p>Do 2 pages per second really count as &quot;intense&quot; activity? Even if I was hosting a website on a $5 VPS, I don&#x27;t think I&#x27;d even notice anything short of 100 requests per second, in terms of resource usage.</div><br/></div></div></div></div></div></div><div id="41441770" class="c"><input type="checkbox" id="c-41441770" checked=""/><div class="controls bullet"><span class="by">itsbjoern</span><span>|</span><a href="#41441104">parent</a><span>|</span><a href="#41441655">prev</a><span>|</span><a href="#41441533">next</a><span>|</span><label class="collapse" for="c-41441770">[-]</label><label class="expand" for="c-41441770">[1 more]</label></div><br/><div class="children"><div class="content">Always find it amusing when people write about „blocking“ requests using robots.txt as if they are deploying a firewall</div><br/></div></div><div id="41441434" class="c"><input type="checkbox" id="c-41441434" checked=""/><div class="controls bullet"><span class="by">aftbit</span><span>|</span><a href="#41441104">parent</a><span>|</span><a href="#41441533">prev</a><span>|</span><a href="#41441436">next</a><span>|</span><label class="collapse" for="c-41441434">[-]</label><label class="expand" for="c-41441434">[2 more]</label></div><br/><div class="children"><div class="content">s&#x2F;consider of&#x2F;consider if&#x2F;</div><br/><div id="41443340" class="c"><input type="checkbox" id="c-41443340" checked=""/><div class="controls bullet"><span class="by">jeffhuys</span><span>|</span><a href="#41441104">root</a><span>|</span><a href="#41441434">parent</a><span>|</span><a href="#41441436">next</a><span>|</span><label class="collapse" for="c-41443340">[-]</label><label class="expand" for="c-41443340">[1 more]</label></div><br/><div class="children"><div class="content">Consider if course this</div><br/></div></div></div></div></div></div><div id="41441436" class="c"><input type="checkbox" id="c-41441436" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#41441104">prev</a><span>|</span><a href="#41443206">next</a><span>|</span><label class="collapse" for="c-41441436">[-]</label><label class="expand" for="c-41441436">[1 more]</label></div><br/><div class="children"><div class="content">Can we not put another file in the root please? That&#x27;s what &#x2F;.well-known&#x2F; is for.<p>And while I&#x27;m here, authors of unix tools, please use $XDG_CONFIG_HOME. I&#x27;m tired of things shitting dot-droppings into my home directory.</div><br/></div></div><div id="41443206" class="c"><input type="checkbox" id="c-41443206" checked=""/><div class="controls bullet"><span class="by">mrweasel</span><span>|</span><a href="#41441436">prev</a><span>|</span><a href="#41442092">next</a><span>|</span><label class="collapse" for="c-41443206">[-]</label><label class="expand" for="c-41443206">[3 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t this open up for manipulating LLMs?<p>You have a site, but the crawlers looks at the llms.txt and uses that, except the content is all wrong and bares no resemblance to the actual content of the page.<p>If you really care about your content being picked up by the scrapers, why not structure it better? Most of the LLMs are pretty much black boxes, so we don&#x27;t really know what a better structure would look like, but I would make the guess that involves simplifying your HTML and removing irrelevant tokens.</div><br/><div id="41443332" class="c"><input type="checkbox" id="c-41443332" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41443206">parent</a><span>|</span><a href="#41442092">next</a><span>|</span><label class="collapse" for="c-41443332">[-]</label><label class="expand" for="c-41443332">[2 more]</label></div><br/><div class="children"><div class="content">llms.txt is not for crawlers&#x2F;scrapers, it&#x27;s for creating context documents at inference time. You place it on your own site -- presumably if you create an llms.txt you&#x27;re not looking to manipulate anyone, but to do your best to provide your site&#x27;s key information in an AI-friendly way.</div><br/><div id="41443363" class="c"><input type="checkbox" id="c-41443363" checked=""/><div class="controls bullet"><span class="by">mrweasel</span><span>|</span><a href="#41443206">root</a><span>|</span><a href="#41443332">parent</a><span>|</span><a href="#41442092">next</a><span>|</span><label class="collapse" for="c-41443363">[-]</label><label class="expand" for="c-41443363">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if you create an llms.txt you&#x27;re not looking to manipulate anyone<p>You don&#x27;t know me :-)<p>My suggestion is that someone might want taint the data that goes into an LLM.<p>Let&#x27;s say you have a website with guides, examples and tips and tricks for writing bash. What would prevent you from pointing the LLMs to separate content which would contain broken examples and code with a number of security issues, because you long term would want to exploit the code generated by the LLMs.</div><br/></div></div></div></div></div></div><div id="41442092" class="c"><input type="checkbox" id="c-41442092" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41443206">prev</a><span>|</span><a href="#41441076">next</a><span>|</span><label class="collapse" for="c-41442092">[-]</label><label class="expand" for="c-41442092">[1 more]</label></div><br/><div class="children"><div class="content">Hi Jeremy here. Nice to see this on HN.<p>To explain the reasoning for this proposal, by way of an example: I recently released FastHTML, a small library for creating hypermedia applications, and by far the most common concern I&#x27;ve received from potential users is that language models aren&#x27;t able to help use it, since it was created after the knowledge cutoff of current models.<p>IDEs like Cursor let you add docs to the model context, which is a great solution to this issue -- except what docs should you add? The idea is that if you, as a site creator, want to make it easier for systems like Cursor to use your docs, then you can provide a small text file linking to the AI-friendly documentation you think is most likely to be helpful in the context window.<p>Of course, these systems already are perfectly capable of doing their own automated scraping, but the results aren&#x27;t that great. They don&#x27;t really know what&#x27;s needed to be in context to get the key foundational information, and some of that information might be on external sites anyway. I&#x27;ve found I get dramatically better results by carefully curating the context for my prompts for each system I use, and it seems like a waste of time for everyone to redo the same work of this curation, rather than the site owner doing it once for every visitor that needs it. I&#x27;ve also found this very useful with Claude Projects.<p>llms.txt isn&#x27;t really designed to help with scraping; it&#x27;s designed to help end-users use the information on web sites with the help of AI, for web-site owners interested in doing that. It&#x27;s orthogonal to robots.txt, which is used to let bots know what they may and may not access.<p>(If folks feel like this proposal is helpful, then it might be worth registering with &#x2F;.well-known&#x2F;. Since the RFC for that says &quot;Applications that wish to mint new well-known URIs MUST register them&quot;, and I don&#x27;t even know if people are interested in this, it felt a bit soon to be registering it now.)</div><br/></div></div><div id="41441076" class="c"><input type="checkbox" id="c-41441076" checked=""/><div class="controls bullet"><span class="by">azhenley</span><span>|</span><a href="#41442092">prev</a><span>|</span><a href="#41443156">next</a><span>|</span><label class="collapse" for="c-41441076">[-]</label><label class="expand" for="c-41441076">[4 more]</label></div><br/><div class="children"><div class="content">LLMs.txt should let me specify the $$$ price that companies must send me to train models on my content.</div><br/><div id="41441698" class="c"><input type="checkbox" id="c-41441698" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41441076">parent</a><span>|</span><a href="#41441406">next</a><span>|</span><label class="collapse" for="c-41441698">[-]</label><label class="expand" for="c-41441698">[2 more]</label></div><br/><div class="children"><div class="content">No, see you&#x27;re supposed to create and upload this specially formatted file on all your webservers for free, just to make it a little easier for them to take all your content for free, so that they can then use your content in their products for free, so they can charge other humans money to get your content from their product without any humans ever having to visit your actual website again. What&#x27;s not to like?<p>If they had to pay for all the content they take&#x2F;use&#x2F;redistribute they wouldn&#x27;t be able to make enough money off of your work for it to be worthwhile.</div><br/><div id="41442570" class="c"><input type="checkbox" id="c-41442570" checked=""/><div class="controls bullet"><span class="by">jeroenhd</span><span>|</span><a href="#41441076">root</a><span>|</span><a href="#41441698">parent</a><span>|</span><a href="#41441406">next</a><span>|</span><label class="collapse" for="c-41442570">[-]</label><label class="expand" for="c-41442570">[1 more]</label></div><br/><div class="children"><div class="content">But there is actually a reason to use this standard. See, if your goal is to alter the perception of AI models, like convincing them certain genocides did not exist or that certain people are(n&#x27;t) criminals, you want AI to index your website as efficiently as possible.<p>Together with websites that make money off trying to report the truth shielding their content from plagiarism scrapers, this means that setting up a wide range of (AI generated) websites all configured to be ingested easily will allow you to alter public perception much easier.<p>This spec is very useful in a fairy tale world where everyone wants to help tech giants build better AI models, but also when the goal is to twist the truth rather than improve reliability.<p>Oh, and I guess projects like Wikipedia are interested in easy information distribution like this. But you can just download a copy of the entire database instead.</div><br/></div></div></div></div><div id="41441406" class="c"><input type="checkbox" id="c-41441406" checked=""/><div class="controls bullet"><span class="by">seeknotfind</span><span>|</span><a href="#41441076">parent</a><span>|</span><a href="#41441698">prev</a><span>|</span><a href="#41443156">next</a><span>|</span><label class="collapse" for="c-41441406">[-]</label><label class="expand" for="c-41441406">[1 more]</label></div><br/><div class="children"><div class="content">And a bank account number to send it to!</div><br/></div></div></div></div><div id="41443156" class="c"><input type="checkbox" id="c-41443156" checked=""/><div class="controls bullet"><span class="by">eterevsky</span><span>|</span><a href="#41441076">prev</a><span>|</span><a href="#41443183">next</a><span>|</span><label class="collapse" for="c-41443156">[-]</label><label class="expand" for="c-41443156">[1 more]</label></div><br/><div class="children"><div class="content">Shouldn&#x27;t it be llms.md if it&#x27;s Markdown?</div><br/></div></div><div id="41443183" class="c"><input type="checkbox" id="c-41443183" checked=""/><div class="controls bullet"><span class="by">romantomjak</span><span>|</span><a href="#41443156">prev</a><span>|</span><a href="#41443181">next</a><span>|</span><label class="collapse" for="c-41443183">[-]</label><label class="expand" for="c-41443183">[2 more]</label></div><br/><div class="children"><div class="content">I find it confusing that author proposes llms.txt, but the content is actually markdown? I get that they tried to follow the convention, but then why not make it a simple text file like the robots.txt is?</div><br/><div id="41443357" class="c"><input type="checkbox" id="c-41443357" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41443183">parent</a><span>|</span><a href="#41443181">next</a><span>|</span><label class="collapse" for="c-41443357">[-]</label><label class="expand" for="c-41443357">[1 more]</label></div><br/><div class="children"><div class="content">Markdown is plain text. llms.txt is meant to be displayed in plain text format, not rendered to html.</div><br/></div></div></div></div><div id="41443181" class="c"><input type="checkbox" id="c-41443181" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#41443183">prev</a><span>|</span><a href="#41440067">next</a><span>|</span><label class="collapse" for="c-41443181">[-]</label><label class="expand" for="c-41443181">[1 more]</label></div><br/><div class="children"><div class="content">I &quot;scrape&quot; some sites[0], generally <i>one time</i>, using a single thread, and my crap home internet. On a good day i&#x27;ll set ~2mbit&#x2F;sec throttle on my side. I do this for archival purposes. So is this generally cool with everyone, or am i supposed to be reading humans.txt or whatever? I hope the spirit of my question makes sense.<p>[0] my main catchall textual site rip directory is 17GB; but i have some really large sites i heard in advance were probably shuttering, that size or larger.</div><br/></div></div><div id="41440067" class="c"><input type="checkbox" id="c-41440067" checked=""/><div class="controls bullet"><span class="by">jsheard</span><span>|</span><a href="#41443181">prev</a><span>|</span><a href="#41441657">next</a><span>|</span><label class="collapse" for="c-41440067">[-]</label><label class="expand" for="c-41440067">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;m just left wondering who would volunteer to make their sites <i>easier</i> to scrape. The trend has been the opposite with more and more sites trying to keep LLM scrapers out, whether by politely asking them to go away via robots.txt or proactively blocking their requests entirely.</div><br/><div id="41440180" class="c"><input type="checkbox" id="c-41440180" checked=""/><div class="controls bullet"><span class="by">phren0logy</span><span>|</span><a href="#41440067">parent</a><span>|</span><a href="#41441162">next</a><span>|</span><label class="collapse" for="c-41440180">[-]</label><label class="expand" for="c-41440180">[9 more]</label></div><br/><div class="children"><div class="content">People who have information they want to share? Programming library docs seem like an obvious choice...</div><br/><div id="41441066" class="c"><input type="checkbox" id="c-41441066" checked=""/><div class="controls bullet"><span class="by">ray_v</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41440180">parent</a><span>|</span><a href="#41441192">next</a><span>|</span><label class="collapse" for="c-41441066">[-]</label><label class="expand" for="c-41441066">[6 more]</label></div><br/><div class="children"><div class="content">Ostensibly, everyone posting information on the open web want to share information -- either directly with people or indirectly via search engines _and_ the current crop of llms (which in my mind, serve the same purpose as search engines)<p>I suppose the thing that people maybe don&#x27;t agree with is the lack of attribution when llms regurgitate information back at the user. That, and the fact that these services are also overly aggressive when it comes to spidering your site</div><br/><div id="41441681" class="c"><input type="checkbox" id="c-41441681" checked=""/><div class="controls bullet"><span class="by">haswell</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41441066">parent</a><span>|</span><a href="#41441722">next</a><span>|</span><label class="collapse" for="c-41441681">[-]</label><label class="expand" for="c-41441681">[3 more]</label></div><br/><div class="children"><div class="content">That’s really my primary issue. Google indexing my content and directing traffic to my site is one thing.<p>But unlike search indexing, there is no exchange of value when these LLMs are trained on my content. We all collectively get nothing for our work. It’s theft dressed up as business as usual. I’ll do whatever I reasonably can to avoid feeding the machine and hope some of the ongoing and inevitable legal fights will rein things in a bit.</div><br/><div id="41442646" class="c"><input type="checkbox" id="c-41442646" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41441681">parent</a><span>|</span><a href="#41442065">next</a><span>|</span><label class="collapse" for="c-41442646">[-]</label><label class="expand" for="c-41442646">[1 more]</label></div><br/><div class="children"><div class="content">This proposal isn’t really for training. It’s for end users who want to know what information to include when they’re using models.</div><br/></div></div><div id="41442065" class="c"><input type="checkbox" id="c-41442065" checked=""/><div class="controls bullet"><span class="by">krmboya</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41441681">parent</a><span>|</span><a href="#41442646">prev</a><span>|</span><a href="#41441722">next</a><span>|</span><label class="collapse" for="c-41442065">[-]</label><label class="expand" for="c-41442065">[1 more]</label></div><br/><div class="children"><div class="content">At least with the open source models we do get something back..</div><br/></div></div></div></div><div id="41441722" class="c"><input type="checkbox" id="c-41441722" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41441066">parent</a><span>|</span><a href="#41441681">prev</a><span>|</span><a href="#41441289">next</a><span>|</span><label class="collapse" for="c-41441722">[-]</label><label class="expand" for="c-41441722">[1 more]</label></div><br/><div class="children"><div class="content">I like sharing information. Information wants to be free after all. Companies on the other hand want to charge people money to use their LLMs and associated AI products, so suddenly we&#x27;ve got a bunch of people profiting off of our content, potentially butchering it or hallucinating all over it in the process.</div><br/></div></div></div></div><div id="41441192" class="c"><input type="checkbox" id="c-41441192" checked=""/><div class="controls bullet"><span class="by">ljm</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41440180">parent</a><span>|</span><a href="#41441066">prev</a><span>|</span><a href="#41441162">next</a><span>|</span><label class="collapse" for="c-41441192">[-]</label><label class="expand" for="c-41441192">[2 more]</label></div><br/><div class="children"><div class="content">The only reason legit docs are hard to find is because they don’t have Google ads on them and they don’t do SEO.<p>The solution to the problem isn’t AI. The solution is to break Google’s stranglehold on the web by regulating it.<p>The solution is to get government up to speed by making it contemporary, so it can understand and respond to current issues. Not leaving it up to people who had their time several decades ago and can’t let go.</div><br/><div id="41441805" class="c"><input type="checkbox" id="c-41441805" checked=""/><div class="controls bullet"><span class="by">knowaveragejoe</span><span>|</span><a href="#41440067">root</a><span>|</span><a href="#41441192">parent</a><span>|</span><a href="#41441162">next</a><span>|</span><label class="collapse" for="c-41441805">[-]</label><label class="expand" for="c-41441805">[1 more]</label></div><br/><div class="children"><div class="content">Maybe I don&#x27;t work with niche enough software, but I rarely found docs particularly hard to find. For me, one of the real benefits of using an LLM is in making it easier to find where in the docs to look, or distilling exhaustive docs into common use cases. It&#x27;s akin to the &#x27;tldr&#x27; tool for man pages.</div><br/></div></div></div></div></div></div><div id="41441162" class="c"><input type="checkbox" id="c-41441162" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#41440067">parent</a><span>|</span><a href="#41440180">prev</a><span>|</span><a href="#41441220">next</a><span>|</span><label class="collapse" for="c-41441162">[-]</label><label class="expand" for="c-41441162">[1 more]</label></div><br/><div class="children"><div class="content">I have a bunch of open source projects that I want LLMs to know all of the details of so they can help people use them.</div><br/></div></div><div id="41441220" class="c"><input type="checkbox" id="c-41441220" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#41440067">parent</a><span>|</span><a href="#41441162">prev</a><span>|</span><a href="#41441657">next</a><span>|</span><label class="collapse" for="c-41441220">[-]</label><label class="expand" for="c-41441220">[1 more]</label></div><br/><div class="children"><div class="content">Sites generated by LLMs</div><br/></div></div></div></div><div id="41441657" class="c"><input type="checkbox" id="c-41441657" checked=""/><div class="controls bullet"><span class="by">bawolff</span><span>|</span><a href="#41440067">prev</a><span>|</span><a href="#41441477">next</a><span>|</span><label class="collapse" for="c-41441657">[-]</label><label class="expand" for="c-41441657">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not that familiar with llms, but surely we are already at the point where web pages can be easily scrapped? Is markdown really an easier format to understand than html? If this is actually useful wouldn&#x27;t .txt be supperior to markdown for this usecase?<p>Does this solve a problem llms  actually have?<p>Not trying to be negative, i&#x27;m honestly curious.</div><br/><div id="41442603" class="c"><input type="checkbox" id="c-41442603" checked=""/><div class="controls bullet"><span class="by">dada78641</span><span>|</span><a href="#41441657">parent</a><span>|</span><a href="#41441674">next</a><span>|</span><label class="collapse" for="c-41442603">[-]</label><label class="expand" for="c-41442603">[1 more]</label></div><br/><div class="children"><div class="content">I hate to be this person, but... it&#x27;s scraping, not scrapping. You&#x27;re <i>scraping</i> the information off the page regardless of what its structure is.</div><br/></div></div><div id="41441674" class="c"><input type="checkbox" id="c-41441674" checked=""/><div class="controls bullet"><span class="by">idf00</span><span>|</span><a href="#41441657">parent</a><span>|</span><a href="#41442603">prev</a><span>|</span><a href="#41441666">next</a><span>|</span><label class="collapse" for="c-41441674">[-]</label><label class="expand" for="c-41441674">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  Converting docs to markdown and using them in claude projects, for example, makes a big difference.</div><br/></div></div><div id="41441666" class="c"><input type="checkbox" id="c-41441666" checked=""/><div class="controls bullet"><span class="by">autoexec</span><span>|</span><a href="#41441657">parent</a><span>|</span><a href="#41441674">prev</a><span>|</span><a href="#41441477">next</a><span>|</span><label class="collapse" for="c-41441666">[-]</label><label class="expand" for="c-41441666">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, I&#x27;m not sure what the point of markdown is here either. I would expect that anything that looks remotely like a URL will be collected and scraped no matter what format it&#x27;s in.</div><br/><div id="41443053" class="c"><input type="checkbox" id="c-41443053" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41441657">root</a><span>|</span><a href="#41441666">parent</a><span>|</span><a href="#41441477">next</a><span>|</span><label class="collapse" for="c-41443053">[-]</label><label class="expand" for="c-41443053">[1 more]</label></div><br/><div class="children"><div class="content">Context windows for LLM inference are limited. You can&#x27;t just throw everything into it -- it won&#x27;t all fit, and larger amounts of context are slower and more expensive. So it&#x27;s important to have a carefully curated set of well-formatted documents to work with.</div><br/></div></div></div></div></div></div><div id="41441477" class="c"><input type="checkbox" id="c-41441477" checked=""/><div class="controls bullet"><span class="by">fny</span><span>|</span><a href="#41441657">prev</a><span>|</span><a href="#41441642">next</a><span>|</span><label class="collapse" for="c-41441477">[-]</label><label class="expand" for="c-41441477">[1 more]</label></div><br/><div class="children"><div class="content">There’s a deep irony that I have to make a file to help LLMs scrape content while others claim AI will doom humanity.<p>A few deep ironies actually.</div><br/></div></div><div id="41441642" class="c"><input type="checkbox" id="c-41441642" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#41441477">prev</a><span>|</span><a href="#41442899">next</a><span>|</span><label class="collapse" for="c-41441642">[-]</label><label class="expand" for="c-41441642">[1 more]</label></div><br/><div class="children"><div class="content">If an LLM needs something like this for context after crawling your site then you might have bigger problems with your site.</div><br/></div></div><div id="41442899" class="c"><input type="checkbox" id="c-41442899" checked=""/><div class="controls bullet"><span class="by">greatNespresso</span><span>|</span><a href="#41441642">prev</a><span>|</span><a href="#41442123">next</a><span>|</span><label class="collapse" for="c-41442899">[-]</label><label class="expand" for="c-41442899">[1 more]</label></div><br/><div class="children"><div class="content">Had the exact same thought some time ago now, even proposed it internally at my company. What makes me doubt this will work eventually is that scraping has been going on forever now and yet no standard has been accepted (as you noted robots.txt serves a different purpose, should have been called indexation.txt)</div><br/></div></div><div id="41442123" class="c"><input type="checkbox" id="c-41442123" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#41442899">prev</a><span>|</span><a href="#41442049">next</a><span>|</span><label class="collapse" for="c-41442123">[-]</label><label class="expand" for="c-41442123">[2 more]</label></div><br/><div class="children"><div class="content">I fail to find any benefit to web site owners to follow this. This seems to benefit llm scrapers. Why would people bother to take this extra step?</div><br/></div></div><div id="41442049" class="c"><input type="checkbox" id="c-41442049" checked=""/><div class="controls bullet"><span class="by">Brajeshwar</span><span>|</span><a href="#41442123">prev</a><span>|</span><a href="#41441471">next</a><span>|</span><label class="collapse" for="c-41442049">[-]</label><label class="expand" for="c-41442049">[2 more]</label></div><br/><div class="children"><div class="content">From my experience, I don&#x27;t think any decent indicators on a website (robots.txt, humans.txt, security.txt, etc.) have worked so far. However, this is still a good initiative.<p>Here are a few things that I see;<p>- Please make a proper shareable logo — lightweight (SVG, PNG) with a transparent background. The &quot;logo.png&quot; in the Github repo is just a screenshot from somewhere. Drop the actual source file there so someone can help.<p>- Can we stick to plain text instead of Markdown? I know Markdown is already plain but is not plain enough.<p>- Personally, I feel there is too much complexity going on.</div><br/><div id="41442140" class="c"><input type="checkbox" id="c-41442140" checked=""/><div class="controls bullet"><span class="by">knowitnone</span><span>|</span><a href="#41442049">parent</a><span>|</span><a href="#41441471">next</a><span>|</span><label class="collapse" for="c-41442140">[-]</label><label class="expand" for="c-41442140">[1 more]</label></div><br/><div class="children"><div class="content">Explain to me how it is good if it does not work. You&#x27;ve lost me somewhere.</div><br/></div></div></div></div><div id="41441471" class="c"><input type="checkbox" id="c-41441471" checked=""/><div class="controls bullet"><span class="by">bidder33</span><span>|</span><a href="#41442049">prev</a><span>|</span><a href="#41441545">next</a><span>|</span><label class="collapse" for="c-41441471">[-]</label><label class="expand" for="c-41441471">[1 more]</label></div><br/><div class="children"><div class="content">similar to <a href="https:&#x2F;&#x2F;spawning.ai&#x2F;ai-txt" rel="nofollow">https:&#x2F;&#x2F;spawning.ai&#x2F;ai-txt</a></div><br/></div></div><div id="41441545" class="c"><input type="checkbox" id="c-41441545" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#41441471">prev</a><span>|</span><a href="#41441107">next</a><span>|</span><label class="collapse" for="c-41441545">[-]</label><label class="expand" for="c-41441545">[2 more]</label></div><br/><div class="children"><div class="content">Is this trying to be what the semantic web was supposed to be? Or is it trying to be &quot;OpenAPI for things that aren&#x27;t REST&#x2F;JSON-RPC APIs&quot;? (Are those even any different?)<p>And we already have plenty of standards for library documentation. Man pages, info pages, Perldoc, Javadoc, ...</div><br/><div id="41442512" class="c"><input type="checkbox" id="c-41442512" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#41441545">parent</a><span>|</span><a href="#41441107">next</a><span>|</span><label class="collapse" for="c-41442512">[-]</label><label class="expand" for="c-41442512">[1 more]</label></div><br/><div class="children"><div class="content">It looks like a very poorly thought out HATEOAS and the reason why nobody uses HATEOAS is that for some reason the creator insisted that knowing a set of fields associated with a datatype is evil out of band communication and therefore hinders evolvability.<p>Of course this then leads to a problem. Your API client isn&#x27;t allowed to invoke hard coded actions or access hard coded fields, it must automatically adjust itself whenever the API changes. In practice means that the types of HATEOAS clients you can write is extremely limited. You can write what basically amounts to an API browser plus a form generator, because anything more complicated needs human level intelligence.</div><br/></div></div></div></div><div id="41441107" class="c"><input type="checkbox" id="c-41441107" checked=""/><div class="controls bullet"><span class="by">j0hnyl</span><span>|</span><a href="#41441545">prev</a><span>|</span><a href="#41442347">next</a><span>|</span><label class="collapse" for="c-41441107">[-]</label><label class="expand" for="c-41441107">[1 more]</label></div><br/><div class="children"><div class="content">This should just be some kind of subset of robots.txt</div><br/></div></div><div id="41442347" class="c"><input type="checkbox" id="c-41442347" checked=""/><div class="controls bullet"><span class="by">gdsdfe</span><span>|</span><a href="#41441107">prev</a><span>|</span><a href="#41441527">next</a><span>|</span><label class="collapse" for="c-41442347">[-]</label><label class="expand" for="c-41442347">[1 more]</label></div><br/><div class="children"><div class="content">The very idea is a bit silly, why would you help an llm understand a website!? Isn&#x27;t that proof that the llm is less than capable and you should either use or develop a better model? Like the whole premise makes no sense to me</div><br/></div></div><div id="41441527" class="c"><input type="checkbox" id="c-41441527" checked=""/><div class="controls bullet"><span class="by">nutanc</span><span>|</span><a href="#41442347">prev</a><span>|</span><a href="#41441824">next</a><span>|</span><label class="collapse" for="c-41441527">[-]</label><label class="expand" for="c-41441527">[5 more]</label></div><br/><div class="children"><div class="content">Actually what is also needed is a notLLMs.txt.<p>robots.txt exists, but is mainly for crawling and also not sure anyone follows it or even if they don&#x27;t follow what&#x27;s the punishment.</div><br/><div id="41442712" class="c"><input type="checkbox" id="c-41442712" checked=""/><div class="controls bullet"><span class="by">kilian</span><span>|</span><a href="#41441527">parent</a><span>|</span><a href="#41441574">next</a><span>|</span><label class="collapse" for="c-41442712">[-]</label><label class="expand" for="c-41442712">[1 more]</label></div><br/><div class="children"><div class="content">There is a proposal for that too: <a href="https:&#x2F;&#x2F;site.spawning.ai&#x2F;spawning-ai-txt" rel="nofollow">https:&#x2F;&#x2F;site.spawning.ai&#x2F;spawning-ai-txt</a> but it&#x27;s wholly unclear if AI companies actually do something with this or if it&#x27;s just wishful thinking...<p>Some AI companies follow robots.txt (OpenAI and Google, for example) but others ignore it. There&#x27;s also other limitations around using robots.txt to sole this problem: <a href="https:&#x2F;&#x2F;searchengineland.com&#x2F;robots-txt-new-meta-tag-llm-ai-429510" rel="nofollow">https:&#x2F;&#x2F;searchengineland.com&#x2F;robots-txt-new-meta-tag-llm-ai-...</a></div><br/></div></div><div id="41441574" class="c"><input type="checkbox" id="c-41441574" checked=""/><div class="controls bullet"><span class="by">bnchrch</span><span>|</span><a href="#41441527">parent</a><span>|</span><a href="#41442712">prev</a><span>|</span><a href="#41441824">next</a><span>|</span><label class="collapse" for="c-41441574">[-]</label><label class="expand" for="c-41441574">[3 more]</label></div><br/><div class="children"><div class="content">Exactly. robots.txt is useless and those that think its useful for preventing unwanted crawling are clueless</div><br/><div id="41441639" class="c"><input type="checkbox" id="c-41441639" checked=""/><div class="controls bullet"><span class="by">bdcravens</span><span>|</span><a href="#41441527">root</a><span>|</span><a href="#41441574">parent</a><span>|</span><a href="#41441824">next</a><span>|</span><label class="collapse" for="c-41441639">[-]</label><label class="expand" for="c-41441639">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a &quot;Keep off the grass&quot; sign. The polite will obey, the ones who are truly the problem will not.</div><br/><div id="41443358" class="c"><input type="checkbox" id="c-41443358" checked=""/><div class="controls bullet"><span class="by">immibis</span><span>|</span><a href="#41441527">root</a><span>|</span><a href="#41441639">parent</a><span>|</span><a href="#41441824">next</a><span>|</span><label class="collapse" for="c-41443358">[-]</label><label class="expand" for="c-41443358">[1 more]</label></div><br/><div class="children"><div class="content">So your site ends up not being findable by Google, and missing from the Wayback Machine once it&#x27;s dead, but it does nothing for LLMs.</div><br/></div></div></div></div></div></div></div></div><div id="41441203" class="c"><input type="checkbox" id="c-41441203" checked=""/><div class="controls bullet"><span class="by">TZubiri</span><span>|</span><a href="#41441824">prev</a><span>|</span><a href="#41441105">next</a><span>|</span><label class="collapse" for="c-41441203">[-]</label><label class="expand" for="c-41441203">[5 more]</label></div><br/><div class="children"><div class="content">What problem does this solve?</div><br/><div id="41443062" class="c"><input type="checkbox" id="c-41443062" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41441203">parent</a><span>|</span><a href="#41441563">next</a><span>|</span><label class="collapse" for="c-41443062">[-]</label><label class="expand" for="c-41443062">[1 more]</label></div><br/><div class="children"><div class="content">From the post describing llms.txt (<a href="https:&#x2F;&#x2F;www.answer.ai&#x2F;posts&#x2F;2024-09-03-llmstxt.html" rel="nofollow">https:&#x2F;&#x2F;www.answer.ai&#x2F;posts&#x2F;2024-09-03-llmstxt.html</a>):<p>&quot;The problem this solves is that today, constructing the right context for LLMs based on a website is ambiguous — do you:<p>1. Crawl the sitemap and include every page, trying to automatically format into an LLM-friendly form?<p>2. Selectively include external links in addition to the sitemap?<p>3. For specific domains like software documentation should you also try to include all the source code?<p>Site authors know best, and can provide a list of content that an LLM should use.&quot;<p>(There&#x27;s quite a bit more info there that answers this question in more detail.)</div><br/></div></div><div id="41441563" class="c"><input type="checkbox" id="c-41441563" checked=""/><div class="controls bullet"><span class="by">tbrownaw</span><span>|</span><a href="#41441203">parent</a><span>|</span><a href="#41443062">prev</a><span>|</span><a href="#41441254">next</a><span>|</span><label class="collapse" for="c-41441563">[-]</label><label class="expand" for="c-41441563">[1 more]</label></div><br/><div class="children"><div class="content">I think the idea is that LLMs aren&#x27;t actually that good, so adding a semi-machine-readable version of your site can make it easier for them to surface your work to their own users.</div><br/></div></div><div id="41441254" class="c"><input type="checkbox" id="c-41441254" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#41441203">parent</a><span>|</span><a href="#41441563">prev</a><span>|</span><a href="#41441105">next</a><span>|</span><label class="collapse" for="c-41441254">[-]</label><label class="expand" for="c-41441254">[2 more]</label></div><br/><div class="children"><div class="content">It solves the problem of the cat&amp;mouse game of LLMs updating their scrapers by making site owners provide them the data in a format the LLMs have developed around already.<p>You&#x27;re clearly looking at this from the incorrect point of view. Silly human. Think like a bot. --The bot makers</div><br/><div id="41442658" class="c"><input type="checkbox" id="c-41442658" checked=""/><div class="controls bullet"><span class="by">jph00</span><span>|</span><a href="#41441203">root</a><span>|</span><a href="#41441254">parent</a><span>|</span><a href="#41441105">next</a><span>|</span><label class="collapse" for="c-41442658">[-]</label><label class="expand" for="c-41442658">[1 more]</label></div><br/><div class="children"><div class="content">This proposal isn’t designed to help training. It’s designed to help end-users.</div><br/></div></div></div></div></div></div><div id="41442176" class="c"><input type="checkbox" id="c-41442176" checked=""/><div class="controls bullet"><span class="by">ironfootnz</span><span>|</span><a href="#41441105">prev</a><span>|</span><label class="collapse" for="c-41442176">[-]</label><label class="expand" for="c-41442176">[1 more]</label></div><br/><div class="children"><div class="content">What a useless way of proposing something to the web. robots.txt is the way to go to anyone on the web.</div><br/></div></div></div></div></div></div></div></body></html>